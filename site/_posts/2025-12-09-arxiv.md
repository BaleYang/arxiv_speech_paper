---
layout: post
title: "arXiv Daily – 2025-12-09"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-09（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-08 08:50 — 2025-12-09 08:50
- 抓取总数：10 篇 | 本页显示：10 篇（去重/过滤后）

## A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data
- **Authors**: Agnes Norbury, George Fairs, Alexandra L. Georgescu, Matthew M. Nour, Emilia Molimpakis, Stefano Goria
- **Categories**: cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.07741v1](https://arxiv.org/abs/2512.07741v1)
- **PDF**: [https://arxiv.org/pdf/2512.07741v1](https://arxiv.org/pdf/2512.07741v1)

在精神科评估中，临床医生不仅关注患者的主诉，还会观察重要的非言语信号，如语调、语速、流畅性、应答反应和身体语言。对这些不同信息源进行权衡与整合是一项具有挑战性的任务，适合通过智能工具辅助实现——然而目前临床实践中尚未广泛应用。本文提出，采用贝叶斯网络建模可以克服当前应用中的若干关键障碍。为验证此观点，我们基于大规模数据集（包含30,135名独立说话人）中的语音特征，构建了用于预测抑郁与焦虑症状的模型并进行评估。除在疾病整体及核心症状层面的预测性能表现（抑郁与焦虑的ROC-AUC分别为0.842和0.831，预期校准误差分别为0.018和0.015；核心个体症状ROC-AUC均高于0.74）外，我们还评估了模型在不同人口统计学群体中的公平性，并探究了多输入模态间的整合效应与冗余性。同时，对临床实用性指标及心理健康服务使用者的接受度进行了探讨。研究表明，当获得足够丰富的大规模多模态数据流，并将模型设定为在症状层面（而非疾病层面）表征常见精神状况时，此类模型可成为构建稳健评估支持工具的理论基础：其以透明、可解释的形式提供临床相关输出，并可直接接受临床专家的监督指导。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2512.07741v1）
</div>

</details>

---

## Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization
- **Authors**: Maximos Kaliakatsos-Papakostas, Konstantinos Soiledis, Theodoros Tsamis, Dimos Makris, Vassilis Katsouros, Emilios Cambouropoulos
- **Categories**: cs.SD, cs.AI, cs.SC
- **arXiv**: [https://arxiv.org/abs/2512.07627v1](https://arxiv.org/abs/2512.07627v1)
- **PDF**: [https://arxiv.org/pdf/2512.07627v1](https://arxiv.org/pdf/2512.07627v1)

Transformer架构在符号音乐生成方面展现出显著优势，其整合用户偏好至生成过程的能力正从多角度被深入研究。本文探讨在旋律和声中引入预定义和弦约束的方法，即在输入旋律的同时，在特定位置提供期望和弦，要求自回归Transformer模型在生成的和声中融入该和弦。文中分析了此类约束引入的特殊性，并提出一种应对该任务的算法。该算法称为B*，它融合了束搜索与A*算法的特性，并结合回溯机制，以强制预训练Transformer模型在正确小节内的准确起始位置满足和弦约束。该算法采用暴力搜索方式，在最坏情况下具有指数级复杂度；然而，本文作为首次系统性揭示该问题难点的尝试，所提出的算法因允许启发式策略的融入，为后续优化提供了广阔空间。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2512.07627v1）
</div>

</details>

---

## Introduction to Ambisonics, Part 1: The Part With No Math
- **Authors**: Jens Ahrens
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.07570v1](https://arxiv.org/abs/2512.07570v1)
- **PDF**: [https://arxiv.org/pdf/2512.07570v1](https://arxiv.org/pdf/2512.07570v1)

本文档是两篇环绕声技术导论的第一部分，面向希望实际应用环绕声技术的读者。本部分略去深层次技术细节，重点帮助读者建立对基础概念的直观理解。我们将阐释环绕声信号的定义、获取方式、可施加的信号处理方法，以及如何为听者实现声音重放。文中提供了丰富的音频示例以辅助说明。本导论的第二部分收录于独立文档，旨在为希望理解数学细节的读者提供深入解析。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2512.07570v1）
</div>

</details>

---

## MultiAPI Spoof: A Multi-API Dataset and Local-Attention Network for Speech Anti-spoofing Detection
- **Authors**: Xueping Zhang, Zhenshan Zhang, Yechen Wang, Linxi Li, Liwei Jin, Ming Li
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.07352v1](https://arxiv.org/abs/2512.07352v1)
- **PDF**: [https://arxiv.org/pdf/2512.07352v1](https://arxiv.org/pdf/2512.07352v1)

现有语音反欺骗基准主要依赖少量公开模型，导致其与真实场景存在显著差距——实际商用系统往往采用多样化的专有API。为解决这一问题，我们提出了MultiAPI Spoof数据集，该多API音频反欺骗数据集包含约230小时由30种不同API生成的合成语音，涵盖商业服务、开源模型及在线平台。基于此数据集，我们定义了API溯源任务，实现对欺骗音频生成来源的细粒度归因。进一步，我们提出Nes2Net-LA——一种局部注意力增强的Nes2Net变体，通过优化局部上下文建模与细粒度欺骗特征提取提升性能。实验表明，Nes2Net-LA在多项指标上达到最优水平，并在多样化和未知欺骗条件下表现出卓越的鲁棒性。代码与数据集均已开源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音反欺骗研究面临现实差距。现有基准数据集（如ASVspoof）主要基于少数公开的TTS/VC模型生成欺骗语音，无法覆盖现实中大量使用的商业、专有或闭源API。
- **既有问题**：
  - 数据多样性不足，导致模型在真实API生成的语音上泛化能力差。
  - 快速发展的生成技术（如扩散模型）加剧了研究基准与现实攻击之间的领域鸿沟。

2)  
论文通过构建新数据集并提出改进网络，系统性地解决上述问题。

- **构建MultiAPI Spoof数据集**：
  - 包含约230小时由**30种不同API**生成的合成语音，涵盖商业服务、开源模型及在线平台。
  - 按API划分训练/开发/评估集，并专门设置**未见API**用于评估模型跨源泛化能力。
  - 该数据集弥补了现有基准的多样性缺口，为模型提供了更接近真实场景的训练数据。

- **提出Nes2Net-LA（局部注意力增强网络）**：
  - **动机**：基础模型Nes2Net虽能提取多尺度特征，但其嵌套块间仅进行层次化交互，限制了**长距离跨块信息交流**，这对高维语音表征至关重要。
  - **方法**：在Nes2Net的嵌套块之间插入**局部注意力模块**。
    - 为每个块定义一个滑动窗口邻域（如窗口半径K=1）。
    - 通过注意力机制聚合邻域内块的特征信息。
    - 通过残差连接将增强的局部特征与原始特征融合。
  - **优势**：
    - 相比全局注意力，计算更高效，适合嵌套块的长序列。
    - 增强了局部上下文建模能力，能提取更**细粒度**的欺骗特征。
    - 提升了特征的**一致性与鲁棒性**，从而改善模型在多样及未见欺骗条件下的检测性能。

3)  
- **任务与效果**：
  - **语音反欺骗检测**：在多个基准（ITW, AI4T）及MultiAPI Spoof测试集上，Nes2Net-LA均取得**最优性能**。例如，在加入MultiAPI Spoof训练后，在ITW数据集上EER降至1.42%。
  - **跨域与未见条件鲁棒性**：加入MultiAPI Spoof训练显著提升了模型在**未见API**上的泛化能力，证明了数据多样性的价值。
  - **API溯源任务**：论文首次定义了此细粒度任务。在MultiAPI Spoof数据集上，模型对**已见API**的溯源F1分数高达0.936，但对**未见API**的识别（零样本泛化）仍具挑战，为未来研究指明了方向。
</div>

</details>

---

## DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection
- **Authors**: Sayeem Been Zaman, Wasimul Karim, Arefin Ittesafun Abian, Reem E. Mohamed, Md Rafiqul Islam, Asif Karim, Sami Azam
- **Categories**: cs.CV, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.07351v1](https://arxiv.org/abs/2512.07351v1)
- **PDF**: [https://arxiv.org/pdf/2512.07351v1](https://arxiv.org/pdf/2512.07351v1)

合成媒体（尤其是深度伪造内容）的日益普及，对数字内容验证提出了新的挑战。尽管近期研究已同时利用音频与视觉信息，但多数方法将多模态线索集成于单一模型中，仍易受模态失配、噪声及篡改的影响。为此，本文提出DeepAgent——一种先进的多智能体协同框架，通过同时融合视觉与音频模态以实现高效的深度伪造检测。DeepAgent包含两个互补的智能体：智能体1采用基于AlexNet的轻量化卷积神经网络对视频进行检测，以识别深度伪造篡改的痕迹；智能体2则通过结合声学特征、Whisper生成的音频转录文本以及EasyOCR提取的图像帧序列，检测音画不一致性。两者的判定结果通过随机森林元分类器进行融合，该分类器利用各智能体学习到的不同决策边界，从而提升最终性能。本研究在三个基准数据集上评估了所提框架，展示了组件级与融合后的性能表现。智能体1在Celeb-DF与FakeAVCeleb合并数据集上取得了94.35%的测试准确率；在FakeAVCeleb数据集上，智能体2与最终元分类器的准确率分别为93.69%与81.56%。此外，在DeepFakeTIMIT数据集上的跨数据集验证进一步证实了元分类器的鲁棒性，其最终准确率达到97.49%，表明该框架在不同数据集上均具备较强的泛化能力。这些结果证明，基于层级结构的融合策略能够通过弥补单模态的缺陷来增强系统鲁棒性，同时验证了多智能体方法在应对多样化深度伪造篡改类型中的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：深度伪造技术日益泛滥，对数字内容真实性构成严重威胁。现有检测方法多依赖单一模态（仅视觉或仅音频），或虽融合多模态但通常在单一模型内进行特征级融合。
- **既有问题**：这些方法难以有效处理模态间不匹配、噪声和复杂操纵。它们缺乏对音频-视觉语义一致性的显式建模，且融合策略多为单阶段，缺少鲁棒的决策级聚合机制来应对跨模态冲突证据，导致在多样化数据集上性能不稳定。

2)  
论文提出的 **DeepAgent** 是一个双智能体协作框架，通过专门的智能体分工和决策级融合来解决上述问题：
- **双智能体分工设计**：
    - **Agent-1（视觉检测器）**：采用轻量级、基于AlexNet的CNN，专注于从视频帧中捕捉像素级和帧级的视觉伪造痕迹（如纹理异常、时空不一致性）。其设计保证了计算效率和对视觉伪影的强判别力。
    - **Agent-2（音频-视觉语义一致性检测器）**：专门检测跨模态不一致性。它融合三种信息：
        1.  **声学特征**：提取MFCC系数，表征音频的频谱特性。
        2.  **语义文本**：使用Whisper模型将音频转录为文本。
        3.  **视觉文本**：使用EasyOCR从视频帧中提取文本。
        通过计算音频转录文本与视觉提取文本之间的词汇相似度，显式地量化“所见”与“所闻”之间的语义对齐程度，从而识别唇语不同步或内容矛盾等深层伪造特征。
- **鲁棒的决策级融合**：
    - 两个智能体独立处理输入并输出各自的预测概率。
    - 这些概率被拼接成一个**元特征向量**，输入到一个**随机森林元分类器**中。
    - 随机森林作为决策级聚合层，能够学习并整合两个智能体不同的决策边界，从而在最终决策中权衡视觉证据和跨模态语义证据。
- **解决思路总结**：该框架通过专业化分工（视觉伪影 vs. 跨模态一致性）实现了对多层面证据的并行分析，再通过基于随机森林的决策级融合，自适应地结合互补信息，弥补单一模态或单一智能体的弱点，提升了系统对噪声、模态缺失及多样化伪造类型的鲁棒性。

3)  
论文在三个基准数据集上评估了DeepAgent，取得了以下效果：
- **组件性能**：在Celeb-DF和FakeAVCeleb的混合数据集上，纯视觉的Agent-1测试准确率达到**94.35%**。在FakeAVCeleb数据集上，跨模态的Agent-2测试准确率达到**93.69%**。
- **融合性能**：随机森林元分类器在FakeAVCeleb数据集上取得了**81.56%**的准确率。更重要的是，在**跨数据集验证**中，在DeepFakeTIMIT数据集上取得了**97.49%**的准确率和97.52%的F1分数，展现了出色的泛化能力。
- **总体效果**：实验表明，基于层次化智能体融合的DeepAgent框架，在平衡准确率、模型复杂度和鲁棒性方面具有优势，特别是在跨数据集场景下能保持稳定的性能，有效应对多种类型的深度伪造。
</div>

</details>

---

## Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data
- **Authors**: Srihari Bandarupalli, Bhavana Akkiraju, Charan Devarakonda, Vamsiraghusimha Narsinga, Anil Kumar Vuppala
- **Categories**: cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.07277v1](https://arxiv.org/abs/2512.07277v1)
- **PDF**: [https://arxiv.org/pdf/2512.07277v1](https://arxiv.org/pdf/2512.07277v1)

面向低资源语言的自动语音识别，其发展始终受限于标注数据的稀缺以及前沿模型对计算资源的高需求。本文以波斯-阿拉伯语系（波斯语、阿拉伯语和乌尔都语）为主要研究对象，系统探讨了跨语言持续预训练在低资源语言上的应用。我们的方法表明，通过策略性地利用无标注语音数据，可以在不牺牲识别准确率的前提下有效弥补资源缺口。我们通过可扩展的无标注数据收集流程构建了一个3000小时的多语言语料库，并采用有针对性的持续预训练结合形态感知的分词技术，训练出一个3亿参数的模型。该模型在性能上可与规模大5倍的系统相媲美：在波斯语上表现优于Whisper Large v3（15亿参数），在阿拉伯语和乌尔都语上也取得了具有竞争力的结果，而所用参数量和标注数据量均显著更少。这些发现挑战了当前“ASR质量主要随模型规模扩展”的主流假设，揭示了在低资源场景下，数据的相关性和策略性的预训练才是更为关键的因素。本工作为发展包容性语音技术提供了一条实用路径，使得在无需依赖大规模计算基础设施或专有数据集的条件下，也能为资源不足的语言实现有效的自动语音识别。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：针对波斯语、阿拉伯语和乌尔都语等形态复杂、标注数据稀缺的低资源语言，自动语音识别面临巨大挑战。  
- **既有方法问题**：当前最先进的ASR模型（如Whisper、Seamless）参数量巨大（1.5B+），计算成本高，且主要受益于高资源语言；现有持续预训练研究多聚焦于高资源场景或单一语言，缺乏对低资源语言家族的系统性探索。

2)  
论文提出了一套结合数据、预训练策略与分词方法的系统性解决方案：  
- **构建跨语言无标注语料库**：通过可扩展的流水线（图1），从公开多媒体资源中收集并处理波斯语、阿拉伯语和乌尔都语语音，构建了一个3000小时的高质量、多领域无标注语料库，用于自监督预训练。  
- **采用针对性的持续预训练策略**：比较了三种初始化策略：  
  - **CS**：从头训练Wav2Vec 2.0 Base（95M参数），仅使用上述3000小时语料。  
  - **CP1**：基于多语言预训练模型XLS-R（300M参数，436K小时128种语言）进行持续预训练。  
  - **CP2**：基于以英语为中心的Wav2Vec 2.0 Large（300M参数，65K小时）进行持续预训练。  
  所有模型均在上述无标注语料库上预训练后，再使用标注数据进行语言特定的微调。  
- **引入形态感知的分词方法**：针对波斯-阿拉伯语系的拼写复杂性和丰富形态，采用SentencePiece子词分词（替代字符级分词）初始化CTC输出层，以更好地建模语言结构，降低词错误率。  
- **核心解决思路**：通过**领域相关的无标注数据预训练**和**针对性的模型初始化**，使紧凑模型（300M参数）能够学习到更有效的声学-语言表示，从而弥补标注数据不足和模型规模限制，实现高效且准确的ASR。

3)  
在波斯语、阿拉伯语和乌尔都语的ASR任务上进行了系统评估：  
- **主要效果**：参数量仅300M的CP2模型（基于Wav2Vec 2.0 Large持续预训练）取得了与庞大模型相竞争甚至更优的性能：  
  - **波斯语**：WER为17.1%，**优于**参数量5倍大的Whisper Large v3（21.4%）。  
  - **阿拉伯语和乌尔都语**：WER分别为32.9%和20.6%，与Whisper Large v3（27.2%和17.2%）差距在3-6%以内，但参数量显著更少。  
- **效率优势**：模型仅使用约3000小时无标注数据和有限的标注数据进行训练，证明了通过**数据相关性和战略预训练**，可以在低资源场景下实现参数高效的高质量ASR。
</div>

</details>

---

## TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation
- **Authors**: Bhavana Akkiraju, Srihari Bandarupalli, Swathi Sambangi, Vasavi Ravuri, R Vijaya Saraswathi, Anil Kumar Vuppala
- **Categories**: cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.07265v1](https://arxiv.org/abs/2512.07265v1)
- **PDF**: [https://arxiv.org/pdf/2512.07265v1](https://arxiv.org/pdf/2512.07265v1)

尽管泰卢固语使用者超过8000万人，但针对这一形态丰富语言的语音翻译研究仍严重不足。本研究通过46小时人工校验的CSTD语料库数据（按30小时/8小时/8小时划分训练集/开发集/测试集），构建了高质量的泰卢固语-英语语音翻译基准。我们对级联架构与端到端架构的系统性对比表明：虽然IndicWhisper + IndicMT凭借大量泰卢固语专项训练数据获得最优性能，但经过微调的SeamlessM4T模型在使用显著更少的泰卢固语专项数据时仍展现出卓越竞争力。这一发现表明，通过精细的超参数调优和适量的平行数据（可能低于100小时），端到端系统在低资源场景下能达到与级联方法相媲美的性能。我们通过人工评估对比BLEU、METEOR、ChrF++、ROUGE-L、TER和BERTScore等指标的可靠性，发现传统指标在泰卢固语-英语翻译任务中比BERTScore具有更优的质量区分能力。本研究的三大核心贡献包括：一个可复现的泰卢固语-英语基准数据集、低资源场景下端到端系统具备竞争潜力的实证依据，以及针对形态复杂语言对自动评估的实践指导。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：泰卢固语使用者超8000万，但其语音翻译研究因缺乏高质量平行数据、评估标准不统一而严重不足。该语言形态复杂，传统评估指标难以准确衡量翻译质量。  
- **既有方法问题**：现有方法主要分为级联系统（ASR+MT）和端到端系统。级联系统依赖大量语言特定数据，而端到端系统（如SeamlessM4T）在泰卢固语上训练数据有限（仅466小时），导致在低资源场景下性能受限。同时，自动评估指标的可靠性在泰卢固语-英语翻译中未得到系统验证。

2)  
- **构建高质量基准数据集**：从CSTD语料库中精选50小时泰卢固语音频，通过人工验证（5点Likert量表）得到46小时高质量平行数据（训练/开发/测试集为30h/8h/8h），解决了数据稀缺和质量问题。  
- **系统对比与优化**：  
  - **级联系统**：评估了IndicWhisper（800+小时泰卢固ASR数据）+ IndicMT（大量平行数据）的组合，充分利用语言特定数据优势。  
  - **端到端系统**：对SeamlessM4T进行精细微调，探索不同数据配置（仅验证数据、平衡混合数据、大规模混合数据）和超参数（学习率、批次大小），证明即使数据有限，通过优化也能提升性能。  
- **评估指标可靠性分析**：对比BLEU、METEOR、ChrF++、ROUGE-L、TER和BERTScore与人类评估（ChatGPT式评分）的相关性，为形态丰富语言的评估提供实践指导。

3)  
- **任务**：泰卢固语到英语的语音翻译。  
- **效果**：  
  - 级联系统IndicWhisper+IndicMT取得最佳性能（BLEU 24.0，METEOR 47.2）。  
  - 微调后的端到端系统SeamlessM4T（配置4）以较少数据达到竞争性结果（BLEU 17.1，METEOR 43.8），与级联系统的BLEU差距仅29%。  
  - 评估指标中，ROUGE-L和ChrF++与人类评分相关性最高（Pearson系数分别为0.97和0.96），适用于低资源翻译评估。
</div>

</details>

---

## Unsupervised Single-Channel Audio Separation with Diffusion Source Priors
- **Authors**: Runwu Shi, Chang Li, Jiang Wang, Rui Zhang, Nabeela Khan, Benjamin Yen, Takeshi Ashizawa, Kazuhiro Nakadai
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.07226v1](https://arxiv.org/abs/2512.07226v1)
- **PDF**: [https://arxiv.org/pdf/2512.07226v1](https://arxiv.org/pdf/2512.07226v1)

单通道音频分离旨在从单通道混合信号中分离出各个独立声源。现有方法大多依赖于使用合成生成的配对数据进行监督学习。然而，在实际场景中获取高质量的配对数据往往十分困难。这种数据稀缺问题可能导致模型在未见条件下的性能下降，并限制其泛化能力。为此，本研究从无监督角度出发，将该问题构建为概率逆问题求解框架。我们的方法仅需在独立声源上训练的扩散先验模型，通过重构引导将初始状态迭代推向解空间，从而实现分离。特别地，我们提出了一种专为分离任务设计的先进逆问题求解器，能够缓解逆去噪过程中扩散先验与重构引导相互干扰导致的梯度冲突问题，从而确保各声源获得高质量且均衡的分离效果。此外，我们发现使用增强混合信号而非纯高斯噪声初始化去噪过程，能够提供信息量更丰富的起点，显著提升最终性能。为进一步增强音频先验建模能力，我们设计了一种新颖的基于时频注意力机制的神经网络架构，该架构展现出强大的音频建模能力。综合以上改进，本方法在语音-声音事件分离、声音事件分离及语音分离任务中均取得了显著的性能提升，并通过实验验证了其有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：单通道音频分离旨在从单通道混合音频中分离出各个独立声源。现有方法大多依赖监督学习，需要大量高质量的成对（混合音频-干净源）数据进行训练。
- **既有问题**：
  - 在真实场景中获取高质量成对数据困难，导致模型在未见条件下的性能下降和泛化能力受限。
  - 传统的无监督方法（如基于NMF、HMM、VAE、GAN等源模型）受限于其底层模型的建模和生成能力，性能存在瓶颈。

2)  
论文提出了一种基于扩散模型的无监督音频分离框架，将分离任务构建为一个概率逆问题，核心方法通过以下设计解决上述问题：

- **采用扩散模型作为源先验**：
  - 为每种声源（如语音、声音事件）独立训练无条件或条件扩散模型，作为强大的源先验知识。这避免了对成对混合数据的需求，实现了无监督学习。

- **设计混合梯度引导强度调度**：
  - 在逆向分离过程中，扩散先验梯度与基于混合音频重建的引导梯度之间存在冲突，损害分离质量。
  - 论文分析了梯度冲突现象，并提出一种混合调度策略：在早期高噪声阶段采用与噪声水平成比例的引导（借鉴DSG），以缓解初始冲突；在后期低噪声阶段平滑过渡到恒定引导（借鉴DPS），以维持稳定性和源间平衡。该调度通过SmoothMax函数实现，有效管理了冲突。

- **引入噪声增强的混合音频初始化**：
  - 摒弃从纯高斯噪声开始采样，改为从一个经过噪声增强的混合音频波形初始化扩散过程。这为分离提供了一个信息丰富的起点，显著提升了最终分离的保真度。

- **提出新颖的时频注意力网络架构**：
  - 设计了一个三重路径自注意力U-Net作为扩散主干网络，直接在波形空间操作。
  - 该架构包含**帧内注意力**（建模单帧内频点间关系）、**频带内注意力**（建模单一频带的时间演变）和**全局时间注意力**（建模跨所有频带的时间依赖），专门用于捕捉音频细粒度的谱时特征，增强了模型对多样音频类型的先验学习能力。

3)  
论文在三个音频分离任务上评估了所提方法，使用VCTK和FSD-Kaggle2018数据集：
- **语音-声音事件分离**：在“1语音+1声音”和“1语音+2声音”混合上，SI-SDR分别达到14.31 dB和8.58 dB，PESQ分别达到2.12和1.80，显著优于其他无监督基线。
- **声音事件分离**：在分离2个和3个声音事件的混合上，SI-SDR分别达到10.44 dB和5.15 dB。
- **语音分离**：在分离2个语音的混合上，SI-SDR达到6.11 dB，失败率降至29.8%。
- **总体效果**：该方法在多项指标上超越了其他无监督方法（如DPS、DSG、解析采样），性能甚至可与完全监督的Conv-TasNet相媲美，验证了其有效性。
</div>

</details>

---

## Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits
- **Authors**: Masato Ishii, Akio Hayakawa, Takashi Shibuya, Yuki Mitsufuji
- **Categories**: cs.MM, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.07209v1](https://arxiv.org/abs/2512.07209v1)
- **PDF**: [https://arxiv.org/pdf/2512.07209v1](https://arxiv.org/pdf/2512.07209v1)

本文提出了一种新颖的音视频联合编辑流程，旨在增强编辑后视频与其伴音之间的协调性。该方法首先应用先进的视频编辑技术生成目标视频，随后进行音频编辑以匹配视觉变化。为实现这一目标，我们提出了一种新的视频到音频生成模型，该模型以源音频、目标视频和文本提示为条件。我们扩展了模型架构以融入条件音频输入，并提出了一种提升训练效率的数据增强策略。此外，该模型能根据编辑复杂度动态调整源音频的影响权重，尽可能保留原始音频结构。实验结果表明，本方法在保持音视频同步性与内容完整性方面优于现有方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：视频编辑技术已能精细处理视觉内容，但大多忽略音频，导致编辑后音视频不协调。现有音视频联合编辑方法存在局限：
  - 独立编辑音视频常导致模态间不对齐。
  - 对编辑后视频直接应用视频到音频生成模型会破坏原始音频结构。
  - 现有联合编辑方法受限于低帧率视频（如4 fps），不适用于高帧率场景。

2)  
论文提出一种**顺序式音视频联合编辑流程**，核心是**条件音频生成模型**，以解决上述问题：
- **流程设计**：
  - 首先使用先进视频编辑方法（如RAVE、VACE）生成目标视频。
  - 随后，通过新提出的视频到音频生成模型，根据**源音频、目标视频和文本提示**生成与之对齐的编辑后音频。
- **模型架构改进**：
  - 基于MMAudio架构，引入**分层声学特征提取**：从源音频计算多层级响度特征，以保留其声学结构。
  - **双重调制机制**：
    - 将处理后的声学特征调制到音频潜在表示中。
    - 同时调制Synchformer提取的视频同步特征，以增强时域对齐。
- **训练策略**：
  - 采用**细节-时间掩码数据增强**：随机掩码声学特征的细节层级和时间段，防止模型过度依赖源音频，确保其对文本和视频条件的响应。
- **自适应生成**：
  - 引入**可编辑性评分**，通过计算源音频与目标视频的语义相似度（使用ImageBind），动态选择声学特征的保留细节层级。编辑难度高时保留较少细节，反之保留较多，以平衡结构保持与目标对齐。

3)  
在**AvED-Bench数据集**的音频编辑任务上评估，相比基线方法（如独立编辑的SDEdit、ZETA，及简单视频到音频生成V2A），本方法取得了以下效果：
- **音视频对齐（IB-AV）显著提升**：通过顺序编辑并参考目标视频，实现了更好的模态同步。
- **结构保持（LPAPS）更优**：自适应声学特征使用有效保留了源音频的声学结构。
- **文本保真度（IB-TA）保持良好**：与独立编辑方法相当，同时避免了后者对齐性差的问题。
- **性能受视频编辑质量影响**：当使用更先进的视频编辑方法（如VACE）时，整体效果更佳。
</div>

</details>

---

## JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention
- **Authors**: Georgios Ioannides, Christos Constantinou, Aman Chadha, Aaron Elkins, Linsey Pang, Ravid Shwartz-Ziv, Yann LeCun
- **Categories**: cs.SD, cs.AI, cs.LG, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.07168v1](https://arxiv.org/abs/2512.07168v1)
- **PDF**: [https://arxiv.org/pdf/2512.07168v1](https://arxiv.org/pdf/2512.07168v1)

本文提出一种两阶段自监督框架，将联合嵌入预测架构（JEPA）与密度自适应注意力机制（DAAM）相结合，用于学习鲁棒的语音表征。第一阶段采用集成DAAM的JEPA，通过在隐空间进行掩码预测来学习语义音频特征，完全解耦于波形重建任务。第二阶段利用这些表征，通过有限标量量化（FSQ）与混合基数打包方案实现高效分词，再结合HiFi-GAN解码器进行高保真波形重建。通过将基于高斯混合的密度自适应门控机制融入JEPA编码器，该模型能以低至2.5 Hz的帧率实现自适应时序特征选择，并发现语音的层次化结构。最终生成的分词（47.5 词元/秒）具有可逆性、高压缩率和语言模型友好性，其性能与现有神经音频编解码器相当，且通常更为高效。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：传统神经音频编解码器在训练时将表征学习与波形重建目标耦合，迫使编码器优先考虑最小化波形级损失的特征。这混淆了两个不同的目标：学习捕捉语言和声学结构的语义表征，以及为高保真重建保留感知质量。
- **既有方法的问题**：这种耦合导致编码器可能过度关注低层次波形细节，而非高层次语义内容，限制了学习到的表征在下游任务（如语音识别、语音合成）中的有效性和灵活性。

2)  
论文提出一个两阶段自监督框架，核心方法如下：
- **第一阶段：JEPA编码器与DAAM**  
  - 使用联合嵌入预测架构（JEPA），通过潜在空间中的掩码预测来学习语义音频特征，完全与波形重建解耦。  
  - 引入密度自适应注意力机制（DAAM），基于高斯混合模型对时间统计特性进行门控，自适应地选择信息丰富的时间区域和特征。  
  - 编码器采用卷积-Transformer混合设计，通过渐进下采样实现低帧率（2.5 Hz）的层次化语音结构建模。
- **第二阶段：量化与重建**  
  - 利用第一阶段学习到的表征，通过有限标量量化（FSQ）进行高效离散化，无需学习码本。  
  - 采用混合基数打包方案，将多个FSQ维度压缩为单个整数令牌，实现高压缩率（47.5令牌/秒）。  
  - 使用HiFi-GAN解码器将量化后的特征重建为高保真波形，训练目标结合L1重建损失、多分辨率STFT损失和对抗损失。
- **解决思路**：通过解耦表征学习与重建，编码器专注于语义内容；DAAM提供自适应特征选择；FSQ与混合基数打包实现高效、语言模型友好的令牌化；最终获得可逆、高压缩且鲁棒的表征。

3)  
- **任务与效果**：该框架作为一个神经令牌化器，在语音表征学习任务上取得了竞争性效果。  
- **具体表现**：生成的离散令牌速率仅为47.5令牌/秒，显著低于许多现有神经音频编解码器（如EnCodec的75 Hz），同时保持了高重建质量。学习到的表征在语义丰富性和压缩效率之间取得了良好平衡，适用于语音合成、语音识别等下游任务，并易于与大型语言模型集成。
</div>

</details>

---
