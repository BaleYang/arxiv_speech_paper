---
layout: post
title: "arXiv Daily – 2025-12-16"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-16（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-15 08:50 — 2025-12-16 08:50
- 抓取总数：5 篇 | 本页显示：5 篇（去重/过滤后）

## REVERB-FL: Server-Side Adversarial and Reserve-Enhanced Federated Learning for Robust Audio Classification
- **Authors**: Sathwika Peechara, Rajeev Sahay
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.13647v1](https://arxiv.org/abs/2512.13647v1)
- **PDF**: [https://arxiv.org/pdf/2512.13647v1](https://arxiv.org/pdf/2512.13647v1)

联邦学习（FL）为音频分类提供了一种隐私保护的训练范式，但其对客户端异构性和投毒攻击高度敏感，其中被恶意攻陷的客户端可能使全局模型产生偏差，进而损害音频分类器的性能。为减轻模型投毒对音频信号分类的影响，本文提出REVERB-FL——一种轻量级的服务器端防御方法，该方法将小型储备数据集（约5%）与聚合前后的重训练及对抗训练相结合。在每轮本地训练后，服务器利用储备数据集（包括原始数据或额外添加的对抗扰动数据）对全局模型进行精调，从而抵消非独立同分布的数据偏移，并缓解潜在的模型投毒问题，且无需显著增加客户端开销或改变聚合流程。我们从理论上证明了该框架的可行性，相较于基准联邦平均算法，其收敛速度更快且稳态误差更低。我们在两个开源音频分类数据集上验证了该框架，数据集包含不同的独立同分布及狄利克雷非独立同分布划分，实验表明REVERB-FL在多种本地数据投毒设计下均能有效缓解全局模型投毒问题。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：联邦学习为音频分类提供了隐私保护的训练范式，但面临两大挑战：
  - **客户端异构性**：音频数据在边缘设备上自然分布，且常呈现非独立同分布特性，导致模型聚合时出现漂移，影响收敛。
  - **投毒攻击**：恶意客户端通过在本地训练数据中注入对抗性扰动（如FGSM、PGD、噪声），进行模型投毒，从而偏置全局模型，降低分类性能。
- **既有方法问题**：现有防御方法（如基于过滤的鲁棒聚合、客户端对抗训练）常依赖不现实的信任假设，在非独立同分布条件下难以区分恶意与异构更新，且可能丢弃有用数据或增加客户端计算开销。

2)  
论文提出 **REVERB-FL**，一种轻量级、服务器端的防御框架，通过结合储备集重训练与对抗训练来解决问题：
- **核心机制**：
  - **储备集重训练**：服务器在训练前从所有客户端分层采样约5%的数据作为干净的储备集，并在每轮联邦平均聚合后，使用该储备集对全局模型进行额外的一轮SGD重训练。这纠正了因非独立同分布或投毒导致的模型漂移，稳定收敛。
  - **对抗增强**：在服务器端，对储备集数据生成对抗性样本（使用FGSM、PGD或加性高斯噪声等方法），并将这些对抗样本与干净数据一起用于重训练，从而增强模型对输入扰动的鲁棒性，而无需修改客户端计算或聚合过程。
- **理论保证**：通过收敛性分析证明，在满足平滑性、强凸性等标准假设下，REVERB-FL相比基准FedAvg具有更快的收缩速度和更低的稳态误差，即使在存在对抗性偏差的条件下也能保持收敛。
- **优势**：
  - **服务器端实现**：完全在服务器端操作，不增加客户端开销，也不依赖对客户端更新的强假设。
  - **兼容性与轻量**：与标准联邦学习流程兼容，仅需少量额外计算，适用于资源受限的边缘音频应用。

3)  
- **任务与数据集**：在**AudioMNIST**（数字语音分类）和**UrbanSound8K**（环境声音分类）两个开源音频数据集上进行了评估，数据划分为独立同分布和基于狄利克雷的非独立同分布。
- **攻击场景**：针对训练时模型投毒攻击，包括FGSM、PGD和加性高斯噪声攻击，设定50%的客户端为恶意。
- **效果**：
  - 在独立同分布和非独立同分布条件下，REVERB-FL（尤其是结合对抗增强的变体）均显著优于基准FedAvg及现有防御方法（如USD-FL、Deep SVDD、UDFed）。
  - 在梯度攻击（如PGD）下，性能提升最为明显，在AudioMNIST上准确率比基线提高15–30%，在UrbanSound8K上提高10–15%。
  - 混合对抗增强版本（Retrain (All Adversarial)）在多种攻击类型下均表现出稳定且优越的跨攻击鲁棒性，同时保持了更快的收敛速度和更高的最终准确率。
</div>

</details>

---

## SAMAY: System for Acoustic Measurement and Analysis
- **Authors**: Adheep Arya G R, Vaibhav Pratap Singh, Mayank Kumar, Niyathi Shenoy, Tejas Suryawanshi, Ruchi Juyal, Sangit Saha, Kaushik Nanda, Hari Babu Pasupuleti, S D Sudarsan
- **Categories**: cs.SD, cs.RO
- **arXiv**: [https://arxiv.org/abs/2512.13284v1](https://arxiv.org/abs/2512.13284v1)
- **PDF**: [https://arxiv.org/pdf/2512.13284v1](https://arxiv.org/pdf/2512.13284v1)

本文介绍了一种名为SAMAY的自动鸟类鸣叫记录系统，该系统旨在通过构建大规模鸟类声学数据库来研究鸟类物种。通过对记录的鸟类鸣叫数据进行分析，该系统还可用于鸟类物种的自动分类、种群监测以及环境变化影响分析。该系统采用高性能STM32F407系列微控制器驱动，支持4个麦克风，配备128 GB存储容量，并通过连接太阳能充电器的10400 mAh电池组供电。此外，设备在运行期间可通过USB和Wi-Fi进行用户配置，确保野外部署时的易操作性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：鸟类是生态系统健康的重要指示物种，但其种群因栖息地破坏、气候变化等因素而衰退，急需长期、非侵入式的监测手段。声学监测通过记录鸟鸣来研究鸟类多样性、行为及种群变化，是有效的监测方法。  
- **既有方法的问题**：现有自主录音单元（ARU）在**长期部署的鲁棒性、存储容量、功耗管理及本地高级处理能力**方面存在不足。部分商业设备（如SongMeter4）虽性能强但成本高；低成本方案（如AudioMoth）则在存储、续航或处理能力上受限。此外，许多系统缺乏**太阳能充电、多存储管理及用户友好配置**等特性，难以在多样环境中实现高效、长期的鸟类声学监测。

2)  
论文提出的SAMAY系统通过**硬件设计、低功耗架构与智能软件流程**的综合方案，系统性地解决了上述问题：  
- **硬件架构**：  
  - 采用**STM32F407微控制器**作为核心，提供强大的处理能力（168 MHz Cortex-M4，带FPU），支持实时音频处理与存储管理。  
  - 集成**四通道音频编解码器（ADAU1978）**，支持高分辨率（24位）音频采集，采样率范围宽（8–192 kHz），确保高质量录音。  
  - 配备**128 GB存储**（通过SD多路复用器支持多SD卡动态切换），满足长期部署的海量数据存储需求。  
  - 内置**太阳能充电模块（LT3652）** 与高容量电池（10400 mAh），实现能源自给，适合偏远地区长期运行。  
- **低功耗与续航优化**：  
  - 利用微控制器的**低功耗睡眠模式**，仅在RTC定时唤醒时进行录音，大幅降低平均功耗。  
  - 关键组件（如RTC、EEPROM）选用低功耗型号，整体功耗在非录音状态降至179 mA，并通过太阳能持续补能。  
- **智能操作与配置**：  
  - 提供**双配置模式**（Wi-Fi/USB），用户可通过移动应用灵活设置采样率、录音计划（每日/每小时模式）等参数，配置存储在EEPROM中，断电不丢失。  
  - 采用**双缓冲DMA技术**录制音频，避免数据丢失，并支持.wav/.mp3格式输出。  
- **本地高级处理能力**：  
  - 利用STM32的算力，实现**实时带通滤波**以提升音频质量，并支持**基于振幅阈值的无效数据自动剔除**，优化存储空间。  
  - 系统为未来功能（如基于四通道的鸟类定位三角测量、机器学习分类）预留了扩展接口。

3)  
SAMAY系统在**野外鸟类声学监测任务**中进行了功能验证，取得了以下效果：  
- **长期自主录音**：成功在野外环境中实现按计划自动唤醒与录音，生成高质量的.wav文件（如48 kHz采样率下，10分钟录音约22 MB），128 GB存储支持约1092次10分钟录音会话。  
- **低功耗续航**：结合太阳能充电，系统可持续运行，适合偏远地区长期部署。  
- **用户友好操作**：通过移动应用实现远程配置与监控，支持JSON配置共享，便于大规模部署。  
- **音频处理增强**：实时噪声滤波与无效数据剔除功能提升了录音质量与存储效率，为后续鸟类识别与行为研究提供了可靠数据基础。
</div>

</details>

---

## DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec
- **Authors**: Tao Li, Wengshuo Ge, Zhichao Wang, Zihao Cui, Yong Ma, Yingying Gao, Chao Deng, Shilei Zhang, Junlan Feng
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.13251v1](https://arxiv.org/abs/2512.13251v1)
- **PDF**: [https://arxiv.org/pdf/2512.13251v1](https://arxiv.org/pdf/2512.13251v1)

基于编解码器的语言模型近期在文本转语音领域取得了突破性进展。然而，由于标准编解码器将音色与韵律特征紧密耦合，基于续写机制的语言模型不可避免地延续了这种纠缠，导致独立控制难以实现。现有研究尝试通过编解码器设计解耦这些特征，但解耦不充分仍是关键瓶颈。为应对这一挑战，本文提出DisCo-Speech——一个基于解耦语音编解码器与语言模型生成器的零样本可控语音合成框架。其核心组件DisCodec包含两个关键阶段：1）三因子解耦：通过并行编码器与混合损失函数，将语音显式分解为内容、韵律和音色三个独立子空间；2）融合与重建：将内容与韵律融合为适合语言模型预测的统一内容-韵律表征，同时联合优化重建质量以平衡解耦与重建的权衡。基于此设计，语言模型可根据风格提示进行韵律续写，而解码器负责目标音色注入，从而实现灵活的零样本控制。实验表明，DisCo-Speech在达到先进音色克隆性能的同时，在零样本韵律控制任务上优于基线方法。通过在编解码器层面解决核心的耦合问题，本框架为可控语音合成奠定了坚实基础。音频样本详见https://github.com/disco-speech/DisCo-Speech，代码与模型权重将通过同一链接发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于编解码器的语言模型（LM）推动了文本转语音（TTS）发展，但现有声学或混合编解码器将音色与韵律紧密耦合。  
- **既有问题**：这种耦合导致基于延续生成的LM在复制提示语音时，无法对音色和韵律进行独立、精细的控制。虽然已有研究尝试通过编解码器设计解耦，但解耦不充分、解耦与重建之间的权衡，以及表示对下游任务不友好，仍是关键瓶颈。

2)  
论文提出 **DisCo-Speech** 框架，其核心是解耦语音编解码器 **DisCodec** 和一个标准Transformer LM。该方法通过两阶段训练解决上述问题：  

- **第一阶段：三因子解耦**  
  - 使用三个并行编码器分别提取内容、韵律和音色表示。  
  - 通过混合约束确保解耦：内容分支使用音素识别损失；韵律分支使用基频回归损失、相关性损失及梯度反转层（GRL）去除音色；音色分支使用说话人分类损失。  
  - 引入**软正交约束**，根据属性间固有依赖关系（如韵律与内容存在动态关联，而音色与韵律可近乎独立）调整解耦强度，避免信息丢失或泄漏。  

- **第二阶段：融合与重建**  
  - 将解耦后的内容与韵律表示融合为统一的、与音色无关的内容-韵律词元，便于LM预测。  
  - 训练一个专门的解码器，在给定目标音色的条件下，从融合词元重建高质量波形，直接优化解耦-重建的权衡。  

- **整体协作**：训练完成后，LM根据文本和韵律提示进行内容-韵律词元的延续生成（控制韵律），DisCodec解码器则根据目标音色条件重建波形（控制音色），从而实现独立的零样本控制。

3)  
- **语音重建**：在LibriSpeech测试集上，DisCodec在50词元/秒的速率下取得了具有竞争力的重建质量（UTMOS: 4.10）和说话人相似度（SSIM: 0.81）。  
- **解耦能力**：在零样本语音转换任务中，DisCodec在保持源韵律的同时，实现了最高的目标音色相似度（SSIM: 61.1）和基频轮廓一致性（F0cor: 0.58）。  
- **可控生成**：  
  - **零样本韵律控制**：在AB偏好测试中，DisCo-Speech在风格场景的音色一致性和韵律相似性上均优于基线；在情感场景，其音色保持能力更强。  
  - **零样本语音克隆**：在SEED-TTS-Eval基准测试中，其语音清晰度与一流自回归模型相当，说话人相似度表现良好。
</div>

</details>

---

## Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning
- **Authors**: Xin Guo, Yifan Zhao, Jia Li
- **Categories**: cs.AI, cs.CV, cs.GR, cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.13131v1](https://arxiv.org/abs/2512.13131v1)
- **PDF**: [https://arxiv.org/pdf/2512.13131v1](https://arxiv.org/pdf/2512.13131v1)

基于语音生成三维人体动作在众多下游应用中展现出巨大潜力，但如何模拟真实人体动作仍面临挑战。当前主流研究集中于通过端到端生成方案（涵盖生成对抗网络、矢量量化变分自编码器及近期扩散模型）来生成伴随语音的肢体动作。本文认为，这一不适定问题中，现有主流学习方案未能有效建模不同动作单元（如头部、身体和手部）之间及内部的关键关联，导致生成动作不自然且协调性差。为深入探究这些内在关联，我们提出一种统一的层次化隐式周期性学习方法，用于音频驱动的三维动作生成。与主流研究不同，本方法通过两项显式技术路径建模多模态隐式关系：其一，为解构复杂动作，我们首先利用周期性自编码器探索动作相位流形，从真实分布中模拟人体自然运动规律，同时结合当前隐状态中的非周期性特征以增强实例级多样性；其二，通过层级化建模面部动作、身体姿态与手部运动的关联，在学习过程中以级联引导机制驱动动画生成。我们在三维虚拟人像上验证了所提方法，大量实验表明，本方法在定量与定性评估中均优于当前最优的伴随语音动作生成方法。代码与模型将公开提供。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音驱动的3D手势生成是一个不适定的一对多映射问题，需要建模不同运动单元（如头部、身体、手部）之间的内部和相互关联，以生成协调自然的动作。  
- **既有方法的问题**：  
  - 主流方法（如端到端、两阶段模型）常将身体运动视为单一聚合信号，忽略了结构化关联，导致生成动作不自然、语义不对齐。  
  - 现有分层方法虽尝试分别处理不同身体部位，但缺乏清晰的关联层次，未能有效整合从强关联单元到弱关联单元的渐进机制。  
  - 同时，这些方法普遍忽视了周期性运动与非周期性运动的区别，影响了生成动作的多样性和自然度。

2)  
论文提出了一种统一的**分层隐式周期性学习（HIP）框架**，通过以下核心方法解决上述问题：  
- **周期性解缠模块**：  
  - 为建模每个运动单元内部的时域物理关联，该模块将复杂手势运动解耦为**周期性成分**和**非周期性成分**。  
  - 使用周期性自编码器学习从真实数据分布中提取常见的周期性相位流形，以模仿人类运动的自然规律。  
  - 同时，通过一个非周期性分支编码实例级的潜在特征，以融入多样化的非周期性运动，从而增强生成动作的个体多样性和自然度。  
- **分层属性引导模块**：  
  - 为建模不同运动单元（面部、身体、手部）之间的层次化关联，该模块采用**级联引导机制**。  
  - 首先，构建一个音频到面部的预测模型，提取与语音内容及情感强相关的面部特征。  
  - 然后，以面部特征作为主导引导，驱动身体运动的生成；身体运动进一步引导手部动作的生成，从而显式地建模从强关联到弱关联的依赖关系。  
- **统一学习框架**：  
  - 整合多模态输入（音频、文本、说话人ID、情感标签），通过特征融合网络和基于TCN的生成器，以分层方式合成协调的面部、身体和手部动画。

3)  
- **任务与效果**：  
  - 在**BEAT**和**BEATv2**数据集上，本方法在语音驱动的3D全身手势生成任务中进行了评估。  
  - **定量结果**：在BEAT数据集上，本方法在FGD（动作质量）和SRGR（语义关联）指标上达到最优，同时保持了高多样性和良好的节奏对齐（BeatAlign）。在BEATv2数据集上，本方法在FGD、多样性和BeatAlign上均优于现有方法。  
  - **定性结果**：生成的手势在视觉上更自然、连续，且与语音内容和节奏更对齐；面部动画的生成也更为准确和真实。  
  - **用户研究**：在真实感和手势-语音同步性方面，本方法显著优于现有代表性方法。
</div>

</details>

---

## HQ-MPSD: A Multilingual Artifact-Controlled Benchmark for Partial Deepfake Speech Detection
- **Authors**: Menglu Li, Majd Alber, Ramtin Asgarianamiri, Lian Zhao, Xiao-Ping Zhang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.13012v1](https://arxiv.org/abs/2512.13012v1)
- **PDF**: [https://arxiv.org/pdf/2512.13012v1](https://arxiv.org/pdf/2512.13012v1)

局部深度伪造语音检测面临的核心挑战在于，篡改仅发生在短暂片段中，而周围音频仍保持真实。然而，现有检测方法从根本上受限于可用数据集的质量：许多数据集依赖过时的合成系统与生成流程，引入了数据集特有的伪影，而非真实的篡改线索。为填补这一空白，我们提出了HQ-MPSD——一个高质量的多语言局部深度伪造语音数据集。该数据集基于细粒度强制对齐得到的语言学连贯拼接点构建，保持了韵律与语义的连续性，并最大程度减少了听觉与视觉上的边界伪影。数据集涵盖8种语言、550名说话人，总时长350.8小时，并添加了背景音效以更好地反映真实声学环境。平均意见分评估与语谱图分析均证实了样本具有较高的感知自然度。通过对前沿检测模型进行跨语言与跨数据集评估，我们发现所有模型在HQ-MPSD上的性能下降均超过80%。这些结果表明，一旦去除低层次伪影并引入多语言及声学多样性，HQ-MPSD即暴露出显著的泛化挑战，从而为局部深度伪造检测提供了一个更贴近现实、要求更高的基准。数据集可通过以下链接获取：https://zenodo.org/records/17929533。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：部分深度伪造语音检测面临严峻挑战，因其仅在短片段内进行篡改，周围音频仍为真实。现有检测方法的发展受限于数据集质量。
- **既有方法的问题**：
  - **数据集质量低**：现有数据集多基于过时合成系统或简单生成策略，引入了数据集特有的拼接伪影（如频谱不连续），而非真实的伪造线索。
  - **样本长度不足**：许多样本过短（如<5秒），缺乏有意义的语音学或韵律结构，限制了模型捕捉上下文依赖的能力。
  - **泛化能力有限**：数据集多为单语言且在纯净实验室环境下创建，无法涵盖真实世界中的多语言、口音及复杂声学环境多样性。

2)  
论文提出了高质量多语言部分深度伪造语音数据集 **HQ-MPSD**，通过以下核心方法解决上述问题：
- **提升样本质量与自然度**：
  - 采用**细粒度强制对齐**确定语言学连贯的拼接点（如词边界中点），避免破坏韵律和语义连续性。
  - 在拼接前进行**响度和频谱归一化**，减少真实与伪造片段间的浅层声学差异。
  - 使用重叠相加和余弦淡入淡出平滑边界，最小化可听和可视的拼接伪影。
- **确保样本长度与上下文**：
  - 将样本长度限制在**5-15秒**，提供有意义的语言学上下文，支持模型学习长程依赖。
- **增强多样性与泛化能力**：
  - 涵盖**八种语言**（荷兰语、英语、法语等）和**550名说话人**，引入语音学与韵律多样性。
  - 添加**背景效果**（房间脉冲响应和噪声），模拟真实声学条件，减少纯净实验室偏差。
- **提供精细标注**：
  - 除话语级标签外，提供**帧级标注**（真实、伪造、过渡区域），支持细粒度检测分析。
- **验证设计有效性**：
  - 通过MOS评分（平均3.68）和频谱图分析确认样本的高感知自然度，视觉上无明显不连续。

3)  
- **评估任务**：在**跨语言泛化**与**跨数据集泛化**任务上测试了SOTA检测模型（如GAT-ST、TDAM）。
- **取得效果**：
  - **跨语言评估**：模型在英语训练后，在其余七种未见语言上测试，性能显著下降（如EER从<1%升至约20-50%），揭示了严重的语言泛化缺陷。
  - **跨数据集评估**：在PartialSpoof数据集上训练的模型（如TDAM、Nes2Net）迁移至HQ-MPSD时，性能急剧退化（EER从<1%升至约50-57%，接近随机猜测），表明现有模型过度依赖数据集的特定伪影，而非真实伪造线索。
- **总体结论**：HQ-MPSD通过消除低层伪影并引入多语言和声学多样性，暴露了当前检测模型的重大泛化挑战，为开发更鲁棒的检测方法提供了更现实、更严格的基准。
</div>

</details>

---
