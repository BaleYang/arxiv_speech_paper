---
layout: post
title: "arXiv Daily – 2025-10-29"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-29（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-28 08:50 — 2025-10-29 08:50
- 抓取总数：12 篇 | 本页显示：12 篇（去重/过滤后）

## STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence
- **Authors**: Zihan Liu, Zhikang Niu, Qiuyang Xiao, Zhisheng Zheng, Ruoqi Yuan, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Jianze Liang, Xie Chen, Leilei Sun, Dahua Lin, Jiaqi Wang
- **Categories**: cs.SD, cs.CL, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.24693v1](http://arxiv.org/abs/2510.24693v1)
- **PDF**: [http://arxiv.org/pdf/2510.24693v1](http://arxiv.org/pdf/2510.24693v1)

尽管多模态大语言模型与音频大语言模型发展迅速，但现有音频基准大多仅检验可从文本描述中还原的语义信息，掩盖了细粒度感知推理能力的不足。本研究提出"音频4D智能"概念，即对声音在时间与三维空间中动态变化的推理能力，并构建STAR-Bench基准进行系统评估。该基准包含基础听觉感知（涵盖绝对与相对两种范式下的六类属性）与整体时空推理两大模块，后者涉及连续/离散过程的片段重组任务，以及静态定位、多源关系、动态轨迹三类空间任务。

我们采用双路径数据构建方法确保样本质量：基础任务使用程序化合成与物理仿真音频；整体数据则通过四阶段流程构建，包含人工标注与基于人类表现的最终筛选。与现有基准仅导致准确率微降不同，STAR-Bench在时序（-31.5%）与空间（-35.2%）任务上引发显著性能落差，证明其聚焦于语言难以描述的感知线索。对19个模型的评估揭示了与人类的巨大差距及能力分层现象：闭源模型受限于细粒度感知，开源模型则在感知、知识与推理层面全面落后。STAR-Bench为开发具有物理世界稳健理解能力的新一代模型提供了关键洞见与明确路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：多模态大语言模型和大型音频语言模型发展迅速，但现有音频基准主要测试可从文本描述中恢复的语义信息，掩盖了细粒度感知推理的不足。  
- **既有方法问题**：  
  - 现有基准依赖粗粒度语义内容，模型仅通过音频文本描述即可达到接近原始音频的性能（如准确率下降仅5.9%-9.0%）。  
  - 缺乏对时间动态和三维空间声音的深度推理能力评估，无法捕捉人类听觉中难以用语言描述的线索（如通过倒水声判断水位）。  

2)  
- **核心方法**：提出STAR-Bench基准，通过分层任务结构评估音频4D智能（时间+三维空间推理）。  
  - **基础声学感知任务**：  
    - 定量评估六项核心音频属性（音高、响度、时长、方位角、仰角、距离）。  
    - 包含绝对感知范围和相对区分敏感度测试，通过程序化合成和物理模拟音频生成可控样本。  
  - **整体时空推理任务**：  
    - **时间推理**：通过片段重排序评估连续过程（如多普勒效应）和离散事件序列（如工具操作）。  
    - **空间推理**：涵盖静态定位、多源关系及动态轨迹跟踪，要求模型整合时空线索。  
  - **数据构建流程**：  
    - 基础任务使用参数化合成音频；整体任务采用四阶段流程（分类构建、AI辅助过滤、人工标注、专家验证），确保样本质量和人类可解性。  
- **解决既有问题**：  
  - 通过细粒度属性测试和复杂场景推理，直接针对非语言可描述的音频线索，使模型性能显著下降（时间任务-31.5%，空间任务-35.2%）。  
  - 引入多音频输入和双通道处理策略（原生输入与分通道输入），弥补现有模型因单声道预处理导致的空间信息丢失。  

3)  
- **评估任务与效果**：  
  - 在基础感知、时间推理和空间推理三大任务上评估19个模型（16个开源、3个闭源）。  
  - **关键结果**：  
    - 人类表现优异（感知75.6%、时间88.0%、空间73.7%），而所有模型均远低于此基线。  
    - 最佳闭源模型Gemini 2.5 Pro平均准确率仅49.59%，开源模型接近随机猜测水平。  
    - 时空任务中，模型因多通道信息丢失普遍表现不佳，凸显空间推理能力薄弱。  
  - **实际意义**：揭示了模型在细粒度感知、多音频推理和空间处理方面的关键缺陷，为未来模型开发提供明确改进方向。
</div>

</details>

---

## Audio Signal Processing Using Time Domain Mel-Frequency Wavelet Coefficient
- **Authors**: Rinku Sebastian, Simon O'Keefe, Martin Trefzer
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.24519v1](http://arxiv.org/abs/2510.24519v1)
- **PDF**: [http://arxiv.org/pdf/2510.24519v1](http://arxiv.org/pdf/2510.24519v1)

在语音信号处理中，特征提取是最关键的环节。梅尔频率倒谱系数（MFCC）因其滤波机制与人耳听觉特性相似，已成为说话人识别与语音识别领域应用最广泛的特征。但该特征的局限性在于仅能提供信号的频域信息，无法反映特定频率成分的时间分布。小波变换凭借其灵活的时频窗口，可同时捕获信号的时频特征，成为分析语音等非平稳信号的理想工具。然而传统小波变换采用等比例频率缩放，对语音信号的分析效率较低，存在低频段分辨率不足、与人类听觉感知匹配度不高等问题。因此亟需开发一种能融合MFCC与小波变换优势的特征提取方法。

现有基于小波变换的梅尔尺度特征提取方法需在梅尔滤波基础上叠加小波变换，增加了额外计算步骤。本文提出一种在时域中结合小波变换概念的梅尔尺度特征提取方法，通过时域梅尔频率小波系数（TMFWC）技术，有效降低时频转换的计算负担与小波特征提取的复杂度。将该技术与储层计算相结合，可显著提升音频信号处理的效率。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频信号处理中，Mel频率倒谱系数（MFCC）因模拟人耳听觉感知而被广泛应用，但仅提供频域信息，缺乏时域定位能力。  
- **既有方法问题**：小波变换虽能同时提供时频信息，但其均匀频率缩放导致低频分辨率不足，且与听觉感知匹配较差；现有融合方法（如Wavelet-MFCC）需分步计算，复杂度高。  

2)  
- **核心方法**：提出时域Mel频率小波系数（TMFWC），直接在时域合成Mel尺度对应的正弦与余弦波，通过卷积计算小波系数，避免频域转换。  
- **解决思路**：  
  - 利用Mel滤波器组参数在时域生成波形，叠加后得到小波变换的实部与虚部。  
  - 通过幅度计算（$\sqrt{\text{imaginary}^2 + \text{real}^2}$）获取TMFWC，保留时频局部特征。  
  - 结合储层计算框架，仅训练输出层，降低计算负担，并采用最大池化进行数据降维。  
- **优势**：消除傅里叶与小波变换的复杂步骤，减少计算开销，同时兼容人耳感知特性与高时频分辨率。  

3)  
- **任务与效果**：在Ti-46和Audio-Mnist数据集上验证TMFWC性能：  
  - **数字识别**：准确率显著提升，优于传统MFCC或小波特征。  
  - **说话人识别**：特征判别力增强，结合储层计算实现高效分类。  
- **结论**：TMFWC在降低计算复杂度的同时，在语音相关任务中达到竞争性性能。
</div>

</details>

---

## Online neural fusion of distortionless differential beamformers for robust speech enhancement
- **Authors**: Yuanhang Qian, Kunlong Zhao, Jilu Jin, Xueqin Luo, Gongping Huang, Jingdong Chen, Jacob Benesty
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.24497v1](http://arxiv.org/abs/2510.24497v1)
- **PDF**: [http://arxiv.org/pdf/2510.24497v1](http://arxiv.org/pdf/2510.24497v1)

固定波束成形技术因无需估计噪声统计量且性能稳定，在实践中获得广泛应用。然而单一波束成形器难以适应变化的声学环境，其干扰抑制能力存在局限。为提升鲁棒性，学界提出自适应凸组合算法，通过线性组合多个固定波束成形器的输出实现性能优化。但该算法在高度非平稳场景（如快速移动干扰）中表现不佳，因其自适应更新难以可靠追踪急剧变化。针对此缺陷，本文提出面向多路无失真差分波束成形器的帧级在线神经融合框架，通过神经网络估计组合权重。相较于传统自适应凸组合方法，本方案能更有效适应动态声学环境，在保持无失真约束的同时实现更强的干扰抑制能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强中，固定波束形成器（如差分麦克风阵列）因结构紧凑、响应频率无关而被广泛应用，但无法适应动态声学环境，对移动干扰源抑制能力有限。  
- **既有方法问题**：自适应凸组合（ACC）方法通过线性组合多个固定波束形成器提升鲁棒性，但其依赖梯度更新的自适应机制在高度非平稳场景（如快速移动干扰）中跟踪能力不足，导致权重估计不优、性能下降。  

2)  
- **核心方法**：提出一种帧级在线神经融合框架（BeamFusion），通过神经网络动态预测多个固定波束形成器输出的最优组合权重。  
- **解决思路**：  
  - **网络结构**：基于多波束输出的实部、虚部和幅度谱特征，采用编码器-分组双路径RNN-解码器架构，交替建模帧内频点关联和帧间时序演化，保证因果性。  
  - **权重约束**：输出层通过SoftMax强制组合权重和为1，满足目标方向无失真约束。  
  - **损失函数**：以增强信号与目标信号的均方误差为损失，确保语音保真度。  
- **优势**：相比ACC的渐进式更新，神经网络能够即时学习复杂声学场景下的最优权重，显著提升对动态干扰的适应能力。  

3)  
- **任务与效果**：  
  - **移动干扰场景**：在混响时间300ms/700ms下，△SNR提升至10.24dB/12.07dB，显著优于ACC（6.56dB/7.08dB）及单波束形成器。  
  - **多干扰环境**：在双干扰源实验中，△SNR达12.16dB，STOI提升至0.68，均优于对比方法。  
  - **干扰抑制**：跨角度SIR曲线显示，BeamFusion在90°–180°范围内始终保持最高干扰抑制水平。
</div>

</details>

---

## Forward Convolutive Prediction for Frame Online Monaural Speech Dereverberation Based on Kronecker Product Decomposition
- **Authors**: Yujie Zhu, Jilu Jin, Xueqin Luo, Wenxing Yang, Zhong-Qiu Wang, Gongping Huang, Jingdong Chen, Jacob Benesty
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.24471v1](http://arxiv.org/abs/2510.24471v1)
- **PDF**: [http://arxiv.org/pdf/2510.24471v1](http://arxiv.org/pdf/2510.24471v1)

语音去混响一直是语音处理领域的关键研究课题，旨在减轻混响对语音通信与交互系统的不利影响。在前向卷积预测（FCP）这类新兴方法中，通常采用深度神经网络预测直达路径信号，再通过线性预测滤波器抑制残余混响。然而，该方法需使用过长的线性预测滤波器，导致计算复杂度显著增加。为此，本文提出基于克罗内克积（KP）分解的新型FCP方法：将长预测滤波器建模为两个短滤波器的克罗内克积，从而大幅降低计算开销。我们进一步提出自适应算法，在线迭代更新这两个短滤波器。实验表明，与传统方法相比，本方法在保持去混响性能竞争力的同时，显著降低了计算成本。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：室内语音信号常受混响干扰，影响语音清晰度与系统性能。传统方法如加权预测误差（WPE）及其自适应版本（AWPE）虽有效，但计算复杂度高，难以实际部署。  
- **既有方法问题**：  
  - 线性预测滤波器需较长阶数，导致计算负担大。  
  - 单通道WPE在噪声或干扰下性能显著下降。  
  - 结合深度学习的FCP方法虽提升性能，仍因滤波器长度问题计算成本高昂。  

2)  
- **核心方法**：提出基于克罗内克积分解的KP-FCP方法，将长预测滤波器建模为两个短滤波器的克罗内克积，显著降低参数数量与计算复杂度。  
- **解决步骤**：  
  - **两阶段架构**：  
    - 第一阶段使用因果深度神经网络在线估计直达路径信号。  
    - 第二阶段通过分解后的短滤波器迭代抑制残余混响。  
  - **自适应更新**：基于递归最小二乘算法在线更新短滤波器，确保动态环境中的稳定性。  
  - **性能保障**：分解保留滤波器低秩特性，在减少计算量的同时维持去混响效果。  

3)  
- **任务与效果**：在单通道语音去混响任务中，基于VCTK数据集测试：  
  - **性能指标**：PESQ和FWSNR显著提升，尤其在混响时间较长时表现优异。  
  - **计算效率**：复杂度从O(K²)降至O(P²K₁² + P²K₂²)，当P较小时计算量大幅减少。  
  - **结果对比**：在P≥4时，KP-FCP性能匹配或超越传统FCP，同时计算成本降低超过50%。
</div>

</details>

---

## Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers
- **Authors**: Yan Meng, Jiachun Li, Matthew Pillari, Arjun Deopujari, Liam Brennan, Hafsah Shamsie, Haojin Zhu, Yuan Tian
- **Categories**: cs.CR, cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.24393v1](http://arxiv.org/abs/2510.24393v1)
- **PDF**: [http://arxiv.org/pdf/2510.24393v1](http://arxiv.org/pdf/2510.24393v1)

作为智能家居系统的核心组件，智能音箱易受语音欺骗攻击。被动活体检测技术仅通过采集的音频（而非依赖传感器）区分真人语音与重放语音，近年来备受关注。然而该方法面临两大挑战：环境因素导致的性能衰减，以及对固定用户姿态的严苛要求。

本研究提出一种新型活体特征——阵列指纹，通过智能音箱内置的麦克风阵列来判定音频来源身份。理论分析表明：相较于现有方案，利用环形麦克风布局的阵列指纹在环境变化与用户移动场景下具有更强鲁棒性。基于此特征，我们设计出轻量级被动检测系统ARRAYID，并构建了与阵列指纹协同工作的特征组合。在包含32,780条音频样本和14种欺骗设备的测试集上，ARRAYID实现了99.84%的准确率，显著优于现有被动活体检测方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：智能音箱易受语音欺骗攻击，现有活体检测方法面临环境变化和用户姿态固定的挑战。  
- **既有方法问题**：  
  - 多因素认证需用户携带传感器或主动发射信号，增加负担。  
  - 被动方案依赖单通道音频频谱，易受传播路径变化和调制攻击影响；双通道方案需用户保持固定姿态，限制实际应用场景。  

2)  
- **核心方法**：提出基于麦克风阵列的**阵列指纹**特征，结合多通道音频差异消除环境失真，并设计轻量级系统ARRAYID。  
- **解决思路**：  
  - **理论分析**：通过声波传播模型证明阵列指纹对距离和方向变化不敏感，仅依赖音频源特性。  
  - **特征设计**：  
    - **频谱阵列指纹**：计算多通道频谱标准差，提取与音频源相关的鲁棒特征。  
    - **频谱分布特征**：分析通道间频谱分布差异，增强区分度。  
    - **LPCC特征**：保留通道特定物理属性，提升检测精度。  
  - **系统集成**：结合轻量神经网络分类器，实现低延迟实时检测。  

3)  
- **任务与效果**：  
  - **活体检测**：在包含32,780样本的自建数据集上准确率达99.84%，在第三方ReMasc数据集上达97.78%，优于现有方案。  
  - **鲁棒性验证**：在距离、方向、用户移动、噪声及不同欺骗设备下均保持高精度，并能有效防御调制攻击等高级威胁。
</div>

</details>

---

## Bayesian Speech synthesizers Can Learn from Multiple Teachers
- **Authors**: Ziyang Zhang, Yifan Gao, Xuenan Xu, Baoxiangli, Wen Wu, Chao Zhang
- **Categories**: cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.24372v1](http://arxiv.org/abs/2510.24372v1)
- **PDF**: [http://arxiv.org/pdf/2510.24372v1](http://arxiv.org/pdf/2510.24372v1)

基于编解码器的文本转语音（TTS）模型因其在语音克隆任务中的高效性与优异表现而备受关注。然而，这类模型面临两大挑战：鲁棒语音编解码器的预训练难度，以及量化误差导致的音质下降。最新研究表明，连续值生成模型可有效缓解上述问题，成为潜力巨大的替代方案。但如何有效建模多样化语音模式，并为连续值自回归TTS开发可靠采样策略，仍是待探索的方向。本文提出BELLE——一种融合语言建模与贝叶斯证据学习的连续值自回归框架，能够直接从文本输入预测梅尔频谱。该框架将每帧梅尔频谱视为从习得超分布中采样的高斯分布，从而实现对不确定性的量化建模，尤其在并行数据场景（即单一文本-音频提示对应多组语音样本）中表现突出。为构建此类数据，我们使用多个预训练TTS模型基于相同文本-音频提示合成多样化语音样本，并通过贝叶斯证据学习将其知识蒸馏至BELLE。实验结果表明，尽管BELLE仅使用约十分之一的训练数据且基于大量合成数据训练，其性能仍可与当前最优开源TTS模型相媲美。生成音频样本详见https://belletts.github.io/Belle/，代码、模型权重与合成数据将在论文录用后公开。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于编解码器的文本转语音模型因量化误差和预训练鲁棒性问题，面临语音质量下降的挑战。连续值生成模型虽能缓解量化误差，但其自回归建模中的采样策略和多样化语音模式建模仍研究不足。  
- **既有方法问题**：传统方法依赖离散编码，导致信息损失；连续值自回归模型缺乏有效的随机性引入机制，限制了生成多样性和质量。

2)  
- **核心方法**：提出BELLE框架，首次将贝叶斯证据深度学习应用于TTS，直接预测梅尔频谱图。  
  - **分层分布建模**：将每帧梅尔频谱视为高斯分布，其参数来自学习的超分布（Normal-Inverse-Gamma），通过分层采样（方差→均值→输出）引入可控不确定性。  
  - **多教师知识蒸馏**：使用多个预训练TTS模型生成同一文本的多样化语音样本，通过加权损失融合真实与合成数据，解决单一样本训练的不确定性估计问题。  
  - **训练优化**：结合回归损失、证据损失、频谱通量损失和停止预测损失，提升生成自然度和时序连贯性。  
- **创新点**：贝叶斯采样替代传统高斯采样，多教师数据增强提升分布估计鲁棒性，支持流式生成低延迟应用。

3)  
- **任务与效果**：在零样本TTS任务（LibriSpeech数据集）中评估：  
  - **语音质量**：MOS达4.21，接近真实语音（4.20），优于基线MELLE（4.02）；说话人相似度（SMOS）最高（4.13）。  
  - **智能性**：词错误率（WER-C/H）最低（1.63%/2.13%），显着提升可懂度。  
  - **流式生成**：BELLE-stream在仅5k小时数据上实现低延迟（RTF=0.55），性能优于同类流式模型。  
  - **多样性**：通过调整参数β可控生成多样性，优于确定性模型（如F5-TTS）。
</div>

</details>

---

## Sound Source Localization for Spatial Mapping of Surgical Actions in Dynamic Scenes
- **Authors**: Jonas Hein, Lazaros Vlachopoulos, Maurits Geert Laurent Olthof, Bastian Sigrist, Philipp Fürnstahl, Matthias Seibold
- **Categories**: cs.SD, cs.CV, eess.AS, eess.IV
- **arXiv**: [http://arxiv.org/abs/2510.24332v1](http://arxiv.org/abs/2510.24332v1)
- **PDF**: [http://arxiv.org/pdf/2510.24332v1](http://arxiv.org/pdf/2510.24332v1)

目的：手术场景理解是提升计算机辅助与智能手术系统的关键。现有方法主要依赖视觉数据或端到端学习，限制了细粒度上下文建模。本研究通过融合三维声学信息，构建具备时空感知能力的多模态手术场景理解体系。

方法：提出一种新颖框架，通过将相控麦克风阵列的声源定位信息映射至RGB-D相机生成的动态点云，构建手术场景的四维视听表征。基于Transformer的声学事件检测模块识别包含工具-组织交互的关键时段，并在视听场景表征中实现空间定位。该系统由专家在模拟手术过程中通过真实手术室环境进行实验验证。

结果：本方法成功实现了手术声学事件的三维空间定位及其与视觉场景元素的关联。实验评估表明，该系统具备精确的空间声源定位能力与多模态数据鲁棒融合特性，可生成动态完整的手术活动表征。

结论：本研究首创动态手术场景中的空间声学定位方法，标志着多模态手术场景表征取得重要突破。通过声视觉数据的深度融合，该框架不仅增强了场景上下文理解能力，更为未来智能自主手术系统奠定了技术基础。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：手术场景理解依赖视觉数据或端端学习，但存在以下问题：  
  - 视觉方法难以建模细粒度上下文，如工具与组织的交互细节；  
  - 端端学习易受非因果关联和数据集偏差影响，缺乏可解释性；  
  - 现有数字手术表征多忽略声音等模态，限制了多模态场景建模能力。  

2)  
- **核心方法**：提出融合声学与视觉数据的4D动态场景表征框架，具体包括：  
  - **多模态表征生成**：通过相位麦克风阵列获取声源定位信息，与RGB-D相机动态点云融合，生成带声学热图的4D场景表示；  
  - **声学事件检测**：基于Transformer架构分析音频谱图，识别手术工具交互（如凿、钻、锯）的时序片段，触发空间定位；  
  - **事件空间定位**：通过加权聚类（DBSCAN）在点云中定位高振幅声源区域，生成3D边界框，关联视觉场景元素；  
  - **系统集成**：设备经标定与同步，确保时空数据对齐，支持离线生成动态音频-视觉数字孪生。  

3)  
- **任务与效果**：  
  - **声学事件检测**：在凿、锯动作中检测准确（F1分数达0.948），钻孔因声音连续性表现稍弱；  
  - **空间定位**：77%的事件在3D交并比≥0.1时成功定位，凿击动作召回率最高（0.914）；  
  - **多模态融合**：实现了声源与视觉元素的动态关联，为手术场景图、工具跟踪等任务提供基础。
</div>

</details>

---

## TsetlinKWS: A 65nm 16.58uW, 0.63mm2 State-Driven Convolutional Tsetlin Machine-Based Accelerator For Keyword Spotting
- **Authors**: Baizhou Lin, Yuetong Fang, Renjing Xu, Rishad Shafik, Jagmohan Chauhan
- **Categories**: cs.SD, cs.AR, eess.AS, B.7; C.3; I.2
- **arXiv**: [http://arxiv.org/abs/2510.24282v1](http://arxiv.org/abs/2510.24282v1)
- **PDF**: [http://arxiv.org/pdf/2510.24282v1](http://arxiv.org/pdf/2510.24282v1)

作为神经网络的低功耗替代方案，崔斯林机因其简洁可解释的推理机制近年备受关注，但在语音任务中的性能仍有局限。本研究提出首个面向12关键词检测任务的卷积崔斯林机算法-硬件协同设计框架TsetlinKWS。首先，通过创新性地结合梅尔频谱系数与频谱通量特征提取方案及频谱卷积技术，使CTM在12关键词检测任务中首次达到87.35%的竞争性准确率。其次，开发了优化分组块压缩稀疏行算法，实现模型规模9.84倍的压缩，显著提升存储效率。最后，设计专用于CTM的状态驱动架构，通过同时利用数据复用与稀疏性实现高能效。该完整系统基于65纳米工艺验证，在0.7V电压下功耗仅16.58微瓦，核心面积0.63平方毫米。单次推理仅需90.7万次逻辑运算，较最先进关键词检测加速器降低10倍运算量，确立了CTM在超低功耗语音应用中的高效候选地位。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：Tsetlin机器（TM）作为低功耗神经网络替代方案，在语音任务中性能受限，现有方法如Vanilla TM在复杂关键词识别任务中准确率不足，且硬件加速器未充分利用TM的稀疏性优势。  
- **既有问题**：传统TM依赖布尔特征匹配，泛化能力弱；现有加速器架构（如数据驱动和事件驱动）无法同时实现高效数据重用和稀疏计算，导致能效低下。  

2)  
- **特征提取改进**：提出MFSC-SF算法，结合梅尔频谱系数和频谱通量，增强特征判别性，并通过动态二值化提升鲁棒性，使CTM在12关键词任务中准确率达87.35%。  
- **模型压缩优化**：设计OG-BCSR算法，将模型大小压缩9.84倍，通过分块和分组策略降低存储开销，并基于图论匹配优化压缩效率。  
- **硬件架构创新**：采用状态驱动架构，通过优先级编码和并行处理单元（PE）实现稀疏数据的高效计算与数据重用，支持同步解压缩和负载均衡调度，功耗仅16.58µW。  

3)  
- **任务与效果**：在Google语音命令数据集（12关键词）上，TsetlinKWS达到87.35%准确率；硬件在65nm工艺下实现0.63mm²核心面积，每推理仅需907k逻辑操作，功耗和计算量较现有加速器降低10倍，适用于超低功耗边缘语音应用。
</div>

</details>

---

## HergNet: a Fast Neural Surrogate Model for Sound Field Predictions via Superposition of Plane Waves
- **Authors**: Matteo Calafà, Yuanxin Xia, Cheol-Ho Jeong
- **Categories**: cs.SD, cs.CE, cs.LG, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.24279v1](http://arxiv.org/abs/2510.24279v1)
- **PDF**: [http://arxiv.org/pdf/2510.24279v1](http://arxiv.org/pdf/2510.24279v1)

本文提出一种新型神经网络架构，用于高效预测二维与三维空间中的声场。该网络通过结构设计自动满足亥姆霍兹方程，确保输出结果符合物理规律。该方法能有效求解声学、光学和电磁学等领域中各类波动现象的边值问题。数值实验表明，在室内声学仿真领域，尤其在中高频段范围内，本方法具有超越现有先进技术的潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：声场模拟对声学、光学等领域至关重要，传统数值方法（如有限元法、边界元法）计算成本高，尤其在高频场景下效率不足。  
- **既有方法问题**：几何声学方法仅适用于高频与大空间；物理信息神经网络等方法在直接数值计算中效果有限，且需大量内部点采样，计算负担重。  

2.  
- **核心方法**：提出HergNet神经网络架构，基于Herglotz波函数理论，通过平面波叠加自动满足亥姆霍兹方程，确保解的物理有效性。  
- **关键设计**：  
  - 使用可训练参数与复数前馈网络学习Herglotz密度函数，将积分近似为离散求和。  
  - 引入相位参数提升收敛速度，网络仅需边界条件训练，无需内部点评估。  
  - 支持非齐次问题，通过叠加点源基本解处理非零声源。  
- **优势**：  
  - 损失函数仅依赖边界误差，显著降低计算成本。  
  - 参数规模随频率平方缩放，优于传统体积方法（如有限元法需数百万单元）。  
  - 网络结构简单（如两层复数隐藏层），结合Adam优化器实现快速收敛。  

3.  
- **任务与效果**：在三维房间声场预测任务中，对比解析解：  
  - **中高频段（如6000 Hz）**：声压实部、虚部误差低于10%，相位完全匹配，声压级误差小于1 dB JND阈值。  
  - **脉冲响应**：通过逆傅里叶变换还原，相对误差低于10%，感知上与解析解一致。  
  - **计算效率**：训练时间短（如124秒），但内存消耗随频率增长；显著优于传统有限元法与物理信息神经网络。
</div>

</details>

---

## Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation
- **Authors**: Kang Zhang, Trung X. Pham, Suyeon Lee, Axi Niu, Arda Senocak, Joon Son Chung
- **Categories**: cs.SD, cs.AI, cs.MM, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.24103v1](http://arxiv.org/abs/2510.24103v1)
- **PDF**: [http://arxiv.org/pdf/2510.24103v1](http://arxiv.org/pdf/2510.24103v1)

本文提出MGAudio——一种基于流模型的新型开放域视频生成音频框架，其核心创新在于引入模型引导的双角色对齐机制。与传统基于分类器或无分类器引导的方法不同，MGAudio通过专为视频条件音频生成设计的训练目标，使生成模型具备自我引导能力。该框架包含三大核心组件：（1）可扩展的基于流的Transformer模型；（2）双角色对齐机制，令视听编码器同时承担条件输入模块与特征对齐器的双重职责以提升生成质量；（3）增强跨模态连贯性与音频真实性的模型引导目标。在VGGSound数据集上，MGAudio取得突破性进展，将FAD指标降至0.40，显著超越最佳无分类器引导基线，并在FD、IS及对齐指标上全面优于现有方法。在极具挑战性的UnAV-100基准测试中亦展现出色泛化能力。这些成果验证了模型引导的双角色对齐机制为条件式视频生成音频任务提供了强大且可扩展的新范式。代码已开源：https://github.com/pantheon5100/mgaudio

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：视频到音频生成任务在视频制作和沉浸式体验中至关重要，但现有方法在语义对齐和时间同步方面存在挑战。  
- **既有方法问题**：  
  - 早期方法（如GAN或自回归Transformer）生成质量低、同步性差。  
  - 近期扩散模型依赖分类器自由引导，但多任务训练可能稀释模型能力，导致推理时采样行为不匹配。  
  - 部分方法参数量大（数十亿），计算效率低。  

2)  
- **核心方法**：MGAudio提出模型引导的双角色对齐框架，包含三个关键组件：  
  - **可扩展流匹配Transformer**：基于流匹配学习从噪声到音频的连续传输，提升去噪稳定性和效率。  
  - **双角色对齐机制**：  
    - 视频编码器同时作为条件模块和特征对齐器，通过对比音频-视觉预训练编码器提取全局视频特征。  
    - 音频编码器提供中间特征对齐损失，增强生成语义一致性。  
  - **模型引导目标**：  
    - 用自蒸馏目标替代分类器自由引导，直接优化条件与无条件预测的差异，提升训练效率和跨模态对齐。  
    - 结合对齐损失和流匹配损失，平衡生成质量与特征一致性。  
- **优势**：  
  - 减少对大规模参数的依赖，提升数据利用效率。  
  - 在训练中引入显式对齐监督，避免多任务冲突。  

3)  
- **任务与效果**：  
  - **VGGSound基准**：FAD降至0.40，在FD、IS和对齐准确率上均超越现有方法（如MMAudio、FRIEREN）。  
  - **UnAV-100基准**：无需微调即实现最优泛化性能（FAD=0.54），保持高对齐准确率（97.54%）。  
  - **低资源场景**：仅用10%训练数据时，性能仍优于全数据基线，凸显数据效率。
</div>

</details>

---

## Listening without Looking: Modality Bias in Audio-Visual Captioning
- **Authors**: Yuchi Ishikawa, Toranosuke Manabe, Tatsuya Komatsu, Yoshimitsu Aoki
- **Categories**: eess.AS, cs.CV, eess.IV
- **arXiv**: [http://arxiv.org/abs/2510.24024v1](http://arxiv.org/abs/2510.24024v1)
- **PDF**: [http://arxiv.org/pdf/2510.24024v1](http://arxiv.org/pdf/2510.24024v1)

视听描述技术旨在通过联合建模声音与视觉信息生成完整的场景描述。尽管现有方法通过复杂的模态融合机制提升了性能，但当前模型在多大程度上能实现双模态互补、以及当某一模态质量下降时模型是否保持稳健，这些问题尚未明确。本研究通过对前沿视听描述模型LAVCap进行系统性模态鲁棒性测试，通过选择性抑制或干扰音频/视觉流以量化敏感度与互补性，发现该模型存在明显的音频模态偏好。为评估模型对双模态的均衡利用程度，我们在AudioCaps数据集基础上新增同步描述音频与视觉流的文本标注，构建AudioVisualCaps数据集。实验不仅报告了LAVCap在AudioVisualCaps上的基线结果，还通过模态鲁棒性测试表明：相较于基于AudioCaps训练的模型，使用AudioVisualCaps训练的LAVCap展现出更弱的模态偏好。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频-视觉描述任务旨在结合声音与视觉信息生成场景描述，但现有模型对多模态融合的互补性与鲁棒性缺乏系统评估。  
- **既有方法问题**：当前模型（如LAVCap）在常用数据集（如AudioCaps）上训练时，存在明显的**听觉模态偏向**，即模型过度依赖音频信息，视觉信息利用不足，导致模态使用不均衡。

---

2)  
- **核心方法**：论文通过构建**AudioVisualCaps数据集**并改进训练策略，以缓解模态偏向问题。  
- **数据集构建**：  
  - 基于AudioCaps，通过半自动流程生成同时描述音频与视觉内容的标注：  
    - 使用图像描述模型（BLIP-2）生成视觉描述；  
    - 结合原始音频描述，利用大语言模型（如GPT）融合生成多模态描述；  
    - 对测试集进行人工验证，训练集则直接使用分模态描述作为独立参考。  
- **训练策略优化**：  
  - 在训练中采用**分模态标注拼接**（如用“and”连接音频与视觉描述），迫使模型同时学习两类信息；  
  - 通过**模态鲁棒性测试**（如替换/干扰单一模态输入）评估模型对双模态的均衡利用能力。  
- **解决效果**：  
  - 模型在AudioVisualCaps上训练后，对视觉退化的敏感性显著提升，表现出更对称的模态依赖，减少了听觉偏向。

---

3)  
- **任务与效果**：  
  - 在**音频-视觉描述任务**中，模型在AudioVisualCaps测试集上取得显著提升（如CIDEr从0.204升至0.665）；  
  - **模态鲁棒性测试**表明：训练后模型对音频或视觉干扰均表现对称的性能下降，证明双模态利用更均衡；  
  - 定性结果显示生成描述能同时涵盖听觉与视觉细节，改善了原有模型的描述完整性。
</div>

</details>

---

## emg2speech: synthesizing speech from electromyography using self-supervised speech models
- **Authors**: Harshavardhana T. Gowda, Lee M. Miller
- **Categories**: cs.SD, cs.CL, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.23969v1](http://arxiv.org/abs/2510.23969v1)
- **PDF**: [http://arxiv.org/pdf/2510.23969v1](http://arxiv.org/pdf/2510.23969v1)

本研究提出一种神经肌肉语音接口，可将发音时面部肌肉采集的肌电信号直接转换为音频。研究发现：自监督语音表征与肌肉动作电位电功率存在强线性关联（$r=0.85$），其特征可通过线性映射精准对应肌电功率；不同发音姿态对应的肌电功率向量在特征空间中形成结构化可分离簇。这种“自监督特征→线性映射→肌电功率→姿态聚类→发音运动”的关联揭示出自监督模型隐式编码了发音机制。基于此特性，我们直接将肌电信号映射至自监督特征空间并合成语音，实现了无需显式发音模型与声码器训练的端到端肌电-语音生成系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经肌肉接口（如脑机接口）可帮助失语患者恢复言语功能，但现有侵入式方法存在手术风险、成本高和适用性有限的问题。  
- **既有方法问题**：  
  - 依赖已知的输入信号（如神经活动）与音频的严格对齐，而实际场景中EMG与语音对齐关系未知。  
  - 使用小词汇库（如1,024词）和受限语速（45–78词/分钟），远低于正常对话水平。  
  - 模型架构不透明、数据集未公开，导致可复现性和公平比较困难。  

2)  
- **核心方法**：利用自监督语音模型（如HuBERT）隐式编码发音机制的特性，构建轻量级EMG到语音的转换模型。  
- **解决思路**：  
  - **特征映射**：发现自监督语音特征（H）与EMG功率（D(E)）存在强线性关系（r=0.85），证明H隐含发音信息。  
  - **结构化表示**：通过EMG协方差矩阵（E）及其对角线D(E)提取发音动作的判别性特征，形成可分簇群。  
  - **端到端生成**：  
    - 使用时间深度可分离卷积网络，将EMG特征（vec(E)、D(E)等）映射到HuBERT离散单元。  
    - 采用CTC损失解决未知对齐问题，避免显式发音模型和声码器训练。  
    - 最终通过预训练Tacotron声码器合成音频。  

3)  
- **任务与效果**：  
  - **离散单元解码**：在预测HuBERT离散单元任务中，vec(E)输入的单位错误率最低（58.7%）。  
  - **音素解码**：音素错误率显著降低，vec(E)输入达41.42%，优于其他EMG特征。  
  - **数据贡献**：构建大规模EMG语音数据集（6,800词、9小时数据），支持自然语速（115词/分钟）下的零样本/少样本学习。
</div>

</details>

---
