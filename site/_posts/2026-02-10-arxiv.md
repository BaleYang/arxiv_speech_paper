---
layout: post
title: "arXiv Daily – 2026-02-10"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-10（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-09 08:50 — 2026-02-10 08:50
- 抓取总数：12 篇 | 本页显示：12 篇（去重/过滤后）

## Beyond Transcripts: A Renewed Perspective on Audio Chaptering
- **Authors**: Fabian Retkowski, Maike Züfle, Thai Binh Nguyen, Jan Niehues, Alexander Waibel
- **Categories**: cs.SD, cs.CL
- **arXiv**: [https://arxiv.org/abs/2602.08979v1](https://arxiv.org/abs/2602.08979v1)
- **PDF**: [https://arxiv.org/pdf/2602.08979v1](https://arxiv.org/pdf/2602.08979v1)

音频章节划分旨在将长音频自动分割为连贯的段落，对于播客、讲座和视频的导航日益重要。尽管其应用价值显著，相关研究仍较为有限且主要基于文本，导致在利用音频信息、处理自动语音识别（ASR）错误以及无需转录文本的评估等关键问题上尚未得到充分探索。本文通过以下三方面贡献填补这些空白：（1）系统比较了基于文本的模型（结合声学特征）、一种基于学习音频表征的新型纯音频架构（AudioSeg）以及多模态大语言模型；（2）实证分析了影响性能的因素，包括转录文本质量、声学特征、时长和说话人构成；（3）提出了形式化的评估方案，对比了依赖转录文本的文本空间方案与不依赖转录文本的时间空间方案。在YTSeg数据集上的实验表明：AudioSeg显著优于基于文本的方法；停顿信息带来的声学增益最大；多模态大语言模型仍受限于上下文长度和指令跟随能力，但在较短音频上表现出潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
研究背景与既有方法的问题：
- **背景**：随着播客、讲座等长音频内容日益普及，自动将其分割为连贯章节（音频章节化）的需求增长，以提升导航与信息定位效率。
- **既有方法的问题**：
  - **过度依赖文本**：现有研究主要基于转录文本进行分割，忽视了音频信号本身的潜在价值。
  - **评估协议存在缺陷**：传统基于文本的评估方法假设存在固定、准确的转录稿。在实际中，系统依赖自动语音识别（ASR）输出，其错误和句子分割差异会导致评估结果不可比，且可能因转录稿粒度不同而产生误导。
  - **边界对齐失真**：章节边界本质是连续时间点，但现有流程常将其“对齐”到句子边界，这种有损对齐会偏移真实时间位置并引入评估偏差。

2)  
论文核心方法如何解决上述问题：
本文通过三个核心贡献系统性地解决了上述问题：

- **系统性地比较了三种建模范式**：
  - **基于文本的模型**：以MiniSeg为基线，探索了在纯文本模型基础上**融合手工设计的声学特征**（如停顿、语速、音高、响度、说话人信息）的效果。
  - **纯音频模型（AudioSeg）**：提出了一种新颖的、**不依赖转录稿的架构**。它直接对学习到的音频表征（如使用Whisper、HuBERT等预训练编码器提取）进行操作，通过帧编码、片段编码和文档编码三阶段流程预测章节边界。
  - **多模态大语言模型（MLLM）**：探索了如Qwen-Omni等MLLM在零样本、上下文学习（ICL）及微调（LoRA）设置下执行该任务的能力。

- **实证分析影响性能的因素**：
  - 分析了文本模型对**ASR错误的鲁棒性**，发现联合在参考转录稿和ASR转录稿上训练能提升鲁棒性。
  - 量化了**不同声学特征的贡献**，发现**停顿时长**带来的增益最大，而说话人特征主要在多人讲话内容中有效。
  - 考察了**音频时长和说话人构成**的影响，发现纯音频模型在短音频上优势明显，而所有模型在长音频和多人讲话场景下性能均下降。

- **系统化并提出了新的评估协议**：
  - **形式化了基于文本空间的评估协议**（如R1, H1-H3），并分析了其依赖转录稿、存在粒度差异等局限性。
  - **引入了基于时间空间的评估协议**（T1, T2），特别是**离散时间评估（T1）**，它将音频离散化为固定时长块（如6秒），在此网格上计算分割指标。这使得评估**不依赖于任何特定转录稿**，实现了文本、音频及多模态模型之间的公平、可比较的评估。

3)  
在哪些任务上取得了怎样的效果：
- **任务**：在YT-Seg数据集上的音频章节化（分割）任务。
- **效果**：
  - **纯音频模型（AudioSeg）** 表现最佳，特别是使用Whisper Large作为音频编码器时，其F1分数（45.52）**显著超越了所有基于文本的模型**，证明了不依赖转录稿进行章节化的可行性与优越性。
  - **基于文本的模型融合声学特征**后，性能得到提升，其中**停顿特征贡献最大**。
  - **多模态大语言模型（如Qwen3-Omni）** 在短音频（<30分钟）上通过上下文学习取得了有竞争力的结果（F1=41.30），但其性能受上下文长度限制。
  - 新提出的**基于时间的评估协议（T1）** 被验证可用于公平比较不同范式的模型，并揭示了传统文本协议固有的信息损失。
</div>

</details>

---

## No Word Left Behind: Mitigating Prefix Bias in Open-Vocabulary Keyword Spotting
- **Authors**: Yi Liu, Chuan-Che, Huang, Xiao Quan
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.08930v1](https://arxiv.org/abs/2602.08930v1)
- **PDF**: [https://arxiv.org/pdf/2602.08930v1](https://arxiv.org/pdf/2602.08930v1)

开放词汇关键词检测（OV-KWS）技术允许用户通过任意语音指令实现个性化设备控制。近期研究探索了音频-文本联合嵌入方法，使用户能够通过文本注册短语，并提出了区分相似语音的技术。我们发现，现有OV-KWS方案往往对注册短语的起始音素存在过度偏倚，导致当负样本注册-查询对共享前缀时（如“调高音量”与“调低音量”）产生误触发。经分析，这一现象源于两个因素：训练数据偏倚和位置偏倚的跨模态评分机制。为应对这些局限，我们提出了包含POB-Spark和POB-LibriPhrase（POB-LP）两个数据集的局部重叠基准（POB），其中包含具有共享前缀的不匹配音频-文本对，并设计了轻量级决策层——等权位置评分（EPS）方法。仅使用EPS即可将POB-Spark的等错误率从64.4%降至29.3%，并将POB-LP准确率从87.6%提升至96.8%，同时在LibriPhrase和Google语音指令（GSC）数据集上保持原有性能。通过在训练中加入POB数据，我们的方法在POB基准上取得了最优结果，且在基线模型中对于原有指标的负面影响最小。这种负面影响在仅包含单词指令的GSC数据集中最为显著。如何缓解这种性能权衡将成为未来研究的重点。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：开放词汇关键词检测（OV-KWS）允许用户通过任意语音命令控制设备。现有方法利用音频-文本联合嵌入，通过文本注册短语并进行消歧。
- **既有问题**：现有方案存在**前缀偏差**，即模型过度关注注册短语的起始音素。当负样本对（如“turn the volume up”与“turn the volume down”）共享前缀时，易导致误触发。这源于：
    - **训练数据偏差**：常用数据集（如LibriPhrase、Google Speech Commands）中长短语及共享前缀的样本不足。
    - **评分偏差**：现有评分层倾向于赋予序列早期位置更高权重，加剧了前缀混淆。

2)  
论文通过引入**新评测基准**和**轻量级评分模块**共同解决上述问题。

- **提出部分重叠基准（POB）**：
    - 包含两个数据集：**POB-LP**（基于LibriPhrase构建，通过追加常见词模拟前缀重叠）和**POB-Spark**（使用先进TTS模型合成，可控地生成多种重叠模式与说话人特征）。
    - POB专门设计用于评估模型在处理**共享前缀的长短语对**时的性能，弥补了现有数据集中此类案例的不足。

- **提出等权重位置评分（EPS）模块**：
    - **设计**：替换原有评分层。EPS对对齐模块输出的每个位置特征应用**相同的线性变换权重**，然后进行平均池化，得到最终匹配分数。
    - **作用机制**：
        - **消除位置依赖偏差**：传统方法（如全连接层或GRU）会学习位置依赖的权重，导致早期音素权重过高。EPS强制所有位置贡献均等，从而缓解前缀偏差。
        - **轻量且通用**：EPS不增加额外参数，仅修改最终评分层，易于集成到现有OV-KWS架构（如SLiCK）中，保持计算效率。
    - **效果**：EPS使模型能够更均衡地考虑整个序列的匹配信息，而不仅仅是前缀部分，从而更好地区分仅在尾部不同的短语对。

3)  
论文在多个任务上验证了所提方法的有效性：
- **部分重叠任务（POB基准）**：在仅使用LibriPhrase训练时，将EPS集成到SLiCK模型（SLiCK-EPS）后，在POB-Spark数据集上的等错误率（EER）从64.4%大幅降至29.3%，在POB-LP上的准确率从87.6%提升至96.8%。
- **原有基准任务**：在LibriPhrase（含easy/hard子集）和Google Speech Commands（GSC）数据集上，SLiCK-EPS保持了与基线模型相当甚至略优的性能。
- **综合训练**：当在训练中加入POB数据后，SLiCK-EPS取得了POB基准上的最佳结果（如POB-Spark EER降至16.2%），同时在原有基准上的性能下降幅度在基线中最小。
</div>

</details>

---

## MOVA: Towards Scalable and Synchronized Video-Audio Generation
- **Authors**: SII-OpenMOSS Team, :, Donghua Yu, Mingshu Chen, Qi Chen, Qi Luo, Qianyi Wu, Qinyuan Cheng, Ruixiao Li, Tianyi Liang, Wenbo Zhang, Wenming Tu, Xiangyu Peng, Yang Gao, Yanru Huo, Ying Zhu, Yinze Luo, Yiyang Zhang, Yuerong Song, Zhe Xu, Zhiyu Zhang, Chenchen Yang, Cheng Chang, Chushu Zhou, Hanfu Chen, Hongnan Ma, Jiaxi Li, Jingqi Tong, Junxi Liu, Ke Chen, Shimin Li, Songlin Wang, Wei Jiang, Zhaoye Fei, Zhiyuan Ning, Chunguo Li, Chenhui Li, Ziwei He, Zengfeng Huang, Xie Chen, Xipeng Qiu
- **Categories**: cs.CV, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.08794v1](https://arxiv.org/abs/2602.08794v1)
- **PDF**: [https://arxiv.org/pdf/2602.08794v1](https://arxiv.org/pdf/2602.08794v1)

音频对于真实世界视频至关重要，然而生成模型在很大程度上忽视了音频成分。当前生成视听内容的方法通常依赖于级联流水线，这会增加成本、累积误差并降低整体质量。尽管Veo 3和Sora 2等系统强调了同步生成的价值，但联合多模态建模在架构、数据和训练方面带来了独特挑战。此外，现有系统的闭源特性限制了该领域的发展。本研究提出MOVA（MOSS视频与音频），这是一个能够生成高质量同步视听内容的开源模型，包括逼真的唇语同步语音、环境感知音效以及内容匹配的音乐。MOVA采用混合专家架构，总参数量达320亿，其中推理时激活参数为180亿。该模型支持图像-文本到视频-音频的生成任务。通过开源模型权重与代码，我们旨在推动研究发展并培育活跃的创作者社区。发布的代码库全面支持高效推理、LoRA微调和提示增强功能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：视频生成模型已能合成高保真视频，但普遍忽视音频成分。音频对于真实视频至关重要。
- **既有方法问题**：
  - **级联流水线**：主流方法先单独生成视频或音频，再合成。这增加了成本、累积误差，并损害整体质量。
  - **闭源限制**：如Veo 3、Sora 2等能同步生成音视频的先进系统均为闭源，阻碍了该领域的研究进展。
  - **联合建模挑战**：同时生成音视频面临架构、数据和训练方面的独特挑战，现有开源模型在规模和质量上均显不足。

2) **论文核心方法如何解决上述问题**
论文提出了MOVA模型，通过以下核心设计解决上述问题：
- **非对称双塔架构**：
  - 利用预训练模型：视频塔采用Wan2.2 I2V A14B模型，音频塔采用1.3B文本到音频扩散模型。这继承了强大的单模态生成先验。
  - 轻量级桥接模块：通过一个2.6B参数的双向桥接模块连接双塔，实现音视频隐状态间的双向交叉注意力，促进模态间信息交互，实现同步生成。
- **对齐的RoPE**：为解决音视频潜在表示时间粒度不同导致的错位问题，修改了旋转位置编码，将视频和音频令牌映射到同一时间尺度，确保跨模态交互时物理时间对齐。
- **精细化的数据工程**：
  - 构建了三阶段数据流水线，对原始视频进行预处理、质量评估（音频质量、视频质量、音视频对齐）和精细化标注。
  - 使用多模态大模型（如Qwen3-Omni、MiMo-VL）分别生成音频和视频描述，再用LLM（GPT-OSS）合并为统一的、语义丰富的多模态标注。
  - 最终构建了大规模、高质量、对齐良好的音视频训练数据集。
- **渐进式联合训练策略**：
  - 分阶段训练：先预训练音频塔，再进行包含视频塔、音频塔和桥接模块的联合训练。
  - 联合训练又分为三个阶段，逐步提升数据质量和输出分辨率（360p -> 720p），并调整噪声调度和文本丢弃率以优化对齐和音质。
  - 采用异构学习率（桥接模块学习率更高）以加速跨模态对齐学习，同时保持预训练塔的稳定性。
  - 引入双Sigma偏移，允许视频和音频采用独立且可调的噪声调度，以适应不同模态的降噪需求。
- **双重无分类器引导**：在推理时，设计了一种双重CFG机制，可以独立调节文本引导强度和跨模态（桥接）引导强度，从而在生成质量和对齐精度之间进行灵活权衡。

3) **在哪些任务上取得了怎样的效果**
MOVA在**同步音视频生成**任务上取得了显著效果，具体包括：
- **多语种唇语同步语音**：在Verse-Bench等基准测试中，其唇同步误差（LSE-D/C）指标优于LTX-2、Ovi等基线模型，能生成中英文的高质量唇语同步视频。
- **环境感知音效与内容对齐音乐**：模型能生成与视觉事件精确对齐的物理音效。在跨模态语义对齐（ImageBind分数）和时序同步（DeSync分数）指标上表现优异。
- **多说话人场景**：在构建的评测集上，其说话人身份与语音内容匹配的准确率（cpCER）优于对比模型。
- **主观评估**：在Arena式的人类偏好评估中，MOVA的ELO评分显著高于所有基线模型，在视频质量、音频保真度和同步性等方面更受人类青睐。
- **开源贡献**：模型完全开源，支持从图像/文本生成音视频，并提供了高效的推理、LoRA微调和提示词增强工具。
</div>

</details>

---

## Prototype-Based Disentanglement for Controllable Dysarthric Speech Synthesis
- **Authors**: Haoshen Wang, Xueli Zhong, Bingbing Lin, Jia Huang, Xingduo Pan, Shengxiang Liang, Nizhuan Wang, Wai Ting Siok
- **Categories**: cs.SD, cs.CL
- **arXiv**: [https://arxiv.org/abs/2602.08696v1](https://arxiv.org/abs/2602.08696v1)
- **PDF**: [https://arxiv.org/pdf/2602.08696v1](https://arxiv.org/pdf/2602.08696v1)

构音障碍语音具有高度变异性和有限标注数据的特点，这对自动语音识别（ASR）及辅助语音技术均构成重大挑战。现有方法多依赖于合成数据增强或语音重建，但常将说话人身份与病理发音特征相互纠缠，限制了可控性与鲁棒性。

本文提出ProtoDisent-TTS，一种基于原型解耦的语音合成框架。该框架以预训练的文本转语音模型为骨干，在统一隐空间中分解说话人音色与构音障碍发音特征。通过病理原型码本提供健康与障碍语音模式的可解释、可控表征，并借助梯度反转层的双分类器目标，强制说话人嵌入对病理属性保持不变性。在TORGO数据集上的实验表明，该设计能够实现健康语音与构音障碍语音之间的双向转换，从而带来一致的ASR性能提升，并实现鲁棒且兼顾说话人特性的语音重建。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：构音障碍语音存在高变异性且标注数据稀缺，给自动语音识别（ASR）和辅助语音技术带来挑战。  
- **既有方法问题**：现有方法（如合成数据增强或语音重建）通常将**说话人音色**与**病理发音特征**纠缠在一起，导致：
  - 合成语音的多样性和可控性受限；
  - 语音重建时可能出现说话人身份漂移，影响鲁棒性。

2)  
论文提出 **ProtoDisent-TTS** 框架，基于预训练的 Index-TTS 骨干网络，通过以下设计解决上述问题：  
- **原型码本与解耦表示**：
  - 构建一个可学习的病理原型码本，其中包含健康语音和多种构音障碍语音的原型嵌入，提供可解释、可控的发音模式表示。
  - 说话人编码器从提示音频中提取音色嵌入，与选定的病理原型嵌入通过元素加法结合，形成联合表征用于语音合成。  
- **双分类器与梯度反转层**：
  - **病理分类器**作用于联合表征，鼓励病理属性主要由原型嵌入捕获；
  - **对抗分类器**直接作用于说话人嵌入，并配备梯度反转层，迫使说话人编码器去除病理相关线索，确保音色嵌入对病理属性保持不变。  
- **训练策略**：
  - 引入基于语音转换的跨条件训练，构建音色转换的语音对，进一步分离说话人身份与病理特征；
  - 总损失函数结合 TTS 损失、病理分类损失和对抗分类损失，共同优化解耦效果。  
该设计在统一潜空间中实现了**说话人音色**与**病理发音**的显式分离，从而支持双向可控转换。

3)  
在 **TORGO** 数据集上进行了三项任务评估：  
- **ASR 数据增强**：
  - 使用合成构音障碍语音训练 Whisper 模型，词错误率显著降低（例如严重障碍组 WER 从 0.4161 降至 0.2488）；
  - 在多数严重程度分组上优于现有基准（如 TTDS）。  
- **语音重建**：
  - 将构音障碍语音重建为健康语音时，说话人相似性（余弦相似度）显著高于基线（如 CMHR 0.1671 vs. 本方法 0.3438），有效保持身份信息。  
- **可控合成**：
  - 通过选择不同病理原型，可灵活生成多样化的构音障碍语音，支持按需数据增强与重建。
</div>

</details>

---

## Input-Adaptive Spectral Feature Compression by Sequence Modeling for Source Separation
- **Authors**: Kohei Saijo, Yoshiaki Bando
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.08671v1](https://arxiv.org/abs/2602.08671v1)
- **PDF**: [https://arxiv.org/pdf/2602.08671v1](https://arxiv.org/pdf/2602.08671v1)

时频域双路径模型在源分离任务中展现出优异性能并得到广泛应用。由于计算成本随频点数量增长，这类模型在处理高采样率任务（如音乐源分离和电影音频源分离）时常采用频带分割模块。该模块的编码器通过为预定义子带分别编码特征来实现频率信息压缩，其引入的归纳偏置更侧重低频部分，从而达成有效压缩。然而频带分割模块存在两个固有局限：（1）缺乏输入自适应性，无法利用输入相关信息；（2）参数量较大，每个子带需独立模块处理。为解决这些问题，我们提出谱特征压缩方法。该方法采用单一序列建模模块压缩输入，兼具输入自适应性与参数高效性。我们研究了基于交叉注意力与基于Mamba的两种变体，并引入受频带分割模块启发的归纳偏置以适配频率信息压缩需求。在音乐源分离和电影音频源分离任务上的实验表明，谱特征压缩模块在不同分离器规模与压缩比下均持续优于频带分割模块。分析结果进一步证实该方法能自适应地捕捉输入信号的频率模式。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：时频域双路径模型在音频源分离中性能优越，但计算成本随频率点数增加而升高。在高采样率任务（如音乐源分离和电影音频源分离）中，常使用频带分割模块来压缩频率信息。  
- **既有方法问题**：  
  - **非输入自适应**：频带分割模块基于预定义子带处理，无法利用输入相关的频率模式。  
  - **参数量大**：每个子带需专用编码器/解码器，导致总参数过多。

2)  
- **核心方法**：提出谱特征压缩，通过单一序列建模模块压缩输入，使其兼具输入自适应性和参数效率。  
- **解决思路**：  
  - **输入自适应**：使用可学习查询的序列建模（如交叉注意力或Mamba），动态捕获输入依赖的频率结构。  
  - **参数高效**：单一模块处理所有频带，大幅减少参数。  
  - **引入归纳偏置**：  
    - 在交叉注意力变体中，设计位置偏置，使查询优先关注对应频带内的频率点。  
    - 在Mamba变体中，通过特定策略插入查询（如频带中间或边界），模拟频带分割的局部性。  
  - **保持性能**：通过上述偏置设计，确保频率信息压缩的有效性，避免直接替换导致的性能下降。

3)  
- **任务与效果**：  
  - **音乐源分离**：在MUSDB18-HQ数据集上，SFC（尤其交叉注意力变体）在分离人声、贝斯、鼓等音轨时，平均cSDR和uSDR均优于频带分割模块，且参数量更少。  
  - **电影音频源分离**：在Divide-and-remaster数据集上，SFC在分离语音、音乐、音效时，SNR和SISDR指标全面超越基线，验证了其泛化能力。  
  - **高压缩比场景**：即使在较少频带数下，SFC性能下降更小，显示其压缩效率优势。
</div>

</details>

---

## VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling
- **Authors**: Ziyang Cheng, Yuhao Wang, Heyang Liu, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.08607v1](https://arxiv.org/abs/2602.08607v1)
- **PDF**: [https://arxiv.org/pdf/2602.08607v1](https://arxiv.org/pdf/2602.08607v1)

近期，语音大语言模型在端到端语音交互中展现出卓越能力。然而，主流自回归范式存在严格的序列约束，限制了生成效率并引入曝光偏差。本文探索将掩码扩散建模作为语音大语言模型的非自回归范式，并提出VocalNet-MDM。为适配流式语音交互需求，我们解决了两个关键挑战：训练-推理失配问题与迭代计算开销。通过分层块掩码策略，使训练目标与块扩散解码过程中的渐进掩码状态对齐；采用迭代自蒸馏技术，将多步优化压缩至更少步骤以实现低延迟推理。在仅6千小时语音数据的小规模训练下，VocalNet-MDM相比自回归基线实现3.7-10倍解码加速，首块延迟降低34%。该模型在保持竞争力的识别准确率同时，实现了最优的文本质量与语音自然度，证明掩码扩散建模是构建低延迟高效语音大语言模型的可扩展替代方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前语音大语言模型主要采用自回归范式，存在序列依赖导致的生成效率低、曝光偏差等问题。  
- **既有方法问题**：  
  - 自回归方法延迟随序列长度线性增长，限制实时交互。  
  - 现有非自回归方法（如条件独立预测）难以建模长程依赖和全局一致性，且常需额外对齐组件或控制信号，增加训练复杂性。  

2)  
论文提出 **VocalNet-MDM**，基于掩码扩散建模实现非自回归语音生成，通过两项核心机制解决训练与推理挑战：  

- **分层分块掩码策略**：  
  - 问题：标准全局伯努利掩码在训练时对所有位置均匀掩码，与流式生成中“历史块完全可见、当前块逐步去掩码”的推理状态不匹配。  
  - 解决：训练时分层采样——先按比例选择部分块，再在每个选中块内按比例掩码部分词元。这使得训练覆盖了块扩散解码中从全掩码到全可见的中间状态，对齐训练与推理分布。  

- **迭代自蒸馏**：  
  - 问题：扩散解码需多步迭代去掩码以保证质量，但累积延迟高。现有加速方法（如KV缓存）无法降低首块延迟，蒸馏方法则需复杂多模型协调。  
  - 解决：  
    - 使用固定教师模型进行多步块细化，收集后续去掩码步中更确定的预测分布作为目标。  
    - 学生模型通过单次前向匹配这些目标，将多步细化压缩到少步推理。  
    - 采用块内并行更新策略，避免全局排序开销，实现低延迟生成。  

- **整体架构**：沿用Thinker-Talker范式，Thinker自回归生成文本和语义隐状态，Talker通过MDM并行预测掩码语音词元，支持流式分块生成。  

3)  
在**英语语音交互任务**上评估，仅使用6K小时数据训练，相比自回归基线：  
- **效率**：解码速度提升3.7–10倍，首块延迟降低34%，达到最佳吞吐量（TPS）与实时因子（RTF）。  
- **质量**：  
  - 文本质量在AlpacaEval等基准上取得最优或竞争性得分。  
  - 语音自然度（UTMOS）达到最优水平，词错误率（WER）保持竞争性。  
- **灵活性**：通过调整扩散步数（如默认4步）平衡效率与质量，为低延迟语音LLM提供了新范式。
</div>

</details>

---

## Global Rotation Equivariant Phase Modeling for Speech Enhancement with Deep Magnitude-Phase Interaction
- **Authors**: Chengzhong Wang, Andong Li, Dingding Yao, Junfeng Li
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.08556v1](https://arxiv.org/abs/2602.08556v1)
- **PDF**: [https://arxiv.org/pdf/2602.08556v1](https://arxiv.org/pdf/2602.08556v1)

尽管深度学习推动了语音增强技术的发展，但有效的相位建模仍具挑战性，因为传统网络通常在平坦的欧几里得特征空间中操作，难以刻画相位内在的循环拓扑结构。为此，我们提出了一种流形感知的幅度-相位双流框架，通过强制全局旋转等变性特征，使相位流与其内在的循环几何结构对齐。具体而言，我们引入了幅度-相位交互卷积模块，用于基于模长的信息交换，以及混合注意力双前馈网络瓶颈层，用于统一特征融合，二者均设计为在相位流中保持全局旋转等变性。我们在相位恢复、去噪、去混响和带宽扩展任务上进行了全面评估，验证了所提方法相对于多种先进基线的优越性。值得注意的是，该架构在相位恢复任务中将相位距离降低了超过20%，在零样本跨语料去噪评估中将PESQ提升了0.1以上。在涉及混合失真的通用语音增强任务中也确立了整体优势。定性分析进一步表明，学习到的相位特征呈现出清晰的周期性模式，与相位内在的循环特性一致。源代码已公开于 https://github.com/wangchengzhong/RENet。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强中，相位建模至关重要，尤其在去混响和带宽扩展等任务中。传统深度网络在欧几里得特征空间中操作，难以有效建模相位固有的循环拓扑结构（即相位角在0到2π间周期性变化）。
- **既有方法问题**：现有解耦架构（如MP-SENet）虽分离了幅度和相位处理，但在网络内部仍将相位视为通用数据分布，仅通过损失函数约束其结构。这导致网络与相位的几何本质不匹配，可能学习到无关的绝对相位方向，浪费模型容量并限制泛化能力。

2)  
论文提出了一种**流形感知的幅度-相位双流框架**，通过强制**全局旋转等变性**来使相位流与其内在循环几何对齐。核心方法包括：
- **全局旋转等变性设计**：将相位空间视为旋转对称的流形，确保网络对全局相位旋转（即所有频率施加恒定相移）具有等变性。这意味着输入相位整体旋转时，网络输出的相位也同步旋转，从而迫使网络专注于学习相对相位结构，而非绝对方向。
- **幅度-相位深度交互方案**：在保持双流拓扑差异的同时，设计两个新模块实现严格的信息交换：
    - **幅度-相位交互卷积模块**：通过**基于模长的门控机制**进行跨流交互。相位到幅度的门控信号来自相位特征的模长（旋转不变），幅度到相位的门控直接来自幅度特征。这确保了交互不破坏相位流的旋转等变性。
    - **混合注意力双FFN瓶颈**：在**注意力得分域**进行统一融合。将复数相位查询/键分解后与幅度特征拼接，计算共享的注意力图（该操作对全局旋转保持不变），再用此图分别调制幅度和相位的值向量。后续的FFN则针对各流特性设计（幅度流用GRU捕获序列依赖，相位流用复数卷积保持局部几何）。
- **整体架构**：编码器/解码器使用MPICM进行局部特征提取与交互；瓶颈层堆叠HADF模块以建模长程上下文依赖。整个相位流严格保持全局旋转等变性。

3)  
论文在多个任务上验证了方法的有效性：
- **相位恢复**：在VoiceBank语料上，**相位距离降低超过20%**，证明了其精确建模相位内在几何的能力。
- **语音去噪**：在VoiceBank+DEMAND和DNS-2020基准上取得竞争性结果；在**零样本跨语料库评估**中，PESQ提升超过0.1，显示了优异的泛化能力。
- **通用语音增强**：在包含去噪、去混响、带宽扩展及其混合失真的任务上（使用WSJ0+WHAMR!测试），方法在多项指标（如PESQ、STOI、SI-SDR）上超越先进基线，尤其在去混响和复合失真场景中**相位精度提升显著**。
</div>

</details>

---

## Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets
- **Authors**: Fredrik Cumlin
- **Categories**: cs.LG, eess.AS, stat.ML
- **arXiv**: [https://arxiv.org/abs/2602.08552v1](https://arxiv.org/abs/2602.08552v1)
- **PDF**: [https://arxiv.org/pdf/2602.08552v1](https://arxiv.org/pdf/2602.08552v1)

主观评分包含固有噪声，这会限制模型与人类评价之间的相关性，但该可靠性问题鲜少被量化。本文提出ρ-Perfect方法，用于实际估计主观评分数据集上模型可达到的最高相关性。我们将ρ-Perfect定义为完美预测器与人类评分之间的相关性，并基于异方差噪声场景（常见于主观评分数据集）推导其估计值。研究表明，ρ-Perfect的平方可估计重测相关性，并利用此特性验证估计结果。通过在语音质量数据集上的应用，展示了ρ-Perfect如何有效区分模型局限性问题与数据质量问题。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在语音质量评估、图像美学等需要模拟主观评价的领域，开发客观模型时，一个核心挑战是确定模型与人类评分之间可能达到的最高相关性。主观评分本身存在固有噪声，这导致模型-人类相关性的理论上限低于1.0。
- **既有方法的问题**：现有可靠性度量方法（如皮尔逊相关比η²、组内相关系数ICC、克朗巴哈α系数）存在局限：
  - 它们通常假设同方差噪声（即所有项目的评分噪声方差相等），这与主观数据中常见的异方差噪声（不同项目间评分者分歧程度不同）现实不符。
  - 它们难以直接解释为模型性能的上限，或未在模型性能评估的背景下被广泛讨论。

2)  
论文提出了 **ρ-Perfect** 度量，旨在为不平衡的主观评分数据集提供一个实用的模型-人类相关性上限估计。其核心方法及解决思路如下：

- **定义完美预测器**：首先从理论上定义一个“完美预测器”，即给定项目时，能够预测其平均评分条件期望的最优回归函数。ρ-Perfect 被定义为这个完美预测器与观测到的项目平均评分之间的皮尔逊相关系数。

- **关键推导与估计**：
  - 利用**全方差定律**，将观测评分的总方差分解为两部分：完美预测器方差（信号）和给定项目时平均评分的条件期望方差（噪声）。
  - 核心公式为：ρ-Perfect = √[ Var(Ŷ) / Var(Y) ]。其中，Var(Y) 是观测平均评分的方差，可直接计算。
  - Var(Ŷ) 的估计是关键创新。通过从总方差 Var(Y) 中减去估计的噪声方差 E[Var(Y|X)] 得到。噪声方差针对每个项目单独计算，公式为项目内评分样本方差除以其评分次数 mi。这**自然地处理了异方差噪声**，因为每个项目可以根据其自身的评分者分歧和评分次数贡献不同的噪声估计。

- **理论验证与实用性**：
  - 论文证明了 ρ-Perfect 的平方近似等于对同一项目集进行两次独立但相似的主观评估之间的相关性（即重测信度）。这为 ρ-Perfect 作为相关性上限的合理性提供了理论依据和间接验证途径。
  - 计算仅需单次评估的数据（每个项目的评分分布），复杂度低，易于实现。
  - 通过在实际数据集（BVCC, MovieLens等）上的实验，验证了 ρ-Perfect² 与模拟的重测相关性高度吻合，且优于或不同于现有可靠性度量（如ICC(2,k)在评分次数不平衡时失效，子采样信度则倾向于高估）。

- **核心解决能力**：通过直接建模并量化主观评分中的异方差噪声，ρ-Perfect 提供了一个**数据依赖的、可解释的相关性上限**。这使得研究者能够区分模型性能不佳是由于**模型本身的缺陷**，还是由于**数据（人类评分）本身的可靠性低**所致。

3)  
- **验证任务**：在多个公开的主观评分数据集上验证了 ρ-Perfect 作为相关性上限估计的有效性，包括：
  - **语音质量**：BVCC、SOMOS 数据集。
  - **推荐系统**：MovieLens 数据集。
  - **音乐情感**：MERP 数据集（唤醒度维度）。
- **取得的效果**：ρ-Perfect 的平方值与通过分割评分者/评分模拟得到的“重测相关性”高度匹配，证实了其作为可靠性度量及相关性上限估计的准确性。
- **应用案例**：在 **NISQA 语音质量评估数据集**上，将 DNSMOS Pro 模型的性能与 ρ-Perfect 计算的上限对比。成功揭示出：
  - 在整体数据上，模型性能（PCC=0.873）接近上限（0.954），表明模型强且数据可靠。
  - 在“突发性失真”子集上，模型性能很低（0.392），但上限也仅为中等（0.701），这表明性能差是**模型缺陷和数据可靠性共同**导致的问题，为改进指明了方向。
</div>

</details>

---

## Physics-Guided Variational Model for Unsupervised Sound Source Tracking
- **Authors**: Luan Vinícius Fiorio, Ivana Nikoloska, Bruno Defraene, Alex Young, Johan David, Ronald M. Aarts
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.08484v1](https://arxiv.org/abs/2602.08484v1)
- **PDF**: [https://arxiv.org/pdf/2602.08484v1](https://arxiv.org/pdf/2602.08484v1)

声源追踪通常采用经典阵列处理算法实现。基于机器学习等替代方法则依赖于真实位置标签，其获取成本较高。本文提出一种变分模型，可在物理驱动解码器的辅助下，在潜空间实现单声源无监督追踪。实验表明，所提方法超越传统基线模型，在性能与计算复杂度方面均达到与先进监督模型相当的水平。该方法对麦克风阵列构型改变及麦克风位置元数据损坏表现出显著鲁棒性。最后，本文进一步将方法扩展至多声源追踪场景，并提出了基础理论框架的改进方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声源追踪传统依赖阵列信号处理方法（如SRP-PHAT、MUSIC），这些方法通常需要网格搜索、精确阵列校准，且对初始化敏感。  
- **既有方法问题**：  
  - 监督学习方法（如Cross3D、Neural-SRP）依赖大量带标签数据，标注成本高，且难以在设备端进行训练或微调。  
  - 现有无监督方法（如基于自编码器的方法）计算成本高、参数多，难以在硬件受限的音频设备上部署，且设计选择较为随意。

2)  
- **核心方法**：提出一种物理引导的变分模型，结合变分自编码器（VAE）与物理解码器，实现无监督声源追踪。  
- **解决思路**：  
  - **变分编码器**：输入为GCC-PHAT特征和麦克风阵列几何信息，输出服从von Mises-Fisher分布的潜变量（表示声源方向）。  
  - **物理解码器**：仅用于训练，根据潜变量预测麦克风对之间的时间延迟，并通过高斯分布建模延迟似然，无需可训练参数。  
  - **损失函数**：基于证据下界（ELBO）改进，包含物理损失（输入与预测延迟分布的KL散度）和KL正则项（约束潜变量分布）。  
- **关键优势**：  
  - 无需标注数据，通过物理模型注入空间信息，实现无监督方向估计。  
  - 变分框架能建模不确定性，提升对噪声和阵列几何变化的鲁棒性。  
  - 编码器采用权重共享的并行架构，支持可变麦克风数量，计算效率高。

3)  
- **任务与效果**：在单声源追踪任务中，使用真实数据集（LOCATA）评估：  
  - 在加性白噪声和方向性噪声实验中，性能优于传统方法（SRP-PHAT），接近监督学习方法（Cross3D、Neural-SRP）。  
  - 在阵列几何不确定的实验中，性能优于Cross3D，与Neural-SRP相当，且对麦克风位置元数据损坏更具鲁棒性。  
  - 模型参数量（0.89M）和计算复杂度与Neural-SRP相似，适合实际部署。
</div>

</details>

---

## Cross-Modal Bottleneck Fusion For Noise Robust Audio-Visual Speech Recognition
- **Authors**: Seaone Ok, Min Jun Choi, Eungbeom Kim, Seungu Han, Kyogu Lee
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.08293v1](https://arxiv.org/abs/2602.08293v1)
- **PDF**: [https://arxiv.org/pdf/2602.08293v1](https://arxiv.org/pdf/2602.08293v1)

视听语音识别（AVSR）通过结合声学与视觉线索，提升噪声环境下的语音识别性能。其核心问题在于如何设计融合机制，使得模型在音频信号受损时能有效利用视觉信息，同时在纯净语音上保持优异表现。本文提出CoBRA（面向鲁棒AVSR的跨模态瓶颈），一种基于瓶颈的融合框架，通过引入少量可学习的令牌作为跨模态信息交换的中介。这些令牌通过调节信息流，使音频流即使在恶劣或域外噪声条件下也能可靠获取关键视觉线索。尽管训练数据有限，我们的模型通过噪声自适应融合机制，在性能上超越了同类基线模型，并与大规模系统保持竞争力，展现出高效性与鲁棒性。消融实验表明，融合深度是最关键的因素，这为设计鲁棒的AVSR系统提供了重要启示。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：传统语音识别在噪声环境下性能严重下降。视听语音识别通过融合音频和视觉线索来提升鲁棒性，但现有方法存在两个主要问题：
  - **融合机制欠佳**：跨模态交互常依赖于简单的拼接或交叉注意力，未能最优地利用互补信息。
  - **计算开销大**：现有融合方法（如交叉注意力）通常引入高计算成本，限制了效率。

2)  
论文提出 **CoBRA** 框架，通过瓶颈令牌实现高效、鲁棒的跨模态融合，具体解决方式如下：

- **核心机制**：
  - 在基于Conformer的双流编码器之上，引入一组**可学习的瓶颈令牌**作为跨模态交互的媒介。
  - 音频和视觉流不直接交互，而是通过这组紧凑的令牌进行信息交换，从而**约束信息流**，避免冗余并降低计算复杂度。

- **融合策略**：
  - 探索了两种更新策略：**顺序融合**（音频和视觉依次与瓶颈交互）和**平均融合**（各模态独立更新瓶颈后取平均）。
  - 通过**控制融合层深度**（如中层融合），在特征充分提取后引入跨模态交互，平衡了模态特异性和互补性。

- **优势体现**：
  - **噪声鲁棒性**：瓶颈机制使模型在音频受损时仍能可靠地访问关键视觉线索，适应训练集内外的噪声。
  - **计算高效**：由于瓶颈令牌数量远少于特征序列长度，注意力计算复杂度显著低于传统全序列交叉注意力。
  - **数据高效**：在相对有限的训练数据下，通过噪声自适应融合实现强大性能。

3)  
- **任务与数据集**：在LRS2和LRS3标准视听语音识别数据集上进行评估。
- **效果**：
  - **干净语音**：在LRS3和LRS2上分别达到1.6%和2.8%的词错误率，与使用更庞大训练数据的大规模系统性能相当。
  - **噪声环境**：在多种噪声类型和信噪比下均优于基线，尤其在低信噪比时提升显著（例如在-7.5dB babble噪声下相对改进达40%）。
  - **跨噪声泛化**：对训练集外的噪声（如粉噪、白噪）也表现出稳定的鲁棒性。
</div>

</details>

---

## PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition
- **Authors**: Xun Su, Huamin Wang, Qi Zhang
- **Categories**: cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.08240v1](https://arxiv.org/abs/2602.08240v1)
- **PDF**: [https://arxiv.org/pdf/2602.08240v1](https://arxiv.org/pdf/2602.08240v1)

语音情感识别（SER）广泛应用于人机交互领域，但传统模型的高计算成本阻碍了其在资源受限的边缘设备上的部署。脉冲神经网络（SNNs）凭借其事件驱动的特性，提供了一种高能效的替代方案；然而，其与连续自监督学习（SSL）表征的融合面临分布失配的根本性挑战，即高动态范围的嵌入会降低基于阈值的神经元的信息编码能力。为解决这一问题，我们提出了提示调优脉冲神经网络（PTS-SNN），一种参数高效的神经形态适应框架，用于将冻结的SSL主干网络与脉冲动态特性对齐。具体而言，我们引入了一种时序移位脉冲编码器，通过无需参数的通道移位捕捉局部时序依赖关系，从而建立稳定的特征基础。为弥合领域差距，我们设计了一种上下文感知膜电位校准策略。该机制利用脉冲稀疏线性注意力模块，将全局语义上下文聚合为可学习的软提示，动态调节参数化泄漏积分发放（PLIF）神经元的偏置电压。这种调节能有效将异构输入分布集中在响应发放范围内，缓解功能静默或饱和问题。在五个多语言数据集（如IEMOCAP、CASIA、EMODB）上的大量实验表明，PTS-SNN在IEMOCAP上达到了73.34%的准确率，与具有竞争力的人工神经网络（ANNs）性能相当，同时仅需119万个可训练参数，每样本推理能耗为0.35毫焦。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音情感识别（SER）是人机交互的关键技术，但传统人工神经网络（ANN）模型计算成本高，难以部署在资源受限的边缘设备上。脉冲神经网络（SNN）因其事件驱动特性，能效更高，是潜在替代方案。  
- **既有问题**：将SNN与连续的自监督学习（SSL）表征结合时，存在**分布不匹配**问题。SSL特征通常具有高动态范围，而SNN的阈值神经元对输入强度敏感，导致神经元易陷入**功能静默**或**发放饱和**，损害信息编码能力。

2)  
论文提出**PTS-SNN**框架，通过参数高效的神经形态适配解决上述问题。其核心方法包括两个关键模块：  
- **时序移位脉冲编码器**：  
  - 采用**无参数的通道移位操作**，在残差脉冲块中沿时间轴移动部分特征通道。  
  - 捕获局部时序依赖，平滑特征流形的高频波动，为后续处理提供稳定的输入基础。  
- **上下文感知膜电位校准策略**：  
  - 设计**脉冲稀疏线性注意力模块**，将全局语义上下文聚合到可学习的软提示中。  
  - 软提示通过**稳态偏置生成器**转换为动态偏置电压，注入到参数化泄漏积分发放（PLIF）神经元。  
  - 动态偏置主动调节神经元的基线膜电位，使异质输入分布**对齐到神经元的敏感响应范围**，避免静默或饱和，从而弥合连续特征与离散脉冲动力学之间的域间隙。  
整个框架保持预训练SSL主干冻结，仅优化轻量适配器，实现了高效的特征对齐与校准。

3)  
- **任务与效果**：在五个多语言SER数据集上评估，包括IEMOCAP、CASIA、EMODB等。  
- **性能**：在IEMOCAP上达到73.34%的加权准确率，与先进的ANN方法相当，同时在CASIA、EMODB等数据集上均优于基线。  
- **效率**：仅需**119万可训练参数**，每样本推理能耗低至**0.35 mJ**，显著优于传统ANN模型，验证了其在边缘设备上高效部署的潜力。
</div>

</details>

---

## Tutti: Expressive Multi-Singer Synthesis via Structure-Level Timbre Control and Vocal Texture Modeling
- **Authors**: Jiatao Chen, Xing Tang, Xiaoyue Duan, Yutang Feng, Jinchao Zhang, Jie Zhou
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.08233v1](https://arxiv.org/abs/2602.08233v1)
- **PDF**: [https://arxiv.org/pdf/2602.08233v1](https://arxiv.org/pdf/2602.08233v1)

现有歌唱合成系统虽能实现高保真的独唱表现，但受限于全局音色控制，难以在同一歌曲中处理动态的多歌手编排与人声音质细节。为此，我们提出Tutti——一个面向结构化多歌手生成的统一框架。具体而言，我们设计了结构感知的歌手提示模块，使其能够依据音乐结构灵活调度歌手音色演变；同时提出基于条件引导变分自编码器的互补音质学习方法，以捕捉与显式控制形成互补的隐式声学纹理特征（如空间混响与频谱融合）。实验表明，Tutti在多歌手精确调度方面表现优异，显著提升了合唱生成的声学真实感，为复杂多歌手编排提供了新的范式。音频示例请访问：https://annoauth123-ctrl.github.io/Tutii_Demo/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
现有歌唱合成系统虽能实现高保真独唱，但存在两大局限：
- **歌手编排**：多数模型遵循“单曲单歌手”范式，将音色作为全局固定条件，无法根据歌曲结构（如主歌、副歌）动态调度多位歌手，无法处理独唱与合唱间的复杂转换。
- **人声质感**：模型主要针对独唱设计，缺乏对多人合唱中声学交互（如空间混响、频谱融合、谐波叠加）的显式建模，导致生成的合唱段听起来像人工混合的独立音轨，缺乏整体凝聚力。

2)  
论文提出 **Tutti** 框架，基于潜在扩散Transformer构建，通过两个核心模块解决上述问题：

- **结构感知歌手提示**：
    - **提示构建**：利用音乐结构分析工具将歌曲分段，通过声纹识别区分不同歌手，并依据结构标签（如主歌、副歌）为每个段落分配歌手身份。主歌段通常分配单一歌手以增强音色稳定性，副歌段在识别出多位歌手时则标记为合唱段。
    - **自适应融合器**：采用基于自注意力的融合模块，将当前段落的多位歌手嵌入动态加权融合为一个统一的条件表示。这使模型能自适应地处理主唱与和声之间的主次关系，实现精确的歌手调度与过渡。

- **基于条件引导VAE的互补质感学习**：
    - **训练机制**：训练一个专门的人声VAE。在训练时，向VAE编码器产生的潜在表示添加高斯噪声进行扰动，同时向解码器显式提供歌词、歌手提示、基频等条件。这迫使编码器丢弃这些已由显式条件提供的信息（如音高、内容、音色），转而专注于提取**互补的声学质感**（如混响效果、空间氛围、高频谐波）。
    - **推理控制**：在生成时，可选择性提供参考音频，通过该VAE编码器提取质感潜在表示。由于质感与内容/音色已解耦，此表示主要增强生成音频的声学真实感和丰富度，而音乐结构和歌手身份则由DiT主干和显式条件主导。

3)  
Tutti在以下任务中取得显著效果：
- **多歌手合成**：在包含乐队合唱、男女对唱等场景的测试集上，其**多歌手MOS**得分达4.02（5分制），表明能清晰区分并融合不同歌手音色。
- **生成质量**：在客观指标上，**词错误率**（13.50%）优于基线Vevo2（16.80%），**说话人相似度**（0.691）更高。主观**音频质量MOS**达4.12，自然度良好。
- **声学重建**：其专用人声VAE在波形重建任务的所有指标（如STOI、WB-PESQ）上均优于现有先进音频压缩模型，为高质量生成奠定基础。
</div>

</details>

---
