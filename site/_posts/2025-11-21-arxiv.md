---
layout: post
title: "arXiv Daily – 2025-11-21"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-11-21（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-11-20 08:50 — 2025-11-21 08:50
- 抓取总数：5 篇 | 本页显示：5 篇（去重/过滤后）

## Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech Codecs
- **Authors**: Wei-Cheng Tseng, David Harwath
- **Categories**: eess.AS, cs.CL
- **arXiv**: [https://arxiv.org/abs/2511.16639v1](https://arxiv.org/abs/2511.16639v1)
- **PDF**: [https://arxiv.org/pdf/2511.16639v1](https://arxiv.org/pdf/2511.16639v1)

近年来，神经音频编解码器的进展不仅实现了卓越的音频压缩性能，还推动了语音合成技术的提升。研究者正探索其作为通用声学特征提取器在更广泛语音处理任务中的应用潜力。基于这一趋势，我们提出Codec2Vec——首个完全基于离散音频编解码单元的语音表征学习框架。该方法具备多重优势：提升数据存储与传输效率、加速训练过程、增强数据隐私保护。通过结合多种训练目标衍生策略的掩码预测任务，我们系统评估了该框架的有效性。在SUPERB基准测试中，Codec2Vec在保持与连续输入模型相当性能的同时，将存储需求降低至16.5倍，训练速度提升2.3倍，充分展现了其可扩展性与高效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自监督学习（SSL）已成为语音处理的主流方法，但传统模型依赖原始波形或高维连续特征（如梅尔频谱图），导致计算和存储成本高昂。  
- **既有问题**：  
  - 预训练阶段需处理大量未标注音频，计算开销巨大。  
  - 连续输入特征占用存储空间多，传输效率低，限制了大规模应用的可行性。  
  - 现有基于神经音频编解码器离散单元的研究多局限于监督任务，缺乏通用自监督框架，且离散单元本身缺乏上下文建模能力。  

2)  
- **核心方法**：Codec2Vec提出首个完全基于离散音频编解码单元的自监督语音表示学习框架。其核心包括：  
  - **输入处理**：使用预训练的神经音频编解码器（如DAC）将音频压缩为离散单元序列，替代传统连续输入。  
  - **掩码预测任务**：采用Transformer编码器对部分掩码的离散单元序列进行上下文建模，通过预测被掩码单元学习表示。  
  - **训练目标策略**：探索多种目标生成方式：  
    - **重构目标**：直接预测原始离散单元。  
    - **迭代聚类**：基于模型中间表示进行k-means聚类，迭代优化目标。  
    - **在线聚类**：通过“教师-学生”框架动态更新聚类目标，提升表示质量。  
  - **效率优化**：预计算离散单元大幅减少存储和I/O开销，并省去在线特征提取模块，加速训练。  

3)  
- **任务与效果**：在SUPERB基准测试中，Codec2Vec在多项任务上达到与连续输入模型相当的性能：  
  - **内容任务**：音素识别（PR）错误率5.2%，自动语音识别（ASR）词错误率6.9%。  
  - **语义任务**：关键词检测（KS）准确率96.7%，意图分类（IC）准确率98.0%。  
  - **说话人任务**：说话人日志（SD）错误率5.4%，说话人验证（SV）错误率5.1%。  
  - **副语言任务**：情感识别（ER）准确率65.4%。  
- **效率提升**：存储需求降低16.5倍，训练时间加速2.3倍，凸显其在大规模场景下的实用性。
</div>

</details>

---

## Difficulty-Controlled Simplification of Piano Scores with Synthetic Data for Inclusive Music Education
- **Authors**: Pedro Ramoneda, Emilia Parada-Cabaleiro, Dasaem Jeong, Xavier Serra
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.16228v1](https://arxiv.org/abs/2511.16228v1)
- **PDF**: [https://arxiv.org/pdf/2511.16228v1](https://arxiv.org/pdf/2511.16228v1)

尽管人工智能在音乐教育领域潜力巨大，但专有系统的限制阻碍了该领域技术的民主化进程。其中，基于AI的乐曲难度调控技术尤具前景——通过简化复杂曲目，能使不同年龄和背景的学习者更平等地获得音乐教育机会。然而现有研究多依赖私有数据集，导致学界难以复现、比较或拓展当前最优成果。此外，尽管生成方法潜力显著，但多数采用MIDI格式。与MusicXML等格式相比，MIDI缺乏可读性与版式信息，限制了其在人工演奏中的实际应用。

本文提出基于Transformer的MusicXML钢琴谱难度调控方法。区别于依赖标注数据的现有方案，我们构建了按预估难度排序的合成数据集，其中每对数据包含同一曲目的高难度与简易编曲版本。通过基于相同旋律与和声生成变体，并利用预训练模型评估难度与风格以确保配对合理性。定性定量实验表明，该方法能精确控制可演奏性与目标难度。与既往研究不同，我们公开所有资源（代码、数据集与模型），在确保可复现性的同时推动开源创新，助力弥合数字鸿沟。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：AI在音乐教育中的应用潜力受限于专有系统，阻碍技术民主化。现有方法依赖私有数据集，导致研究难以复现和扩展。  
- **既有问题**：  
  - 生成方法多基于MIDI格式，缺乏可读性和乐谱布局信息，限制实际教学应用。  
  - 数据标注依赖专家，成本高昂，且版权问题限制了数据集的公开与共享。  

2)  
- **核心方法**：提出基于Transformer的模型，通过合成数据实现钢琴谱难度控制简化，具体包括：  
  - **合成数据集构建**：基于固定旋律与和声，为每首原曲生成128个变体，利用预训练模型评估难度与风格相似性，筛选出高一致性且难度不同的配对数据（PianoPairs）。  
  - **输入表示优化**：采用Linearized MusicXML（LMX）编码，保留乐谱可读性并适配序列生成任务。  
  - **条件生成机制**：通过旋律轮廓与和声轮廓作为条件输入，引导模型生成结构一致、难度可控的简化乐谱。  
  - **难度适配训练**：使用配对数据训练模型，以序列到序列任务形式学习从高难度到低难度的“翻译”，并引入特殊标记控制简化方向。  

3)  
- **任务与效果**：  
  - **乐谱简化**：在包含多流派（流行、古典等）的测试集上，模型成功生成简化版本，过滤策略下74.4%的变体难度降低（差距≥2级）。  
  - **风格保持**：生成乐谱与原始作品风格一致性高（CLaMP余弦距离降低）。  
  - **主观评估**：专家评测显示简化版本在可读性、可演奏性及教学适用性上表现优异，支持个性化音乐教育。
</div>

</details>

---

## SUNAC: Source-aware Unified Neural Audio Codec
- **Authors**: Ryo Aihara, Yoshiki Masuyama, Francesco Paissan, François G. Germain, Gordon Wichern, Jonathan Le Roux
- **Categories**: eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2511.16126v1](https://arxiv.org/abs/2511.16126v1)
- **PDF**: [https://arxiv.org/pdf/2511.16126v1](https://arxiv.org/pdf/2511.16126v1)

神经音频编解码器（NAC）可为下游应用（尤其是大语言模型）提供紧凑的音频表征。然而现有NAC大多以混合方式编码多源信号，导致需要处理特定源信号的应用（如单一类型声音分析、指定说话人转录等）时效率受限。为此，我们提出一种源感知编解码器，通过源类型提示直接从混合信号中编码各独立源。该方法支持用户驱动选择待编码源（包括同类型多源信号，如多路语音），实验表明：相较于传统“源分离+NAC”级联方案，本模型以更低计算成本实现了具有竞争力的重合成与分离质量。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经音频编解码器（NAC）广泛用于音频信号压缩，但传统方法对混合音频中的多源信号进行纠缠编码，导致下游任务（如单源分析或转录）效率低下。  
- **既有问题**：现有方法如SDCodec虽能分离不同类型源（如语音、音乐），但无法处理同一类型多源（如多人语音），且编码过程计算冗余，限制应用灵活性。  

2)  
- **核心方法**：SUNAC提出端到端源感知统一神经音频编解码器，通过提示机制在潜在空间直接分离多源信号，无需显式分离步骤。  
  - **条件特征提取**：利用可学习提示（如<Speech>）对编码后的时频特征进行调制，通过FiLM和Transformer模块实现源特定特征提取。  
  - **共享量化与解码**：所有源共享同一残差向量量化器和解码器，减少参数冗余；采用排列不变训练解决同类型多源排列模糊性问题。  
  - **效率优化**：集成设计避免级联系统（如分离器+NAC）的重复计算，显著降低计算成本。  

3)  
- **任务与效果**：  
  - **多源分离**：在语音、音乐、音效混合任务中，SUNAC在SI-SDR和ViSQOL指标上与传统方法（如SDCodec）相当，且能分离同类型多源（如双人语音）。  
  - **计算效率**：参数量为69.2M，计算成本（3.5 GMACs）低于级联系统，在保持重建质量的同时提升效率。
</div>

</details>

---

## SceneGuard: Training-Time Voice Protection with Scene-Consistent Audible Background Noise
- **Authors**: Rui Sang, Yuxuan Liu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.16114v1](https://arxiv.org/abs/2511.16114v1)
- **PDF**: [https://arxiv.org/pdf/2511.16114v1](https://arxiv.org/pdf/2511.16114v1)

语音克隆技术仅需少量音频样本即可实现未经授权的语音合成，对个人隐私构成严重威胁。现有基于不可感知对抗扰动的防御方法易受去噪、压缩等常见音频预处理技术的影响。本文提出SceneGuard，一种训练时段语音保护方法，通过在语音录制中添加与场景一致的可听背景噪声进行防护。与不可感知扰动不同，该方法利用自然声学场景（如机场、街道、公园）生成符合上下文环境的保护性噪声，既保持语境合理性又具备对抗防御措施的鲁棒性。在文本转语音训练攻击测试中，SceneGuard使说话人相似度降低5.5%（统计显著性极高：p < 10^{-15}，Cohen's d = 2.18），同时保持98.6%的语音可懂度（STOI = 0.986）。鲁棒性评估表明，在MP3压缩、谱减法、低通滤波及降采样等五种常见对抗措施下，该方法均保持或增强保护效果。研究结果证实，可听且场景一致的噪声为训练时段语音保护提供了比不可感知扰动更可靠的解决方案。源代码已开源：https://github.com/richael-sang/SceneGuard。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- 语音克隆技术快速发展，仅需少量音频即可合成目标说话人语音，带来隐私泄露风险。
- 现有防御方法主要基于不可感知的对抗扰动，但存在明显缺陷：
  - 对常见音频预处理（如降噪、压缩）敏感，易被消除。
  - 依赖代理指标（如Lp范数）约束不可感知性，无法保证鲁棒性。

2) **论文核心方法如何解决上述问题**
SceneGuard通过场景一致的可听背景噪声实现训练时语音保护，其核心方法包括：
- **场景一致性噪声设计**：
  - 利用声学场景分类识别输入语音环境（如机场、街道），从对应场景噪声库中采样背景噪声。
  - 通过梯度优化联合学习时域掩码m(t)和噪声强度γ，在信噪比约束下最小化说话人相似性。
- **优化框架**：
  - 目标函数结合说话人相似性损失（基于ECAPA-TDNN编码器）、掩码平滑正则化及能量约束。
  - 参数化映射确保信噪比自动满足[10,20] dB范围，平衡保护强度与语音可用性。
- **鲁棒性机制**：
  - 场景一致性噪声与语音共享频谱特性，难以被降噪算法分离而不损伤语音质量。
  - 预处理操作（如低通滤波、下采样）可能增强保护效果，因高频语音细节被移除而噪声保留。

3) **在哪些任务上取得了怎样的效果**
- **训练攻击防御**：在TTS模型微调攻击中，说话人相似性降低5.5%（p<10⁻¹⁵），同时保持98.6%可懂度（STOI=0.986）和3.6%词错误率。
- **零样本攻击防御**：攻击成功率从20.0%降至13.3%，相似性降低5.0%。
- **鲁棒性验证**：在MP3压缩、谱减降噪、低通滤波及下采样五种对策下，保护效果均维持或增强（相似性最低降至0.688）。
</div>

</details>

---

## Train Short, Infer Long: Speech-LLM Enables Zero-Shot Streamable Joint ASR and Diarization on Long Audio
- **Authors**: Mohan Shi, Xiong Xiao, Ruchao Fan, Shaoshi Ling, Jinyu Li
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.16046v1](https://arxiv.org/abs/2511.16046v1)
- **PDF**: [https://arxiv.org/pdf/2511.16046v1](https://arxiv.org/pdf/2511.16046v1)

联合自动语音识别与说话人日志旨在解决多说话人场景中“谁在何时说了什么”的问题。本文提出一种端到端语音大语言模型Speech-LLM，用于实现可流式联合日志与识别任务。该模型仅使用20秒以内的短音频训练，却能对长音频进行流式推理而无需额外训练。这一突破通过引入说话人提示缓存实现，该缓存在分块流式推理过程中采用动态更新机制，其设计灵感源于大语言模型的自回归特性。该缓存机制还支持无缝集成预注册说话人信息，这在会议转录等场景中具有重要应用价值。为增强日志能力，我们在训练阶段向语音编码器注入词级说话人监督信号。实验结果表明：在20秒短音频场景下，本系统优于Sortformer、Meta-Cat等强基线模型；在长音频场景下，其性能超越采用级联离线流程的DiarizationLM。值得注意的是，本工作首次实现了仅用短音频训练的语音大语言模型对长音频进行零样本流式联合识别与日志，并达到最先进性能水平。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：多说话人场景（如会议）需联合自动语音识别（ASR）与说话人日志（Diarization），以解决“谁在何时说了什么”的问题。  
- **既有方法问题**：  
  - 级联系统（如独立ASR与日志组合）存在错误传播，且无法端到端优化。  
  - 近期方法（如Sortformer、Meta-Cat）虽支持端到端训练，但依赖分离的编码器，且仅适用于短音频（≤20秒），无法处理长音频的全局日志需求。  
  - 现有Speech-LLM未集成说话人日志，而DiarizationLM需后处理且非端到端，限制了实时流式应用。  

2.  
- **核心方法**：提出端到端语音大模型（JEDIS-LLM），通过以下设计解决长音频流式推理问题：  
  - **说话人提示缓存（SPC）**：  
    - 在分块流式推理中，SPC存储每个说话人的代表性音频片段及其文本，作为上下文提示。  
    - 利用LLM自回归特性，将缓存内容与当前音频块结合，维持跨块说话人标签一致性，无需显式聚类或重训练。  
    - 支持动态更新机制，根据音频质量（如长度、标点）优化缓存内容。  
  - **说话人档案集成**：  
    - SPC可替换为预注册的高质量说话人档案，直接映射真实姓名，提升实际应用灵活性。  
  - **训练优化**：  
    - 采用**词级说话人监督**：在语音编码器中添加辅助任务，预测每个词对应的说话人ID，增强日志能力而不损害ASR性能。  
    - 结合段级目标训练LLM，避免词级目标导致的序列过长与上下文断裂问题。  

3.  
- **任务与效果**：  
  - **短音频任务（≤20秒）**：在AMI、CH109等数据集上，WDER与cpWER均优于Sortformer和Meta-Cat，例如WDER降低至6.97%（AMI）。  
  - **长音频任务**：在CH109和Fisher测试集上，流式推理显著超越DiarizationLM（如WDER达1.73%），且支持实时处理。  
  - **说话人档案集成**：进一步缩小SA-WER与cpWER差距，提升说话人ID与真实名称的匹配精度。
</div>

</details>

---
