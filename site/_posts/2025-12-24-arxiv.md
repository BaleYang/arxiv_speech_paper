---
layout: post
title: "arXiv Daily – 2025-12-24"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-24（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-23 08:50 — 2025-12-24 08:50
- 抓取总数：12 篇 | 本页显示：12 篇（去重/过滤后）

## AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition
- **Authors**: Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra
- **Categories**: cs.SD, cs.AI, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.20407v1](https://arxiv.org/abs/2512.20407v1)
- **PDF**: [https://arxiv.org/pdf/2512.20407v1](https://arxiv.org/pdf/2512.20407v1)

无人机（UAV）在物流、农业、监控及国防等领域的应用日益广泛，其滥用也引发了安全与安防隐患，因此发展有效的检测机制至关重要。基于声学的传感技术提供了一种低成本、非侵入式的检测方案，可替代视觉或雷达方法，因为无人机螺旋桨会产生独特的声学特征。本研究提出AUDRON（基于音频的无人机识别网络），一种用于无人机声音检测的混合深度学习框架。该框架融合了梅尔频率倒谱系数（MFCC）与经卷积神经网络（CNN）处理的短时傅里叶变换（STFT）频谱图，并结合循环层进行时序建模以及基于自编码器的特征表示。在分类前通过特征级融合整合互补信息。实验评估表明，AUDRON能有效区分无人机声学特征与背景噪声，在不同条件下均保持较高的准确性与泛化能力。在二分类与多分类任务中，AUDRON分别达到98.51%和97.11%的准确率。研究结果凸显了将多特征表示与深度学习相结合在可靠声学无人机检测中的优势，表明该框架在视觉或雷达感知受限的安全监控场景中具有部署潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：无人机广泛应用带来安全与隐私风险，需有效检测机制。传统雷达、视觉系统成本高、易受环境（如光线、遮挡）限制，且小型或快速无人机可能规避探测。
- **既有方法问题**：现有声学检测方法多依赖单一特征（如MFCC）或简单分类器（如SVM），在复杂噪声环境中鲁棒性不足；部分融合方法依赖多模态（如音视频），在单一传感器数据质量下降时性能受限。

2)  
论文提出AUDRON框架，通过多模态特征融合解决上述问题：
- **多分支特征提取**：并行处理原始音频，提取互补特征：
  - **MFCC提取器**：使用1D CNN捕捉音色纹理特征。
  - **STFT-CNN提取器**：将STFT谱图作为图像，用2D CNN学习层次化谱时模式。
  - **RNN提取器**：采用双向LSTM与注意力机制，建模时间依赖关系。
  - **音频自编码器**：以自监督方式学习原始音频的压缩潜在表示。
- **特征级融合**：将四路特征向量拼接，经融合层（含Dropout与批归一化）整合后输入分类头，增强对无人机声学特征与背景噪声的区分能力。
- **数据增强与合成**：使用合成音频模拟不同无人机类型（如四旋翼、六旋翼）的谐波特征，并融合真实数据集与环境噪声，提升模型泛化性。

3)  
AUDRON在以下任务取得显著效果：
- **二元分类**：区分无人机与非无人机声音，最高准确率达98.51%（F1分数0.9837）。
- **多类分类**：识别特定无人机类型（如Bebop、Membo）与环境噪声，准确率达97.11%。
- **合成数据评估**：在合成数据上达到99.92%准确率，验证模型对清晰声学特征的有效捕捉。  
- **对比基准**：显著优于CNN、RNN、CRNN等基线模型，在二元与多类任务中分别提升约2%与4%准确率。
</div>

</details>

---

## EnvSSLAM-FFN: Lightweight Layer-Fused System for ESDD 2026 Challenge
- **Authors**: Xiaoxuan Guo, Hengyan Huang, Jiayi Zhou, Renhe Sun, Jian Liu, Haonan Cheng, Long Ye, Qin Zhang
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.20369v1](https://arxiv.org/abs/2512.20369v1)
- **PDF**: [https://arxiv.org/pdf/2512.20369v1](https://arxiv.org/pdf/2512.20369v1)

生成式音频模型的最新进展已能实现高保真环境音合成，这引发了音频安全领域的严重关切。为此，ESDD 2026挑战赛聚焦于两种场景下的环境音深度伪造检测：未知生成器场景（赛道1）与黑盒低资源场景（赛道2）。本研究提出EnvSSLAM-FFN系统，其核心在于将冻结的SSLAM自监督编码器与轻量级前馈网络后端相结合。为在严重数据不均衡条件下有效捕捉伪造痕迹，我们融合了SSLAM编码器第4至9层的中间表征，并采用类别加权训练目标。实验结果表明，所提系统在两个赛道上均稳定优于官方基线，测试等错误率分别达到1.20%与1.05%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：生成式音频模型（如AudioLDM）能合成高保真环境音，对音频安全构成严重威胁。环境音在频谱和时间上变化巨大，导致基于语音的自监督模型和基准（如ASVspoof）难以泛化。  
- **既有方法问题**：现有检测器主要针对结构化语音设计，在应对环境音深度伪造时，面临**数据严重不平衡**和**对未知生成器泛化能力不足**的挑战。

2)  
论文提出EnvSSLAM-FFN系统，通过以下核心设计解决上述问题：  
- **轻量级架构**：采用**冻结的SSLAM自监督编码器**作为前端，提取通用音频特征；后端连接一个**轻量级FFN**（含两个投影层、ReLU激活和Dropout），降低计算成本并避免过拟合。  
- **层融合策略**：设计可学习的融合模块，动态聚合SSLAM中间层（第4-9层）的表征。实验发现这些层能提供更有效的反欺骗线索，而非浅层的声学模式或深层的语义特征。  
- **处理数据不平衡**：训练时使用**类别加权的二元交叉熵损失**，根据类别频率的倒数赋予真实样本更高权重，缓解标签不平衡的影响。  
- **时序信息聚合**：通过**注意力统计池化**计算帧序列的加权均值和标准差，有效汇总时序信息。  
- **适应低资源场景**：在Track 2（黑盒低资源检测）中，系统使用Track 1的最佳权重初始化，并以更低学习率微调，实现快速适应。

3)  
在ESDD 2026挑战赛的两个任务上取得了显著效果：  
- **Track 1（未知生成器检测）**：在测试集上达到**1.20%的等错误率（EER）**，相比最佳基线（13.20%）有大幅提升。  
- **Track 2（黑盒低资源检测）**：在测试集上达到**1.05%的EER**，同样显著优于基线（12.48%）。  
- 结果表明，该系统在数据不平衡和低资源条件下均能实现高效、鲁棒的检测。
</div>

</details>

---

## MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model
- **Authors**: Ye Tao, Xuenan Xu, Wen Wu, Shuai Wang, Mengyue Wu, Chao Zhang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.20339v1](https://arxiv.org/abs/2512.20339v1)
- **PDF**: [https://arxiv.org/pdf/2512.20339v1](https://arxiv.org/pdf/2512.20339v1)

本文提出MMEdit，一种基于音频语言模型的统一音频编辑框架。现有文本引导音频编辑方法存在明显局限：免训练方法常因扩散反演导致信号质量下降，而基于训练的方法虽生成质量较高，却受限于高质量配对数据的稀缺性，且任务定义仅覆盖有限编辑操作类型。此外，常规架构通常将文本与音频处理解耦，难以实现指令与具体声学上下文的精准对齐。

为解决上述问题，我们系统性地扩展了任务定义，涵盖添加、替换、删除、重排序及属性修改等完整编辑操作体系。通过设计可扩展的数据合成流程，构建了具有细粒度事件级标注的大规模配对数据集。为捕捉复杂编辑语义，我们融合Qwen2-Audio编码器与基于MMDiT的生成器，实现精准的跨模态对齐与局部化编辑控制。

实验结果表明，本方法在编辑定位精度、指令跟随鲁棒性以及非编辑区域保真度方面均达到优异性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本引导的音频编辑旨在根据文本指令修改音频中的特定声学事件，同时严格保留非目标内容。  
- **既有方法的问题**：  
  - **免训练方法**：依赖扩散逆过程，易导致信号退化，且推理成本高、依赖完整描述或手工掩码。  
  - **基于训练的方法**：虽生成质量更好，但受限于高质量配对数据稀缺，且任务定义仅覆盖少量编辑操作。  
  - **模型架构**：通常将文本与音频处理解耦，限制了指令与声学上下文的对齐能力。

2)  
- **统一任务定义与数据构建**：  
  - 系统扩展了六类编辑操作：添加、移除、替换、重排序、响度调整和速度修改。  
  - 开发了可扩展的合成流程，生成大规模、带细粒度事件级标注的配对数据，并建立了开源评测基准。  
- **ALM驱动的多模态理解**：  
  - 采用Qwen2-Audio作为多模态编码器，联合处理音频和文本输入，实现跨模态对齐。  
  - 提取全局上下文和细粒度序列特征，为生成提供双重粒度条件信号。  
- **MMDiT主干网络设计**：  
  - 基于MMDiT架构，通过联合注意力块使音频潜在表示与指令嵌入直接交互，实现事件级编辑。  
  - 全局条件通过自适应层归一化调制整个网络，单一块则专注于保持音频的时序连续性与谱相干性。  
- **潜在扩散训练策略**：  
  - 在训练中将噪声潜在与源音频潜在在通道维度拼接，使去噪器能感知未编辑内容。  
  - 采用速度预测目标，并在推理时使用分类器无引导采样，通过保留源音频潜在优化引导方向。

3)  
- **支持任务**：在添加、移除、替换、重排序、响度调整和速度修改六类操作上均进行了评测。  
- **效果**：  
  - 在合成测试集上，多数客观指标（如LSD、FAD）优于基线，显示更接近真实目标。  
  - 在真实音频的主观评测中，MMEdit在指令遵循（R-MOS）和未编辑区域保真度（F-MOS）上均获最高分。  
  - 能够泛化到基线不支持的复杂操作（如重排序、响度/速度调整），并在编辑定位与内容保留间取得更好平衡。
</div>

</details>

---

## LP-CFM: Perceptual Invariance-Aware Conditional Flow Matching for Speech Modeling
- **Authors**: Doyeop Kwak, Youngjoon Jang, Joon Son Chung
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.20314v1](https://arxiv.org/abs/2512.20314v1)
- **PDF**: [https://arxiv.org/pdf/2512.20314v1](https://arxiv.org/pdf/2512.20314v1)

本文旨在通过引入幅度缩放与时移等感知不变性，为语音建模提供新视角。传统生成式建模通常将每个数据样本视为目标分布的固定代表，但从生成角度看，这些样本仅是真实语音分布中众多感知等价变体之一。为此，我们提出线性投影条件流匹配（LP-CFM），将目标建模为沿感知等价变体方向投影对齐的拉长高斯分布。进一步引入向量校准采样（VCS）以保持采样过程与线性投影路径对齐。在不同模型规模、数据量和采样步数的神经声码器实验中，所提方法相较于传统最优传输条件流匹配均取得稳定提升，在低资源与少步数场景下增益尤为显著。这些结果凸显了LP-CFM与VCS在构建更鲁棒、更符合感知特性的语音生成模型方面的潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音生成中，人类听觉对全局幅度缩放和微小时间偏移具有感知不变性，即这些变化不影响感知质量。  
- **既有方法问题**：传统生成模型（包括条件流匹配CFM）将每个数据样本视为目标分布的固定代表，强制模型学习单一实例，即使存在感知等效的变体也会惩罚偏差。这导致数据和模型容量的低效利用，未能充分利用感知不变性。

2)  
论文提出**线性投影条件流匹配（LP-CFM）**和**向量校准采样（VCS）**，核心方法如下：  
- **LP-CFM**：  
  - **目标分布重构**：将目标分布建模为沿感知等效变体直线（如幅度缩放或时间偏移形成的线性集合）的**拉长高斯分布**，而非以单个数据点为中心的球形高斯。  
  - **训练目标**：通过条件概率路径和速度场，引导样本向直线上的最近有效点移动，而非强制收敛到任意单一实例。这减少了传输路径长度和变异性，优化更高效。  
- **VCS**：  
  - **采样校正**：在推理阶段移除预测向量中与目标直线平行的误差分量，确保采样过程与投影几何一致，作为安全机制提升稳定性。  
- **应用具体化**：在语音建模中，利用短时傅里叶变换（STFT）特性，将幅度缩放（对数幅度谱上表现为加法）和时间偏移（相位谱上表现为线性关系）表达为直线方程，从而实例化LP-CFM。

3)  
- **任务**：在神经声码任务（将梅尔频谱转换为波形）中评估，使用LJ Speech数据集。  
- **效果**：  
  - **一致提升**：在多种模型大小、数据规模和采样步数下，LP-CFM均优于传统最优传输CFM（OT-CFM），指标包括M-STFT、PESQ、MCD、Periodicity、V/UV F1和UTMOS。  
  - **显著优势**：在低资源场景（如小模型、少数据、少采样步数）中改进尤为明显，例如在UNet-16模型上UTMOS提升约0.14，在33%数据训练下性能仍优于OT-CFM使用全数据。  
  - **主观验证**：CMOS测试显示听者显著偏好LP-CFM，尤其在低资源条件下。
</div>

</details>

---

## SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision
- **Authors**: Maxime Poli, Mahi Luthra, Youssef Benchekroun, Yosuke Higuchi, Martin Gleize, Jiayi Shen, Robin Algayres, Yu-An Chung, Mido Assran, Juan Pino, Emmanuel Dupoux
- **Categories**: cs.CL, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.20308v1](https://arxiv.org/abs/2512.20308v1)
- **PDF**: [https://arxiv.org/pdf/2512.20308v1](https://arxiv.org/pdf/2512.20308v1)

语言建模与语音表征学习的并行进展，使得无需文本中介、直接从语音中学习语言成为可能。这要求直接从语音中提取语义表征。本文的贡献包括三个方面。首先，我们提出 SpidR，一种自监督语音表征模型，能够高效学习具有高度可访问性语音信息的表征，特别适用于无文本口语语言建模。该模型基于原始波形进行训练，采用掩码预测目标，并结合自蒸馏与在线聚类方法。学生模型的中间层学习预测来自教师模型中间层的分配结果。与先前方法相比，该学习目标稳定了在线聚类过程，从而得到更高质量的码本。在语言建模下游任务（sWUGGY、sBLIMP、tSC）上，SpidR 的表现优于 wav2vec 2.0、HuBERT、WavLM 和 DinoSR。其次，我们系统评估了不同模型及层级中语音单元质量（ABX、PNMI）与语言建模性能之间的相关性，验证了这些指标作为可靠代理的有效性。最后，与 HuBERT 相比，SpidR 显著缩短了预训练时间，仅需在 16 个 GPU 上训练一天，而非一周。这一加速得益于预训练方法及高效的代码库，使得迭代更快、实验更便捷。训练代码与模型检查点已开源：https://github.com/facebookresearch/spidr。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：纯语音语言建模旨在直接从音频学习语言，无需文本中介。这需要从语音中提取语义表征。现有方法（如HuBERT、DinoSR）通常使用自监督语音模型作为语音分词器，但其学习到的语音单元存在以下问题：
- **既有问题**：
  - 单元对声学变化和协同发音导致的上下文变化缺乏鲁棒性，更接近上下文相关的音素状态而非真正的语言单元。
  - 训练成本高：例如HuBERT需要多轮迭代训练和聚类，过程繁琐；DinoSR虽为单轮训练，但原始实现仍需一周时间，限制了探索效率。
  - 对语音单元质量的评估与下游语言建模性能之间的关联缺乏系统理解。

2)  
论文提出**SpidR**模型，通过改进学习目标和训练流程解决上述问题：
- **核心架构**：基于DinoSR，采用学生-教师自蒸馏框架和在线聚类。学生和教师均为Transformer编码器。关键创新在于**层间对齐预测**：
  - 对于最后K层，学生每一层的输出直接预测教师同一层通过码本得到的伪标签（即码本分配），而非像DinoSR那样仅用学生最后一层输出来预测所有教师层的标签。
  - 这种设计减少了学生表征与码本之间的分布偏移，使预测任务更直接。
- **稳定训练**：
  - 层间对齐预测显著提高了在线聚类的稳定性，有效防止了码本崩溃（codebook collapse），如图2所示，SpidR在各层的码本困惑度和预测困惑度均高于DinoSR，表明码本使用更均衡、多样性更好。
  - 训练流程简化：单轮训练即可完成，无需HuBERT式的多轮迭代和手动选择中间层。
- **高效实现**：
  - 提供了纯PyTorch的轻量级代码库，充分利用`torch.compile`等现代特性，大幅减少主机-设备同步，提升了训练速度。
  - 移除了注意力层中的偏置项，避免了训练中可能出现的权重范数激增和模型崩溃问题。
  - 采用了更平滑的教师模型指数移动平均更新策略。
- **效果关联验证**：论文系统评估了语音单元质量（ABX、PNMI）与下游语言建模性能（词汇、句法、语义层面）之间的相关性，证实这些指标可作为SLM性能的可靠代理，指导模型开发。

3)  
SpidR在以下任务和指标上取得了显著效果：
- **语音表征质量**：在LibriSpeech上，其连续表征的ABX错误率（衡量音素区分度）和MAP单词检索准确率均优于wav2vec 2.0、HuBERT、data2vec和DinoSR等基线模型。
- **下游纯语音语言建模**：使用SpidR提取的离散单元（通过码本预测或K-means聚类）训练OPT-125M模型，在多个零样本评估基准上超越所有对比的SSL编码器：
  - **词汇层面**（sWUGGY）：准确率最高达71.89%（K-means）。
  - **句法层面**（sBLIMP）：准确率最高达82.46%（K-means）。
  - **语义层面**（tSC）：准确率最高达70.46%（K-means）。
- **训练效率**：在16张A100 GPU上，在LibriSpeech 960h上完成预训练仅需约1天（23小时），远快于HuBERT（约1周）和原始DinoSR。代码库开源，加速了研究迭代。
</div>

</details>

---

## TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation
- **Authors**: Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe
- **Categories**: cs.CV, cs.AI, eess.AS, eess.IV
- **arXiv**: [https://arxiv.org/abs/2512.20296v1](https://arxiv.org/abs/2512.20296v1)
- **PDF**: [https://arxiv.org/pdf/2512.20296v1](https://arxiv.org/pdf/2512.20296v1)

本文旨在从文本和参考图像中联合生成交互式视频与会话语音。为构建类人对话系统，近期研究已探索了说话/倾听头部生成及会话语音生成。然而这些工作通常孤立开展，忽视了人类对话的多模态本质——其涉及紧密耦合的视听交互。本文提出TAVID，一个同步生成交互式面部与会话语音的统一框架。TAVID通过两个跨模态映射器（运动映射器与说话人映射器）整合面部与语音生成流程，实现音频与视觉模态间互补信息的双向交换。我们从四个维度评估系统性能：说话面部真实感、倾听头部响应性、双人交互流畅度及语音质量。大量实验证明，本方法在上述所有方面均具显著有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：构建类人对话系统是AI研究的重要目标，但现有方法多局限于单一模态（如纯文本或纯语音），忽略了人类对话固有的多模态（音频-视觉）耦合特性。  
- **既有问题**：  
  - **任务割裂**：先前研究将“说话头生成”（生成说话者面部视频）和“倾听头生成”（生成倾听者反馈）作为独立任务，无法模拟真实的双向交互对话。  
  - **依赖预录音频**：现有交互式头部生成方法需依赖预录语音驱动，无法自由生成新的语音内容；若级联文本转语音系统，则会导致错误累积和额外的说话人建模负担。  
  - **交互不自然**：早期方法通过手动角色切换器控制说/听状态，导致转换生硬，且难以处理同时说话或沉默等真实对话状态。

2)  
TAVID 提出一个**统一框架**，从文本对话和参考图像中**同步生成**交互式视频与会话语音，其核心方法通过两个跨模态映射器解决上述问题：  

- **整体架构**：  
  - 框架包含**交互视频生成**与**会话语音合成**两条并行的流水线，二者通过共享的**多流语义令牌**（代表对话双方）进行初步同步。  
  - 为强化跨模态关联，引入了**运动映射器**和**说话人映射器**，在音视频流之间建立双向信息交换通路。  

- **运动映射器**：  
  - **功能**：将多流语义令牌转换为包含精确唇动与倾听反馈的**交互式运动特征**，驱动视频生成中的面部动态。  
  - **关键设计**：采用**联合自注意力机制**（而非简单的拼接或双注意力），同时捕捉每个说话流的特定信息与双流间的相互依赖关系，从而自然建模说/听状态的动态转换与协调。  
  - **输入优化**：使用**韵律感知的语义令牌**（提取自XLS-R模型），其包含的韵律信息有助于生成更准确、富有表现力的唇部运动与面部动态。  

- **说话人映射器**：  
  - **功能**：从参考图像中提取视觉特征（人脸嵌入 + ReferenceNet空间特征），预测与之匹配的**说话人声学特征**，确保合成语音的音色与视觉身份一致。  
  - **优势**：实现了**从人脸到声音的自动建模**，无需额外音频提示，解决了级联系统中语音与视觉身份对齐的难题。  

- **协同生成**：  
  - 两条流水线以上述映射器为桥梁，形成协同环路：运动映射器利用语义信息驱动视觉动态；说话人映射器利用视觉信息塑造声音特征。  
  - 整个框架以**端到端**方式训练，避免了级联系统的错误累积，实现了对对话内容、说话人身份和交互行为的灵活控制。

3)  
TAVID 在多个任务上取得了显著优于基线方法的效果：  

- **交互式头部生成**：在Seamless Interaction测试集上，在**视觉质量、唇部同步准确性、话轮转换自然度**等主观与客观指标上均大幅领先音频驱动的基线方法（如DIM）。其生成的视频能准确捕捉唇动、倾听反馈（如点头）、表情变化甚至自然的话语重叠。  
- **单角色头部生成**：  
  - **说话头生成**：在HDTF数据集上，取得了最佳的**视频质量指标**，唇部同步分数与当前最优方法相当。  
  - **倾听头生成**：在ViCo数据集上，在**运动真实感**与**协调性**指标上优于或媲美最新方法。  
- **人脸风格化语音生成**：在VoxCeleb2数据集上，其合成语音的**自然度**优于所有基线，在**人脸匹配度**上领先其他人脸驱动方法，表明其能有效将视觉身份转化为一致的声学特征。
</div>

</details>

---

## Aliasing-Free Neural Audio Synthesis
- **Authors**: Yicheng Gu, Junan Zhang, Chaoren Wang, Jerry Li, Zhizheng Wu, Lauri Juvela
- **Categories**: cs.SD, eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2512.20211v1](https://arxiv.org/abs/2512.20211v1)
- **PDF**: [https://arxiv.org/pdf/2512.20211v1](https://arxiv.org/pdf/2512.20211v1)

神经声码器与编解码器通过声学表征重构波形，其设计直接影响音频质量。在现有方法中，基于上采样的时域模型在推理速度与合成质量上均表现优异，达到了当前最优性能。然而，尽管这些模型在生成感知自然的音频方面已取得成功，其合成保真度仍受限于架构设计缺陷导致的混叠伪影。具体而言，无约束的非线性激活函数会产生超出奈奎斯特频率的无限谐波，引发“折返式”混叠伪影；广泛使用的转置卷积上采样层会将镜像低频成分复制填充至高频区域，导致“镜像式”混叠伪影；同时，其固有的周期性与镜像直流偏置结合还会产生“音调伪影”，引发恒定频率的振铃效应。本文旨在从信号处理角度解决上述问题：通过过采样与反导数抗混叠技术对激活函数进行抗混叠化处理，并以重采样层替代存在问题的转置卷积层，从而消除音调伪影并抑制混叠成分。基于所提出的抗混叠模块，我们构建了Pupu声码器与Pupu编解码器，并发布高质量预训练模型以推动音频生成研究。通过构建测试信号基准验证了抗混叠模块的有效性，并在语音、歌声、音乐及通用音频数据上进行了实验验证。结果表明，轻量化的Pupu声码器与Pupu编解码器在歌声、音乐及通用音频任务上显著优于现有系统，同时在语音任务上达到可比性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于上采样的时域神经声码器和编解码器在推理速度和合成质量上表现出色，但合成保真度受限于模型架构设计不当引入的混叠伪影。  
- **既有方法问题**：  
  - **非线性激活函数**：无约束的非线性激活（如ReLU）产生无限谐波，超过奈奎斯特频率，导致“折叠”混叠伪影。  
  - **上采样层**：广泛使用的ConvTranspose层将低频部分镜像复制到高频区域，导致“镜像”混叠伪影，并结合其固有周期性和镜像直流偏置，引入“音调伪影”（恒定频率振铃）。  
  - **现有解决方案**：如过采样或线性插值，要么牺牲合成速度或质量，要么无法完全消除伪影。

2)  
论文从信号处理角度提出抗混叠模块，核心方法如下：  
- **抗混叠激活函数**：  
  - 采用**过采样**和**抗导数抗混叠（ADAA）** 技术，将离散信号转换为连续信号后再应用激活函数，避免采样频率约束。  
  - 以SnakeBeta激活为例，推导出其ADAA形式，消除分母项，确保数值稳定性，无需阈值切换，同时梯度有界，避免爆炸。  
  - 仅需2倍过采样即可达到4倍过采样的抗混叠效果，兼顾计算效率。  
- **抗混叠上采样层**：  
  - 用**重采样（零交织+低通滤波）** 替代ConvTranspose层，避免“音调伪影”并抑制混叠分量。  
  - 通过通道扩展卷积层将零交织的潜在表示转换为类似噪声的确定性先验，填充高频区域，提升训练稳定性。  
- **模型集成**：  
  - 基于上述模块，构建Pupu-Vocoder和Pupu-Codec模型，分别用于声码器和编解码任务。  
  - 模型保持轻量级设计，在解码器中用“AF Conv Blocks”替换原始卷积块，集成抗混叠激活和上采样模块。

3)  
- **任务与效果**：  
  - **歌唱语音、音乐、通用音频**：Pupu-Vocoder和Pupu-Codec在客观指标（如MOS-Pred、FAD）和主观听测（MUSHRA）上均超越现有系统（如BigVGAN、DAC），尤其在高质量工业数据上表现突出。  
  - **语音**：达到与现有先进模型（如DAC、BigCodec）相当的性能。  
  - **动态码率编码**：在低码率场景下保持稳健性能，优于EnCodec，与更大参数模型持平。  
  - **抗混叠验证**：通过测试信号基准（AHR指标）和频谱可视化，证实模块有效消除混叠伪影，准确重建高频谐波结构。
</div>

</details>

---

## Spectral or spatial? Leveraging both for speaker extraction in challenging data conditions
- **Authors**: Aviad Eisenberg, Sharon Gannot, Shlomo E. Chazan
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.20165v1](https://arxiv.org/abs/2512.20165v1)
- **PDF**: [https://arxiv.org/pdf/2512.20165v1](https://arxiv.org/pdf/2512.20165v1)

本文提出一种鲁棒的多通道说话人提取算法，旨在处理参考信息不准确的情况。现有方法通常仅依赖空间或频谱线索来识别目标说话人，而本方法通过融合两种信息源以增强鲁棒性。该算法的核心在于强调稳定性，即使在某一特征退化或存在误导时仍能保持可靠性能。给定含噪混合信号及两个可能不可靠的线索，我们训练专用网络动态平衡二者的贡献——或在必要时忽略信息量较低的特征。通过使用简易到达方向估计器和含噪频谱注册过程模拟推理阶段的误差，我们在挑战性条件下评估系统性能。实验结果表明，即使在参考信息存在显著误差时，所提模型仍能成功提取目标说话人。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：多通道说话人提取任务依赖目标说话人的先验信息（如参考语音或方向）。现有方法通常**单独依赖**空间线索（如波达方向DOA）或谱线索（如参考语音的频谱特征）来识别目标。
- **既有问题**：在**参考信息不准确或部分缺失**的挑战性条件下（例如DOA估计错误、参考语音质量差、说话人空间距离近），仅依赖单一线索的方法性能会显著下降，缺乏鲁棒性。

2)  
论文提出一种**集成空间与谱线索**的鲁棒多通道说话人提取方法，其核心设计如下：
- **统一嵌入与动态调制**：
    - 模型主干为带自注意力机制的U-Net。输入包括多通道混合信号、单通道谱参考语音以及目标DOA。
    - 谱参考通过编码器生成嵌入向量；DOA通过查找表转化为嵌入表示。两者相加形成**统一参考嵌入**。
    - 该嵌入通过**特征线性调制（FiLM）** 条件化混合信号的表示，使模型能同时利用两种信息。
- **轻量级分类器与自适应选择**：
    - 在瓶颈层后引入一个**三分类器**，将当前场景判为三类之一：①两种参考均相关；②仅空间参考相关；③仅谱参考相关。
    - 分类器的输出嵌入用于**第二次自注意力迭代的引导**，通过FiLM操作动态调整模型对两种参考的依赖程度，从而在必要时**抑制不可靠的参考**。
- **鲁棒性训练策略**：
    - 训练时**主动引入错误参考**（如随机DOA或错误说话人的谱参考），并与正确参考配置一同训练。
    - 同时为DOA添加小扰动，以提升对估计误差的容忍度。
    - 总体损失结合了提取任务的**尺度不变信噪比损失**和分类器的**交叉熵损失**，迫使模型学会在参考信息不可靠时自适应地倚重更可靠的线索。

3)  
- **测试任务与效果**：模型在六种挑战性测试集上评估，包括说话人空间接近、同性别混合、参考信息错误等场景。
- **性能表现**：所提模型（带分类器）在多数情况下取得最佳**SI-SDR改进值**（例如在中等空间接近条件下达10.3 dB），显著优于仅用空间或谱线索的基线。
- **关键优势**：在**DOA随机错误**或**谱参考低信噪比**时，模型仍能稳定提取目标，证明了其**对错误参考的鲁棒性**。仅当说话人极度接近时，性能有轻微下降。
</div>

</details>

---

## Fun-Audio-Chat Technical Report
- **Authors**: Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li, Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang, Qiquan Zhang, Jingren Zhou
- **Categories**: cs.CL, cs.AI, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.20156v1](https://arxiv.org/abs/2512.20156v1)
- **PDF**: [https://arxiv.org/pdf/2512.20156v1](https://arxiv.org/pdf/2512.20156v1)

近期，联合语音-文本模型的发展为实现无缝语音交互展现出巨大潜力。然而，现有模型面临关键挑战：语音标记（25Hz）与文本标记（约3Hz）之间的时间分辨率不匹配会稀释语义信息、带来高昂计算成本，并导致文本大语言模型知识的灾难性遗忘。我们推出Fun-Audio-Chat，这是一个大型音频语言模型，通过我们在先前工作DrVoice中的两项创新来解决这些局限。首先，双分辨率语音表示（DRSR）：共享大语言模型以高效的5Hz（通过标记分组）处理音频，而语音精化头部则以25Hz生成高质量标记，在效率（GPU使用降低约50%）与质量之间取得平衡。其次，核心-鸡尾酒训练，这是一种包含中间合并的两阶段微调方法，能够缓解灾难性遗忘。随后，我们应用多任务DPO训练来增强模型的鲁棒性、音频理解、指令跟随及语音共情能力。这种多阶段后训练使Fun-Audio-Chat能够保留文本大语言模型知识，同时获得强大的音频理解、推理与生成能力。与近期需要大规模音频-文本预训练的大型音频语言模型不同，Fun-Audio-Chat充分利用预训练模型并进行广泛的后训练。Fun-Audio-Chat 8B和MoE 30B-A3B在语音到文本及语音到语音任务上表现优异，在口语问答基准测试中位列同规模模型前列。同时，在音频理解、语音功能调用、指令跟随和语音共情方面也达到竞争性乃至领先的性能。我们还开发了Fun-Audio-Chat-Duplex，这是一个全双工变体，在口语问答和全双工交互中表现出色。我们开源了Fun-Audio-Chat-8B及其训练与推理代码，并提供了交互式演示。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
现有联合语音-文本模型面临三个核心挑战：
- **时序分辨率不匹配**：语音标记（通常25Hz）与文本标记（约3Hz）的频率差异导致语义信息被稀释，阻碍了LLM核心能力的充分利用。
- **灾难性遗忘**：将文本LLM主干训练为多模态模型时，常导致其原有文本知识严重丢失。
- **计算成本高昂**：高音频帧率（通常12.5Hz或25Hz）带来巨大计算开销，限制了实际部署。

2)  
论文通过两项关键创新解决上述问题：

- **采用双分辨率语音表示（DRSR）架构**：
  - **语音标记分组**：将25Hz的语音标记通过分组机制映射为5Hz的表示，使共享LLM主干能以高效的5Hz帧率处理音频，显著降低计算量（训练GPU小时数减少近50%）。
  - **语音精炼头（SRH）**：在生成阶段，SRH利用LLM的隐藏状态，以完整的25Hz分辨率自回归生成高质量的语音标记。这种设计在保持LLM语义推理能力的同时，实现了高保真语音合成。

- **实施多阶段后训练策略**：
  - **核心-鸡尾酒训练**：采用两阶段全监督微调。第一阶段使用高学习率快速适应多模态任务；随后通过中间模型合并（与原始预训练LLM参数加权插值）来保留知识；第二阶段使用低学习率进行稳定优化。此策略有效缓解了灾难性遗忘。
  - **多任务DPO训练**：在核心-鸡尾酒训练后引入，通过偏好学习目标增强模型在多个维度的能力：
    - 对真实语音数据的鲁棒性。
    - 音频理解能力。
    - 语音指令跟随能力（如控制情感、风格、韵律）。
    - 语音共情能力（理解用户情感并生成共情回应）。
  - 整个流程**仅依赖后训练**，无需大规模音频-文本预训练，在保留文本LLM能力的同时获得了强大的音频理解、推理与生成技能。

3)  
Fun-Audio-Chat在多个任务上取得了优异效果：
- **语音问答**：在相似规模（8B和30B-A3B）模型中，于OpenAudioBench、VoiceBench、UltraEval-Audio等多个基准的语音转文本和语音转语音任务上排名前列。
- **音频理解**：在MMAU、MMAU-Pro、MMSU基准上超越多个开源基线，达到最佳或极具竞争力的性能。
- **语音功能调用**：在Speech-ACEBench、Speech-BFCL等基准上表现优异，尤其在并行函数调用场景中展现出强大能力。
- **语音指令跟随与共情**：在VStyle等基准上，其语音属性控制、指令跟随及共情能力显著优于其他开源模型，并与商业模型竞争。
- **全双工交互**：其变体Fun-Audio-Chat-Duplex在全双工知识理解和轮流对话成功率指标上表现强劲，支持自然、实时的双向语音交流。
</div>

</details>

---

## QuarkAudio Technical Report
- **Authors**: Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Xiaofu Chen, Bin Gong, Zheng Xue, Gang Song
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.20151v1](https://arxiv.org/abs/2512.20151v1)
- **PDF**: [https://arxiv.org/pdf/2512.20151v1](https://arxiv.org/pdf/2512.20151v1)

现有音频处理与生成模型多依赖任务专用架构，导致开发工作碎片化且扩展性受限。因此，设计一个能够处理多任务、同时具备强大指令与音频理解能力及高质量音频生成能力的统一框架具有重要意义。这需要兼容的范式设计、强大的骨干网络以及高保真音频重建模块。为满足这些需求，本技术报告提出QuarkAudio——一个基于仅解码器自回归语言模型的生成式统一框架。该框架包含统一的离散音频分词器H-Codec，其将自监督学习表征融入分词与重建过程。我们进一步对H-Codec提出多项改进，包括动态帧率机制并将音频采样率扩展至48 kHz。QuarkAudio通过将任务特定条件信息作为仅解码器语言模型的条件序列，并以自回归方式预测离散目标音频令牌，实现了多任务统一。该框架支持广泛的音频处理与生成任务，包括语音修复、目标说话人提取、语音分离、语音转换以及语言查询的音频源分离。此外，我们将下游任务扩展至自然语言指令引导的通用自由形式音频编辑（包括语音语义编辑与音频事件编辑）。实验结果表明，H-Codec能够以低帧率实现高质量音频重建，提升下游音频生成的效率与性能；QuarkAudio在多项任务中达到与当前最优任务专用或多任务系统相当或更具竞争力的性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频领域存在任务碎片化问题，现有模型多为任务专用架构，导致开发冗余、扩展性受限。  
- **既有方法问题**：  
  - 离散音频编解码器在低帧率下难以实现高保真重建，单层量化面临音质损失挑战。  
  - 连续表示方法需复杂设计以融合多模态条件，限制了任务扩展性。  
  - 统一音频生成框架在音频质量、跨任务泛化能力（如语音分离）和自由形式音频编辑方面仍存在不足。

2)  
- **核心方法**：提出**QuarkAudio**，一个基于仅解码器自回归语言模型的统一生成框架。  
- **关键组件与创新**：  
  - **H-Codec音频分词器**：  
    - 采用双流（声学+语义）设计，分别量化波形特征和自监督学习（SSL）表征，提升重建质量。  
    - 引入动态帧率机制（H-Codec-1.5），根据语义相似性自适应聚合帧，降低帧率。  
    - 扩展采样率至48 kHz（H-Codec-2.0），支持更高保真度音频。  
    - 使用两阶段训练策略，先在大规模音频上预训练，再微调解码器以优化重建。  
  - **统一音频语言模型**：  
    - 将任务特定条件信息（如文本、参考音频）作为连续嵌入序列，输入仅解码器LM。  
    - 通过特殊任务令牌区分七种操作模式（如语音修复、语音分离、编辑等）。  
    - 使用延迟模式并行预测多层编解码器令牌，平衡性能与计算成本。  
- **解决思路**：  
  - 通过H-Codec实现高效、高质量的音符化，为下游生成提供低帧率、高保真的离散表示。  
  - 利用连续条件嵌入保留丰富语义信息，结合自回归LM统一建模多任务生成过程，避免任务专用架构。

3)  
- **支持任务**：语音修复（SR）、目标说话人提取（TSE）、语音分离（SS）、语音转换（VC）、语言查询音频源分离（LASS）、语音语义编辑（EDIT-S）、音频事件编辑（EDIT-A）。  
- **取得效果**：  
  - H-Codec在LibriSpeech等基准上实现竞争性的重建质量（如PESQ、STOI、说话人相似度）。  
  - QuarkAudio在多项任务上达到或超越专用模型性能，例如在语音修复、语音分离等任务中指标优于或接近LLaSE-G1等基线。  
  - 在自由形式音频编辑任务中，实现了基于自然语言指令的语义和事件编辑，展示了潜在应用能力。
</div>

</details>

---

## SpatialNet with Binaural Loss Function for Correcting Binaural Signal Matching Outputs under Head Rotations
- **Authors**: Dor Shamay, Boaz Rafaely
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.20122v1](https://arxiv.org/abs/2512.20122v1)
- **PDF**: [https://arxiv.org/pdf/2512.20122v1](https://arxiv.org/pdf/2512.20122v1)

随着虚拟现实头显、智能眼镜及头部追踪耳机等设备的兴起，双耳声重放技术日益受到关注。由于此类系统常采用空间分辨率有限的任意传声器阵列，实现精确的双耳信号仍具挑战性。为改进早期双耳信号匹配方法的局限，提升高频及头部旋转条件下的重放效果，研究者提出了基于幅度最小二乘的双耳信号匹配方法。然而，随着头部旋转角度增大，其精度仍会下降，导致空间感和音色失真，尤其在虚拟听者耳朵远离最近传声器时更为明显。本研究提出将深度学习与幅度最小二乘双耳信号匹配方法相结合以缓解此类失真。通过采用基于SpatialNet网络的后续处理框架，充分利用其高效处理空间信息的能力，并引入信号层面损失与基于人类双耳听觉理论模型的感知驱动双耳损失共同指导训练。在六传声器半圆形阵列的仿真研究中验证了该方法的有效性，表明其能在头部旋转过程中保持稳定性能。进一步在不同混响环境及头部旋转条件下的听觉实验表明，所提框架能有效抑制幅度最小二乘双耳信号匹配方法的性能衰减，并在大幅头部旋转下实现鲁棒的校正效果。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：双耳音频重放对VR/AR等沉浸式应用至关重要。传统方法如Ambisonics依赖特定麦克风阵列，不适用于可穿戴设备等灵活配置。  
- **既有方法问题**：BSM-MagLS方法虽改善了高频性能和头部旋转补偿，但当虚拟听者耳朵远离麦克风时（尤其在头部旋转较大时），仍会产生空间和音色伪影，导致信号质量下降。

2)  
- **核心方法**：提出一种结合深度学习与BSM-MagLS的后处理框架，使用SpatialNet神经网络校正头部旋转引起的失真。  
- **关键设计**：  
  - **网络输入与目标**：输入为经BSM-MagLS补偿后的双耳信号；目标为将麦克风阵列旋转至与头部对齐后BSM-MagLS生成的准确信号。  
  - **损失函数**：结合信号级损失（如SI-SDR、STFT损失）与基于听觉模型的双耳损失（提取ILD、IPD、IVS等空间线索），以同时优化信号保真度与空间感知。  
- **解决思路**：利用靠近麦克风阵列的耳朵信号更可靠的特点，通过SpatialNet（含窄带与跨带处理模块）调整双耳信号，减少伪影并保持空间线索一致性。

3)  
- **任务与效果**：在仿真和主观听音实验中评估，使用六麦克风半圆阵列。  
  - **客观指标**：SpatialNet（尤其采用听觉损失变体）在SI-SDR、STFT损失及双耳线索误差上均优于原始BSM-MagLS，且在头部旋转增大时性能下降更缓。  
  - **主观听音**：在MUSHRA测试中，所提方法在60°和90°旋转下均接近参考信号质量，显著优于BSM-MagLS及STFT损失变体，展现了良好的泛化能力与鲁棒性。
</div>

</details>

---

## DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation
- **Authors**: Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang
- **Categories**: cs.CV, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.20117v1](https://arxiv.org/abs/2512.20117v1)
- **PDF**: [https://arxiv.org/pdf/2512.20117v1](https://arxiv.org/pdf/2512.20117v1)

视听分割（AVS）旨在通过联合利用听觉与视觉信息，在像素级别定位发声物体。然而，现有方法常受多源纠缠与视听错位问题困扰，导致模型偏向于更响亮或更大的物体，而忽略较弱、较小或同时出现的声源。为应对这些挑战，本文提出DDAVS——一种解耦音频语义与延迟双向对齐的框架。针对多源纠缠问题，DDAVS采用可学习查询从音频中提取语义，并将其锚定在基于音频原型记忆库构建的结构化语义空间中，进一步通过对比学习优化以增强区分性与鲁棒性。针对视听错位问题，DDAVS引入延迟模态交互的双重交叉注意力机制，提升多模态对齐的鲁棒性。在AVS-Objects与VPO基准上的大量实验表明，DDAVS在单源、多源及多实例场景中均持续优于现有方法，展现出卓越性能。这些结果验证了本框架在复杂真实世界视听分割条件下的有效性与泛化能力。项目页面：https://trilarflagz.github.io/DDAVS-page/

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：视听分割旨在利用音频和视觉信息在像素级定位发声物体。  
- **既有方法问题**：现有方法存在**多源纠缠**（多个声源在音频特征中混杂，导致模型偏向响亮或大型物体，忽略较弱、较小或共现声源）和**视听错位**（音频与视觉模态无法准确关联，影响对小型、远距离或屏外声源的定位）。  

2)  
论文提出DDAVS框架，通过两个核心模块解决上述问题：  
- **音频解纠缠模块**：  
  - 使用可学习查询从音频特征中提取多个语义，并通过交叉注意力将其**锚定到预构建的原型记忆库**中。该记忆库包含单一声源的音频嵌入，提供了结构化、稳定的语义空间。  
  - 引入**对比学习优化**：对原始音频和增强音频（添加噪声、混响等）生成的查询进行对比损失计算，增强解纠缠后音频语义的区分性和鲁棒性，避免弱声源被主导声源压制。  
- **延迟双向对齐模块**：  
  - 采用**延迟的双向交叉注意力机制**：在视觉编码器的较深层（如第3、4层）进行音频与视觉特征的双向交互，而非早期层。延迟交互能过滤低层噪声，双向设计则允许音频和视觉相互增强，捕获模态间依赖关系。  
  - 具体流程：音频查询先关注视觉特征以提取声音相关区域，再以更新后的音频特征作为键值对视觉特征进行增强，从而提升跨模态对齐的鲁棒性和分割精度。  

3)  
在**AVSBench**和**VPO**基准测试中，DDAVS在多项任务上取得最优效果：  
- **单声源场景**：在AVS-Objects-S4上J&F达到92.4%，优于之前最佳方法（91.1%）。  
- **多声源场景**：在AVS-Objects-MS3上J&F为75.1%，在VPO-MS上J&F为76.11%，均显著领先。  
- **复杂场景**：在涉及多实例、多类别的VPO-MSMI任务上，J&F达到72.84%，较之前最佳提升3.54%。定性结果显示，模型能更准确分割共现声源并抑制非发声物体。
</div>

</details>

---
