---
layout: post
title: "arXiv Daily – 2026-01-09"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-09（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-08 08:50 — 2026-01-09 08:50
- 抓取总数：11 篇 | 本页显示：11 篇（去重/过滤后）

## Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification
- **Authors**: Karim El Khoury, Maxime Zanella, Tiffanie Godelaine, Christophe De Vleeschouwer, Benoit Macq
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.05011v1](https://arxiv.org/abs/2601.05011v1)
- **PDF**: [https://arxiv.org/pdf/2601.05011v1](https://arxiv.org/pdf/2601.05011v1)

音频-语言模型近期通过利用自然语言监督实现零样本音频事件分类，展现出强大的零样本能力。然而，其性能对文本提示的表述极为敏感，细微的措辞变化会导致准确率大幅波动。先前研究通过提示学习或提示集成缓解了这一问题，但这些策略要么需要标注数据，要么未能考虑某些提示可能对性能产生负面影响。本研究提出一种基于熵的提示加权方法，旨在通过优化提示贡献的组合来提升预测置信度。为此，我们设计了一种定制化的目标函数，通过最小化预测熵来生成新的提示权重，将低熵值作为高置信度的代理指标。该方法可应用于单个音频样本或批量样本，无需额外标注数据且计算开销可忽略不计。在涵盖环境声、城市声和人声的五类音频分类数据集上的实验表明，在零样本场景下，相比传统提示集成方法，本方法取得了稳定的性能提升，在整个基准测试中准确率提升幅度达到传统方法的五倍。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频-语言模型在零样本分类中展现出潜力，但性能对文本提示的措辞高度敏感，细微变化会导致准确率大幅波动。  
- **既有方法问题**：  
  - 提示学习需要标注数据且计算成本高。  
  - 提示集成方法（如多数投票或嵌入平均）未考虑部分提示可能对性能产生负面影响，在零样本设置中难以识别低质量提示。  

2)  
- **核心思路**：提出一种基于熵最小化的提示加权方法，通过优化目标函数自动学习各提示模板的权重，以生成高置信度（低熵）的预测，无需额外标注数据。  
- **方法细节**：  
  - **问题建模**：将音频样本特征与多个提示模板的文本嵌入计算相似度，通过加权平均得到类别logits，再经softmax生成预测概率。  
  - **目标函数**：包含三个部分：  
    - **预测置信度项**：最小化预测向量的熵，促使模型输出高置信度结果。  
    - **零样本正则项**：防止优化后的预测偏离初始零样本预测，保持稳定性。  
    - **权重熵正则项**：鼓励权重分布平滑，避免过度稀疏。  
  - **优化流程**：采用迭代固定点更新规则优化权重向量，仅需预计算一次特征嵌入，计算开销可忽略。  
- **创新点**：  
  - 将提示集成转化为权重优化问题，以熵为置信度代理指标。  
  - 支持单样本或批量样本的权重学习，灵活适应不同场景。  

3)  
- **任务与效果**：在五个音频分类数据集（涵盖环境、城市、人声任务）上评估：  
  - 相比传统提示集成方法（如多数投票、嵌入平均），平均准确率提升显著（最高达1.4%）。  
  - 在ESC-Actions数据集上，较零样本基线提升3.8%。  
  - 方法在零样本设置中一致优于基线，且计算效率高（运行时仅约0.2秒）。
</div>

</details>

---

## A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction
- **Authors**: Qing Wang, Zehan Li, Yaodong Song, Hongjie Chen, Jian Kang, Jie Lian, Jie Li, Yongxiang Li, Xuelong Li
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04960v1](https://arxiv.org/abs/2601.04960v1)
- **PDF**: [https://arxiv.org/pdf/2601.04960v1](https://arxiv.org/pdf/2601.04960v1)

本文提出了一种面向情感智能的统一口语模型，其通过一种称为“情感归因注入思维”（IEAT）的新型数据构建策略得到增强。IEAT将用户情感状态及其潜在成因融入模型的内部推理过程，使情感感知推理得以内化，而非作为显式监督信号处理。模型采用两阶段渐进策略进行训练：第一阶段通过自蒸馏实现语音-文本对齐与情感属性建模；第二阶段进行端到端的跨模态联合优化，以确保文本与口语情感表达的一致性。在类人对话系统挑战赛（HumDial）情感智能基准测试上的实验表明，所提方法在基于大语言模型和人工评估中，于情感轨迹建模、情感推理及共情回应生成等任务上均取得了领先性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：口语对话系统正从任务导向向情感智能演进，要求模型能理解并回应动态的人类情感。  
- **既有问题**：在统一的语音-语言框架内，实现语义与情感的一致性仍是一个重大挑战。现有方法往往将情感信息作为显式监督处理，未能将其内化为模型推理过程的一部分，导致情感理解与表达不够自然和连贯。

2)  
- **核心方法**：本文提出了一种名为**注入式情感归因思维（IEAT）**的数据构建与训练策略，并将其与一个基于GOAT-SLM架构的统一口语语言模型相结合。该方法通过两阶段渐进式训练策略实现。  
- **解决思路**：  
  - **IEAT策略**：在数据构建阶段，将用户的情感状态及其诱发原因直接“注入”到模型内部推理链（thinking）中。这使得情感感知推理被内化，成为模型认知过程的一部分，而非外部强加的标签。  
  - **两阶段训练**：  
    - **第一阶段**：专注于语音-文本对齐和情感属性建模。通过自蒸馏技术，从语义对齐逐步扩展到对语言内容、副语言信息和说话人特征的联合建模。  
    - **第二阶段**：进行端到端的跨模态联合优化。同时训练文本和语音生成路径，以提升跨模态的情感连贯性与语义一致性。语音生成分支采用模块化微调，在冻结大语言模型底层以保留基础语言知识的同时，微调顶层以预测语音token，并引入多token预测以支持实时流式TTS。  
- **整体优势**：这种统一架构结合IEAT，使模型能在单一框架内有效进行情感轨迹建模、情感推理和共情回应生成，解决了语义与情感在多模态表达中的一致性问题。

3)  
- **评测任务**：在Human-like Spoken Dialogue Systems Challenge (HumDial) 情感智能赛道的三个子任务上进行了评测，包括：情感轨迹检测、情感推理、共情回应生成。  
- **取得效果**：  
  - 在官方开发集和测试集上，该模型在**所有三个子任务**（中英文）上均取得了**排名第一或顶尖**的性能（基于LLM的评分，0-5分制）。  
  - 在最终的测试集综合评分中，以**4.27分**的总成绩获得**第一名**，超越了包括Qwen3-omni在内的多个强基线模型。  
  - 结果证明了该方法在实现情感智能方面的有效性和一致性。
</div>

</details>

---

## ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models
- **Authors**: Kaiwen Luo, Liang Lin, Yibo Zhang, Moayad Aloqaily, Dexian Wang, Zhenhong Zhou, Junwei Zhang, Kun Wang, Li Sun, Qingsong Wen
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04876v1](https://arxiv.org/abs/2601.04876v1)
- **PDF**: [https://arxiv.org/pdf/2601.04876v1](https://arxiv.org/pdf/2601.04876v1)

尽管音频大语言模型（ALLMs）已取得显著进展，但其长音频理解能力尚未得到充分探索。现有大量基准测试主要针对通用音频任务，且多聚焦于短音频片段，导致在评估ALLMs处理长音频方面缺乏共识。本文提出ChronosAudio，这是首个专为ALLMs长音频理解设计的综合性多任务基准测试。该基准涵盖六大任务类别，包含总计超过200小时音频的36,000个测试实例，并按短、中、长音频分层设计，以全面评估模型在长度泛化方面的表现。基于ChronosAudio对16个前沿模型进行广泛实验，得出三个关键发现：1. **长上下文急剧崩溃**：ALLMs在长音频场景下性能严重下降，从短音频转向长音频时，特定任务性能降幅超过90%；2. **注意力结构稀释**：性能下降源于模型无法保持时间局部性，注意力机制在序列后期出现显著扩散；3. **缓解策略的恢复上限**：现有优化方法仅能实现50%的性能恢复。这些发现揭示了长音频理解面临的重大挑战，凸显了开发具备稳健文档级音频推理能力的方法的迫切需求。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频大语言模型（ALLMs）发展迅速，但现有评估基准（如AudioBench、AIR-Bench）主要关注短音频片段（秒到分钟级），缺乏对长音频（数分钟至数十分钟）理解能力的系统评估。
- **既有问题**：当前ALLMs在长音频场景下的表现未知，且缺乏专门的多任务长音频基准来全面检验模型在扩展时间上下文中的性能、注意力机制稳定性及泛化能力。

2)  
- **基准构建**：论文提出了首个面向ALLMs的长音频多任务基准**ChronosAudio**，旨在系统评估模型的长上下文理解能力。
    - **任务设计**：涵盖六大任务类别，分为**时间感知**（听写、定位）、**逐字序列生成**（转录、多说话人识别）和**高级推理**（理解、摘要），覆盖关键长音频应用场景。
    - **数据分层**：包含36,000个测试实例，总时长超过200小时。音频按时长分为短（30秒~5分钟）、中（5~10分钟）、长（10~20分钟）三组，以全面评估长度泛化能力。
- **问题诊断**：通过在该基准上对16个先进模型进行实验，揭示了长音频理解中的三大关键问题：
    - **长上下文崩溃**：从短上下文过渡到长上下文时，模型性能急剧下降（特定任务降幅超过90%），且“中间丢失”现象比文本模型出现得更早。
    - **注意力稀释**：性能下降源于无法保持时间局部性；注意力机制在序列后期出现显著扩散，导致高分辨率对齐丢失。
    - **缓解策略的天花板**：现有长上下文策略（如稀疏注意力）仅能恢复模型短上下文能力的约50%，表明简单的架构调整不足以实现文档级音频推理。

3)  
- **评估任务**：在ChronosAudio涵盖的六大任务（听写、定位、转录、多说话人识别、理解、摘要）上，对16个开源与闭源SOTA模型进行了全面评估。
- **取得的效果**：
    - **揭示了性能缺陷**：所有模型在长音频上均表现不佳，尤其是开源模型在长音频摘要任务中平均得分仅为14.14，而闭源模型为59.22，差距显著。
    - **量化了性能下降**：实验表明，从短音频切换到长音频时，模型性能平均下降约18个点，转录任务得分从42.66（短）暴跌至3.86（长）。
    - **验证了缓解策略的局限性**：稀疏注意力等机制能部分提升性能（如在听写任务中恢复至短上下文能力的93%），但对保真度要求高的任务（如转录）仅能恢复约50%的能力，凸显了当前方法的不足。
</div>

</details>

---

## Gradient-based Optimisation of Modulation Effects
- **Authors**: Alistair Carson, Alec Wright, Stefan Bilbao
- **Categories**: eess.AS, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04867v1](https://arxiv.org/abs/2601.04867v1)
- **PDF**: [https://arxiv.org/pdf/2601.04867v1](https://arxiv.org/pdf/2601.04867v1)

调制效果器（如移相器、镶边器和合唱效果器）在电吉他演奏中被广泛使用。近年来，基于机器学习的模拟调制单元仿真技术已得到研究，但多数方法要么仅限于单一类型的效果器，要么相比经典数字实现方式存在高计算成本或延迟问题。本文基于先前研究，提出了一种基于可微分数字信号处理的镶边、合唱及移相效果建模框架。该模型在训练阶段使用时频域方法，而在推理阶段完全在时域运行，实现零延迟。我们探讨了此类效果器基于梯度优化的挑战，并证明在损失函数中引入低频加权可避免学习延迟时间时陷入局部极小值。实验表明，当针对模拟效果器单元进行训练时，模型输出的声音在某些情况下与参考信号在感知上难以区分，但对于具有长延迟时间和反馈的效果器，仍存在优化挑战。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：调制效果器（如镶边、合唱、移相）在音乐制作中广泛应用。近年来，基于机器学习的模拟调制单元仿真受到关注。
- **既有问题**：
  - 现有方法通常仅针对单一效果类别，缺乏通用性。
  - 许多方法（如基于神经网络的模型）计算成本高或存在延迟，难以满足实时处理需求。
  - 循环神经网络等模型在模拟长延迟时间（如镶边效果）时存在困难，且对线性时变效果的建模不够理想。

2)  
- **核心框架**：提出基于可微分数字信号处理（DDSP）的通用建模框架，可统一处理镶边、合唱和移相效果。
- **训练与推理分离**：
  - **训练阶段**：在时频域进行，使用短时帧的频率采样方法近似时变线性滤波器，通过损失函数优化参数。
  - **推理阶段**：完全在时域运行，实现零延迟处理，满足实时性需求。
- **关键技术点**：
  - **低频加权损失函数**：针对延迟时间估计问题，分析发现梯度下降易陷入局部最优。通过使用低通滤波核（如三角脉冲）作为输入信号或在损失函数中加入预加重滤波，对低频成分进行加权，拓宽损失曲面的凸区域，显著提升优化稳定性。
  - **参数化模型结构**：模型核心是一个相位偏移滤波器，可以是插值延迟线（用于镶边/合唱）或级联的一阶全通滤波器（用于移相）。结构包含可学习的双二阶滤波器和由查找表+MLP生成的低频振荡器控制信号。
  - **反馈配置灵活性**：提供了两种反馈路径配置（滤波器在反馈环内或外），以适应不同效果需求。
- **解决既有问题**：该框架避免了黑箱神经网络的高计算开销，通过引入信号处理先验知识实现轻量化；通用结构支持多种调制效果；时域推理消除了延迟；低频加权策略有效解决了长延迟时间优化中的收敛难题。

3)  
- **任务与效果**：
  - **数字效果参数复现（简化问题）**：在已知目标参数的数字化镶边和移相效果上，模型能准确恢复参数，验证了框架有效性。
  - **模拟效果器建模**：
    - **Boss BF-2镶边踏板**：在无反馈设置下，最佳模型输出与目标信号的误差谱比（ESR）达-16 dB，感知评估中与参考无法区分。但对于带反馈的长延迟情况，建模仍存挑战。
    - **Marshall SV-1合唱踏板**：模型成功复现了不同“Wave”控制设置下的效果，需至少2个通道以获得良好效果。
    - **EHX Small Stone移相踏板**：无论是否带反馈，模型输出在感知评估中均与参考无法区分，最佳ESR达-13.57 dB。
- **总结**：该方法在多数情况下能实现与模拟参考感知上无法区分的建模效果，尤其在移相和无反馈镶边/合唱任务上表现优异，但对带反馈的长延迟效果建模仍需进一步改进。
</div>

</details>

---

## Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling
- **Authors**: Xingyuan Li, Mengyue Wu
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.04744v1](https://arxiv.org/abs/2601.04744v1)
- **PDF**: [https://arxiv.org/pdf/2601.04744v1](https://arxiv.org/pdf/2601.04744v1)

从语音声学特征中检测医疗状况本质上是一个弱监督学习问题：单一且通常带有噪声的会话级标签必须与长而复杂的音频记录中的细微模式相关联。该任务还受到数据严重稀缺和临床标注主观性的进一步阻碍。尽管半监督学习为利用未标注数据提供了可行路径，但现有音频方法往往未能解决核心挑战——病理特征在患者语音中并非均匀表达。我们提出了一种新颖的纯音频半监督学习框架，通过从未分割的临床对话中联合学习帧级、片段级和会话级表征，显式建模这种层次结构。我们的端到端方法动态聚合这些多粒度特征，并生成高质量伪标签以高效利用未标注数据。大量实验表明，该框架具有模型无关性，在跨语言和跨病症场景中表现稳健，且数据效率极高——例如仅使用11个标注样本即可达到全监督性能的90%。本研究为医学语音分析中基于弱远端监督的学习提供了系统化方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：利用语音声学特征进行疾病检测面临数据稀缺、标签噪声和弱监督三大挑战。临床对话通常只有一个会话级标签，但病理特征在长对话中稀疏、非均匀地出现。
- **既有方法问题**：现有方法常将长录音分割为独立片段处理，隐含假设症状均匀表达，这与实际不符。同时，通用半监督学习方法难以直接迁移到存在显著领域偏移的临床语音分析中。

2)  
论文提出一种新颖的、仅使用音频的半监督学习框架，通过分层建模多粒度信息来解决上述问题：
- **核心架构**：框架同时建模**会话级、片段级和帧级**三个层次。会话级主干使用Transformer聚合整个对话的表示；片段级利用RNN和从主干生成的伪标签学习话语级特征；帧级采用师生网络和对比损失，增强对底层声学模式的鲁棒性。
- **动态伪标签生成**：采用**单阶段、端到端**的训练方式，在线动态更新会话级和片段级的高质量伪标签，有效利用未标注数据，无需额外推理成本。
- **解决弱监督与稀疏性**：该方法**不假设症状在每段话语中均匀出现**，而是通过多粒度建模和动态聚合，让模型学会识别对话中与病理最相关的关键片段，从而匹配远端的会话级监督信号。
- **增强鲁棒性与数据效率**：多层级损失（会话分类损失、片段分类损失、帧级一致性损失）的加权组合，使模型在标注数据极少和标签有噪声的情况下仍能稳健学习。

3)  
该方法在以下任务上取得了显著效果：
- **抑郁症检测**（中文EATD-Corpus数据集）和**阿尔茨海默症检测**（英文ADReSSo21数据集）。
- **数据效率极高**：仅使用约**11个标注样本（10%数据）** 即可达到全监督性能的90%；使用**30%的标注数据**时，性能已与使用100%数据的全监督基线相当甚至更优。
- **性能提升**：在全部标注比例下均显著超越基线，例如在50%数据比例时抑郁症检测F1值提升4.59%。方法在完全监督设定下也优于基线，证明了多粒度建模本身的价值。
- **模型无关性与鲁棒性**：在wav2vec2、HuBERT、WavLM等多种语音编码器上均有效，且能处理包含调查者语音的原始对话，无需说话人分离。
</div>

</details>

---

## LAMB: LLM-based Audio Captioning with Modality Gap Bridging via Cauchy-Schwarz Divergence
- **Authors**: Hyeongkeun Lee, Jongmin Choi, KiHyun Nam, Joon Son Chung
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.04658v1](https://arxiv.org/abs/2601.04658v1)
- **PDF**: [https://arxiv.org/pdf/2601.04658v1](https://arxiv.org/pdf/2601.04658v1)

自动音频描述旨在对输入音频的语义内容进行描述。近期研究采用大语言模型作为文本解码器以利用其推理能力。然而，先前方法将音频特征直接投影至LLM嵌入空间而未考虑跨模态对齐，导致无法充分发挥这些能力。为此，我们提出LAMB——一种基于LLM的音频描述框架，通过柯西-施瓦茨散度桥接音频嵌入与LLM文本嵌入空间之间的模态鸿沟。LAMB引入跨模态对齐器，在最大化互信息的同时最小化柯西-施瓦茨散度，实现音频与文本在全局和词元层面的紧密对齐。我们进一步设计双流适配器，提取语义增强的音频嵌入，从而为跨模态对齐器提供更丰富的信息。最终，利用对齐后的音频嵌入，所提出的词元引导器直接在LLM文本嵌入空间内计算得分，以引导生成描述的输出逻辑。实验结果表明，我们的框架增强了LLM解码器的推理能力，在AudioCaps数据集上取得了最先进的性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动音频描述（AAC）旨在生成音频内容的文本描述。近期研究采用大语言模型（LLM）作为文本解码器，以利用其推理能力。  
- **既有问题**：现有方法（如线性投影或Q-Former）将音频特征映射到LLM嵌入空间时，未考虑跨模态对齐，导致音频表示与LLM文本嵌入空间之间存在模态鸿沟，限制了LLM解码器对音频语义的理解能力。

2)  
- **核心方法**：LAMB框架通过三个关键组件解决模态鸿沟问题：  
  - **双流适配器**：并行提取音频的语义特征（通过可学习查询的注意力机制）和时序特征（通过卷积与GRU），融合后生成信息更丰富的音频嵌入。  
  - **跨模态对齐器**：使用柯西-施瓦茨散度与互信息损失，在全局和词元级别对齐音频嵌入与文本嵌入分布，最小化模态差异。  
  - **词元引导器**：在LLM文本嵌入空间中，计算对齐后音频嵌入与LLM词元嵌入的L2距离作为引导分数，直接调整解码器输出的logits，无需外部模块。  
- **训练目标**：结合跨模态对齐损失、引导生成损失和解码器交叉熵损失进行端到端优化。

3)  
- **任务与效果**：在AudioCaps和Clotho数据集上进行评测。  
  - **AudioCaps**：在METEOR、CIDEr、SPICE、SPIDEr等指标上均达到最优性能，显著超越之前方法（如CIDEr提升至91.1）。  
  - **Clotho**：在多个指标上取得竞争性结果，部分指标（如CIDEr）优于现有方法。  
- **结论**：通过弥合模态鸿沟，LAMB有效提升了LLM在音频理解任务中的推理能力，生成更准确、连贯的描述。
</div>

</details>

---

## FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions
- **Authors**: Dekun Chen, Xueyao Zhang, Yuancheng Wang, Kenan Dai, Li Ma, Zhizheng Wu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04656v1](https://arxiv.org/abs/2601.04656v1)
- **PDF**: [https://arxiv.org/pdf/2601.04656v1](https://arxiv.org/pdf/2601.04656v1)

本研究提出FlexiVoice，一种具备零样本语音克隆能力的文本转语音合成系统，能够通过自然语言指令实现灵活的语音风格控制。该系统以大型语言模型为核心，接收文本输入，并可选择性地接受自然语言指令和语音参考，分别控制语音风格与音色特征。FlexiVoice采用创新的渐进式后训练方案，逐步实现精准灵活的可控性：首先通过直接偏好优化技术，使系统能同时准确遵循自然语言指令与语音参考；随后采用多目标群体相对策略优化方法，解耦风格指令、参考音色与文本内容；最后通过指令优化策略进一步提升复杂指令跟随能力。实验结果表明，FlexiVoice在各项基准测试中均优于现有方法，展现出优异的控制因子解耦能力。人工评估进一步证实了该系统在自然度、可控性与鲁棒性方面的卓越表现。音频示例详见https://flexi-voice.github.io。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：零样本TTS和基于指令的TTS技术快速发展，但现有方法在同时控制音色和风格时面临挑战。  
- **既有问题**：模型常出现“风格-音色-内容冲突”，即过度依赖参考语音的音色先验（音色泄漏）或从文本推断韵律（内容泄漏），而忽略显式的风格指令，导致指令跟随不准确、音色一致性不稳定。

2)  
论文提出 **FlexiVoice** 系统，其核心是通过 **渐进式后训练（PPT）** 框架解决多模态冲突问题，具体分为三个阶段：  
- **S1: 多模态DPO**：使用直接偏好优化（DPO），在情感任务上对齐模型，使其能同时跟随自然语言指令和参考语音。  
- **S2: 解耦GRPO**：采用多目标组相对策略优化（GRPO），构建指令与参考语音/文本内容冲突的场景，通过奖励函数（如情感识别奖励、说话人验证奖励）强制模型分离风格、音色和内容。  
- **S3: 指令GRPO**：基于音频语言模型（ALM）的奖励，扩展模型对复杂、开放域指令的跟随能力，同时混合S2数据以防止灾难性遗忘。  

PPT框架通过从简单到复杂的课程学习，将指令跟随从简单的条件控制转变为严格的解耦过程，从而实现对风格、音色和内容的鲁棒解耦与控制。

3)  
- **多模态控制与解耦任务**：在情感TTS任务上，FlexiVoice在指令跟随准确率（ACC-I）上大幅超越基线（如英文TR-Hard任务达78.2%），同时有效降低文本/参考语音的干扰（ACC-T/R显著降低），证明了其解耦能力。  
- **复杂指令跟随任务**：在InstructTTSEval基准测试中，FlexiVoice在英语和中文任务上平均准确率分别达到79.3%和70.8%，超越所有开源基线，并接近闭源商业系统（如Gemini-pro）。  
- **主观评估**：人类评估确认其生成语音的自然度、可控性和鲁棒性均优于基线，且保持了较高的音质MOS分数。
</div>

</details>

---

## LLMs-Integrated Automatic Hate Speech Recognition Using Controllable Text Generation Models
- **Authors**: Ryutaro Oshima, Yuya Hosoda, Youji Iiguni
- **Categories**: eess.AS, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04654v1](https://arxiv.org/abs/2601.04654v1)
- **PDF**: [https://arxiv.org/pdf/2601.04654v1](https://arxiv.org/pdf/2601.04654v1)

本文提出一种基于大语言模型（LLM）的仇恨语音自动识别（ASR）模型。该方法将ASR模型的编码器与LLM的解码器相结合，能够同步完成语音转写与内容审查任务，从而避免有害内容的传播。为通过指令微调使LLM能够用特定标记屏蔽仇恨相关词汇，需要标注的仇恨语音数据集，但此类数据较为稀缺。为此，我们采用思维链（CoT）提示技术，结合文化背景与示例指导LLM生成文本样本，再通过文本转语音（TTS）系统将其转换为语音样本。然而，部分生成样本虽包含仇恨相关词汇却实属非仇恨内容，这会降低审查性能。本文通过文本分类模型筛选出被正确标注为仇恨内容的样本，并通过调整正确分类模型数量的阈值，可控制生成数据集的仇恨程度，从而以课程学习方式渐进训练LLM。实验结果表明，所提方法对仇恨相关词汇的屏蔽准确率达到58.6%，优于现有基线方法。研究同时验证了课程训练对提升转写与审查任务效率的积极作用。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频社交媒体平台（如YouTube）上仇恨言论的快速传播对社会和商业环境构成威胁。现有方法主要针对文本内容，或采用级联系统（ASR转录后文本分类）进行检测。
- **既有问题**：
  - 级联方法（如ASR+BERT）依赖LIME等事后解释技术，难以准确识别惯用语、微妙语境及单个语句中的多个仇恨相关词。
  - 缺乏大规模带标注的仇恨语音数据集，限制了基于大语言模型（LLM）的指令微调。
  - 直接使用LLM生成训练数据时，可能产生包含仇恨词但非仇恨内容的样本，降低模型审查性能。

2)  
论文提出一种集成ASR编码器与LLM解码器的多任务模型，实现语音转录与仇恨词屏蔽的同步处理。其核心方法通过数据生成、过滤与课程学习解决上述问题：

- **可控数据生成与过滤**：
  - 基于思维链（CoT）提示技术，引导LLM生成包含指定仇恨关键词的文本样本，模拟真实仇恨言论的语境与文化背景。
  - 使用五个预训练文本分类模型对生成样本进行打分，根据模型一致判定为“仇恨”的数量分配仇恨等级（0-5）。
  - 通过设定阈值过滤低质量样本，控制生成数据集的仇恨强度，为课程学习提供分层数据。

- **模型架构与训练策略**：
  - 架构上集成ASR编码器（Whisper）与LLM解码器（Qwen2.5），通过Q-Former对齐语音与文本特征。
  - 采用分阶段训练：先微调ASR编码器以准确识别仇恨词；再冻结编码器，通过指令微调（配合多样化提示）和激活调优（KL散度损失）训练解码器，使其同时完成转录与屏蔽任务。
  - 引入课程学习：按仇恨等级由低到高逐步训练模型，提升对复杂仇恨内容的学习效率与多任务平衡能力。

3)  
- **任务**：在自动语音识别（ASR）与仇恨言论审查的双任务场景中，评估模型在转录准确性与仇恨词屏蔽能力上的表现。
- **效果**：
  - 在屏蔽准确率（MAR）上达到58.6%，优于所有基线模型（ASR-BERT: 36.7%, ASR*-BERT: 54.4%）。
  - 课程学习策略（Curriculum-2）进一步将MAR提升至59.4%，并降低非屏蔽词错误率（UMWER），表明其能更高效地平衡转录与审查任务。
  - 模型成功实现单语句中多个仇恨词的同步屏蔽，解决了传统方法（如LIME）的局限性。
</div>

</details>

---

## Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony
- **Authors**: Joonwon Seo, Mariana Montiel
- **Categories**: cs.LG, cs.SD, math-ph
- **arXiv**: [https://arxiv.org/abs/2601.04592v1](https://arxiv.org/abs/2601.04592v1)
- **PDF**: [https://arxiv.org/pdf/2601.04592v1](https://arxiv.org/pdf/2601.04592v1)

传统循环神经网络（RNN）将音乐语境压缩为确定性的隐藏状态向量，这种信息瓶颈无法捕捉音乐固有的模糊性。本文提出密度矩阵循环神经网络（DM-RNN），这是一种基于密度矩阵的新型理论架构。该模型能够维持音乐解释的统计系综（混合态），同时捕捉经典概率与量子相干性。我们采用量子通道（CPTP映射）严格定义了时序动态特性，并基于Choi-Jamiołkowski同构详细阐述了参数化策略，确保通过学习获得的动态特性在结构上始终保持物理有效性（CPTP）。通过引入冯·诺依曼熵量化音乐不确定性的分析框架，以及利用量子互信息（QMI）测量声部间纠缠关系，DM-RNN为建模复杂模糊的音乐结构提供了数学严谨的理论框架。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐序列建模面临核心挑战是音乐固有的**模糊性**（如一个和弦后存在多种可能的旋律走向）。  
- **既有方法问题**：传统RNN/LSTM等模型将历史上下文压缩为**单一确定性隐藏状态向量**，这形成了一个信息瓶颈，迫使模型坍缩为一个点估计，无法表示音乐中可能性的统计分布，从而难以捕捉结构性的模糊与多义性。

2)  
论文提出**密度矩阵RNN**，以量子信息论中的密度矩阵作为隐藏状态，从根本上改变了表示与演化方式：  
- **状态表示**：用密度矩阵 ρ_t ∈ D(ℋ_d) 替代传统向量 h_t。  
  - **对角元**表示系统处于不同潜在基态（如不同音乐解释）的经典概率。  
  - **非对角元**（相干性）编码不同基态间的相位关系，能表示纯叠加态，从而区分结构性模糊与认知不确定性。  
- **动态演化**：通过**量子通道**（完全正定保迹映射）定义状态演化 ρ_{t-1} → ρ_t，确保变换的物理有效性。  
- **参数化实现**：利用**Choi-Jamiołkowski同构**，通过神经网络生成Choi矩阵的Cholesky因子，再经可微归一化得到Kraus算子，从而**从构造上保证**学到的动态是CPTP的。  
- **预测机制**：通过POVM测量（玻恩规则）从 ρ_t 得到下一个符号的概率分布。  
- **分析框架**：引入冯诺依曼熵量化音乐不确定性，引入量子互信息量化多声部间的关联/纠缠。

3)  
- **任务**：本论文为**理论框架**，未报告具体实验，但明确了其建模目标为**音乐序列建模**，特别是捕捉**模糊性**与**复调关联**。  
- **效果**：  
  - 框架**理论上能区分**经典混合态与相干叠加态，从而更精细地表示音乐中的结构性模糊。  
  - 提供了基于矩阵谱分析和量子信息度量（熵、互信息）的**全新分析工具**，用于量化音乐张力、解决以及声部间相关性。  
  - 指出了实际应用需解决计算复杂度问题，并建议未来使用**矩阵乘积算子**等张量网络方法实现。
</div>

</details>

---

## When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict
- **Authors**: Dawei Huang, Yongjie Lv, Ruijie Xiong, Chunxiang Jin, Xiaojiang Peng
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04564v1](https://arxiv.org/abs/2601.04564v1)
- **PDF**: [https://arxiv.org/pdf/2601.04564v1](https://arxiv.org/pdf/2601.04564v1)

语音情感识别系统通常默认语音情感与词汇语义之间存在一致性。然而在实际交互中，声学与语义的冲突普遍存在却常被忽视——即语调传递的情感与字面含义相互矛盾。本研究发现，当前最先进的语音情感识别模型（包括基于自动语音识别的方法、自监督学习模型及音频语言模型）在此类冲突下均会出现性能下降，其原因可归结为语义偏差或声学-语义表征的纠缠。为解决这一问题，我们提出融合声学-语义框架，通过显式解耦声学与语义通路，并借助轻量级基于查询的注意力模块建立二者关联。为支持系统化评估，我们构建了首个以多场景下清晰可解释的声学-语义冲突为核心的数据集。大量实验表明，该框架在领域内与零样本场景下均持续优于现有方法。值得注意的是，在冲突数据集基准测试中，传统语音情感识别模型表现严重失效，而本框架以59.38%的准确率创造了新的性能纪录。相关代码与数据集已开源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：真实语音交互中，声学（语调）与语义（字面意思）常存在冲突（如讽刺、幸灾乐祸），而现有语音情感识别（SER）方法假设二者一致，导致实际应用受限。  
- **既有问题**：  
  - 基于ASR或语音-文本预训练的编码器（如Whisper、CLAP）存在语义偏差，易被字面意思误导。  
  - 自监督学习模型（如HuBERT、WavLM）的表示中声学与语义信息纠缠，难以区分冲突。  
  - 现有多模态融合方法缺乏有效仲裁机制，且评测数据集中冲突样本稀疏、不系统。

2)  
论文提出**融合声学-语义框架**，通过解耦与交互解决上述问题：  
- **双路解耦**：  
  - **声学通路**：采用神经音频分词器（如MingTok-Audio）提取低维、纯净的声学表征，专注捕捉韵律、音色等情感线索。  
  - **语义通路**：使用预训练语音识别编码器（如Whisper）提取高维语义表征，专注文本内容。  
- **动态融合模块**：  
  - **特征蒸馏**：对两路特征进行分块化后，基于L2范数计算显著度得分，筛选Top-K高能量片段，保留情感关键信息。  
  - **查询式注意力**：引入可学习查询向量，通过跨注意力机制主动从蒸馏后的声学与语义序列中交互信息，生成融合表征。  
  - **预测头**：通过轻量级MLP输出最终情感分类结果。  
- **优势**：  
  - 显式解耦避免语义偏差或特征纠缠。  
  - 动态融合能仲裁冲突信号，增强对讽刺等复杂情感的鲁棒性。  
  - 框架轻量且可扩展，支持不同声学/语义编码器组合。

3)  
- **评测任务**：在内部训练集（MELD、RAVDESS、ESD）和零样本跨域数据集（CASE、Emo-Emilia、EMOVO、EmoDB）上进行评估。  
- **效果**：  
  - 在内部测试集上平均准确率达71.92%，优于现有SOTA方法。  
  - 在专门针对声学-语义冲突构建的**CASE基准**上，达到59.38%准确率，显著超越传统模型（如HuBERT 32.90%、Whisper 47.26%）。  
  - 在跨语言零样本任务（如EmoDB、EMOVO）中表现稳健，验证了框架的泛化能力。
</div>

</details>

---

## WESR: Scaling and Evaluating Word-level Event-Speech Recognition
- **Authors**: Chenchen Yang, Kexin Huang, Liwei Fan, Qian Tu, Botian Jiang, Dong Zhang, Linqi Yin, Shimin Li, Zhaoye Fei, Qinyuan Cheng, Xipeng Qiu
- **Categories**: cs.CL, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04508v1](https://arxiv.org/abs/2601.04508v1)
- **PDF**: [https://arxiv.org/pdf/2601.04508v1](https://arxiv.org/pdf/2601.04508v1)

语音不仅传递语言信息，还包含丰富的非语言声音事件，如笑声和哭泣。尽管语义转录已得到充分研究，但非语言事件的精确定位仍是一个关键而尚未深入探索的挑战。现有方法存在任务定义不足的问题，表现为类别覆盖有限、时间粒度模糊，且缺乏标准化评估框架，阻碍了下游应用的发展。为填补这一空白，我们首先构建了一个包含21类声音事件的精细化分类体系，将其重新划分为离散型（独立存在）与连续型（与语音混合）两类。基于该分类体系，我们提出了WESR-Bench——一个由专家标注的评估集（包含900余条话语），采用创新的位置感知标注协议，将语音识别错误与事件检测分离，从而实现对离散和连续事件的精确定位评估。此外，我们构建了一个超过1,700小时的数据集，并训练了专用模型，在保持语音识别质量的同时，性能超越了开源音频-语言模型及商业API。我们期待WESR能为未来建模丰富真实听觉场景的研究提供基础性资源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音不仅传递语义信息，还包含丰富的非语言声音事件（如笑声、咳嗽）。现有方法在事件感知的语音识别方面存在不足。  
- **既有问题**：  
  - **任务定义不充分**：现有数据集事件类别覆盖有限，时间粒度模糊，且缺乏对连续事件（如边笑边说）的明确定义。  
  - **评估框架缺失**：当前评估多依赖句子级分类准确率或词错误率，无法精确衡量事件在词级的位置对齐，尤其难以处理同一语句中的多个事件或连续事件。  

2)  
论文通过构建**WESR**（词级事件-语音识别）任务体系来解决上述问题，具体包括：  

- **完善任务定义与分类体系**：  
  - 提出包含21类声音事件的精细分类法，并将其区分为**离散事件**（如`[laughs]`）和**连续事件**（如`<laughing>...</laughing>`），为建模提供了清晰框架。  

- **建立标准化评估基准WESR-Bench**：  
  - 构建由专家标注的900+语句双语测试集，涵盖21类事件。  
  - 设计**位置感知的评估协议**：  
    - 通过**事件保留对齐**步骤，将假设文本与参考文本在词级对齐，同时保留事件标签。  
    - 将事件映射到**词位置**（连续事件）或**词间位置**（离散事件），统一表示。  
    - 基于对齐后的位置计算每个事件类别的精确率、召回率与F1分数，支持对多事件和连续事件的精确评估。  

- **构建大规模训练数据与强基线模型**：  
  - 整合现有数据集并利用Gemini API自动标注，构建1,700+小时的**WESR-Train**训练语料。  
  - 在多个骨干模型（如Whisper、Qwen3-Omni）上进行监督微调，得到专用WESR模型。  
  - 模型能够同时输出转录文本和词级对齐的事件标签，在保持ASR质量的同时，显著提升事件检测与定位性能。  

3)  
- **任务**：在双语（英/中）词级事件-语音识别任务上进行了评估。  
- **效果**：  
  - 微调后的WESR模型（如WESR-Qwen）在WESR-Bench上取得了最佳性能，**宏观平均F1达到38.0%**，显著超过开源音频语言模型（如Qwen3-Omni）和商业API（如Gemini-2.5-Pro）。  
  - 在连续事件（如耳语、唱歌）和离散事件（如咳嗽、笑声）上均表现优异，同时**在Common Voice基准上的ASR词错误率仅轻微上升**，表明其在提升事件识别能力的同时基本保持了语音转文本的准确性。
</div>

</details>

---
