---
layout: post
title: "arXiv Daily – 2026-01-09"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-09（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-08 08:50 — 2026-01-09 08:50
- 抓取总数：11 篇 | 本页显示：11 篇（去重/过滤后）

## Leveraging Prediction Entropy for Automatic Prompt Weighting in Zero-Shot Audio-Language Classification
- **Authors**: Karim El Khoury, Maxime Zanella, Tiffanie Godelaine, Christophe De Vleeschouwer, Benoit Macq
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.05011v1](https://arxiv.org/abs/2601.05011v1)
- **PDF**: [https://arxiv.org/pdf/2601.05011v1](https://arxiv.org/pdf/2601.05011v1)

音频-语言模型近期通过利用自然语言监督实现无标注训练数据的音频事件分类，展现出强大的零样本能力。然而，其性能对文本提示的措辞极为敏感，微小的变动会导致准确率大幅波动。先前研究通过提示学习或提示集成缓解了这一问题，但这些策略要么需要标注数据，要么未能考虑某些提示可能对性能产生负面影响。本研究提出一种基于熵的提示加权方法，旨在通过优化提示贡献的组合来最大化预测置信度。为此，我们设计了一个定制化的目标函数，通过最小化预测熵来生成新的提示权重，将低熵值作为高置信度的代理指标。该方法可应用于单个音频样本或样本批次，无需额外标注且计算开销可忽略不计。在涵盖环境声、城市声和人声的五类音频分类数据集上的实验表明，在零样本场景下，相比传统提示集成方法，本方法实现了稳定的性能提升，在整个基准测试中准确率提升幅度达到传统方法的五倍。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：音频-语言模型在零样本分类中展现出潜力，但其性能对文本提示的措辞高度敏感，细微变化会导致准确率大幅波动。  
- **既有方法问题**：现有方法如提示学习需要标注数据，计算成本高；提示集成方法（如多数投票或嵌入平均）未考虑部分提示可能对性能产生负面影响，在零样本设置下难以有效识别并处理低质量提示。  

2.  
- **核心思路**：提出一种基于熵最小化的提示加权方法，将提示集成转化为寻找最优权重组合的优化问题，以低熵作为高预测置信度的代理指标，无需额外标注数据。  
- **方法框架**：  
  - **问题建模**：对每个音频样本，计算其与所有提示模板的相似度（logits），通过加权平均得到最终logits，再经softmax生成预测概率。权重向量β需满足概率单纯形约束。  
  - **目标函数**：包含三项：  
    （i）**预测置信度项**：最小化预测向量的熵，促使模型输出高置信度预测；  
    （ii）**零样本正则项**：防止预测过度偏离初始零样本预测（使用单一标准模板），保持稳定性；  
    （iii）**熵正则项**：对权重向量β施加熵正则化，避免权重过于稀疏，促进平滑分布。  
  - **优化流程**：采用迭代固定点更新规则优化β（见算法1），所有音频和文本嵌入仅需计算一次，计算开销可忽略。  
- **扩展策略**：  
  - 支持**单样本优化**（为每个样本学习独立β）或**数据集级优化**（为整个数据集学习共享β）。  
  - 引入**剪枝机制**：多轮优化中逐步剔除低权重提示，进一步提升性能。  

3.  
- **任务与效果**：在涵盖环境声音（ESC-50、ESC-Actions）、城市声音（US8K、SESA）和声音（VS）的五类零样本音频分类数据集上评估。  
- **性能提升**：相比基线集成方法（如多数投票、嵌入平均），所提方法在平均准确率上获得显著提升（最高提升1.4%），其中在ESC-Actions数据集上相对零样本预测提升达3.8%。优化后的权重能自动降低低质量提示的影响，且计算效率高（运行时延可忽略）。
</div>

</details>

---

## A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction
- **Authors**: Qing Wang, Zehan Li, Yaodong Song, Hongjie Chen, Jian Kang, Jie Lian, Jie Li, Yongxiang Li, Xuelong Li
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04960v1](https://arxiv.org/abs/2601.04960v1)
- **PDF**: [https://arxiv.org/pdf/2601.04960v1](https://arxiv.org/pdf/2601.04960v1)

本文提出了一种面向情感智能的统一口语语言模型，其核心创新在于引入了一种名为“情感归因注入思维”的数据构建策略。该策略将用户情感状态及其潜在成因融入模型的内部推理过程，使情感感知推理内化为模型能力，而非依赖显式监督信号。模型采用两阶段渐进式训练策略：第一阶段通过自蒸馏实现语音-文本对齐与情感属性建模；第二阶段进行端到端跨模态联合优化，确保文本与语音情感表达的一致性。在类人对话系统挑战赛情感智能基准上的实验表明，该方法在情感轨迹建模、情感推理及共情回应生成任务中均取得领先性能，该结论在基于大语言模型与人工评估中均得到验证。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：口语对话系统正从任务导向向情感智能演进，要求模型能理解并回应动态的人类情感。然而，在统一的语音-语言框架内实现语义与情感的一致性仍是一个重大挑战。
- **既有方法的问题**：现有方法通常将情感信息作为显式监督信号处理，未能将其内化为模型内部推理过程的一部分，导致情感理解与生成在跨模态（语音与文本）之间可能缺乏连贯性。

2)  
论文通过提出**注入式情感归因思维（IEAT）** 和**两阶段渐进式训练策略**来解决上述问题，具体如下：
- **核心方法：IEAT数据构建策略**
    - **情感内化**：IEAT将用户的情感状态及其诱发原因直接注入到模型的内部“思考”过程中，使情感感知推理成为模型认知的一部分，而非外部监督标签。
    - **数据构造**：利用LLM（如Qwen3-8B）生成训练数据时，模仿其内部思考模式，将情感信息（如通过emo2vec模型从音频中提取的情感标签）插入到推理链中，引导模型生成包含情感归因的响应。
- **两阶段渐进式训练**
    - **第一阶段**：专注于语音-文本对齐和情感属性建模。
        - 首先通过自蒸馏进行跨模态语义对齐。
        - 随后引入IEAT，联合建模语言内容、副语言信息和说话人特征，使模型能从语音中直接提取情感线索。
    - **第二阶段**：进行端到端的跨模态联合优化。
        - 同时训练文本和语音生成路径，确保跨模态的情感一致性与语义连贯性。
        - 语音生成分支采用模块化微调（微调LLM的顶层以预测语音token，冻结底层以保留基础语言知识），并引入多token预测以支持实时流式TTS合成。
- **统一架构**：模型基于GOAT-SLM架构，在共享语义空间中处理语音和文本，遵循“听-思-写-说”范式，实现统一的推理与生成。

3)  
论文在**HumDial挑战赛情感智能赛道**的三个子任务上进行了评估，并取得了最佳效果：
- **任务表现**：在官方开发集和测试集上，模型在**情感轨迹建模（Task1）、情感推理（Task2）和共情响应生成（Task3）** 上均取得了排名第一或顶尖的综合性能（评分范围0-5，分数越高越好）。
- **评估结果**：模型在中文和英文测试中均表现优异，并且在基于LLM的自动评估和人工评估中均保持领先。最终测试集总得分（4.27）排名第一，证明了所提方法的有效性。
</div>

</details>

---

## ChronosAudio: A Comprehensive Long-Audio Benchmark for Evaluating Audio-Large Language Models
- **Authors**: Kaiwen Luo, Liang Lin, Yibo Zhang, Moayad Aloqaily, Dexian Wang, Zhenhong Zhou, Junwei Zhang, Kun Wang, Li Sun, Qingsong Wen
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04876v1](https://arxiv.org/abs/2601.04876v1)
- **PDF**: [https://arxiv.org/pdf/2601.04876v1](https://arxiv.org/pdf/2601.04876v1)

尽管音频大语言模型（ALLMs）已取得显著进展，但其长音频理解能力尚未得到充分探索。现有大量基准测试主要针对通用音频任务，且多聚焦于短片段音频，导致在评估ALLMs处理长音频方面缺乏共识。本文提出ChronosAudio，这是首个专为ALLMs长音频理解设计的综合性多任务基准测试。该基准涵盖六大任务类别，包含总计超过200小时音频的36,000个测试实例，并按短、中、长三种时长分层，以全面评估模型在长度泛化方面的表现。基于ChronosAudio对16个前沿模型进行广泛实验，得出三个关键发现：1. **长上下文性能骤降**：ALLMs在长音频场景下表现严重受限，从短上下文过渡到长上下文时，特定任务性能下降幅度超过90%；2. **注意力结构稀释**：性能下降源于模型无法保持时间局部性，注意力机制在序列后期出现显著扩散；3. **缓解策略的恢复上限**：现有优化方法仅能实现约50%的性能恢复。这些发现揭示了长音频理解面临的重大挑战，凸显了开发具备稳健文档级音频推理能力的方法的迫切需求。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频大语言模型（ALLMs）发展迅速，但其长音频理解能力尚未被系统评估。现有音频基准（如AudioBench、AIR-Bench）主要关注短音频片段（秒到分钟级），缺乏针对长音频（数分钟至数十分钟）的统一评估标准。
- **既有问题**：由于缺乏专门的长音频基准，ALLMs在长上下文中的表现未知，存在性能潜在退化风险，且当前模型架构是否能够维持长序列的时序局部性和注意力集中度尚不明确。

2)  
- **基准构建**：论文提出了首个面向ALLMs的长音频多任务基准**ChronosAudio**，旨在系统评估模型在长音频上的理解能力。
- **核心设计**：
  - **任务覆盖**：包含六大任务类别，涵盖**时序感知**（听写、定位）、**逐字序列生成**（转录、多说话人识别）和**高层推理**（理解、摘要）三类关键场景。
  - **长度分层**：数据按时长分为短（30秒~5分钟）、中（5~10分钟）、长（10~20分钟）三组，共36,000个测试实例，总时长超过200小时，以全面评估长度泛化能力。
  - **评估指标**：为每类任务设计了细粒度指标（如听写准确率、定位误差、混合词错误率、说话人识别F1、问答精确匹配、摘要覆盖度与事实性），确保评估全面且可量化。
- **问题解决机制**：
  - **揭示性能退化**：通过系统实验，首次量化了ALLMs在长音频上的“长上下文崩溃”现象——从短音频切换到长音频时，特定任务性能下降超过90%。
  - **诊断注意力稀释**：通过可视化注意力权重，发现模型在序列后期出现注意力显著扩散，难以维持时序局部性，导致高分辨率对齐丢失。
  - **检验缓解策略上限**：测试了稀疏注意力和滑动窗口注意力等主流长上下文策略，发现它们仅能恢复约50%的短上下文性能，表明当前架构调整不足以解决根本问题。

3)  
- **评估任务**：在ChronosAudio涵盖的六大任务（听写、定位、转录、多说话人识别、理解、摘要）上，对16个先进ALLMs进行了全面评估。
- **取得效果**：
  - **量化性能崩溃**：实验表明，所有模型在长音频上均出现严重性能下降。例如，在转录任务中，闭源模型分数从短音频的42.66骤降至长音频的3.86；开源模型在摘要任务中平均分从40.96（短）降至14.14（长）。
  - **揭示模型差距**：闭源模型在长音频推理任务（如摘要）中保持较强鲁棒性（平均59.22），显著优于开源模型（平均14.14），差距超45分。
  - **验证缓解策略局限性**：稀疏注意力虽能提升部分任务表现（如听写任务恢复至短上下文水平的93%），但对保真度要求高的任务（如转录）仅能恢复约50%能力，证实当前方法存在“恢复上限”。
</div>

</details>

---

## Gradient-based Optimisation of Modulation Effects
- **Authors**: Alistair Carson, Alec Wright, Stefan Bilbao
- **Categories**: eess.AS, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04867v1](https://arxiv.org/abs/2601.04867v1)
- **PDF**: [https://arxiv.org/pdf/2601.04867v1](https://arxiv.org/pdf/2601.04867v1)

调制效果（如移相器、镶边和合唱效果）在电吉他演奏中应用广泛。近年来，基于机器学习的模拟调制单元仿真技术已得到研究，但多数方法要么局限于单一效果类型，要么相比经典数字实现方式存在高计算成本或延迟问题。本文在先前研究基础上，提出一种基于可微分数字信号处理的镶边、合唱及移相效果建模框架。该模型在训练阶段使用时频域方法，而在推理阶段完全在时域运行，实现零延迟处理。我们深入探讨了此类效果基于梯度优化的技术挑战，证明损失函数的低频加权策略能有效避免学习延迟时间时陷入局部极小值。实验表明，当以模拟效果单元为训练目标时，模型生成的音频在某些情况下与参考信号在感知上难以区分，但对于长延迟时间及带反馈的效果类型，仍存在优化挑战。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：调制效果器（如镶边、合唱、移相）在音乐制作中广泛应用。现有模拟效果器的数字建模方法存在局限：
  - 传统电路仿真方法虽轻量但设备特定，缺乏通用性。
  - 神经网络黑箱模型通用性强但计算成本高、延迟大。
  - 现有方法多限于单一效果类别，或在处理长延迟时间（如镶边效果）时精度不足。

2)  
- **核心方法**：提出一个基于可微分数字信号处理（DDSP）的统一框架，用于建模镶边、合唱和移相效果。该方法通过以下设计解决既有问题：
  - **训练与推理分离**：
    - 训练时在时频域进行，使用短时傅里叶变换的帧基频率采样方法，以学习时变滤波器的参数。
    - 推理时完全在时域运行，实现零延迟，满足实时处理需求。
  - **优化挑战应对**：
    - 针对延迟时间估计中梯度下降易陷入局部最小值的问题，提出对损失函数进行**低频加权**（如使用低通核或预加重滤波器），拓宽损失曲面的凸区域，使优化更稳定。
    - 针对移相效果中的全通滤波器系数估计，使用频谱平坦的输入信号（如线性调频信号）也能取得良好效果。
  - **模型结构**：
    - 引入可学习的低频振荡器（LFO）模块，通过查找表与小型MLP生成控制信号，调制延迟线或全通滤波器级联。
    - 包含可学习的双二阶滤波器，用于增强建模灵活性。
    - 支持多通道扩展以处理更复杂的效果（如合唱）。

3)  
- **任务与效果**：
  - **数字效果参数恢复**（玩具问题）：模型能准确重建已知参数的镶边和移相效果，验证了框架的有效性。
  - **模拟效果器建模**：
    - **Boss BF-2镶边踏板**：在无反馈设置下，输出与参考在感知上难以区分；但带反馈的长延迟建模仍存挑战。
    - **Marshall SV-1合唱踏板**：多通道模型能较好匹配目标，感知评分优异。
    - **EHX Small Stone移相踏板**：使用全频带信号训练时，模型输出与参考在感知上无显著差异。
  - **感知评估**：在多数案例中，模型输出与模拟参考在听感上高度相似，部分效果达到感知不可区分的水平。
</div>

</details>

---

## Semi-Supervised Diseased Detection from Speech Dialogues with Multi-Level Data Modeling
- **Authors**: Xingyuan Li, Mengyue Wu
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.04744v1](https://arxiv.org/abs/2601.04744v1)
- **PDF**: [https://arxiv.org/pdf/2601.04744v1](https://arxiv.org/pdf/2601.04744v1)

从语音声学特征中检测医疗状况本质上是一个弱监督学习问题：单一且通常带有噪声的会话级标签必须与长而复杂的音频记录中的细微模式相关联。该任务还受到数据严重匮乏和临床标注主观性的进一步制约。尽管半监督学习为利用未标注数据提供了可行路径，但现有音频方法往往未能解决核心挑战——病理特征在患者语音中并非均匀表达。我们提出了一种新颖的纯音频半监督学习框架，通过从未分割的临床对话中联合学习帧级、片段级和会话级表征，显式建模这种层次结构。我们的端到端方法动态聚合这些多粒度特征，并生成高质量伪标签以高效利用未标注数据。大量实验表明，该框架具有模型无关性，在跨语言和跨病症场景中表现稳健，且具备极高的数据效率——例如仅使用11个标注样本即可达到全监督性能的90%。本研究为医学语音分析中基于弱远端监督的学习提供了系统化方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：利用语音声学特征进行疾病检测面临数据稀缺、标签噪声和弱监督三大挑战。临床诊断通常只提供会话级别的标签，但病理特征在长对话中稀疏且不均匀地出现。
- **既有方法问题**：现有方法常将长录音分割为独立片段处理，隐含假设症状均匀表达，这与实际不符。同时，通用半监督学习方法因领域差异难以直接迁移，且未能有效建模会话内的层次结构。

2)  
论文提出了一种新颖的、仅使用音频的半监督学习框架，通过分层建模多粒度信息来解决上述问题。其核心方法包括：
- **会话级建模**：作为主流程，使用音频编码器处理整个会话的片段，通过Transformer和多头注意力机制聚合特征，生成会话级表示并进行分类。该过程同时为未标注数据生成高质量的伪标签。
- **片段级建模**：利用主流程生成的伪标签，通过循环神经网络对每个话语片段进行建模。这使得模型能够学习句子级别的特征，而无需假设患者的每一句话都包含病理特征，也无需预先分离参与者与调查者的语音。
- **帧级建模**：采用师生网络架构和对比学习，通过数据增强生成不同视图，并使用均方误差损失来增强帧级别声学特征的一致性学习。这有助于模型捕获更细粒度的模式，并对伪标签噪声具有鲁棒性。
- **单阶段在线训练**：框架以端到端方式联合优化上述三个层次的损失。伪标签在训练过程中动态更新和筛选，形成了一个自我改进的训练循环，高效利用了未标注数据。

3)  
该方法在以下任务上取得了显著效果：
- **抑郁症检测**：在中文EATD-Corpus数据集上，仅使用10%的标注数据即可达到全监督基线90%的性能；使用30%的标注数据时，性能已接近使用100%数据训练的全监督模型。
- **阿尔茨海默症检测**：在英文ADReSSo21数据集上，使用30%的标注数据即可接近全监督性能，使用40%时甚至能超越全监督基线。
- **综合效果**：该方法在数据稀缺和标签噪声场景下表现鲁棒，具有模型无关性，且能有效处理包含调查者语音的原始对话，避免了繁琐的说话人分离预处理。
</div>

</details>

---

## LAMB: LLM-based Audio Captioning with Modality Gap Bridging via Cauchy-Schwarz Divergence
- **Authors**: Hyeongkeun Lee, Jongmin Choi, KiHyun Nam, Joon Son Chung
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.04658v1](https://arxiv.org/abs/2601.04658v1)
- **PDF**: [https://arxiv.org/pdf/2601.04658v1](https://arxiv.org/pdf/2601.04658v1)

自动音频描述旨在对输入音频的语义内容进行描述。近期研究采用大语言模型作为文本解码器以利用其推理能力。然而，先前方法将音频特征直接投影至LLM嵌入空间而未考虑跨模态对齐，导致无法充分发挥这些能力。为此，我们提出LAMB框架——一种基于LLM的音频描述系统，通过柯西-施瓦茨散度桥接音频嵌入与LLM文本嵌入空间之间的模态鸿沟。该框架包含跨模态对齐器，在最大化互信息的同时最小化柯西-施瓦茨散度，实现音频与文本在全局和词元层面的紧密对齐。我们进一步设计双流适配器，提取语义增强的音频嵌入，从而为跨模态对齐器提供更丰富的信息。最终，基于对齐后的音频嵌入，所提出的词元引导器直接在LLM文本嵌入空间内计算得分，以引导生成描述的输出逻辑值。实验结果表明，我们的框架有效增强了LLM解码器的推理能力，在AudioCaps数据集上取得了最先进的性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动音频描述任务旨在为音频生成文本描述。近期研究利用大语言模型作为文本解码器，以发挥其推理能力。  
- **既有方法问题**：现有方法通常通过线性投影或Q-Former将音频特征映射到LLM文本嵌入空间，但缺乏显式的跨模态对齐目标，导致音频表示与LLM嵌入空间之间存在模态鸿沟，限制了模型对音频语义的理解和利用。

2)  
论文提出LAMB框架，通过以下核心方法解决模态鸿沟问题：  
- **双流适配器**：  
  - 设计语义模块（使用可学习查询进行多头注意力）和时间模块（使用卷积与GRU捕获局部与时序模式），分别提取丰富语义和时序上下文。  
  - 融合两模块输出，并通过投影将其映射到LLM文本嵌入空间，为后续对齐提供高质量音频表示。  
- **跨模态对齐器**：  
  - 引入柯西-施瓦茨散度作为对称度量，结合互信息损失，从全局和词元两个层面缩小音频与文本嵌入分布的距离。  
  - 全局损失衡量整体分布差异，词元损失实现细粒度对齐，增强跨模态语义一致性。  
- **词元引导机制**：  
  - 在解码过程中，直接利用LLM自身的词元字典，计算对齐后音频嵌入与各词元嵌入的L2距离作为引导分数。  
  - 通过可学习系数调整解码器的原始逻辑值，使生成更偏向与音频语义相关的词元，无需依赖外部模块。

3)  
- **任务与效果**：在AudioCaps和Clotho数据集上进行评测。  
- **主要成果**：在AudioCaps上取得SOTA性能，各项指标显著提升（如CIDEr达91.1，SPIDEr达55.4）。在Clotho上表现具有竞争力，部分指标优于现有方法。  
- **结论**：跨模态对齐有效提升了LLM解码器的推理能力，生成描述更准确、连贯，且与人类偏好更一致。
</div>

</details>

---

## FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions
- **Authors**: Dekun Chen, Xueyao Zhang, Yuancheng Wang, Kenan Dai, Li Ma, Zhizheng Wu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04656v1](https://arxiv.org/abs/2601.04656v1)
- **PDF**: [https://arxiv.org/pdf/2601.04656v1](https://arxiv.org/pdf/2601.04656v1)

本研究提出FlexiVoice，一种支持零样本语音克隆的灵活风格控制文本转语音（TTS）合成系统。该系统通过自然语言指令控制说话风格，并以零样本方式依据语音参考控制音色。FlexiVoice以大型语言模型为核心架构，接收文本输入，并可选择性地接受自然语言指令和语音参考，分别用于控制风格与音色。系统采用新颖的渐进式后训练方案，逐步实现精准灵活的控制能力：首先通过直接偏好优化技术，使模型能同时准确遵循自然语言指令与语音参考；随后采用多目标群体相对策略优化方法，解耦风格指令、参考音色与文本内容；最后通过指令导向的群体相对策略优化进一步提升指令跟随能力。实验结果表明，FlexiVoice在各项基准测试中均优于现有方法，展现出优异的控制因子解耦能力。人工评估进一步证实了其在自然度、可控性与鲁棒性方面的卓越表现。音频样本详见https://flexi-voice.github.io。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：零样本TTS和基于指令的TTS是当前研究热点，旨在通过短语音参考克隆音色，或通过自然语言指令控制风格。  
- **既有问题**：现有方法存在“风格-音色-内容冲突”。模型容易过度依赖参考语音的声学先验（音色泄漏）或从文本推断韵律（内容泄漏），而忽略显式的风格指令，导致指令跟随不准确、音色一致性不稳定。

2)  
论文提出 **FlexiVoice** 系统，其核心是 **渐进式后训练（PPT）** 方案，分三个阶段解决上述问题：  
- **S1: 多模态DPO**：使用直接偏好优化（DPO），在情感任务上对齐模型，使其能同时准确跟随指令和参考语音。  
- **S2: 解耦GRPO**：采用多目标组相对策略优化（GRPO），构建指令与参考语音/文本内容冲突的训练场景，通过奖励模型（如情感识别、说话人验证）强制模型分离风格、音色和内容。  
- **S3: 指令GRPO**：使用音频语言模型（如Kimi-Audio）作为奖励，扩展模型对复杂、开放域指令的跟随能力，同时混合S2数据以防止灾难性遗忘。  

PPT通过课程学习策略，将指令跟随从简单的条件控制转变为严格的解耦过程，从而系统性地解决多模态冲突。

3)  
- **多模态控制与解耦任务**：在情感TTS任务上，FlexiVoice在指令跟随准确率（ACC-I）上大幅提升（例如英文TO-Hard任务达89.4%），同时显著降低来自文本或参考语音的干扰（ACC-T/R低至6.6%），证明了强大的解耦能力。  
- **复杂指令跟随任务**：在InstructTTSEval基准测试中，FlexiVoice平均准确率（英文79.3%，中文70.8%）超越所有开源基线，接近闭源商业系统（如Gemini-pro），并在声学参数指定、描述性风格指导和角色扮演等任务上表现优异。  
- **主观评估**：人类评估确认其生成语音的自然度、可控性和鲁棒性均优于基线模型。
</div>

</details>

---

## LLMs-Integrated Automatic Hate Speech Recognition Using Controllable Text Generation Models
- **Authors**: Ryutaro Oshima, Yuya Hosoda, Youji Iiguni
- **Categories**: eess.AS, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04654v1](https://arxiv.org/abs/2601.04654v1)
- **PDF**: [https://arxiv.org/pdf/2601.04654v1](https://arxiv.org/pdf/2601.04654v1)

本文提出一种基于大语言模型（LLM）的仇恨语音自动识别（ASR）模型。该方法将ASR模型的编码器与LLM的解码器相结合，能够同步完成语音转写与内容审查任务，从而避免有害内容的传播。为训练LLM通过指令微调将仇恨相关词汇替换为特定标记，需要标注的仇恨语音数据集，但此类数据较为稀缺。我们采用思维链（CoT）提示技术，结合文化背景与示例指导LLM生成文本样本，再通过文本转语音（TTS）系统将其转换为语音样本。然而，部分生成样本虽包含仇恨相关词汇却并非仇恨语音，这会降低审查性能。为此，本文通过文本分类模型筛选出被正确标注为仇恨内容的样本。通过调整正确分类模型数量的阈值，可控制生成数据集中仇恨内容的强度，从而以课程学习的方式渐进训练LLM。实验结果表明，所提方法对仇恨相关词汇的掩码准确率达到58.6%，优于现有基线方法。研究同时验证了课程训练策略对提升转写与审查任务效率的积极作用。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频社交媒体平台上的仇恨言论传播迅速，对受害者和社会造成心理伤害。现有内容审查机制至关重要。
- **既有方法问题**：
  - 传统方法（如基于LIME的解释模型）难以准确识别习语表达或细微语境。
  - 无法检测单个语句中的多个仇恨相关词汇，因为其对分类结果的贡献是分散的。
  - 缺乏带标注的仇恨语音数据集，限制了基于大语言模型（LLM）的指令微调。

2)  
- **核心方法概述**：提出一种集成自动语音识别（ASR）编码器与LLM解码器的多任务模型，实现语音转写与仇恨内容审查（用特定标记屏蔽仇恨词汇）的同步处理。
- **数据生成与过滤**：
  - 使用思维链（CoT）提示技术，结合文化背景和示例，引导LLM生成包含指定仇恨关键词的文本样本。
  - 通过文本转语音（TTS）系统将文本转换为语音样本。
  - 利用五个预训练文本分类模型对生成样本进行评分，根据模型将样本判定为“仇恨”的数量分配仇恨等级（0-5级）。
  - 通过设定仇恨等级阈值过滤样本，控制生成数据集的仇恨程度，用于课程学习。
- **模型训练**：
  - 先微调ASR编码器（使用生成的语音样本），再通过Q-Former将其与LLM解码器集成。
  - 对解码器进行指令微调（使用多样化的提示，要求转写并屏蔽仇恨词汇）并结合激活微调（使用KL散度损失），以保持模型泛化能力并减少过拟合。

3)  
- **任务与效果**：
  - 在自动仇恨语音识别任务上，所提方法（ASR*-LLM）在仇恨词汇屏蔽准确率（MAR）达到58.6%，优于所有基线模型（ASR-BERT: 36.7%, ASR*-BERT: 54.4%, ASR-LLM: 45.8%）。
  - 课程学习（逐步增加仇恨等级）进一步提升了多任务处理效率，其中Curriculum-2模型取得了最佳MAR（59.4%）和较低的未屏蔽词错误率（UMWER: 47.0%）。
  - 方法成功实现了语音转写与仇恨内容审查的同步处理，但存在因LLM幻觉导致过度生成屏蔽标记的问题，未来需优化提示与数据集。
</div>

</details>

---

## Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony
- **Authors**: Joonwon Seo, Mariana Montiel
- **Categories**: cs.LG, cs.SD, math-ph
- **arXiv**: [https://arxiv.org/abs/2601.04592v1](https://arxiv.org/abs/2601.04592v1)
- **PDF**: [https://arxiv.org/pdf/2601.04592v1](https://arxiv.org/pdf/2601.04592v1)

传统循环神经网络（RNN）将音乐上下文信息压缩为确定性的隐藏状态向量，这种信息瓶颈无法捕捉音乐中固有的模糊性。本文提出密度矩阵循环神经网络（DM-RNN），这是一种基于密度矩阵的新型理论架构。该模型能够维护音乐解释的统计集合（混合态），同时捕捉经典概率与量子相干性。我们采用量子通道（CPTP映射）严格定义了时序动态过程，并基于Choi-Jamiołkowski同构设计了参数化策略，确保学习到的动态过程在结构上始终保持物理有效性（CPTP）。通过引入冯·诺依曼熵量化音乐不确定性的分析框架，以及量子互信息（QMI）测量声部间纠缠关系，DM-RNN为建模复杂模糊的音乐结构提供了数学严谨的理论框架。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐序列建模面临核心挑战是音乐固有的**模糊性**（如一个音符后存在多种可能的延续）。  
- **既有方法的问题**：  
  - 传统RNN/LSTM等模型将历史上下文压缩为**单一确定性隐藏状态向量**，这形成了一个信息瓶颈。  
  - 这种点估计迫使模型坍缩掉音乐的可能性分布，无法有效表示和保持**结构性的模糊**（例如，一个和弦同时具有两种调性功能的“叠加”状态，而非仅仅是概率混合）。  
  - 即使贝叶斯RNN等引入了随机性，也主要关注参数或数据的不确定性，其隐藏状态仍是经典概率分布，缺乏对**相位关系（相干性）** 的建模能力。

2)  
论文提出**密度矩阵RNN（DM-RNN）**，以量子信息论中的密度矩阵作为隐藏状态，构建了一个能同时表达概率混合与量子相干性的理论框架。其核心方法通过以下机制解决传统方法的问题：  

- **状态表示升级**：  
  - 将隐藏状态从向量 \( h_t \in \mathbb{R}^d \) 替换为**密度矩阵** \( \rho_t \in \mathcal{D}(\mathcal{H}_d) \)。  
  - **对角元（布居数）** 表示系统处于特定潜在基态的概率（经典不确定性）。  
  - **非对角元（相干性）** 编码不同基态之间的稳定相位关系，能数学地表征不同音乐解释如何“干涉”或结构关联，从而区分**经典混合**与**相干叠加**。  

- **严格的动态与参数化**：  
  - 用时序演化 \( \rho_{t-1} \mapsto \rho_t \) 建模为**量子通道（CPTP映射）**，保证物理有效性。  
  - 关键贡献是利用**Choi-Jamiołkowski同构**进行参数化：  
    - 通过神经网络生成Cholesky因子 \( L_t \) 来构建Choi矩阵，**自动保证完全正性（CP）**。  
    - 通过对Kraus算子进行可微的归一化（计算 \( S_t = \sum_k K'_k^\dagger K'_k \)，然后令 \( K_k = K'_k S_t^{-1/2} \)），**强制保证迹守恒（TP）**。  
  - 该设计从构造上确保学到的动态是合法的CPTP映射，避免了昂贵特征分解。  

- **新的分析框架**：  
  - 引入**冯·诺依曼熵** \( S(\rho) \) 量化音乐上下文的不确定性（混合度）。  
  - 通过**量子互信息（QMI）** 度量多声部之间的关联（纠缠），前提是隐式地对隐藏空间施加张量积结构以对应不同声部。  
  - 这些工具使得DM-RNN不仅能生成，还能**分析**音乐中的模糊性、张力解决以及声部间依赖关系。

3)  
- **建模任务**：该工作为**理论框架**，未报告具体数据集上的性能数值，但明确了其针对的建模任务：  
  - 音乐序列建模（单声部与多声部）。  
  - 捕捉音乐中的**结构性模糊**（如调性模糊、和弦双重功能）与**声部间关联**。  
- **取得的效果/能力**：  
  - 提供了**数学上严格**的表示，能区分经典概率混合与相干叠加，这是传统RNN和经典概率模型无法做到的。  
  - 实现了**可分析的指标**（如冯·诺依曼熵、量子互信息），用于量化音乐不确定性、跟踪张力演变及声部纠缠程度。  
  - 为后续在**计算音乐学分析**与**生成建模**（需结合张量网络等高效实现）奠定了基础。
</div>

</details>

---

## When Tone and Words Disagree: Towards Robust Speech Emotion Recognition under Acoustic-Semantic Conflict
- **Authors**: Dawei Huang, Yongjie Lv, Ruijie Xiong, Chunxiang Jin, Xiaojiang Peng
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04564v1](https://arxiv.org/abs/2601.04564v1)
- **PDF**: [https://arxiv.org/pdf/2601.04564v1](https://arxiv.org/pdf/2601.04564v1)

语音情感识别系统通常默认语音情感与词汇语义之间存在一致性。然而在实际交互中，声学与语义的冲突普遍存在却常被忽视——即语调传递的情感与字面含义相互矛盾。本研究发现，当前最先进的语音情感识别模型（包括基于自动语音识别的方法、自监督学习模型及音频语言模型）在此类冲突下均会出现性能下降，其原因可归结为语义偏差或声学-语义表征的纠缠。为解决这一问题，我们提出融合声学-语义框架，通过显式解耦声学与语义通路，并借助轻量化的基于查询的注意力模块实现二者桥接。为支持系统性评估，我们构建了首个以多场景下清晰可解释的声学-语义冲突为核心的数据集。大量实验表明，该框架在领域内与零样本场景下均持续优于现有方法。值得注意的是，在冲突数据集基准测试中，传统语音情感识别模型表现严重失效，而本框架以59.38%的准确率创造了新的性能标杆。相关代码与数据集已开源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音情感识别（SER）通常假设语音的声学情感与词汇语义一致。然而，现实交流中普遍存在**声学-语义冲突**（如讽刺、幸灾乐祸），即语调传达的情感与字面含义相矛盾。  
- **既有方法的问题**：现有SER模型（包括ASR模型、自监督学习模型和音频语言模型）在此类冲突下性能显著下降，原因在于：  
  - **语义偏差**：基于ASR的编码器（如Whisper）过度依赖文本字面含义。  
  - **表征纠缠**：SSL方法（如HuBERT）的声学与语义信息混杂，难以解耦。  
  - **模态冲突处理不足**：多模态融合方法缺乏有效策略仲裁冲突信号。

2)  
论文提出**融合声学-语义框架**，通过显式解耦与动态融合解决上述问题：  
- **双路解耦**：  
  - **声学通路**：使用神经音频分词器提取低维、纯净的声学表征，专注捕捉韵律、音色等情感线索。  
  - **语义通路**：利用预训练语音文本编码器提取高维语义表征。  
- **动态融合模块**：  
  - **特征蒸馏**：对两路特征进行分块化后，基于L2范数计算显著度得分，选取Top-K关键token，保留情感相关时序信息。  
  - **查询式注意力**：引入可学习查询向量，通过交叉注意力机制主动从蒸馏后的声学与语义上下文中提取信息，生成融合表征。  
  - **预测头**：使用轻量级MLP基于融合特征进行情感分类。  
- **优势**：  
  - 显式解耦避免了语义偏差与表征纠缠。  
  - 动态融合机制能有效仲裁冲突，增强模型对复杂情感场景的鲁棒性。

3)  
- **评估任务与效果**：  
  - **领域内测试**：在MELD、RAVDESS、ESD数据集上，FAS取得最优或接近最优性能（平均ACC 71.92%）。  
  - **零样本泛化**：在专门评估冲突的CASE基准上，FAS以**59.38%的准确率**显著超越所有基线模型（传统SER模型性能大幅下降至25-47%），创下新SOTA。  
  - **跨语言/跨库泛化**：在Emo-Emilia、EMOVO、EmoDB等零样本数据集上，FAS展现出稳定的泛化能力，平均ACC达54.66%。
</div>

</details>

---

## WESR: Scaling and Evaluating Word-level Event-Speech Recognition
- **Authors**: Chenchen Yang, Kexin Huang, Liwei Fan, Qian Tu, Botian Jiang, Dong Zhang, Linqi Yin, Shimin Li, Zhaoye Fei, Qinyuan Cheng, Xipeng Qiu
- **Categories**: cs.CL, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.04508v1](https://arxiv.org/abs/2601.04508v1)
- **PDF**: [https://arxiv.org/pdf/2601.04508v1](https://arxiv.org/pdf/2601.04508v1)

语音不仅传递语言信息，还包含丰富的非语言声音事件，如笑声与哭声。尽管语义转录已得到充分研究，但非语言事件的精确定位仍是一个关键却尚未深入探索的挑战。现有方法存在任务定义不足的问题，表现为类别覆盖有限、时间粒度模糊，且缺乏标准化评估框架，阻碍了下游应用的发展。为填补这一空白，我们首先构建了一个包含21类声音事件的精细化分类体系，将其重新划分为离散型（独立存在）与连续型（与语音混合）两类。基于该体系，我们提出了WESR-Bench——一个由专家标注的评估集（含900余条话语），其采用创新的位置感知标注协议，能够分离语音识别错误与事件检测误差，从而实现对离散与连续事件的精确定位评估。此外，我们构建了一个超过1,700小时的数据集，并训练了专用模型，在保持语音识别质量的同时，性能超越了开源音频-语言模型及商业API。我们期待WESR能为未来真实复杂听觉场景建模研究提供基础性资源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音不仅传递语义信息，还包含丰富的非语言声音事件（如笑声、咳嗽）。现有研究主要关注语义转录，而对非语言事件的细粒度定位仍是一个未被充分探索的挑战。  
- **既有方法的问题**：  
  - **任务定义不足**：现有方法覆盖的事件类别有限，且时间粒度模糊（如句子级或帧级检测），缺乏对连续事件（如边笑边说）与离散事件的明确区分。  
  - **评估框架缺失**：缺乏标准化的评估协议，现有指标（如词错误率WER）无法准确衡量事件在词级别的定位精度，且难以处理同一语句中的多个事件或连续事件。  

2)  
论文通过构建**WESR（词级别事件-语音识别）**框架系统性地解决上述问题，具体包括：  
- **任务形式化与分类体系**：  
  - 提出包含21类声音事件的精细分类法，明确区分**离散事件**（如`[laughs]`）和**连续事件**（如`<laughing>...</laughing>`），为联合建模语音内容与事件提供了统一框架。  
- **评估基准与协议**：  
  - 构建**WESR-Bench**，一个包含900+语句的专家标注双语测试集，涵盖自然语音中的多种事件。  
  - 设计**位置感知的评估协议**：  
    - 通过事件保留对齐（Event-Preserving Alignment）分离ASR错误与事件检测误差。  
    - 将事件映射到词位置或词间位置，支持对连续事件跨词范围的精确评估。  
    - 基于对齐计算每个事件类别的精确率、召回率与F1分数，天然支持多事件和混合事件场景。  
- **数据与模型基线**：  
  - 构建**WESR-Train**大规模训练集（1,700+小时），通过混合检索策略收集数据，并利用大模型（Gemini）自动生成词级别事件标注。  
  - 在多个骨干模型（如Whisper、Qwen3-Omni）上进行监督微调，使模型能同时输出转录文本和词级别事件标签。  

3)  
- **任务**：在**词级别事件-语音识别（WESR）**任务上评估模型性能，涵盖21类事件（15类离散、6类连续）的检测与定位。  
- **效果**：  
  - 微调后的WESR模型（如WESR-Qwen）在WESR-Bench上取得**宏观平均F1分数38.0%**，显著优于开源音频语言模型（如Qwen3-Omni的16.9%）和商业API（如Gemini-2.5-Pro的32.3%）。  
  - 在连续事件（如耳语、唱歌）和离散事件（如咳嗽、笑声）上均表现优异，尤其在连续事件上F1提升18%（达到0.641）。  
  - 模型在提升事件检测能力的同时，在Common Voice ASR基准上仅带来轻微的词错误率上升，保持了语音转录质量。
</div>

</details>

---
