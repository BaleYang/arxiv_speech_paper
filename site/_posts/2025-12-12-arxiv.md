---
layout: post
title: "arXiv Daily – 2025-12-12"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-12（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-11 08:50 — 2025-12-12 08:50
- 抓取总数：6 篇 | 本页显示：6 篇（去重/过滤后）

## Building Audio-Visual Digital Twins with Smartphones
- **Authors**: Zitong Lan, Yiwei Tang, Yuhan Wang, Haowen Lai, Yido Hao, Mingmin Zhao
- **Categories**: cs.SD, cs.MM, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.10778v1](https://arxiv.org/abs/2512.10778v1)
- **PDF**: [https://arxiv.org/pdf/2512.10778v1](https://arxiv.org/pdf/2512.10778v1)

当前数字孪生技术几乎完全局限于视觉维度，忽视了声学这一空间真实感与交互的核心要素。本研究提出AV-Twin系统，首次实现仅通过消费级智能手机即可构建可编辑的视听数字孪生。该系统融合移动式房间脉冲响应采集与视觉辅助声场建模技术，高效重建室内声学环境。通过可微分声学渲染技术，系统进一步还原各表面材料属性，支持用户在修改材质、几何结构与空间布局时，实现声学与视觉效果的同步自动更新。这些能力共同为真实环境构建全维度可修改的视听数字孪生提供了切实可行的技术路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：数字孪生技术广泛应用于AR/VR、机器人等领域，但现有系统主要关注视觉维度，忽略了声音这一塑造空间感知与交互的核心要素。
- **问题**：
  - **采集成本高**：传统声场重建依赖昂贵的专用硬件（如扬声器-麦克风阵列、运动捕捉系统），成本超10万美元，且测量耗时极长。
  - **缺乏可编辑性**：先进的神经隐式声场模型将声学响应编码为黑箱函数，无法支持对材料、布局或家具的单独修改，任何改动都需重新采集数据。

2) **论文核心方法如何解决上述问题**
论文提出 **AV-Twin** 系统，仅使用消费级智能手机，通过一系列创新设计，实现了高效、可编辑的视听数字孪生构建。

- **高效、实用的声场采集**：
  - **仅使用智能手机的RIR采集**：设计声学握手协议，利用线性调频信号实现设备间亚毫秒级同步，无需专用硬件即可采集完整的房间脉冲响应。
  - **视觉数字孪生用于空间锚定**：利用手机SLAM同步构建视觉数字孪生（包含设备轨迹和场景网格），为每个RIR测量提供精确的空间位置。
  - **动态轨迹采集范式**：用户手持手机在环境中自然行走并连续记录RIR，取代传统的密集网格采样。仅需20分钟行走即可重建全局声场，效率提升超100倍。
  - **视觉辅助的声场建模**：提出视觉辅助声学体渲染模型。该模型利用视觉网格的几何信息，仅在射线与表面的首个交点处评估神经网络，而非沿整条射线密集采样。这使渲染速度提升10倍，数据效率提升2倍。

- **构建可编辑的视听场景**：
  - **声学属性估计**：核心创新是将隐式声场转化为显式的、基于对象的视听场景图。通过结合多组RIR测量与**可微分声学渲染**，并利用视觉先验（将外观相似、空间相邻的表面分组），优化估计每个网格表面的材料反射率等声学属性。
  - **支持场景编辑**：上述过程生成了一个将视觉外观与声学属性绑定到同一组网格基元的显式表示。用户随后可以：
    - **编辑材料**：更改表面的反射系数，系统自动更新对应的声学响应（如混响时间）。
    - **编辑几何**：添加/移除家具、修改房间布局，系统会重新计算受影响的声学路径并合成新的RIR。

3) **在哪些任务上取得了怎样的效果**
- **声场重建**：在**新颖视角声学合成**任务中，动态轨迹方法仅用20分钟数据即可达到甚至超过传统网格方法（耗时超3000分钟）的重建质量。视觉辅助AVR模型将单次RIR渲染时间从100ms降至10ms。
- **材料属性估计**：对常见材料（如干墙、玻璃）反射系数的估计，与参考测量值的平均绝对误差为5.6%，皮尔逊相关系数达0.96。
- **应用验证**：
  - **沉浸式音频渲染**：用户研究表明，88%的参与者认为动态轨迹方法合成的音频在整体质量上优于传统网格方法。
  - **视听场景编辑**：超过90%的用户认为编辑后的声学效果与视觉场景的修改相匹配（如增加反射率使声音更混响，添加家具使声音更清晰）。
  - **声学定位**：利用声场模型进行数据增强后，基于CNN的定位模型误差降低50%，达到0.45米的亚米级精度。
</div>

</details>

---

## Exploring Perceptual Audio Quality Measurement on Stereo Processing Using the Open Dataset of Audio Quality
- **Authors**: Pablo M. Delgado, Sascha Dick, Christoph Thompson, Chih-Wei Wu, Phillip A. Williams
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.10689v1](https://arxiv.org/abs/2512.10689v1)
- **PDF**: [https://arxiv.org/pdf/2512.10689v1](https://arxiv.org/pdf/2512.10689v1)

ODAQ（开放音频质量数据集）为探索单声道与双声道音频质量劣化提供了一个综合框架，涵盖多种失真类型及信号，并附有主观质量评分。该数据集近期更新聚焦于中/侧声道（MS）与左/右声道（LR）等立体声处理方法的影响，提供了用于深入评估当前先进客观音频质量指标的测试信号与主观评分。评估结果表明，尽管侧重音色的指标在较简单条件下通常表现稳健，但在呈现语境更复杂的条件下，其预测性能往往下降。本研究结果强调了建模自下而上的心理声学过程与自上而下的语境因素之间相互作用的重要性，为未来研究指明了方向，即开发能更有效融合感知音频质量中音色与空间维度的模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：客观音频质量评估主要依赖基于音色的单声道模型，对立体声处理（如MS/LR编码）引入的复杂空间失真评估不足。现有方法常独立处理左右声道，或简单平均失真指标，难以准确建模双耳空间线索（如ILD、ITD）与音色失真的交互作用。  
- **既有问题**：  
  - 空间音频质量建模局限于定位、宽度等特定维度，对通道间失真关联（如同时/独立失真）研究不足。  
  - 多数指标未考虑呈现上下文（如测试格式、锚点条件）对主观评分的影响，导致在复杂听测场景中预测性能下降。  

2)  
- **核心方法**：基于ODAQ更新数据集，系统评估了多种客观音频质量指标在立体声处理下的性能，重点分析其如何结合单声道与双耳模型。  
  - **指标范围**：涵盖PEAQ、NMR、2f-model、HAAQI、PEMO-Q、MoBi-Q、eMoBi-Q等传统及扩展指标，部分整合了双耳线索提取模型（如ITD、IACC、ILD）。  
  - **立体声处理**：通过LR（独立处理左右声道）和MS（处理中/侧声道）两种方式引入失真，模拟实际编码场景。  
  - **模型结合策略**：  
    - 多数指标对左右声道失真度量进行时间/频率平均后映射为质量分。  
    - MoBi-Q等采用启发式组合（如取单耳/双耳质量分中较差者）生成总体评分。  
    - PEAQ的双耳扩展使用MARS等机器学习方法融合空间线索与音色质量分。  
- **解决思路**：  
  - 通过ODAQ提供的多上下文听测数据，揭示指标在失真类型（如量化噪声QN、频谱空洞SH）、呈现模式（纯LR/MS、混合上下文）下的性能差异。  
  - 强调需同时建模自下而上的心理声学过程（如掩蔽、调制失真）与自上而下的上下文因素，以提升对复杂立体声失真的预测鲁棒性。  

3)  
- **评估任务**：在ODAQ数据集上测试指标对立体声处理失真的预测能力，包括LR/MS独立失真、混合上下文（QNmix/SHmix）及硬声像（如单侧人声）场景。  
- **效果**：  
  - 音色导向指标（如Two-F、PEMO-Q、PEAQ-CSM）在简单上下文（纯LR/MS）中表现稳健（相关系数常＞0.8），凸显调制失真建模的重要性。  
  - 双耳扩展模型（如CSM_Bin_FB）未显著提升性能，甚至低于单耳基线，反映听众更依赖音色保真度。  
  - 多数指标在混合上下文（QNmix/SHmix）中性能大幅下降（平均相关系数约0.44），揭示呈现上下文对主观评分的关键影响。  
  - 部分指标（如NMR）在硬声像项目的MS处理中完全失效，凸显通道平均策略的局限性。
</div>

</details>

---

## BRACE: A Benchmark for Robust Audio Caption Quality Evaluation
- **Authors**: Tianyu Guo, Hongyu Chen, Hao Liang, Meiyi Qiang, Bohan Zeng, Linzhuang Sun, Bin Cui, Wentao Zhang
- **Categories**: cs.SD, cs.CL
- **arXiv**: [https://arxiv.org/abs/2512.10403v1](https://arxiv.org/abs/2512.10403v1)
- **PDF**: [https://arxiv.org/pdf/2512.10403v1](https://arxiv.org/pdf/2512.10403v1)

自动音频描述对于音频理解至关重要，能够支持无障碍访问和内容索引等应用。然而，音频描述质量的评估仍面临重大挑战，尤其是在无参考场景下，高质量的真实描述往往难以获取。尽管CLAPScore是目前最广泛使用的无参考音频描述评估指标，但其在不同条件下的鲁棒性尚未得到系统验证。

为填补这一空白，我们提出了BRACE——一个专为评估无参考场景下音频描述对齐质量而设计的新基准。BRACE主要用于评估音频描述评估指标，也可扩展用于衡量大型音频语言模型的跨模态对齐能力。该基准包含两个子评测集：用于细粒度描述对比的BRACE-Main，以及用于检测细微幻觉内容的BRACE-Hallucination。我们通过高质量数据筛选、基于大语言模型的描述篡改和人工标注构建了这些数据集。

鉴于CLAPScore作为无参考音频描述评估指标的广泛采用，以及大型音频语言模型在音频-语言任务中日益增长的应用，我们使用BRACE基准对这两类方法进行了全面评估：测试了不同CLAP模型变体的CLAPScore，并对多种大型音频语言模型进行了测评。

值得注意的是，在BRACE-Main基准上，表现最佳的CLAP模型仅达到70.01的F1分数，而最优的大型音频语言模型也仅获得63.19分。通过揭示CLAP模型与大型音频语言模型的局限性，BRACE基准为未来研究方向提供了重要参考。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动音频描述对音频理解至关重要，但评估其质量，尤其是在无参考（无高质量真值描述）场景下，仍面临挑战。  
- **既有方法问题**：  
  - 现有基准（如FENSE）仅依赖描述文本进行成对比较，未充分利用音频模态信息，且使用的描述生成模型已过时。  
  - 缺乏专门评估无参考音频描述评价指标（ACEM）鲁棒性的基准，也缺少检测描述中基于对象的幻觉内容的高质量基准。  
  - 当前广泛使用的无参考指标CLAPScore在不同条件下的鲁棒性尚未得到系统验证。

2)  
- **核心方法**：论文提出了BRACE基准，包含两个子基准，通过高质量过滤、基于LLM的破坏和人工标注构建。  
  - **BRACE-Main**：用于细粒度描述比较，包含三类音频-描述对（人-人、人-机、机-机）。通过筛选高质量音频-描述对，利用LALM生成描述并引入语义和语法破坏，再经人工标注达成共识，确保数据质量。  
  - **BRACE-Hallucination**：用于检测细微幻觉内容。利用LLM识别描述中的名词并替换为逻辑一致但语义不同的名词，生成包含幻觉的音频-描述对。  
- **解决思路**：  
  - 通过多类型描述对和人工标注，系统评估ACEM（如CLAPScore）与人类判断的一致性，以及LALM的模态对齐能力。  
  - 引入幻觉基准，挑战模型对细微错误的检测能力，揭示现有模型在细粒度音频-文本对齐上的不足。  
  - 采用滑动窗口策略（SLIDE-CLAP）缓解CLAP模型因固定输入窗口导致的性能不稳定问题，提升评估可复现性。

3)  
- **评估任务与效果**：  
  - 在BRACE-Main上，最佳CLAP模型（LAION-CLAP）F1分数为70.01，最佳LALM（AF2）为63.19，表明现有模型在细粒度描述比较上仍有较大提升空间。  
  - 在BRACE-Hallucination上，最佳CLAP模型（M2D-CLAP）F1分数为88.26，而闭源LALM（GPT-4o-Audio-Preview）达到96.37，开源模型则显著落后（如Qwen-Audio-Chat为77.25），揭示了模型在幻觉检测上的性能差距。  
  - 基准有效区分了模型质量，并暴露了CLAP模型在细粒度声学细节和语法敏感性上的不足，以及LALM在指令遵循、位置偏见等方面的缺陷。
</div>

</details>

---

## Investigating training objective for flow matching-based speech enhancement
- **Authors**: Liusha Yang, Ziru Ge, Gui Zhang, Junan Zhang, Zhizheng Wu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.10382v1](https://arxiv.org/abs/2512.10382v1)
- **PDF**: [https://arxiv.org/pdf/2512.10382v1](https://arxiv.org/pdf/2512.10382v1)

语音增强旨在从含噪录音中恢复纯净语音。尽管生成式方法（如分数匹配和薛定谔桥）已展现出显著效果，但其计算成本往往较高。流匹配通过直接学习从噪声到数据的速度场，提供了一种更高效的替代方案。本研究系统探讨了流匹配在语音增强中的三种训练目标：速度预测、$x_1$预测与预条件化$x_1$预测，并分析了它们对训练动态及整体性能的影响。此外，通过引入感知（PESQ）与信号（SI-SDR）优化目标，我们进一步提升了收敛效率与语音质量，在各项评估指标上均取得显著改进。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强旨在从带噪录音中恢复纯净语音。传统判别式深度学习方法虽有效，但未显式建模语音的生成结构，可能限制其在多变噪声下的泛化能力。
- **既有方法问题**：近年生成式方法（如分数匹配、薛定谔桥）展现出潜力，但通常计算成本高昂，依赖大量采样步骤或迭代优化，导致推理效率低下。

2)  
本文系统研究了流匹配在语音增强中的三种训练目标，并引入感知与信号级损失以提升效果：
- **核心训练目标比较**：
    - **速度预测**：直接回归将噪声映射到数据的向量场，训练稳定但收敛较慢。
    - **x1预测**：直接预测纯净语音目标，理论等效于速度预测，但训练行为不稳定，性能较差。
    - **预条件x1预测**：基于EDM框架，通过对网络输入输出进行缩放，确保时间步上损失权重均匀，显著提升了训练稳定性与效率。
- **损失函数增强**：在基础目标上，额外引入可微分的PESQ（感知语音质量评估）损失和SI-SDR（尺度不变信噪比）损失。两者结合使用，在加速收敛、提升感知质量的同时，避免了单一优化PESQ可能导致的信号失真。
- **解决思路**：预条件x1预测目标通过改进的损失加权机制，解决了原始x1预测的不稳定问题，实现了接近速度预测的性能，且收敛速度快一倍。结合PESQ与SI-SDR损失，则进一步在优化效率和语音质量间取得了更好平衡。

3)  
- **任务**：在VoiceBank-DEMAND数据集上进行语音增强实验。
- **效果**：
    - **不同目标比较**：预条件x1预测在PESQ、SI-SDR、WER（词错误率）等多个指标上与速度预测相当，且收敛速度快约一倍，显著优于原始x1预测。
    - **与基线对比**：仅使用预条件x1预测的流匹配模型，在仅需5步采样的情况下，取得了优于或可比扩散模型基线（SGMSE+、StoRM、SBVE）的综合性能，尤其在PESQ和WER上表现突出。
    - **联合损失效果**：结合PESQ与SI-SDR损失后，模型在保持各项指标良好平衡的同时，进一步提升了PESQ分数并加速了收敛。
</div>

</details>

---

## Neural personal sound zones with flexible bright zone control
- **Authors**: Wenye Zhu, Jun Tang, Xiaofei Li
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2512.10375v1](https://arxiv.org/abs/2512.10375v1)
- **PDF**: [https://arxiv.org/pdf/2512.10375v1](https://arxiv.org/pdf/2512.10375v1)

个人声区（PSZ）再现系统旨在利用单个扬声器阵列，在同一空间区域内为不同位置的听者创建各自独立的虚拟声学场景，是虚拟现实应用中的一项基础技术。在实际应用中，重建目标必须在用于记录从扬声器阵列到各PSZ控制点的局部房间脉冲响应（RIR）的同一固定接收器阵列上进行测量，这导致系统在实际使用中既不便携又成本高昂。本文提出了一种专为PSZ再现设计的3D卷积神经网络（CNN），该网络以虚拟目标场景作为输入，以PSZ预滤波器作为输出，能够灵活控制麦克风网格并支持替代性重建目标。通过与传统方法的实验结果对比表明，所提方法仅需一次训练即可处理灵活控制点网格上的多种重建目标。此外，该方法还展现出从分布在PSZ中的稀疏采样点学习全局空间信息的能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：个人声区（PSZ）系统旨在同一空间内为不同听者创建独立的声学场景。传统方法（如声学对比控制、压力匹配）在应用时存在一个关键限制。
- **既有问题**：为了重现特定的真实声学场景，传统方法通常要求使用**固定的控制点网格**来同时测量远程场景的目标声学传递函数（ATF）和本地房间脉冲响应（RIR）。这意味着，每当目标场景改变时，用于捕捉它的麦克风阵列必须与本地房间RIR测量时的网格模式保持一致。这种对固定网格的依赖使得系统在重现变化的声学场景时**成本高昂且不便**。

2)  
论文提出了一种基于3D卷积神经网络（CNN）的端到端神经PSZ方法，以解决上述灵活性问题。其核心设计如下：

- **网络输入与输出**：
    - **输入**：目标声学传递函数（ATF），具体为亮区（BZ）控制点网格上的ATF数据。
    - **输出**：扬声器阵列所需的预滤波器组。
    - 网络学习从目标ATF到预滤波器的直接映射，其训练目标是最小化重现ATF与目标ATF之间的误差。

- **支持灵活控制的关键设计**：
    - **网格掩码（Grid Masking）**：在训练阶段，对输入的控制点ATF数据应用多种**随机掩码模式**，以模拟不同的、稀疏的控制点网格（例如，从12×12的全网格到单个中心点）。这使得网络能够从**可变且稀疏的采样点**中学习全局空间信息，而非过拟合于特定固定网格。
    - **监控点（Monitor Points）**：在每个声区内引入一个**不与控制点重叠的、更密集的监控点网格**。网络损失函数（均方误差）基于预滤波器在**这些监控点**上产生的ATF与真实目标ATF计算，而非控制点。这迫使网络学习声区的整体声场信息，而不仅仅是匹配有限的输入控制点。

- **网络架构**：
    - 采用基于**3D ResNet**的架构，处理将ATF数据重构而成的四维张量（包含实部/虚部、空间网格维度、频率维度）。
    - 3D卷积能有效捕捉声场在空间和频率上的联合特征。

- **解决的核心问题**：
    - **灵活网格**：通过网格掩码训练，单个训练好的模型能够处理**不同模式、不同稀疏程度**的控制点输入，无需为每种网格重新训练或测量。
    - **替代目标**：网络以目标ATF为输入，因此可以自由更换要重现的**虚拟声源场景**，只需一次训练即可适应多种目标。
    - **从稀疏点学习**：实验表明，即使输入的控制点非常稀疏（如3×3网格），网络仍能有效推断出声区的全局空间信息，从而生成高质量的预滤波器，性能显著优于传统压力匹配（PM）方法在同等稀疏输入下的表现。

3)  
- **任务**：在混响环境中，使用扬声器阵列重现个人声区（包含一个亮区BZ和一个暗区DZ），并评估其性能。
- **效果**：
    - **优于基线**：在多种稀疏控制网格（如Grid-3, Grid-2）上，所提神经PSZ方法在**亮区重现误差（REB）** 和**声学对比度（AC）** 上均显著优于传统压力匹配（PM）方法。
    - **灵活性与稀疏性**：方法仅需一次训练，即可处理**可变的控制网格模式**。即使使用远少于传统方法所需的控制点（如9个点），仍能有效重现BZ声场并保持较高的BZ/DZ分离度（AC约14 dB）。
    - **泛化能力**：与针对单一固定网格训练的模型相比，本方法在应对多种网格时性能略有折衷，但实现了无需重新训练的**强大灵活性**，更适用于实际场景。
</div>

</details>

---

## MR-FlowDPO: Multi-Reward Direct Preference Optimization for Flow-Matching Text-to-Music Generation
- **Authors**: Alon Ziv, Sanyuan Chen, Andros Tjandra, Yossi Adi, Wei-Ning Hsu, Bowen Shi
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.10264v1](https://arxiv.org/abs/2512.10264v1)
- **PDF**: [https://arxiv.org/pdf/2512.10264v1](https://arxiv.org/pdf/2512.10264v1)

音乐生成模型面临的一个核心挑战在于其难以直接与人类偏好对齐，因为音乐评价本质上是主观的，且个体差异显著。本文提出 MR-FlowDPO，一种基于多奖励直接偏好优化的新方法，用于增强流匹配音乐生成模型——这是现代音乐生成模型的主要类别之一。我们设计了多项音乐奖励，分别从文本对齐度、音频制作质量与语义一致性三个关键维度评估音乐质量，并利用可扩展的现成模型进行各奖励的预测。这些奖励通过两种方式被应用：（一）构建用于直接偏好优化的偏好数据；（二）将奖励信息整合至文本提示中。针对音乐性评估的模糊性问题，我们提出一种基于语义自监督表征的新型评分机制，显著提升了生成音乐的节奏稳定性。通过多种音乐专用客观指标及人工听感实验进行综合评估，结果表明 MR-FlowDPO 能显著提升整体音乐生成质量，在音频质量、文本对齐度与音乐性方面均持续优于现有强基线模型。代码已开源：https://github.com/lonzi/mrflow_dpo；生成样本详见演示页面：https://lonzi.github.io/mr_flowdpo_demopage/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本到音乐生成模型（如流匹配模型）在覆盖范围和生成质量上已有进步，但其训练目标通常未直接优化人类偏好，导致生成音乐存在感知缺陷。
- **既有方法问题**：
  - 依赖人工标注的偏好数据，难以扩展。
  - 现有方法（如Tango2、TangoFlux）仅优化单一标准（如文本对齐），无法全面适应多样化的用户偏好。
  - 对于音乐性、录音质量等维度的规模化引导仍未被充分探索。

2)  
论文提出**MR-FLOWDPO**方法，通过多奖励直接偏好优化（DPO）解决上述问题，具体包括：
- **多奖励设计**：构建三个可扩展的自动奖励函数，分别评估：
  - **文本对齐**：使用基于音乐的CLAP模型计算音频与文本描述的余弦相似度。
  - **音频制作质量**：采用回归Transformer预测音频的声学质量分数（如清晰度、动态范围）。
  - **语义一致性**：提出基于HuBERT模型的新颖奖励，通过自监督表征学习量化音乐语义连贯性，显著提升节奏稳定性。
- **偏好数据构建**：提出**多奖励强主导（MRSD）**配对算法，从参考模型生成的样本中筛选正负对，确保正样本在至少一个主要奖励轴上显著优于负样本，且在其余轴上也有较小优势，从而平衡多维度优化。
- **奖励提示机制**：在训练和推理时，将奖励分数以自然语言形式前置到文本提示中，进一步引导模型生成高质量音乐。
- **DPO微调**：将上述多奖励整合到流匹配模型的DPO损失函数中，直接优化模型以符合人类偏好，避免依赖人工标注。

3)  
- **任务**：文本到音乐生成。
- **效果**：
  - 在客观指标上，MR-FLOWDPO显著提升了音频制作质量（Aesthetic Score）和内容享受度（EA Score），并有效降低了节奏不稳定性（BPM-std指标优于或持平基线）。
  - 在人类评估中，模型在整体偏好、音频质量、文本对齐和音乐性上均显著优于多个强基线（如MusicGen、MelodyFlow、AudioLDM2），尤其在音频质量和音乐性上获得最高净胜率。
  - 消融实验验证了多奖励整合、MRSD配对和奖励提示机制的有效性。
</div>

</details>

---
