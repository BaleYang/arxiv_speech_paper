---
layout: post
title: "arXiv Daily – 2025-11-19"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-11-19（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-11-18 08:50 — 2025-11-19 08:50
- 抓取总数：9 篇 | 本页显示：9 篇（去重/过滤后）

## A Controllable Perceptual Feature Generative Model for Melody Harmonization via Conditional Variational Autoencoder
- **Authors**: Dengyun Huang, Yonghua Zhu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.14600v1](https://arxiv.org/abs/2511.14600v1)
- **PDF**: [https://arxiv.org/pdf/2511.14600v1](https://arxiv.org/pdf/2511.14600v1)

尽管大语言模型使符号音乐生成日益普及，但创作具有鲜明特色与丰富表现力的音乐仍是重大挑战。现有研究多通过情感模型引导生成过程，但仍难以实现新颖性与创造性。在音乐信息检索领域，听觉感知被公认为音乐体验的核心维度，可同时揭示创作意图与情感模式。为此，我们提出CPFG-Net神经网络及将感知特征值映射为和弦表征的转换算法，实现旋律和声化。该系统能够从给定旋律中可控地预测感知特征序列与调性结构，进而生成和声连贯的弦进行。我们的网络基于从古典音乐构建的新型感知特征数据集BCPT-220K进行训练。实验结果表明，本模型在感知特征预测方面达到最先进水平，并在和弦推断中展现出卓越的音乐表现力与创造性。本研究为旋律和声化提供了新视角，并对更广泛的音乐生成任务具有促进作用。该符号化模型可轻松扩展至基于音频的模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前基于大语言模型的符号音乐生成方法虽普及，但在生成具有独特作曲风格和丰富表现力的音乐方面仍面临挑战。现有研究多引入情感模型引导生成，但生成结果缺乏新颖性与创造力。  
- **既有问题**：  
  - 现有方法依赖粗粒度特征（如情感、风格）进行控制，难以实现对和弦推断的精细调控。  
  - 部分研究尝试通过概率框架引入随机性以提升新颖性，但削弱了与情感表达的联系。  
  - 基于音乐张力特征的端到端方法缺乏透明性和交互性，且未专门应用于旋律和声化任务。  

2)  
- **核心方法**：提出基于条件变分自编码器的可控感知特征生成模型（CPFG-Net），通过多阶段控制实现旋律和声化。  
- **解决路径**：  
  - **数学转换模型**：基于Spiral Array理论构建感知特征（张力、距离、应变）与和弦表示的双向映射，支持调性调制和任意和弦类型。  
  - **网络设计**：  
    - 使用GRU与注意力机制编码旋律条件，隐变量采样实现多样性生成。  
    - 解码器分四路输出感知特征与调性，通过重构损失与KL散度联合优化。  
  - **可控性实现**：  
    - 支持隐空间采样、特征曲线修改、和弦库约束三阶段控制。  
    - 引入解纠缠策略，识别隐变量维度对特定特征的贡献，实现特征定向调整。  
  - **数据支撑**：构建古典音乐感知数据集BCPT-220K，涵盖22万样本的张力特征标注。  

3)  
- **任务与效果**：  
  - **感知特征预测**：在张力、距离、应变重建上取得最优MSE（0.0085/0.0097/0.010）与SRCC（0.893/0.986/0.978），显著优于基线模型。  
  - **旋律和声化**：生成和弦在客观指标上展现更高多样性（平均和弦覆盖度0.57）与创造性，主观评估中和谐度与Coconet相当，新颖性略低但可控性更强。  
  - **案例验证**：通过特征曲线编辑与隐变量调控，生成具有定制化张力变化的和声进展，证明方法在音乐表达与创造力上的有效性。
</div>

</details>

---

## IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention
- **Authors**: Xinxin Tang, Bin Qin, Yufang Li
- **Categories**: cs.SD, cs.AI, cs.CV
- **arXiv**: [https://arxiv.org/abs/2511.14515v1](https://arxiv.org/abs/2511.14515v1)
- **PDF**: [https://arxiv.org/pdf/2511.14515v1](https://arxiv.org/pdf/2511.14515v1)

在资源受限设备上实现轻量级设计与高性能的平衡仍是语音增强领域的重要挑战。现有先进方法（如MUSE）通过引入多路径增强泰勒变换器与可变形嵌入模块，以仅0.51M参数建立了强基线。但深度分析表明MUSE仍存在效率瓶颈：MET模块依赖复杂的“近似-补偿”机制来弥补泰勒展开注意力缺陷，而可变形嵌入的偏移量计算会引入额外计算负担。本文提出IMSE——系统优化的超轻量网络，其核心创新包括：1）用振幅感知线性注意力替代MET模块，通过显式保留注意力计算中查询向量的范数信息，从根本上解决线性注意力忽略振幅的问题，无需辅助补偿分支即可实现高效全局建模；2）用初始深度卷积替代DE模块，借鉴初始网络思想将大核卷积分解为并行分支（方形、水平与垂直条带），以极低参数量冗余捕获声谱图特征。在VoiceBank+DEMAND数据集上的实验表明，IMSE相较MUSE基线显著降低参数量16.8%（从0.513M降至0.427M），同时在PESQ指标上达到3.373的先进性能，为超轻量语音增强的模型尺寸与音质权衡确立了新基准。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强在资源受限设备上需平衡轻量化与高性能。现有方法如MUSE虽参数少（0.51M），但存在效率瓶颈。  
- **既有问题**：  
  - MET模块依赖复杂的“近似-补偿”机制，泰勒展开注意力忽略查询向量幅值信息，导致注意力分布平滑。  
  - 可变形嵌入的偏移计算与插值操作增加额外计算负担，限制实时部署。  

2)  
- **核心方法**：提出IMSE，通过两项创新优化结构：  
  - **幅值感知线性注意力（MALA）**：  
    - 替换MET模块，引入除法归一化策略，动态计算缩放因子β与偏移项γ。  
    - 保留查询向量幅值信息，使线性注意力生成尖锐分布，无需补偿分支即可实现高效全局建模。  
    - 复杂度保持线性（O(N)），通过预计算上下文向量提升效率。  
  - **初始深度可分离卷积（IDConv）**：  
    - 替换可变形嵌入模块，将大核卷积分解为并行分支（恒等映射、3×3卷积、1×11水平条带、11×1垂直条带）。  
    - 捕获频谱图的各向异性特征（如时间轴连续性、频率轴谐波结构），显著减少参数与计算量。  
- **整体架构**：基于U-Net主干，集成MALA与IDConv，在保持性能的同时压缩模型规模。  

3)  
- **任务与效果**：在VoiceBank+DEMAND数据集上评估：  
  - **参数量**：仅0.427M，较MUSE降低16.8%。  
  - **性能**：PESQ达3.373（与MUSE的3.370相当），CSIG与COVL略有提升，其他指标（CBAK、STOI）保持领先。  
  - **对比轻量模型**：参数量仅为TSTNN（0.92M）的一半，PESQ显著优于其2.96。  
  - **应用潜力**：为资源受限场景提供了参效比最优的语音增强方案。
</div>

</details>

---

## TTA: Transcribe, Translate and Alignment for Cross-lingual Speech Representation
- **Authors**: Wei Liu, Jiahong Li, Yiwen Shao, Dong Yu
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.14410v1](https://arxiv.org/abs/2511.14410v1)
- **PDF**: [https://arxiv.org/pdf/2511.14410v1](https://arxiv.org/pdf/2511.14410v1)

语音大语言模型在多模态与多任务语音理解中展现出卓越性能。典型的语音大语言模型范式是将语音模态与大型语言模型相融合。尽管现有研究常采用Whisper编码器处理语音输入，但该方案在输入格式、模型规模及语义性能方面存在局限。为此，我们提出专精于语音语义的轻量化TTA模型，以实现更高效的大语言模型集成。通过在多语言语音识别、语音翻译及语音-文本对齐三大任务上使用35.8万小时语音数据进行大规模训练，TTA能够生成鲁棒的跨语言语音表征。在语音识别/语音翻译、语音检索及语音识别-大语言模型性能评估等多样化基准测试中的广泛实验表明，TTA性能显著优于Whisper。此外，我们系统验证了跨语言能力与语音识别/语音翻译性能间的关联机制。TTA的模型权重与训练方案将作为音频理解工具包Auden的重要组成部分开源发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：语音大语言模型（Speech-LLM）在多模态语音理解中表现优异，通常将语音模态与大语言模型（LLM）结合。现有方法广泛采用Whisper编码器，但其存在以下问题：  
  - 输入格式受限，默认仅支持30秒语音；  
  - 语义性能不足，尤其在中文任务上表现较弱；  
  - 模型规模庞大，计算效率低。  
- **既有方法局限**：Whisper及其变体主要基于编码器-解码器架构，缺乏对跨语言语义对齐的显式优化，且未充分探索多语言语音识别（MASR）与语音翻译（ST）的联合训练对跨语言表示的影响。

2.  
- **核心方法**：TTA采用轻量级混合架构（ZT-AED），结合Zipformer Transducer与注意力编码器-解码器，并通过对比学习实现跨语言语义对齐。具体包括：  
  - **多任务训练**：使用358k小时多语言数据，联合优化MASR、ST及语音-文本对齐任务，增强语义表示能力；  
  - **跨语言对齐模块**：利用冻结的多语言BERT提取文本嵌入，通过SigLIP对比损失拉近语音与文本在多语言空间中的距离；  
  - **轻量化设计**：参数仅247M，显著小于Whisper系列（762M–1542M），提升计算效率。  
- **问题解决机制**：  
  - 通过ZT-AED架构支持灵活输入长度，克服Whisper的时长限制；  
  - 联合训练与对齐损失显式增强跨语言语义表示，改善中文等语言的性能；  
  - 轻量级模型在保持高性能的同时，提升LLM集成效率。

3.  
- **任务与效果**：  
  - **多语言语音识别（MASR）**：在Aishell、LibriSpeech等基准上，词错误率（WER）显著低于Whisper Medium，部分任务超越Whisper Large；  
  - **语音翻译（ST）**：在CoVoSTv2上BLEU得分优于Whisper Medium，接近Whisper Large；  
  - **跨语言语音检索**：检索准确率超越Whisper Large-v2，验证语义对齐有效性；  
  - **ASR-LLM集成**：作为编码器时，在中文和英语ASR任务中识别错误率更低，训练速度提升1.5–2倍。
</div>

</details>

---

## Accelerating Automatic Differentiation of Direct Form Digital Filters
- **Authors**: Chin-Yun Yu, György Fazekas
- **Categories**: eess.SY, eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2511.14390v1](https://arxiv.org/abs/2511.14390v1)
- **PDF**: [https://arxiv.org/pdf/2511.14390v1](https://arxiv.org/pdf/2511.14390v1)

本文提出一种通用方法，用于实现直接型数字滤波器的自动微分，其闭式反向传播过程包含初始条件梯度。该表达式能同时表征滤波器及其梯度计算，并支持并行运算。基于PyTorch的C++/CUDA实现相比原生Python版本加速超1000倍，且在GPU上始终保持最快运行速度。对于实际应用中常见的低阶滤波器，结合解析梯度的精确时域滤波方法在速度上优于频域方法。源代码已发布于https://github.com/yoyolicoris/philtorch。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：直接形式数字滤波器在信号处理中广泛应用，但集成到深度学习框架（如PyTorch）时，训练效率低下。  
- **既有问题**：  
  - PyTorch原生算子不支持递归滤波，需通过Python函数调用实现，速度慢。  
  - 常用频率采样方法存在时域混叠误差，需增加计算量或近似处理，精度与效率难以兼顾。  

2)  
- **核心方法**：提出通用状态空间形式的自动微分框架，推导解析梯度并实现为PyTorch底层算子。  
- **解决路径**：  
  - **统一表示**：将直接形式与转置直接形式滤波器统一为状态空间模型，简化梯度计算。  
  - **解析梯度**：通过反向传播时间算法推导参数梯度（如$\frac{\partial L}{\partial A}$、$\frac{\partial L}{\partial v(0)}$），避免自动微分框架开销。  
  - **并行优化**：利用结合性扫描算法加速状态递推，支持GPU并行计算。  
  - **实现策略**：  
    - 开发C++/CUDA扩展，注册自定义算子。  
    - 反向传播复用相同核函数，减少实现复杂度。  

3)  
- **任务与效果**：  
  - **任务**：低阶数字滤波器的时域前向计算与梯度反传。  
  - **效果**：  
    - 相比原生Python实现，速度提升超1000倍。  
    - GPU上持续最优性能，显著优于频率采样方法。  
    - 支持短序列优化，梯度计算精度与数值稳定性更优。
</div>

</details>

---

## Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions
- **Authors**: Marcel Gibier, Nolwenn Celton, Raphaël Duroselle, Pierre Serrano, Olivier Boeffard, Jean-François Bonastre
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2511.14307v1](https://arxiv.org/abs/2511.14307v1)
- **PDF**: [https://arxiv.org/pdf/2511.14307v1](https://arxiv.org/pdf/2511.14307v1)

本报告介绍了针对DCASE 2025挑战赛第五赛道音频问答任务所提出的解决方案。该系统采用自监督学习骨干网络BEATs提取帧级音频特征，通过分类头生成基于AudioSet本体的片段级声学事件预测。在生成事件级预测前对片段级预测结果进行校准，最终将校准后的事件预测与问题及候选答案共同构建结构化提示。该提示被输入至经过GRPO算法微调的Qwen2.5-7B-Instruct模型，该算法采用简易奖励函数进行训练。在开发集上达到62.6%的准确率，验证了结合声学事件推理与指令微调大语言模型在音频问答任务中的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频问答任务在多媒体交互中应用广泛，但现有方法主要依赖端到端多模态大语言模型，存在以下问题：  
  - 直接处理高维音频特征，导致模型可解释性差；  
  - 对音频与语言的对齐能力要求高，泛化性能受限；  
  - 依赖复杂后处理技术（如正则匹配）提取答案，效率较低。  

2)  
**核心方法通过以下步骤解决上述问题**：  
- **符号化事件提取与校准**：  
  - 使用预训练BEATs模型提取音频片段级事件概率，生成带时间戳的声学事件列表；  
  - 通过逻辑回归校准似然比，解决概率分数与真实分布不匹配问题，提升事件检测可靠性。  
- **结构化提示与语言模型推理**：  
  - 将校准后的事件、问题及选项组合为严格格式的文本提示；  
  - 采用GRPO算法对Qwen2.5-7B模型进行强化学习微调：  
    - 使用简单奖励函数（答案正确性）计算优势值，避免训练额外评论家网络；  
    - 通过概率比裁剪与KL散度约束，平衡策略更新稳定性与效果。  
- **关键优势**：  
  - 将低可靠性音频特征转化为高可解释性符号描述，降低语言模型推理难度；  
  - GRPO在少样本场景下优于传统监督微调，增强对事件结构化信息的利用能力。  

3)  
- **任务与效果**：  
  - 在DCASE 2025挑战赛的音频问答任务中，系统在开发集上达到62.6%准确率，显著优于最佳基线Gemini-2.0-Flash（52.5%）；  
  - 在时序事件推理（Part 2）和复杂场景问答（Part 3）中提升最显著，分别达到54.0%与67.5%准确率；  
  - 事件校准技术使跨类别检测阈值泛化能力增强，尤其在AudioSet对齐任务中效果突出。
</div>

</details>

---

## Segmentwise Pruning in Audio-Language Models
- **Authors**: Marcel Gibier, Raphaël Duroselle, Pierre Serrano, Olivier Boeffard, Jean-François Bonastre
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2511.14293v1](https://arxiv.org/abs/2511.14293v1)
- **PDF**: [https://arxiv.org/pdf/2511.14293v1](https://arxiv.org/pdf/2511.14293v1)

当前音频-语言模型在各类音频任务中展现出卓越性能，且能处理长音频输入。然而，这些模型的计算成本高度依赖于序列长度，而音频数据的特性会导致序列长度急剧增加。在视觉-语言领域，词元剪枝方法已被证明能有效减少词元数量，同时在标准基准测试中保持强劲性能。本研究探索了此类词元选择策略在音频-语言模型中的适用性与有效性，并通过提出一种考虑时间维度的轻量化策略实现改进。在仅保留四分之一初始词元的情况下，本方法在Clotho v2数据集上的CIDEr指标相对最大降幅为2%，在MMAU数据集上的准确率相对最大降幅为4%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频-语言模型在处理长音频输入时，计算成本随序列长度急剧增加，因为注意力机制的复杂度呈二次方增长。音频的时间维度通常远大于文本，导致计算开销显著。  
- **既有方法问题**：现有方法多基于视觉模型开发，音频领域研究较少；且主要依赖注意力分数进行剪枝，忽略了音频信号的时序特性，可能导致冗余令牌选择，降低信息覆盖的多样性。  

2)  
- **核心方法**：提出Segmentwise Top-K令牌剪枝策略，无需训练，仅用于推理阶段。该方法将音频编码器输出划分为非重叠的时序段，在每个段内选择注意力分数最高的子集令牌，确保令牌在时间上分布更均匀。  
- **解决思路**：  
  - 通过分段约束，减少冗余令牌，提高信息多样性，避免仅依赖全局注意力导致的局部信息丢失。  
  - 结合音频的时序特性，增强对信号连续性的建模，优于单纯基于注意力的方法（如Top-K）或混合方法（如VisionZip）。  
- **优势**：在保持性能的同时，显著降低计算量，且无需额外训练，易于部署。  

3)  
- **任务与效果**：在自动音频描述（Clotho v2、AudioCaps）和音频问答（ClothoAQA、MMAU）任务上评估。  
- **结果**：保留25%令牌时，性能接近全令牌基线（如CIDEr下降≤2%，准确率下降≤4%）；部分任务甚至表现更优（如Qwen2-Audio在AudioCaps上CIDEr提升23%）。方法在多个基准上一致优于对比方法，同时推理速度提升（预填充时间减少达4倍）。
</div>

</details>

---

## Count The Notes: Histogram-Based Supervision for Automatic Music Transcription
- **Authors**: Jonathan Yaffe, Ben Maman, Meinard Müller, Amit H. Bermano
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2511.14250v1](https://arxiv.org/abs/2511.14250v1)
- **PDF**: [https://arxiv.org/pdf/2511.14250v1](https://arxiv.org/pdf/2511.14250v1)

自动音乐转录（AMT）旨在将音频转换为符号化音乐表征。当前基于深度神经网络的AMT方法通常依赖强对齐训练数据，需精确到帧级别的标注。由于此类数据集的构建成本高昂且在多数音乐场景中难以实现，采用片段级标注的弱对齐方法逐渐受到关注。然而现有方法多依赖于动态时间规整或软对齐损失函数，仍要求局部语义对应关系，导致易出错且计算开销大。本文提出CountEM新型AMT框架，通过以音符事件直方图作为监督信号，摆脱显式局部对齐需求，实现更轻量计算与更强灵活性。该框架采用期望最大化方法，仅基于音符出现频次进行迭代优化预测，在显著降低标注成本的同时保持高转录精度。在钢琴、吉他与多乐器数据集上的实验表明，CountEM达到或超越现有弱监督方法，有效提升了AMT的鲁棒性、可扩展性与效率。项目页面详见https://yoni-yaffe.github.io/count-the-notes。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动音乐转录（AMT）依赖深度神经网络，但传统方法需精确的帧级对齐标注，成本高且不适用于复杂音乐场景。  
- **既有方法问题**：弱对齐方法（如动态时间规整DTW）仍需局部语义对应，易产生对齐错误、计算开销大，且假设音符顺序一致，在真实演奏（如琶音）中常失效。  

2)  
- **核心方法**：提出CountEM框架，基于音符事件直方图进行监督，无需显式对齐。  
- **解决过程**：  
  - 使用期望最大化（EM）算法迭代优化：  
    - **E步**：通过峰值检测从模型预测的后验图中选取每个音符最可能的K个起始时间（K由直方图计数决定），生成强对齐标签。  
    - **M步**：用估计标签更新模型参数，采用加权二元交叉熵损失处理标签稀疏性。  
  - 引入数据增强（如音高偏移）提升泛化性。  
- **优势**：直方图监督简化计算，避免DTW的对齐错误，对结构变化（如音符顺序混乱）更鲁棒，显著降低标注需求。  

3)  
- **任务与效果**：  
  - **钢琴转录**（MAESTRO数据集）：F-score达94.2%（30秒窗口），接近全监督水平（96.0%）。  
  - **吉他转录**（GuitarSet/GAPS）：跨数据集F-score提升超15%，最高达93.0%。  
  - **多乐器转录**（MusicNet/URMP）：在弦乐和管乐上F-score达91.6%，优于部分DTW方法。  
- **总结**：方法在减少标注成本下，保持高转录精度，并展现跨乐器与噪声环境的鲁棒性。
</div>

</details>

---

## Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation
- **Authors**: Kumud Tripathi, Aditya Srinivas Menon, Aman Gaurav, Raj Prakash Gohil, Pankaj Wasnik
- **Categories**: cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.14219v1](https://arxiv.org/abs/2511.14219v1)
- **PDF**: [https://arxiv.org/pdf/2511.14219v1](https://arxiv.org/pdf/2511.14219v1)

作为开源自动语音识别系统，Whisper模型因其在多语言和零样本场景下的优异性能而被广泛采用，但该模型经常产生幻听错误，尤其在噪声环境下更为突出。现有针对Whisper类ASR系统的改进主要集中于音频预处理或转录后处理，而通过直接修改模型结构来抑制幻听的研究尚不充分。为此，我们提出一种两阶段架构：首先通过自适应层级注意力机制增强编码器鲁棒性，继而采用多目标知识蒸馏框架进一步抑制幻听。第一阶段通过层间相关性分析将编码器层级划分为语义连贯的模块，利用可学习的多头注意力融合各模块表征，使模型能协同利用低阶与高阶特征实现更鲁棒的编码。第二阶段通过让学生模型在噪声音频上训练，使其语义分布和注意力分布与处理纯净音频的教师模型对齐。在噪声语音基准测试中，该方法在保持纯净语音性能的同时，显著降低了幻听现象与词错误率。自适应层级注意力与知识蒸馏共同为提升Whisper在真实噪声环境下的可靠性提供了系统化解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：Whisper作为开源自动语音识别系统，在多语言和零样本场景下表现优异，但在噪声环境中易产生“幻觉错误”，即生成流利但语义错误的转录。  
- **既有方法问题**：现有方法多依赖音频预处理或转录后处理过滤错误内容，未直接修正模型内部表征。这些方法无法解决噪声导致的编码器-解码器内部表征错位问题，治标不治本。  

2)  
- **自适应层注意力**：  
  - 通过层间相关性分析将编码器层分组为语义连贯的块，例如低层（声学特征）和高层（语义抽象）。  
  - 使用可学习多头注意力动态融合块表征，使模型能联合利用低层与高层特征，提升噪声下的上下文建模能力。  
- **多目标知识蒸馏**：  
  - 以干净语音训练的模型为教师，噪声环境下的学生模型通过复合损失函数对齐教师行为：  
    - 交叉熵损失保证转录准确性；  
    - 编码器/解码器最终层余弦相似度损失对齐语义表征；  
    - 解码器交叉注意力图均方误差损失迁移教师注意力模式。  
- **协同机制**：ALA增强编码器鲁棒性后，知识蒸馏进一步约束解码器在噪声下的语义生成，从表征和注意力层面共同抑制幻觉。  

3)  
- **任务与效果**：在含噪声的阿拉伯语、法语、印地语和英语语音识别任务中：  
  - 显著降低词错误率，尤其在低信噪比条件下提升明显；  
  - 语义一致性指标SeMaScore显著提高，证明幻觉减少；  
  - 干净语音场景下性能保持稳定，未因噪声优化而损失原有能力。
</div>

</details>

---

## FxSearcher: gradient-free text-driven audio transformation
- **Authors**: Hojoon Ki, Jongsuk Kim, Minchan Kwon, Junmo Kim
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.14138v1](https://arxiv.org/abs/2511.14138v1)
- **PDF**: [https://arxiv.org/pdf/2511.14138v1](https://arxiv.org/pdf/2511.14138v1)

【中文摘要】
现有音频转换方法因依赖有限的可微分音效集合，难以通过文本提示实现多样化且高质量的音频转换。本文提出\textbf{FxSearcher}——一种无需梯度的新型框架，通过搜索音频效果器（FX）的最优配置来实现基于文本提示的源信号转换。该方法采用贝叶斯优化与基于CLAP的评分函数实现高效搜索，并引入引导提示以抑制不良伪影并增强人类偏好。为客观评估性能，我们提出了基于人工智能的评估框架。实验结果表明，本方法在各项指标上的最高得分与人类偏好高度吻合。演示内容详见 https://hojoonki.github.io/FxSearcher/

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本驱动音频转换旨在通过自然语言描述修改音频信号。现有方法主要分为两类：基于黑盒神经网络的方法缺乏可解释性且可能损坏原始信号；基于音频效果器的方法虽可解释但受限于可微分性。  
- **既有问题**：梯度依赖方法（如Text2FX）因可微分性约束，仅能使用有限效果器（如均衡器、混响），导致音色多样性不足；基于大语言模型的方法（如LLM2FX）忽略源音频，仅依赖文本生成参数，效果受限。  

2)  
- **核心框架**：FxSearcher采用无梯度优化框架，通过贝叶斯优化搜索音频效果器链的最优参数配置，摆脱可微分性限制，兼容任意商业级效果器。  
- **评分函数设计**：  
  - 基于CLAP模型计算目标提示词与生成音频的语义相似度作为目标分数。  
  - 引入引导提示词（描述失真、浑浊等常见瑕疵），计算瑕疵相似度作为引导分数。  
  - 最终分数为目标分数与引导分数之差，平衡语义匹配与听觉质量。  
- **优化过程**：  
  - 使用高斯过程建模参数与分数的概率关系，通过采集函数平衡探索与利用。  
  - 迭代更新参数，逐步逼近最优解，生成高质量且符合人类偏好的音频。  

3)  
- **任务与效果**：在语音和乐器音频转换任务中，FxSearcher在多项指标上优于基线：  
  - 人类主观评价（MOS）得分最高（语音3.48，乐器3.46），显著提升听觉偏好。  
  - AI评估中，Qwen评分（语音2.73，乐器3.18）和Gemini胜率（语音61.8%，乐器71.6%）均领先。  
  - 语音任务中显著改善清晰度（WER降低至37.5%）和稳定性（响度分布更平滑）。
</div>

</details>

---
