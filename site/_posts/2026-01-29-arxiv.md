---
layout: post
title: "arXiv Daily – 2026-01-29"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-29（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-28 08:50 — 2026-01-29 08:50
- 抓取总数：11 篇 | 本页显示：11 篇（去重/过滤后）

## Gen-SER: When the generative model meets speech emotion recognition
- **Authors**: Taihui Wang, Jinzheng Zhao, Rilin Chen, Tong Lei, Wenwu Wang, Dong Yu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.20573v1](https://arxiv.org/abs/2601.20573v1)
- **PDF**: [https://arxiv.org/pdf/2601.20573v1](https://arxiv.org/pdf/2601.20573v1)

语音情感识别在语音理解与生成中至关重要。现有方法主要基于分类模型或大语言模型。不同于以往思路，本文提出Gen-SER，通过生成模型将语音情感识别重新定义为分布偏移问题。我们将离散类别标签映射至连续空间，并通过正弦分类编码获得目标分布。采用基于目标匹配的生成模型，将初始分布高效转化为目标分布。通过计算生成的目标分布与真实目标分布的相似度实现分类。实验结果验证了该方法的有效性，证明了其可扩展至多种语音理解任务，并展现出在更广泛分类任务中的潜在适用性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音情感识别是语音理解与人机交互的关键任务。传统方法主要分为两类：
  - **分类模型**：使用语音编码器（如HuBERT、WavLM）提取特征后接分类器。
  - **大语言模型**：将语音特征对齐到LLM输入，通过提示生成情感标签。
- **既有问题**：
  - 基于扩散的分类方法将分类视为条件密度估计，但推理延迟随类别数线性增长，计算效率低。
  - 潜在空间投影方法（如自编码器）可提升效率，但多类别任务中自编码器训练困难，重构误差会影响可扩展性。

2)  
论文提出Gen-SER，将SER重构为分布迁移问题，核心方法如下：

- **问题重构**：
  - 假设不同情感的语音信号服从不同的初始分布，每个类别标签对应预定义的终端分布。
  - 分类任务转化为将初始分布迁移至终端分布的问题。

- **连续标签编码**：
  - 提出**正弦分类编码**，将离散类别标签映射为连续空间中的正交向量（公式2）。
  - 优点：避免训练多类别自编码器，生成连续且正交的嵌入向量，便于学习。

- **生成式分布迁移**：
  - 使用预训练HuBERT提取语音特征作为初始样本 \(x_1\)，并提取条件变量 \(X_c\)。
  - 采用**目标匹配生成模型**，直接预测终端嵌入向量 \(x_0\)，而非估计整个向量场。
  - 设计**逻辑均值计划**（公式6）与**桥接方差计划**（公式7），有效扰动信号并控制高斯分布。
  - 训练目标为最小化预测终端向量与真实终端向量的欧氏距离（公式8）。

- **高效推理**：
  - 推理时，通过欧拉ODE求解器迭代估计终端样本 \(\hat{x}_0\)。
  - 分类通过计算 \(\hat{x}_0\) 与各类别终端向量的余弦相似度完成（公式12），选择相似度最高的类别。

- **优势**：
  - 避免了逐类别评估的计算开销，提升了效率。
  - 无需复杂自编码器，通过简单编码生成终端分布，增强了可扩展性。

3)  
- **任务与效果**：
  - **语音情感识别**：在MELD测试集上，准确率达到56.5%，优于多数分类模型（如WavLM的50.6%）和LLM方法（如Qwen2-audio的55.3%），但低于大规模训练的SenseVoice-L（63.1%）。
  - **性别分类**：在内部数据集上，准确率达到90.5%，超越Fbank+分类器（86.6%）、WavLM+分类器（87.5%）及部分LLM方法（如Soundwave的90.3%）。
- **结论**：该方法在SER和性别分类任务上均表现优异，展示了其可扩展性，为语音理解任务提供了新思路。
</div>

</details>

---

## Decoding Speech Envelopes from Electroencephalogram with a Contrastive Pearson Correlation Coefficient Loss
- **Authors**: Yayun Liang, Yuanming Zhang, Fei Chen, Jing Lu, Zhibin Lin
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.20542v1](https://arxiv.org/abs/2601.20542v1)
- **PDF**: [https://arxiv.org/pdf/2601.20542v1](https://arxiv.org/pdf/2601.20542v1)

近年来，从脑电图（EEG）信号重建语音包络的技术进展，使得在多说话人环境中实现连续听觉注意解码（AAD）成为可能。目前大多数基于深度神经网络（DNN）的包络重建模型，其训练目标在于最大化目标语音包络与重建包络之间的皮尔逊相关系数（PCC）。然而，在听觉注意解码中，目标PCC与非目标PCC之间的差异具有关键作用，而现有方法往往仅侧重于最大化目标PCC。为此，我们提出了一种对比PCC损失函数，该函数直接表征目标PCC与非目标PCC之间的差异。我们在三个公开的EEG AAD数据集上，使用四种DNN架构对所提方法进行了评估。实验结果表明，在多种设置下，该目标函数能够有效提升包络可分离性与AAD准确率，同时也揭示了依赖于数据集和网络架构的失效情况。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于脑电图（EEG）的语音包络重建是连续听觉注意解码（AAD）的关键技术，用于在多说话人环境中识别听者关注的语音流。  
- **既有方法的问题**：现有深度神经网络（DNN）模型通常采用皮尔逊相关系数（PCC）损失函数，仅最大化重建包络与关注语音包络之间的相关性（关注PCC）。然而，AAD的准确性实际依赖于关注PCC与未关注PCC之间的差异，而现有方法未显式优化这一差异，导致模型可能同时记住关注与未关注语音的共有特征，从而降低两类PCC的可区分性。

2)  
- **核心方法**：论文提出了一种对比PCC损失函数（L_ΔPCC），其定义为关注PCC与未关注PCC均值之间的差值：  
  `L_ΔPCC = -ρ_a + (1/(N-1)) * Σ ρ_u,j`  
  其中ρ_a为关注PCC，ρ_u,j为与第j个未关注语音包络的PCC。该损失函数同时优化两个目标：  
  - 最大化关注PCC（即-ρ_a项）；  
  - 最小化未关注PCC的均值（即求和项），从而显式扩大关注与未关注语音流之间的相关性差异。  
- **解决思路**：  
  - 通过引入未关注PCC作为对比项，迫使模型学习更具判别性的特征，抑制对未关注语音的编码，而不仅仅是提高对关注语音的拟合。  
  - 为避免模型通过将所有相关性推向负值来“作弊”，损失函数使用未关注PCC的均值而非简单求和，确保了优化的稳定性。  
  - 该方法不依赖网络结构创新，而是通过改进回归目标函数，直接针对AAD的决策指标（PCC差异）进行优化，从而提升模型在多种架构下的泛化性能。

3)  
- **任务与效果**：在三个公开EEG AAD数据集（KUL、DTU、KUL-AV-GC）和四种DNN模型（VLAAI、LSM-CNN、EEGMamba、EEGDeformer）上评估：  
  - **主要效果**：相比传统PCC损失，使用对比PCC损失在多数情况下显著提高了解码准确率（AAD准确率）和PCC差异（ΔPCC），平均ΔPCC相对提升17.84%。  
  - **机制验证**：提升主要源于未关注PCC的显著降低，而非单纯提高关注PCC，增强了包络的可分离性。  
  - **局限性**：效果受数据集特性、模型架构及时间窗长度影响，在部分配置下（如EEGMamba在DTU数据集）性能提升不明显。
</div>

</details>

---

## Audio Deepfake Detection in the Age of Advanced Text-to-Speech models
- **Authors**: Robin Singh, Aditya Yogesh Nair, Fabio Palumbo, Florian Barbaro, Anna Dyka, Lohith Rachakonda
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.20510v1](https://arxiv.org/abs/2601.20510v1)
- **PDF**: [https://arxiv.org/pdf/2601.20510v1](https://arxiv.org/pdf/2601.20510v1)

随着文本转语音（TTS）系统的快速发展，合成语音的真实性显著提升，为音频深度伪造检测带来了新的挑战。本研究对三种前沿TTS模型——代表流式生成、基于大语言模型（LLM）以及非自回归架构的Dia2、Maya1和MeloTTS——进行了对比评估。基于Daily-Dialog数据集生成的12,000个合成音频样本，我们在语义、结构和信号层面四种检测框架下进行了系统测试。结果表明，检测器性能在不同生成机制间存在显著差异：针对某一TTS架构有效的检测模型可能对其他架构（尤其是基于LLM的合成方法）失效。相比之下，融合多层面互补分析的多视角检测方法在所有评估模型中均表现出鲁棒性能。这些发现揭示了单一范式检测器的局限性，并强调了采用集成化检测策略以应对不断演化的音频深度伪造威胁的必要性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：先进的文本转语音模型极大提升了合成语音的真实感，对音频深度伪造检测构成新挑战。现有检测方法主要基于针对早期声码器系统的基准，如ASVspoof 2021。
- **既有问题**：检测器对新型TTS架构（如基于LLM、流式、非自回归的模型）的泛化能力不足。单一检测范式无法应对多样化的生成机制，且缺乏对2024-2025年最新TTS模型（如Dia2、Maya1、MeloTTS）的系统评估。

2)  
- **核心方法**：论文采用多视角检测策略，系统评估了三种前沿TTS模型（代表流式、LLM基、非自回归架构）与四种检测框架的组合。
    - **构建新数据集**：使用DailyDialog语料库生成了12,000个合成音频样本，涵盖三种TTS范式，确保语言和对话真实性。
    - **多层面检测框架**：
        - **语义理解**：使用微调后的Whisper-MesoNet，捕捉高层语义不一致性。
        - **结构图建模**：采用SSL-AASIST，通过图网络分析时频区域关系。
        - **分层表征学习**：应用XLS-R-SLS，动态融合预训练模型不同层的特征，以同时捕获低层声学异常和高层上下文缺陷。
        - **大规模基础模型**：评估了MMS-300M等模型。
    - **评估专有模型**：同时测试了UncovAI的专有检测模型作为对比。
- **解决思路**：通过结合互补的分析层次（语义、结构、信号级），克服单一检测器对特定TTS架构的偏见，从而实现对多样化现代合成语音的鲁棒检测。

3)  
- **检测任务与效果**：在针对Dia2、Maya1、MeloTTS生成的合成音频检测任务中：
    - **单一检测器表现不均**：Whisper基检测器对MeloTTS效果较好（EER 17.05%），但对Maya1效果差（EER 35.95%）；XLS-R-SLS对Dia2效果最佳（EER 7.07%），但对MeloTTS效果较差（EER 27.10%）。
    - **多视角方法的必要性**：结果证实了结合不同分析层次的重要性。
    - **专有模型表现突出**：UncovAI的专有检测模型在所有三类合成音频上均取得近乎完美的性能（F1分数均高于0.98）。
</div>

</details>

---

## Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech
- **Authors**: Myungjin Lee, Eunji Shin, Jiyoung Lee
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.20481v1](https://arxiv.org/abs/2601.20481v1)
- **PDF**: [https://arxiv.org/pdf/2601.20481v1](https://arxiv.org/pdf/2601.20481v1)

现代零样本文本转语音（TTS）模型展现出前所未有的表现力，但也带来了严重的犯罪风险，因为它们能够合成未经本人同意的个体语音。在此背景下，说话人遗忘技术旨在根据请求阻止生成特定说话人身份。现有方法依赖重新训练，成本高昂且仅限于训练集中出现过的说话人。本文提出TruS，一种无需训练的说话人遗忘框架，将范式从数据删除转向推理时控制。TruS通过引导身份特定的隐藏激活来抑制目标说话人，同时保持其他属性（如韵律和情感）。实验结果表明，TruS能有效阻止对已知及未知退出说话人的语音生成，为语音合成建立了可扩展的安全保障。演示与代码详见http://mmai.ewha.ac.kr/trus。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：零样本文本转语音（TTS）技术能合成任意人的声音，带来隐私与安全风险，如未经授权模仿他人声音。  
- **既有方法问题**：  
  - **水印技术**：事后追溯，无法实时阻止合成。  
  - **语音匿名化**：替换身份而非禁止生成特定身份。  
  - **基于再训练的遗忘方法**：计算成本高、难以扩展，且无法处理训练集外的说话人。

2)  
论文提出 **TruS**，一种无需训练、在推理时实现的说话人遗忘框架，核心方法如下：  
- **构建身份原型向量**：  
  - 使用保留说话人的语音样本，在TTS模型的扩散变换器块中提取隐藏激活，计算平均得到“身份原型”向量，作为身份中性的参考基准。  
- **动态选择干预层与时间步**：  
  - 计算目标（选择退出）说话人激活与身份原型之间的余弦相似度。  
  - 根据相似度分布的全局均值与方差动态设定阈值，自动选择相似度较低的层和时间步进行干预，确保只针对身份相关特征。  
- **推理时隐层激活引导**：  
  - 为每个选定的干预点计算身份特定的引导向量，即目标激活与身份原型的归一化差值。  
  - 在去噪过程中，从目标激活中减去其在该引导向量上的投影，以抑制身份信息，同时保留语言内容、韵律和情感等属性。  
- **关键优势**：  
  - **无需训练**：直接操作预训练模型，实现即插即用。  
  - **动态适应性**：根据每个目标说话人的激活模式自动调整干预点，避免过度干扰。  
  - **可扩展性**：支持连续处理新的选择退出请求，适用于训练集内外的说话人。

3)  
- **任务与效果**：  
  - **可见选择退出说话人**：在Emilia数据集上，TruS将说话人相似度（SIM-SO）显著降低至0.477，同时词错误率（WER-SO）为3.25，优于需要大量再训练的基线方法。  
  - **未见选择退出说话人**：在LibriSpeech数据集上，TruS将相似度（SIM-UO）降至0.488，成功阻止训练集外说话人的声音合成，证明了其零样本泛化能力。  
  - **情感保持**：在CREMA-D情感语音数据集上，TruS在有效遗忘身份的同时，保持了与原始模型相近的情感相似度（SIM-Emo约0.723）。
</div>

</details>

---

## On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style
- **Authors**: Adam Štefunko, Carlos Eduardo Cancino-Chacón, Jan Hajič
- **Categories**: cs.SD, cs.IR
- **arXiv**: [https://arxiv.org/abs/2601.20478v1](https://arxiv.org/abs/2601.20478v1)
- **PDF**: [https://arxiv.org/pdf/2601.20478v1](https://arxiv.org/pdf/2601.20478v1)

通奏低音是一种巴洛克时期的即兴伴奏风格，演奏者需在羽管键琴或管风琴上根据乐谱中的给定低音线条即兴演绎多个声部。这一传统并非仅具历史意义，更是一种受历史启发的活态实践；《对齐通奏低音数据集》（ACoRD）首次在符号化领域记录了现代通奏低音演奏样本。该数据集包含7位演奏者对5部通奏低音乐谱的175段MIDI录音，使我们得以开始观察和分析通奏低音即兴演奏带来的多样性。近期提出的通奏低音演奏-乐谱对齐系统，为将即兴演奏音符映射至乐谱音符提供了方法。为研究对齐后的通奏低音演奏，我们需要一种合适的特征表示方法。本文提出“格里夫”表示法，其设计灵感来源于历史上的通奏低音论著。该方法能以移调不变的方式编码通奏低音实现的音高内容与结构。格里夫通过将演奏中对齐到同一乐谱音符的音符按起始时间顺序分组，直接从对齐后的通奏低音演奏中提取，形成有意义的特征单元，构建出可用于分析通奏低音演奏风格的特征空间。我们统计描述了从ACoRD数据集录音中提取的格里夫特征，并通过两项实验展示如何利用格里夫对不同演奏者的通奏低音演奏风格进行个体性统计分析。最后，我们论证了为何需要保持通奏低音即兴演奏的结构以精细分析演奏者的个人风格，以及格里夫为何能构建一个具有历史依据、值得进一步实证验证的有意义特征空间。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：通奏低音是一种巴洛克即兴伴奏风格，基于给定低音线在羽管键琴或管风琴上即兴演奏多个声部。它不仅是历史实践，也是当代受历史启发的活态传统。
- **既有问题**：尽管通奏低音处于成熟音乐风格与口传即兴实践的交汇点，但对其当代演奏的分析研究很少。现有数据集（如ACoRD）虽记录了现代演奏，但缺乏合适的特征表示方法来捕捉即兴演奏的结构与风格差异。

2)  
- **核心方法**：论文提出一种名为 **griff** 的特征表示方法，灵感来源于历史通奏低音论著。它通过将演奏音符映射到乐谱音符来构建，旨在编码演奏的音高内容与结构，同时保持对移调的不变性。
- **具体构建**：
  - **有序griff**：将映射到同一乐谱音符的演奏音符按onset时间顺序分组，用向量序列表示，以区分块状和弦与有意的琶音（使用35ms时间窗）。
  - **池化griff**：将同一乐谱音符对应的所有不同音高合并为一个向量。
  - 为消除调性影响，将MIDI音高转换为相对于乐谱音符的半音音程，并用字符串编码（音程间用下划线分隔，向量间用竖线分隔）。
- **解决思路**：
  - griff保留了演奏的结构化音高信息，而丢弃了绝对时间关系，避免了节奏差异对结构等效模式的掩盖。
  - 它提供了比原始音高或音程更精细的表示粒度，契合历史上对通奏低音的思考方式，有望用于分析不同演奏者的个人风格。

3)  
- **任务与效果**：
  - 在**ACoRD数据集**上提取并统计了griff，共得到17,152个griff（有序表示7041种类型，池化表示2817种类型）。
  - 通过**演奏者风格分析实验**表明：
    - 使用简单的音程频率表示无法区分不同演奏者。
    - 在griff特征空间中，同一演奏者不同演奏之间的相似性高于不同演奏者之间的相似性，表明griff能有效捕捉个人演奏风格的差异。
  - 这些结果为griff作为“历史启发”的特征空间提供了初步实证支持，值得进一步验证（如聚类、分类实验）。
</div>

</details>

---

## Self Voice Conversion as an Attack against Neural Audio Watermarking
- **Authors**: Yigitcan Özer, Wanying Ge, Zhe Zhang, Xin Wang, Junichi Yamagishi
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.20432v1](https://arxiv.org/abs/2601.20432v1)
- **PDF**: [https://arxiv.org/pdf/2601.20432v1](https://arxiv.org/pdf/2601.20432v1)

音频水印技术可在保持说话人身份、语言内容及感知质量的前提下，将辅助信息嵌入语音中。尽管基于神经网络的数字信号处理水印方法在不可感知性和嵌入容量方面已取得进展，但其鲁棒性评估仍主要针对压缩、加性噪声和重采样等传统失真类型。然而，基于深度学习的攻击手段兴起，为水印安全性带来了全新且严峻的威胁。本研究提出将自语音转换作为一种通用且保持内容不变的攻击方式，用于对抗音频水印系统。自语音转换通过语音转换模型在维持说话人身份不变的同时改变声学特征。实验表明，该攻击会严重削弱当前先进水印方法的可靠性，并揭示了其对现代音频水印技术安全性的深远影响。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频水印技术用于在语音中嵌入辅助信息，以进行来源验证和版权保护。随着深度学习的发展，基于神经网络的音频水印方法在不可感知性和嵌入容量上有所提升。  
- **既有方法的问题**：现有水印技术的鲁棒性评估主要针对传统失真（如压缩、噪声、重采样），而忽略了基于深度学习的恶意攻击。特别是，当前评估中考虑的对抗性转换（如音高变换）未能反映现代攻击者利用语音合成与转换技术的能力，导致水印鲁棒性被系统性高估。

2)  
- **核心方法**：本文提出**自语音转换**作为一种针对神经音频水印的通用攻击方法。该方法利用语音转换模型，将输入语音重新映射到同一说话人身份，在保持说话人特征和语言内容不变的同时，通过解耦的隐表示（如内容、说话人身份、音高）重构语音，从而微妙地改变声学特性。  
- **攻击流程**：  
  - 首先，将带水印的语音分解为内容特征、说话人身份和可选音高轮廓。  
  - 然后，通过解码器融合这些表示生成梅尔频谱图，再经神经声码器合成为波形。  
  - 这一重构过程会抑制与水印相关的细粒度信号变化，而保持感知质量。  
- **解决思路**：  
  - 自语音转换攻击模拟了现实中的恶意对手，其能够利用先进的语音转换工具（如kNN-VC和RVC）进行内容保持的转换。  
  - 攻击不依赖于水印系统的内部知识，仅作为水印嵌入与检测之间的中间处理阶段，符合实际部署场景。  
  - 实验表明，该攻击能有效破坏多种水印方法，因为水印信息往往依赖于声学线索，而这些线索在解耦与重构过程中被丢弃。

3)  
- **评估任务**：在LibriTTS数据集上测试了自语音转换攻击对五种水印方法的影响，包括经典DCT方法和基于深度学习的AudioSeal、Timbre、WMCodec、VoiceMark。  
- **取得效果**：  
  - 自语音转换攻击使所有水印方法的比特提取准确率降至接近随机猜测水平（约0.5），显著优于传统声码器攻击。  
  - 攻击在保持高说话人相似性、低词错误率和良好感知质量的同时，实现了水印的失效，证明了其作为内容保持攻击的有效性和实用性。
</div>

</details>

---

## Mix2Morph: Learning Sound Morphing from Noisy Mixes
- **Authors**: Annie Chu, Hugo Flores García, Oriol Nieto, Justin Salamon, Bryan Pardo, Prem Seetharaman
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.20426v1](https://arxiv.org/abs/2601.20426v1)
- **PDF**: [https://arxiv.org/pdf/2601.20426v1](https://arxiv.org/pdf/2601.20426v1)

本文提出Mix2Morph——一种通过微调实现的文本到音频扩散模型，能够在无需专门形态转换数据集的情况下完成声音形态融合。该模型通过在较高扩散步数下对含噪替代混合音频进行微调，能够生成稳定且感知连贯的形态融合结果，有效融合两种源声音的特征。我们特别关注声音注入这一形态融合的子类：在感知与实践层面，该方法以一个声音作为主导源，提供整体时间结构与行为框架，同时将另一声音作为次要源贯穿融合，从而丰富其音色与纹理特性。客观评估与听感实验表明，Mix2Morph在多种声音类别中均能生成高质量的声音注入效果，其性能优于现有基线方法，为构建更具可控性与概念驱动的声音设计工具迈出了重要一步。声音示例请访问 https://anniejchu.github.io/mix2morph 。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声音设计常需将两个声音融合成一个兼具两者特征的“声音变形”。传统方法（如基于感知描述符的特征插值和交叉合成）在处理谐波声音时有效，但在处理非谐波声音（如环境噪声、音效）时效果不佳。  
- **既有方法问题**：  
  - 传统DSP方法难以处理非谐波声音。  
  - 深度学习方法（如MorphFader、SoundMorpher）常出现“中点塌陷”，即中间变形结果不稳定，听起来像简单叠加而非融合。  
  - 现有方法缺乏专门针对“声音注入”的训练数据，难以在保持主声音主导性的同时融入次声音特征。

2)  
Mix2Morph的核心方法是通过微调文本到音频扩散模型，利用噪声混合数据学习声音变形，无需专门的变形数据集。具体解决策略如下：  
- **构建噪声替代数据**：  
  - 将两个声音的简单加性混合作为“坏”的变形数据。  
  - 通过时域和频域增强（如RMS包络对齐、频谱平均）使混合声音在结构和频谱上更接近真实变形，作为训练目标。  
- **基于扩散模型的训练策略**：  
  - 采用“无浪费”数据策略，将噪声替代数据仅分配给扩散过程的高时间步（如t ∈ [0.5, 1]），让模型学习变形的整体结构，而低时间步则依赖预训练模型细化细节。  
  - 这种分配方式使模型能利用不完美数据学习高级变形概念，避免过拟合到混合伪影。  
- **多样化增强模式**：  
  - 定义四种增强模式（仅RMS、仅频谱、两者结合、无增强），并随机应用于训练对，增加数据多样性。  
  - 每种模式配以特定文本描述，引导模型理解不同变形属性（如主声音行为、频谱混合）。  
- **整体优势**：  
  - 避免了收集高质量变形数据集的困难，通过噪声混合数据实现了大规模训练。  
  - 模型能生成感知连贯的“声音注入”，即主声音保持时域结构，次声音丰富音色纹理。

3)  
- **任务**：在“声音注入”任务上评估，即生成主声音主导、次声音融入音色的变形声音。  
- **效果**：  
  - **客观指标**：在50个概念对的测试集上，Mix2Morph在对应性（0.725）、中间性（0.648）和方向性（0.436）上优于基线模型（如LGrS、MorphFader），同时保持较低的FAD分数（1.220），表明生成质量高。  
  - **主观听测**：在25名参与者的测试中，Mix2Morph的变形率最高（77%），平均意见得分（MOS）为3.52，显著优于其他基线，被认为能生成更连贯、高质量的变形声音。
</div>

</details>

---

## Switchcodec: Adaptive residual-expert sparse quantization for high-fidelity neural audio coding
- **Authors**: Xiangbo Wang, Wenbin Jiang, Jin Wang, Yubo You, Sheng Fang, Fei Wen
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.20362v1](https://arxiv.org/abs/2601.20362v1)
- **PDF**: [https://arxiv.org/pdf/2601.20362v1](https://arxiv.org/pdf/2601.20362v1)

近期神经音频压缩模型常采用残差向量量化实现高保真编码，但固定每帧码本数量的方案难以适应音频内容的高度多样性——尤其对于极简或极复杂的信号。为突破此限制，我们提出SwitchCodec，一种基于残差专家向量量化的神经音频编解码器。该方案将共享量化器与动态路由的专家量化器相结合，专家量化器根据输入音频激活，从而解耦比特率与码本容量，提升压缩效率。此设计确保每个量化器都能得到充分训练与利用。此外，通过可变比特率机制在推理阶段动态调整激活的专家量化器数量，无需重新训练即可实现多比特率操作。实验表明，SwitchCodec在客观指标与主观听感测试中均优于现有基线模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：高保真神经音频编码通常依赖残差向量量化，但现有方法采用固定数量的码本，无法适应音频内容的巨大差异。这导致对简单片段过度分配比特，而对复杂内容则量化不足。
- **既有方法问题**：
  - 固定结构的残差向量量化（RVQ）在低比特率下性能显著下降，码本容量有限。
  - 混合专家向量量化（MoE-VQ）缺乏结构化的残差层次，训练不稳定。
  - 自适应RVQ等方法依赖启发式阈值，泛化能力有限，且动态比特率分配通常需要复杂重训练。

2)  
论文提出了**SwitchCodec**，其核心是**残差专家向量量化**框架，通过以下机制解决问题：
- **双路径量化设计**：
  - **共享基础量化器**：首先捕获通用结构，提供稳定的基线表示。
  - **专家量化器池**：一组可路由的专家量化器，通过学习的门控网络根据输入音频特征进行稀疏激活。
- **关键创新：选择与顺序解耦**：
  - **自适应选择**：门控网络根据潜在特征（如频谱平坦度、时间熵）计算亲和度分数，为每个音频窗口选择最相关的top-k个专家。
  - **固定顺序应用**：被选中的专家按照其预定义的索引升序依次应用，而非按选择分数排序。这确保了能量递减的残差层次结构得以保留，继承了传统RVQ的优点。
  - **优势**：此举将比特率与码本容量解耦——比特率取决于激活的专家数量，而非固定的级联深度。避免了在简单片段上浪费比特，同时能在复杂内容上保留细节。
- **轻量级可变比特率机制**：
  - 在推理时，通过调整激活的专家数量（即k值），单个模型即可覆盖从0.89 kbps到8 kbps的比特率范围，无需重新训练。
  - 仅需传输一个标识激活专家的路由掩码作为边信息，其开销极低（例如在2.67 kbps下仅约2.2 bps，占比<0.1%）。

3)  
- **任务**：在44.1 kHz采样率下的高保真神经音频压缩。
- **效果**：
  - **客观指标**：在2.67 kbps和5.33 kbps下，在Mel谱距离、STFT距离、PESQ和ViSQOL等指标上均显著优于EnCodec和DAC等基线模型。
  - **主观听感**：MUSHRA评分分别达到91.7和93.4（接近透明质量），远高于基线模型。
  - **模型效率**：通过扩展专家池并保持稀疏激活，在维持高质量的同时实现了码本的高效利用。
</div>

</details>

---

## ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy
- **Authors**: Ya-Tse Wu, Chi-Chun Lee
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.20319v1](https://arxiv.org/abs/2601.20319v1)
- **PDF**: [https://arxiv.org/pdf/2601.20319v1](https://arxiv.org/pdf/2601.20319v1)

本研究探讨了情感语音与生成策略对自动语音识别（ASR）性能的影响。通过分析三种情感文本转语音（TTS）模型合成的语音，我们发现替换错误占主导地位，且不同模型的情感表现力存在差异。基于此，我们提出了两种生成策略：一种基于转录正确性，另一种基于情感显著性，用于构建微调子集。实验结果表明，在真实情感数据集上词错误率（WER）持续改善，且在纯净的LibriSpeech语料上未出现明显性能下降。结合两种策略取得了最佳效果，尤其在富有表现力的语音上提升显著。这些发现凸显了针对性数据增强对于构建情感感知ASR系统的重要性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动语音识别（ASR）在现实应用中需具备鲁棒性。现有研究多关注外部噪声，而对说话人内部因素（如情感表达）关注较少。情感会显著改变语音的声学和韵律特征，导致传统ASR性能下降。
- **既有问题**：情感语音通常被视为ASR中的“噪声”，其高变异性使得识别困难。此外，标注情感语音数据收集成本高，而使用合成情感语音进行数据增强时，可能引入声学/韵律伪影，且合成语音的情感显著性可能与真实表达不一致。

2)  
论文提出两种生成策略，从合成情感语音中筛选高质量子集用于ASR微调，以解决情感语音导致的错误率上升和合成数据质量不均的问题：

- **TTS正确性策略（TTS-G）**：
  - **目标**：针对情感语音主要引发“替换错误”的问题。
  - **方法**：对比合成语音与原始中性语音（相同文本）的ASR错误。仅保留那些**替换错误增多**，同时**插入和删除错误未增加**的合成语音样本。这确保了所选样本能针对性地提供对ASR模型具有挑战性的、由情感引起的语音变化。

- **情感显著性策略（EMO-G）**：
  - **目标**：确保用于增强的数据具有足够的情感表达力，而非情感中性。
  - **方法**：使用情感回归模型预测合成语音在唤醒度、效价和支配度三个维度上的得分。仅保留**至少有一个维度得分偏离均值超过一个标准差**的样本。这筛选出了情感表达强烈的语音。

- **组合策略（TTS-EMO-G）**：
  - 同时应用上述两个条件，筛选出**既在声学上具有挑战性（导致更多替换错误），又具有明确情感表达**的样本。这种双重过滤构建了更高质量、更具针对性的训练数据子集。
- **实施与验证**：使用这些策略筛选出的数据子集，对预训练的Qwen2-audio模型（冻结大部分参数，仅微调音频编码器的一小部分）进行微调。实验证明，该方法能有效利用合成数据提升模型对情感语音的鲁棒性。

3)  
- **任务与效果**：在**真实情感语音数据集**（MSP-Podcast和IEMOCAP）上进行评估，模型仅使用合成数据微调。
- **主要成果**：
  - **词错率（WER）持续改善**：相比基线模型，在多个测试集上取得WER下降，最高改善达0.99%（MSP Test2）。
  - **组合策略最优**：TTS-EMO-G策略在所有基准测试中表现最佳，尤其在情感表达强烈的极端区域改进最明显。
  - **保持中性语音性能**：在干净的LibriSpeech测试集上，识别性能未出现明显下降，证明了方法的特异性增强能力。
</div>

</details>

---

## MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting
- **Authors**: Jing Xu, Minglin Wu, Xueyuan Chen, Xixin Wu, Helen Meng
- **Categories**: cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.20300v1](https://arxiv.org/abs/2601.20300v1)
- **PDF**: [https://arxiv.org/pdf/2601.20300v1](https://arxiv.org/pdf/2601.20300v1)

自监督学习在语音表征学习方面取得了显著进展，但现有多语言自监督模型仍受限于预训练阶段所涵盖的语言范围。若为纳入新语言而从头开始重新训练，计算成本极高；而若直接采用顺序训练且未引入缓解策略，则常导致灾难性遗忘问题。为此，我们提出MiLorE-SSL——一种轻量级框架，通过结合LoRA模块与软性专家混合机制，实现高效持续多语言训练。其中，LoRA提供高效的低秩自适应能力，软性MoE机制则促进跨语言的灵活专家共享，从而减少语言间干扰。为进一步缓解遗忘现象，我们引入少量现有语言的回放数据，避免依赖大规模历史语料库。在ML-SUPERB基准上的实验表明，MiLorE-SSL仅需训练2.14%的参数，即可在新语言上取得优异性能，并同步提升对已有语言的处理能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自监督学习（SSL）模型在语音表示学习上取得了显著进展，但现有多语言SSL模型（如XLSR、mHuBERT）通常局限于预训练时见过的语言。当需要扩展新语言时，面临两大挑战：
  - **计算成本高**：从头重新训练以纳入新语言的计算开销巨大。
  - **灾难性遗忘**：在没有缓解策略的情况下进行顺序训练，会导致模型对已学语言的性能严重下降。
- **既有方法的问题**：
  - 静态数据重采样策略（如mHuBERT-147）在添加新语言时需要完全重新处理数据和全量重训练，缺乏灵活性且效率低下。
  - 基于回放（replay）的策略需要存储和访问大量历史数据，内存负担重。
  - 适配器（adapter）模块在语言数量增长时扩展性差，且容易引发跨语言干扰。

2)  
论文提出的核心方法 **MiLorE-SSL** 是一个轻量级框架，旨在通过参数高效的方式持续扩展SSL模型的多语言能力，同时缓解灾难性遗忘。其解决方案整合了三个关键设计：

- **LoRA与软混合专家（Soft MoE）的结合**：
  - 在HuBERT等SSL模型的每个Transformer块中，将原有的前馈网络（FFN）替换为 **MiLorE模块**。该模块包含一个**冻结的原始FFN主干**、一组**基于LoRA的专家**，以及一个**软路由器**。
  - **LoRA专家**：每个专家由低秩矩阵参数化，仅引入少量可训练参数（论文中仅占2.14%），从而低成本地扩展模型对新语言的建模能力。
  - **软路由器**：根据输入动态计算各专家的软权重，实现灵活的专家共享。这种设计允许模型在不同语言间既共享通用知识（如底层声学特征），又保留语言特异性表征，有效减少了跨语言干扰。

- **轻量级回放策略**：
  - 在持续训练新语言时，会**混合少量（如100小时）已学语言（如英语）的样本**进行回放训练。
  - 这使模型能够“重温”已学语言的分布，巩固旧知识，而无需依赖或存储完整的历史大型语料库，极大降低了存储和计算负担。

- **整体训练机制**：
  - 仅训练MiLorE模块中的LoRA专家参数和路由器权重，模型主干保持冻结，保护了已有的多语言知识。
  - 训练目标沿用HuBERT的掩码预测损失，同时应用于新语言数据和回放数据，确保模型在适应新语言的同时不遗忘旧语言。

**总结**：MiLorE-SSL通过 **“参数高效的LoRA专家扩展” + “灵活的知识共享路由” + “最小化的数据回放”** 三重机制，实现了高效、可扩展的持续多语言学习，并显著缓解了遗忘问题。

3)  
在**ML-SUPERB**基准测试中，MiLorE-SSL在以下任务上取得了显著效果：
- **单语自动语音识别（ASR）**：在CommonVoice和Fleurs数据集上，相较于基线模型（HuBERT Large和mHuBERT-147），在英语、普通话和粤语上均取得了更低的字符错误率（CER）。例如，在CommonVoice上，平均CER从mHuBERT-147的16.27%和HuBERT Large的16.77%**降至10.67%**。
- **语言识别（LID）**：取得了**99.40%**的平均准确率，优于mHuBERT-147（97.21%）和HuBERT Large（94.94%）。
- **关键优势**：这些性能提升是在**仅使用2.14%的可训练参数**和**少量（100小时）回放数据**的情况下实现的，证明了该方法在高效扩展多语言能力并防止遗忘方面的有效性。
</div>

</details>

---

## Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling
- **Authors**: Husein Zolkepli
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.20185v1](https://arxiv.org/abs/2601.20185v1)
- **PDF**: [https://arxiv.org/pdf/2601.20185v1](https://arxiv.org/pdf/2601.20185v1)

X-Codec-2.0 在神经音频压缩与多语言语音建模中表现出色，其采用冻结的 HuBERT 特征，以 50 Hz 的潜在速率和 16 kHz 的采样率运行。尽管效果显著，该配置在时间效率和音频保真度方面仍存在局限。本研究通过引入额外的池化操作并增大解码器跳跃步长，提出一种简单而有效的改进方案。该方案将潜在速率从 50 Hz 降低至 25 Hz，同时将输出采样率从 16 kHz 提升至 24 kHz，从而在不改变核心架构的前提下，显著提升了系统效率与感知质量。在多语言 Common Voice 17 测试集上的评估结果显示，基于 UTMOSv2 评分，改进后的配置相比原始 X-Codec-2.0 基线实现了 0.29 的平均意见分提升，并在所有运行于 25 Hz 的编解码器中取得了当前最佳性能。相关源代码、模型检查点及生成对比已发布于 \href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：X-Codec-2.0 是一种基于 HuBERT 特征的多语言神经音频编解码器，在 50 Hz 潜在帧率和 16 kHz 采样率下运行。  
- **既有问题**：  
  - 50 Hz 的潜在帧率限制了时间分辨率，可能导致高频内容模糊。  
  - 16 kHz 采样率限制了音频保真度，影响感知质量。  
  - 固定的帧率可能无法充分利用模型捕捉细粒度语音变化的能力。

2)  
- **核心方法**：通过简单的架构调整，在保持核心组件不变的情况下，同时降低潜在帧率并提高输出采样率。  
  - **调整跳步大小与池化**：将编码器的跳步大小从 320 样本增加到 960 样本，并在量化前引入一个平均池化层（核大小为 2，步长为 2）。这使潜在帧率从 50 Hz 降至 25 Hz，同时将输出音频采样率提升至 24 kHz。  
  - **解码器权重插值**：为适应新的跳步大小，对预训练解码器输出层的权重和偏置进行一维线性插值，以保留原始模型的频谱特性。  
  - **参数冻结与微调**：仅微调解码器，语义编码器和编解码器编码器保持冻结，确保训练高效且稳定。  
- **解决效果**：  
  - **效率提升**：潜在帧率减半，使离散令牌序列更紧凑，利于集成到 LLM 管道中。  
  - **质量改善**：更高的采样率（24 kHz）和更精细的时间配置提升了高频重建和整体感知清晰度。

3)  
- **评估任务**：在多语言语音重建任务上，使用 Common Voice 17 测试集（涵盖 116 种语言）进行感知质量评估。  
- **取得效果**：  
  - 在 UTMOSv2 指标上，相比原始 X-Codec-2.0 基线，平均意见得分（MOS）提升 0.29。  
  - 在 25 Hz 潜在帧率的所有编解码器中，取得了最佳报告性能，尤其在英语、法语、意大利语等多种语言上表现一致优于其他对比模型。
</div>

</details>

---
