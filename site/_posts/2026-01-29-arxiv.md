---
layout: post
title: "arXiv Daily – 2026-01-29"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-29（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-28 08:50 — 2026-01-29 08:50
- 抓取总数：11 篇 | 本页显示：11 篇（去重/过滤后）

## Gen-SER: When the generative model meets speech emotion recognition
- **Authors**: Taihui Wang, Jinzheng Zhao, Rilin Chen, Tong Lei, Wenwu Wang, Dong Yu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.20573v1](https://arxiv.org/abs/2601.20573v1)
- **PDF**: [https://arxiv.org/pdf/2601.20573v1](https://arxiv.org/pdf/2601.20573v1)

语音情感识别在语音理解与生成中至关重要。现有方法主要基于分类模型或大语言模型。不同于以往思路，本文提出Gen-SER这一创新方法，通过生成模型将情感识别重构为分布偏移问题。我们将离散类别标签映射至连续空间，并通过正弦分类编码获得目标分布。采用基于目标匹配的生成模型，将初始分布高效转化为目标分布。通过计算生成的目标分布与真实目标分布的相似度实现分类。实验结果验证了该方法的有效性，证明了其可扩展至多种语音理解任务，并展现出在更广泛分类任务中的应用潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音情感识别（SER）是语音理解与人机交互的关键任务。传统方法主要分为两类：
  - **分类模型**：使用语音编码器（如HuBERT、WavLM）后接分类器，依赖判别式学习。
  - **大语言模型（LLM）**：通过提示让LLM生成情感标签，计算成本高且依赖大规模数据。
- **既有问题**：
  - 基于扩散的分类器将分类视为条件密度估计，但推理延迟随类别数线性增长，效率低。
  - 潜在空间投影方法（如自编码器）可提升效率，但多类别任务中训练困难且存在重构误差。

2)  
论文提出**Gen-SER**，将SER重构为**分布迁移问题**，核心方法如下：

- **问题重构**：
  - 假设不同情感的语音信号服从不同的初始分布，每个类别标签对应预定义的终端分布。
  - 目标是将语音样本从初始分布迁移到终端分布，通过计算生成分布与真实终端分布的相似度进行分类。

- **关键设计**：
  - **正弦分类编码**：将离散类别标签映射为连续空间中的正交向量（公式2），避免训练多类别自编码器，保证向量连续且相互正交，便于学习。
  - **目标匹配生成模型**：
    - 使用预训练HuBERT提取语音特征作为初始样本 \(x_1\)，并生成条件变量 \(X_c\)。
    - 采用**逻辑均值计划**（公式6）和**桥接方差计划**（公式7）控制分布迁移过程，有效扰动信号。
    - 直接预测目标嵌入向量 \(x_0\)（而非整个向量场），通过目标匹配损失（公式8）训练，提升稳定性和效率。
  - **高效推理**：
    - 训练后，通过欧拉ODE求解器迭代估计终端样本（算法2），计算与各类别嵌入的余弦相似度进行分类（公式12）。

- **优势**：
  - 避免了逐类别误差估计，推理效率高；正弦编码消除了自编码器训练难题；目标匹配模型简化了学习过程。

3)  
- **任务与效果**：
  - **语音情感识别（SER）**：在MELD测试集上，准确率达到56.5%，优于多数分类模型（如WavLM的50.6%）和LLM方法（如Qwen2-audio的55.3%），但低于大规模LLM SenseVoice-L（63.1%）。
  - **性别分类**：在内部数据集上，准确率达90.5%，超越Fbank、WavLM分类器及部分LLM基线，验证了方法在多种语音理解任务上的可扩展性。
- **其他发现**：单步推理已能取得较好效果（56.13%），增加推理步数可稳步提升性能（20步达56.44%）。
</div>

</details>

---

## Decoding Speech Envelopes from Electroencephalogram with a Contrastive Pearson Correlation Coefficient Loss
- **Authors**: Yayun Liang, Yuanming Zhang, Fei Chen, Jing Lu, Zhibin Lin
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.20542v1](https://arxiv.org/abs/2601.20542v1)
- **PDF**: [https://arxiv.org/pdf/2601.20542v1](https://arxiv.org/pdf/2601.20542v1)

近年来，从脑电图信号中重建语音包络的技术进展，使得在多说话人环境中实现连续听觉注意解码成为可能。大多数基于深度神经网络的包络重建模型，其训练目标在于最大化目标语音包络与重建包络之间的皮尔逊相关系数。然而，在听觉注意解码中，目标语音与干扰语音的相关系数差异至关重要，而现有方法往往仅侧重于最大化目标语音的相关系数。为此，本文提出一种对比皮尔逊相关系数损失函数，该函数直接表征目标语音与干扰语音相关系数之间的差异。我们在三个公开的脑电图听觉注意解码数据集上，使用四种深度神经网络架构对所提方法进行了评估。结果表明，在多种实验设置下，所提出的目标函数能够有效提升包络可分离性与听觉注意解码准确率，同时也揭示了该方法在不同数据集和网络架构中可能存在的失效情况。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于脑电图（EEG）的语音包络重建是连续听觉注意解码（AAD）的关键技术，用于在多说话人环境中识别听者关注的语音流。  
- **既有方法的问题**：现有深度神经网络（DNN）模型通常采用皮尔逊相关系数（PCC）损失，仅最大化重建包络与关注语音包络之间的相关性（关注PCC）。但AAD的准确性实际取决于关注PCC与未关注PCC之间的差异，而现有方法未显式优化这一差异，导致模型可能同时记住关注与未关注语音的共有特征，降低两类PCC的可区分性。

2)  
- **核心方法**：论文提出了一种对比PCC损失函数（L_ΔPCC），其定义为关注PCC与未关注PCC均值之间的差值：  
  **L_ΔPCC = -ρ_a + (1/(N-1)) * Σ ρ_u,j**  
  其中ρ_a为关注PCC，ρ_u,j为与第j个未关注语音包络的PCC。  
- **解决思路**：  
  - **双重优化目标**：该损失同时最大化关注PCC（第一项）并最小化未关注PCC的均值（第二项），从而显式扩大两类PCC的差异。  
  - **避免优化陷阱**：作者发现若简单最大化“-ρ_a + Σρ_u,j”，模型可能将所有相关性推向负值以降低损失，但无法学到有效表征。因此采用未关注PCC的均值作为惩罚项，确保模型真正学习区分性特征。  
  - **增强注意判别性**：通过最小化L_ΔPCC，模型被引导抑制对未关注语音的神经追踪，强化对关注语音的专一解码，从而提升AAD决策所需的包络可分性。  
- **方法验证**：该方法在四种DNN架构（VLAAI、LSM-CNN、SSM2Mel、EEG-Deformer）和三个公开EEG数据集上进行了系统评估，证明了其相对于传统PCC损失的普遍有效性。

3)  
- **任务与效果**：在基于EEG的听觉注意解码任务中，使用对比PCC损失训练模型，在多个数据集（KUL、DTU、KUL-AV-GC）上取得了以下效果：  
  - **解码准确率提升**：在大多数数据集和模型配置下，AAD准确率显著高于传统PCC损失（如表1所示，部分结果具有统计显著性）。  
  - **包络可分性增强**：关注与未关注PCC的差异（ΔPCC）平均相对提升17.84%，表明重建包络对注意状态的判别力更强。  
  - **机制验证**：性能提升主要源于对未关注语音相关性的抑制，而非单纯提高关注语音相关性。
</div>

</details>

---

## Audio Deepfake Detection in the Age of Advanced Text-to-Speech models
- **Authors**: Robin Singh, Aditya Yogesh Nair, Fabio Palumbo, Florian Barbaro, Anna Dyka, Lohith Rachakonda
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.20510v1](https://arxiv.org/abs/2601.20510v1)
- **PDF**: [https://arxiv.org/pdf/2601.20510v1](https://arxiv.org/pdf/2601.20510v1)

随着文本转语音（TTS）系统的快速发展，合成语音的真实性显著提升，为音频深度伪造检测带来了新的挑战。本研究对三种前沿TTS模型——代表流式架构的Dia2、基于大语言模型的Maya1以及非自回归架构的MeloTTS——进行了对比评估。基于Daily-Dialog数据集生成的12,000个合成音频样本，研究从语义、结构和信号层面出发，对四种检测框架进行了系统性测试。结果表明，不同生成机制下的检测器性能存在显著差异：针对特定TTS架构有效的检测模型可能在其他架构（尤其是基于大语言模型的合成方法）上失效。相比之下，融合多层面互补分析的多视角检测方法在所有评估模型中均表现出鲁棒性能。这些发现揭示了单一范式检测器的局限性，并强调了采用集成化检测策略以应对不断演化的音频深度伪造威胁的必要性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：先进的文本转语音模型极大提升了合成语音的真实感，对音频深度伪造检测构成新挑战。现有检测方法主要基于针对早期声码器系统的基准，如ASVspoof 2021，其检测器可能无法泛化到2024-2025年发布的新架构。
- **既有方法问题**：单一检测范式存在局限性。例如，基于语义的检测器对LLM基合成可能失效，而层次特征融合方法对基于流匹配的系统效果未经验证。现有基准缺乏对最新TTS模型（如Dia2、Maya1、MeloTTS）生成伪造音频的覆盖，导致检测器在真实场景中面临泛化差距。

2)  
- **核心方法**：论文采用多视角检测策略，系统评估了三种前沿TTS模型（代表流式、LLM基、非自回归架构）与四种检测框架，以填补对新威胁认知的空白。
    - **构建新数据集**：使用DailyDialog语料库生成了12,000个合成音频样本，涵盖三种不同TTS范式，确保语言和对话真实性。
    - **采用多层面检测策略**：结合了四种互补的分析视角：
        - **语义理解**：使用微调的Whisper-MesoNet，捕捉高层语义不一致性。
        - **层次表征学习**：采用XLS-R-SLS，动态融合预训练模型不同层的特征，以捕获从低级声学到高级上下文的多种伪影。
        - **结构图建模**：使用SSL-AASIST，将音频建模为时频异构图，分析结构关系。
        - **大规模基础模型**：评估了MMS-300M，利用其跨语言、多领域表征。
    - **系统性评估与对比**：通过EER、AUC、F1分数等指标，量化不同检测器针对不同TTS生成机制的效能，揭示了检测性能的显著可变性。
- **如何解决问题**：这种综合方法克服了单一范式检测器的局限。通过同时从语义、结构和信号层面分析，多视角策略能够更全面地捕捉不同TTS架构（如Dia2的流式一致性、Maya1的层次编解码器伪影、MeloTTS的相位不连续性）留下的独特法医特征，从而实现对多样化现代合成语音的鲁棒检测。

3)  
- **检测任务与效果**：在针对Dia2、Maya1、MeloTTS生成的合成音频的检测任务中：
    - **单一检测器表现不均**：Whisper基检测器对MeloTTS效果较好（EER 17.05%），但对Maya1效果差（EER 35.95%）。XLS-R-SLS对Dia2效果最佳（EER 7.07%），但对MeloTTS效果差（EER 27.10%）。
    - **多视角策略的鲁棒性**：综合结果表明，结合互补分析层面的多视角检测方法（如论文中评估的多种框架组合）能够对所有评估模型展现出更稳健的性能。
    - **最优效果**：论文中评估的UncovAI专有模型取得了近乎完美的检测效果，在三个数据集上的F1分数均高于0.98，展示了集成策略应对新型攻击的有效性。
</div>

</details>

---

## Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech
- **Authors**: Myungjin Lee, Eunji Shin, Jiyoung Lee
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.20481v1](https://arxiv.org/abs/2601.20481v1)
- **PDF**: [https://arxiv.org/pdf/2601.20481v1](https://arxiv.org/pdf/2601.20481v1)

现代零样本文本转语音（TTS）模型在提供前所未有的表现力的同时，也带来了严重的犯罪风险，因为它们能够合成未经本人同意的个体语音。在此背景下，说话人遗忘技术旨在根据请求阻止生成特定说话人身份。现有方法依赖重新训练，成本高昂且仅限于训练集中出现过的说话人。本文提出TruS，一种无需训练的说话人遗忘框架，将范式从数据删除转向推理时控制。TruS通过引导身份特定的隐藏激活来抑制目标说话人，同时保持其他属性（如韵律和情感）。实验结果表明，TruS能有效阻止对已知和未知退出说话人的语音生成，为语音合成建立了可扩展的安全保障。演示和代码可在 http://mmai.ewha.ac.kr/trus 获取。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：零样本文本转语音技术虽能合成高表现力的语音，但也带来了严重的滥用风险，例如未经许可合成特定人物的声音。现有保护方法存在明显不足。
- **既有方法的问题**：
  - **水印技术**：属于事后追溯，无法在生成时阻止滥用。
  - **语音匿名化**：旨在替换说话人身份，而非禁止生成特定身份。
  - **基于机器遗忘的方法**：大多需要重新训练模型，计算成本高，且通常只能处理训练集中见过的说话人，无法应对训练集外的新请求。

2)  
论文提出的核心方法 **TruS** 是一个无需训练、在推理时进行控制的说话人遗忘框架。它通过动态干预TTS模型的内部激活来抑制目标说话人的身份信息，从而解决上述问题。具体机制如下：

- **身份原型构建**：
  - 使用一组保留说话人的语音样本，在TTS模型的每个扩散块和时间步上，计算其前馈网络输出的平均激活，形成一个**身份原型**。该原型代表了“非目标”说话人的典型身份特征。

- **动态选择与引导**：
  - **计算引导向量**：对于请求退出的目标说话人样本，将其激活与身份原型相减并进行归一化，得到一个**身份特定的引导向量**。该向量指向目标说话人在潜在空间中的身份方向。
  - **动态选择干预层**：并非所有层对身份信息的贡献都相同。TruS通过计算目标激活与身份原型在各层、各时间步的余弦相似度，动态选择相似度低于特定阈值的层和时间步作为干预点。这确保了干预的精准性和稀疏性。

- **推理时激活修正**：
  - 在选定的干预点和时间步，将目标激活向量投影到引导向量上，并减去该投影分量。这一操作旨在**仅移除与身份相关的成分**，同时最大程度保留语音内容、韵律和情感等其他属性。
  - 整个过程在模型推理时即时完成，**无需任何重新训练或微调**。

- **方法优势**：
  - **无需训练**：避免了重新训练带来的高昂计算成本和潜在性能下降。
  - **处理未见说话人**：通过基于激活相似度的动态机制，能够泛化到训练集之外的、从未见过的请求退出说话人。
  - **保护其他属性**：选择性干预确保了语音的语义内容和非说话人属性（如情感）得以保留。

3)  
TruS在以下任务上取得了显著效果：
- **对训练集内已见退出说话人的遗忘**：在Emilia数据集上，有效降低了目标说话人的身份相似度，同时保持了与基线模型相近的语音内容保真度，性能媲美甚至优于需要大量训练的基线方法。
- **对训练集外未见退出说话人的泛化遗忘**：在LibriSpeech数据集上，成功抑制了未见说话人的身份合成，证明了其零样本泛化能力。
- **情感等副语言属性保留**：在CREMA-D情感语音数据集上的实验表明，在有效遗忘目标身份的同时，生成语音的情感特征得到了很好的保持。
</div>

</details>

---

## On Every Note a Griff: Looking for a Useful Representation of Basso Continuo Performance Style
- **Authors**: Adam Štefunko, Carlos Eduardo Cancino-Chacón, Jan Hajič
- **Categories**: cs.SD, cs.IR
- **arXiv**: [https://arxiv.org/abs/2601.20478v1](https://arxiv.org/abs/2601.20478v1)
- **PDF**: [https://arxiv.org/pdf/2601.20478v1](https://arxiv.org/pdf/2601.20478v1)

通奏低音是一种巴洛克时期的即兴伴奏风格，演奏者需在羽管键琴或管风琴上根据乐谱中的给定低音线条即兴演绎多个声部。这一传统并非仅具历史意义，更是一种受历史启发的活态实践，而《对齐通奏低音数据集》（ACoRD）首次在符号化领域记录了现代通奏低音演奏的样本。该数据集包含7位演奏者对5部通奏低音乐谱的175段MIDI录音，使我们得以开始观察和分析通奏低音即兴演奏带来的多样性。近期提出的通奏低音演奏-乐谱对齐系统，为将即兴演奏音符映射至乐谱音符提供了方法。为研究对齐后的通奏低音演奏，我们需要一种合适的特征表示方法。本文提出“格里夫”（griff）表示法，其灵感来源于历史上的通奏低音论著。该方法能以移调不变的方式编码通奏低音实现的音高内容与结构。格里夫通过将同一乐谱音符对应的演奏音符按起始时间顺序分组，直接从对齐后的通奏低音演奏中提取，形成有意义的特征单元，构建出可用于分析通奏低音演奏风格的特征空间。我们对从ACoRD数据集录音中提取的格里夫进行了统计描述，并通过两项实验展示了如何利用格里夫对不同演奏者的通奏低音演奏风格进行个体性统计分析。最后，我们论证了为何保留通奏低音即兴演奏的结构有助于深入分析演奏者的个人风格，以及格里夫为何能构建出具有历史依据、值得进一步实证检验的有意义特征空间。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**
- **背景**：通奏低音是一种巴洛克时期的即兴伴奏风格，演奏者需基于乐谱中的低音线条即兴创作多声部。虽然它是历史启发的现代实践，但对其当代演奏的分析研究很少。
- **问题**：现有研究缺乏对通奏低音即兴演奏的合适特征表示。虽然已有对齐数据集（ACoRD）和对齐系统，但缺少能够捕捉演奏结构和音高内容的特征编码，这限制了对不同演奏者个人风格的分析。

2)  
**论文核心方法如何解决上述问题**
本文提出了名为 **griff** 的特征表示方法，灵感来源于历史通奏低音论著，旨在编码演奏的音高内容和结构，并具有移调不变性。具体方法如下：

- **构建基础**：基于性能-乐谱对齐系统，将每个乐谱音符对应的演奏音符分组。
- **两种表示形式**：
    - **有序griff**：按音符起始时间顺序分组，通过量化阈值（如35毫秒）区分块状和弦与有目的的琶音，形成向量序列。
    - **池化griff**：将所有对齐到同一乐谱音符的演奏音符音高合并为一个向量。
- **编码与标准化**：将MIDI音高转换为相对于乐谱音符的半音音程，以实现移调不变性；使用字符串编码（用下划线分隔同一向量内的音程，用竖线分隔不同向量）。
- **优势**：
    - 保留演奏的结构化音高内容，同时丢弃绝对时间关系，避免节奏变化掩盖结构等效性。
    - 提供历史相符的特征空间，适合分析个人演奏风格，避免了仅使用原始音高或音程表示的过于粗糙的问题。

3)  
**在哪些任务上取得了怎样的效果**
- **任务**：在ACoRD数据集上分析通奏低音演奏风格，特别是不同演奏者的个体性。
- **效果**：
    - 统计显示，griff能有效表示演奏中的和弦模式（如识别最常见的griff类型）。
    - 通过演奏者griff分布分析（如累积覆盖率和交叉熵相似性矩阵），发现griff表示能区分不同演奏者的风格，而仅使用音程表示则无法区分。
    - 实验表明，griff提供了足够细粒度的特征空间，有助于未来更稳健的实证验证（如聚类和分类）。
</div>

</details>

---

## Self Voice Conversion as an Attack against Neural Audio Watermarking
- **Authors**: Yigitcan Özer, Wanying Ge, Zhe Zhang, Xin Wang, Junichi Yamagishi
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.20432v1](https://arxiv.org/abs/2601.20432v1)
- **PDF**: [https://arxiv.org/pdf/2601.20432v1](https://arxiv.org/pdf/2601.20432v1)

音频水印技术能够在保持说话人身份、语言内容及感知质量的前提下，将辅助信息嵌入语音中。尽管基于神经网络的数字信号处理水印方法在不可感知性和嵌入容量方面取得了进展，但其鲁棒性评估仍主要针对压缩、加性噪声和重采样等传统失真类型。然而，基于深度学习的攻击手段的兴起，为水印安全性带来了全新且严峻的威胁。本研究探讨了自语音转换作为一种通用且保持内容不变的攻击方式对音频水印系统的影响。自语音转换通过语音转换模型在维持说话人身份不变的同时改变声学特征。实验表明，该攻击会严重削弱当前先进水印方法的可靠性，并凸显了其对现代音频水印技术安全性的深远影响。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频水印用于在语音中嵌入辅助信息，以进行来源验证和版权保护。随着深度学习技术的发展，基于神经网络的音频水印方法在不可感知性和嵌入容量上有所提升。  
- **既有方法的问题**：现有水印技术的鲁棒性评估主要针对传统失真（如压缩、加性噪声、重采样），而忽略了基于深度学习的恶意攻击。特别是，当前评估中考虑的对抗性转换（如音高变换、时间拉伸）未能反映现代攻击者利用语音转换等先进技术的能力，导致水印鲁棒性被系统性高估。

2)  
- **核心方法**：论文提出将**自语音转换**作为一种针对神经音频水印的通用攻击方法。自语音转换通过语音转换模型，将说话人的语音重映射到同一身份，在保持说话人身份和语言内容不变的同时，改变声学特征。  
- **攻击流程**：  
  - 攻击者将水印语音输入自语音转换系统，该系统将语音解耦为内容特征、说话人身份和音高轮廓等表示。  
  - 这些表示通过解码器重新合成为梅尔频谱图，再经神经声码器生成波形。  
  - 这一重构过程保留了感知质量和可懂度，但抑制了水印所依赖的细粒度声学线索。  
- **解决思路**：  
  - 自语音转换在表示层面操作，而非直接信号处理，因此能破坏依赖波形或特征级嵌入的水印信息。  
  - 该方法利用了现代语音转换模型（如kNN-VC和RVC）的解耦能力，在保持内容与身份的同时，有效去除水印。  
  - 实验表明，即使结合传输信道失真，自语音转换仍能将水印提取性能推至接近随机猜测的水平。

3)  
- **评估任务**：在LibriTTS数据集上测试了五种水印方法（经典DCT方法及四种基于深度学习的方法：AudioSeal、Timbre、WMCodec、VoiceMark）。  
- **攻击效果**：  
  - 自语音转换攻击使所有测试水印方法的比特提取准确率降至接近0.5（随机猜测水平），显著优于传统声码器攻击。  
  - 攻击在保持高说话人相似性、低词错误率和良好感知质量的同时，实现了水印的失效，证明了其作为内容保持型攻击的有效性和实用性。
</div>

</details>

---

## Mix2Morph: Learning Sound Morphing from Noisy Mixes
- **Authors**: Annie Chu, Hugo Flores García, Oriol Nieto, Justin Salamon, Bryan Pardo, Prem Seetharaman
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.20426v1](https://arxiv.org/abs/2601.20426v1)
- **PDF**: [https://arxiv.org/pdf/2601.20426v1](https://arxiv.org/pdf/2601.20426v1)

本文提出Mix2Morph，这是一种通过微调实现的文本到音频扩散模型，能够在无需专门变形数据集的情况下完成声音形态融合。该模型通过在较高扩散步数下对含噪替代混合音频进行微调，能够生成稳定且感知连贯的变形结果，有效融合两种源声音的特征。我们特别关注声音注入这一子任务——该任务兼具实用价值与感知意义，其特点是以一个主导声源提供整体时序与结构框架，同时将次要声源的特征渗透其中，从而丰富音色与纹理质感。客观评估与听感实验表明，Mix2Morph在多项指标上超越现有基线模型，能够跨声音类别生成高质量的声音注入效果，为构建更可控、更具概念驱动性的声音设计工具迈出重要一步。音频示例详见 https://anniejchu.github.io/mix2morph。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声音设计常需将两个声音融合成一个兼具两者特征的“声音变形”。传统方法（如基于感知描述符的插值或交叉合成）在处理非音高纹理（如环境噪声）时效果不佳。  
- **既有方法问题**：  
  - 近期基于深度学习的模型（如MorphFader、SoundMorpher）常出现“中点塌陷”，即中间变形结果不稳定，听起来更像简单叠加而非融合。  
  - 现有方法缺乏专门用于训练的高质量变形数据集，且难以在保持主音源主导性的同时融入次音源特征（即“声音注入”这一实用场景）。  

2)  
Mix2Morph的核心方法是通过微调文本到音频扩散模型，利用噪声混合数据学习声音变形，无需专门的变形数据集。具体解决策略如下：  
- **构建噪声替代数据**：  
  - 将两个声音的简单加性混合作为“不良”变形数据，并通过时域和频域增强使其更接近真实变形。  
  - **时域对齐**：提取主音源的RMS包络并应用于混合信号，使整体时间结构遵循主音源。  
  - **频域对齐**：计算两个声音的频谱平均值作为目标频谱，通过EQ调整将两者投射到共同的音色空间。  
- **扩散训练策略**：  
  - 采用“无浪费”训练思路，将噪声替代数据仅分配给扩散过程的高时间步（如t ∈ [0.5, 1]）。  
  - 在高时间步，模型学习变形的整体结构概念；在低时间步，依赖预训练知识细化细节，避免对混合伪影过拟合。  
- **多样化训练目标**：  
  - 随机应用四种增强模式（仅RMS、仅频谱、两者兼具、无增强），并配以对应的文本描述，使模型能适应不同的变形需求。  
- **整体流程**：通过上述方法，模型能够从噪声混合中学习生成感知一致的声音注入，其中主音源控制时间动态，次音源丰富音色纹理。  

3)  
- **任务**：在“声音注入”任务上评估，即生成以主音源行为为主导、次音源音色为补充的融合声音。  
- **效果**：  
  - **客观指标**：在50个跨类别概念对上，Mix2Morph在对应性（0.725）、中间性（0.648）和方向性（0.436）上优于基线模型，且FAD分数（1.220）显示高质量生成。  
  - **主观听测**：在25人参与的听感测试中，Mix2Morph的变形被判定为“变形”的比例最高（77%），平均意见得分（MOS）达3.52，显著优于简单混合、LGrS和MorphFader等基线。
</div>

</details>

---

## Switchcodec: Adaptive residual-expert sparse quantization for high-fidelity neural audio coding
- **Authors**: Xiangbo Wang, Wenbin Jiang, Jin Wang, Yubo You, Sheng Fang, Fei Wen
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.20362v1](https://arxiv.org/abs/2601.20362v1)
- **PDF**: [https://arxiv.org/pdf/2601.20362v1](https://arxiv.org/pdf/2601.20362v1)

近期神经音频压缩模型常采用残差向量量化实现高保真编码，但每帧使用固定数量的码本难以适应音频内容的高度可变性——尤其对于极简或极复杂的信号。为此，我们提出SwitchCodec，一种基于残差专家向量量化的神经音频编解码器。该方法将共享量化器与动态路由的专家量化器相结合，专家量化器根据输入音频激活，从而解耦比特率与码本容量，提升压缩效率。该设计确保每个量化器得到充分训练与利用。此外，通过可变比特率机制在推理阶段调整激活的专家量化器数量，无需重新训练即可实现多比特率操作。实验表明，SwitchCodec在客观指标与主观听感测试上均优于现有基线模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：高保真神经音频编码通常依赖残差向量量化（RVQ），但现有方法采用固定数量的码本处理每帧音频。这种固定结构无法适应音频内容复杂度的巨大差异，导致对简单片段过度分配比特，而对复杂片段量化不足，限制了压缩效率与质量。
- **既有方法问题**：传统RVQ及其改进（如分组RVQ）结构僵化；近期稀疏激活方法（如MoE-VQ）缺乏结构化残差层次，训练不稳定；自适应方法（如基于能量阈值的自适应RVQ）泛化能力有限，且多码率支持常需复杂重训练。

2)  
论文提出 **SwitchCodec**，其核心是 **残差专家向量量化（REVQ）**，通过双路径量化框架动态适配音频内容，具体解决方式如下：

- **REVQ 框架设计**：
    - **共享基础量化器**：首先捕获音频的通用结构，提供稳定的基础表示。
    - **专家量化器池与稀疏激活**：一个由多个专家量化器组成的池，通过基于编码器特征学习的门控网络，为每个音频窗口选择亲和度最高的 top-k 个专家进行激活。这实现了**码本容量与比特率的解耦**：比特率取决于激活的专家数量，而非固定的级联深度，从而避免对简单片段浪费比特，同时为复杂内容保留细节。
    - **选择与顺序解耦**：专家的选择基于亲和度分数，但其应用顺序严格遵循预定义的索引升序。这保留了传统RVQ能量递减的残差层次结构，增强了模型的可解释性和训练稳定性。

- **轻量级可变比特率（VBR）机制**：
    - 在推理时，通过调整激活的专家数量（即 top-k 中的 k），即可在单一模型上实现从 0.89 kbps 到 8 kbps 的多码率操作，无需重新训练。
    - 仅需传输一个标识激活专家的路由掩码作为边信息，其开销极低（例如在2.67 kbps下仅约2.2 bps，占比<0.1%）。

- **整体优势**：该设计在保持编码器/解码器复杂度与标准RVQ相近的同时，通过内容自适应的专家选择，显著提升了量化效率与重建质量。

3)  
- **任务**：在44.1 kHz采样率下的高保真神经音频压缩。
- **效果**：在2.67 kbps和5.33 kbps比特率下，SwitchCodec在客观指标（Mel谱距离、STFT距离、PESQ、ViSQOL）和主观听力测试（MUSHRA）上均超越了EnCodec和DAC等基线模型。
    - 例如，在2.67 kbps下，其MUSHRA得分达91.7（接近透明音质），显著高于DAC的86.3和EnCodec的61.3。
    - 分析表明，通过扩展专家池并保持稀疏激活，模型能维持高质量重建的同时，更高效地利用码本资源。
</div>

</details>

---

## ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy
- **Authors**: Ya-Tse Wu, Chi-Chun Lee
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.20319v1](https://arxiv.org/abs/2601.20319v1)
- **PDF**: [https://arxiv.org/pdf/2601.20319v1](https://arxiv.org/pdf/2601.20319v1)

本研究探讨情感语音与生成策略对自动语音识别（ASR）性能的影响。通过分析三种情感文本转语音（TTS）模型合成的语音，发现替换错误占主导地位，且不同模型的情感表现力存在差异。基于此，我们提出两种生成策略：一种基于转录正确性，另一种基于情感显著性，用于构建微调数据子集。实验结果表明，在真实情感数据集上词错误率（WER）持续改善，且在纯净的LibriSpeech语料上未出现明显性能下降。结合两种策略的方法取得了最显著的提升，尤其对于表现力丰富的语音效果更为突出。这些发现表明，针对性的数据增强对于构建情感感知的ASR系统具有重要意义。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动语音识别（ASR）在真实场景中需具备鲁棒性。现有研究多关注外部噪声，而对说话人内部因素（如情感表达）关注较少。情感会显著改变语音的声学特征，给传统ASR系统带来挑战。
- **既有问题**：情感语音通常被视为ASR过程中的“噪声”，导致识别错误率高于中性语音。此外，标注情感语音数据收集成本高，而使用合成情感语音进行数据增强时，可能存在声学伪影或情感显著性不足的问题，且情感语音具体如何影响ASR错误模式尚不明确。

2)  
论文提出两种生成策略，从合成的情感语音数据中筛选出高质量子集用于ASR微调，以解决上述问题：
- **基于转录正确性的策略（TTS-G）**：  
  分析发现，合成情感语音主要导致**替换错误**（Substitution Errors）。该策略筛选那些与原始文本相比，**替换错误增多**，但插入和删除错误未增加的合成语音样本。这确保了所选样本能针对性地提供由情感变化引起的、易出错的语音模式。
- **基于情感显著性的策略（EMO-G）**：  
  分析发现，部分合成语音的情感表达较弱。该策略使用一个情感回归模型，预测语音在唤醒度、效价和支配度三个维度上的得分。筛选那些在**至少一个维度上得分偏离均值超过一个标准差**的样本，确保用于训练的数据具有足够的情感表达力，而非接近中性。
- **组合策略（TTS-EMO-G）**：  
  综合应用以上两个标准，只保留同时满足“引入更多替换错误”和“情感表达显著”的样本。这构建了一个既包含有挑战性的识别错误，又具有清晰情感表达的优质训练子集。
- **方法实施**：  
  使用三种前沿的情感可控TTS模型（CosyVoice2, EmoVoice, MaskGCT）基于LibriSpeech文本合成情感语音。应用上述策略构建子集，并用其对预训练的Qwen2-audio ASR模型进行微调，仅更新音频编码器的少量参数。

3)  
- **任务与效果**：在**真实情感语音数据集**（MSP-Podcast和IEMOCAP）上进行评估，模型仅使用合成数据训练。
- **主要效果**：采用组合策略（TTS-EMO-G）微调的模型取得了最一致的词错误率（WER）提升，在MSP-Podcast和IEMOCAP上分别最高提升0.99%和0.57%。性能提升在情感表达强烈的极端区域尤为明显。
- **附加优势**：该方法在提升情感语音识别性能的同时，**未对干净中性语音**（LibriSpeech）的识别性能造成明显损害，保持了模型的通用性。
</div>

</details>

---

## MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting
- **Authors**: Jing Xu, Minglin Wu, Xueyuan Chen, Xixin Wu, Helen Meng
- **Categories**: cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.20300v1](https://arxiv.org/abs/2601.20300v1)
- **PDF**: [https://arxiv.org/pdf/2601.20300v1](https://arxiv.org/pdf/2601.20300v1)

自监督学习显著推动了语音表征学习的发展，但现有多语言自监督模型仍受限于预训练阶段所接触的语言范围。若为纳入新语言而从头开始重新训练，计算成本极高；而若直接采用顺序训练且未引入缓解策略，则常导致灾难性遗忘。为此，我们提出MiLorE-SSL——一种轻量级框架，其结合了LoRA模块与软性专家混合机制，以实现高效持续的多语言训练。LoRA提供高效的低秩自适应能力，而软性专家混合机制则促进跨语言的灵活专家共享，从而减少跨语言干扰。为进一步缓解遗忘问题，我们引入来自现有语言的有限回放数据，避免了对大规模历史语料的依赖。在ML-SUPERB基准上的实验表明，MiLorE-SSL仅需训练2.14%的参数，即可在新语言上取得优异性能，并同步提升对已有语言的处理能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自监督学习（SSL）模型在语音表示学习上取得了显著进展，但现有多语言SSL模型（如XLSR、mHuBERT）通常局限于预训练时见过的语言。  
- **既有问题**：  
  - **扩展成本高**：为支持新语言，从头开始重新训练模型计算代价高昂。  
  - **灾难性遗忘**：在不采取缓解策略的情况下进行顺序训练，会导致模型遗忘已学语言的知识。  
  - **数据冗余与低效**：现有方法（如数据上采样）虽能提升低资源语言表现，但引入了数据冗余和训练效率低下的问题，且难以动态适应新语言。

2)  
论文提出 **MiLorE-SSL** 框架，通过结合参数高效训练与专家混合机制，以轻量方式实现多语言持续学习，具体解决思路如下：  

- **核心架构设计**：  
  - **LoRA模块**：在Transformer的FFN层中引入低秩适配（LoRA）专家，仅更新少量低秩矩阵（占可训练参数的2.14%），保持主干网络冻结，从而高效扩展模型对新语言的能力。  
  - **软专家混合（Soft MoE）**：设计一个软路由机制，动态为每个输入分配多个专家的权重，促进语言间的知识共享与专业化，减少跨语言干扰。  

- **遗忘缓解策略**：  
  - **有限回放数据**：在持续训练过程中，引入少量已学语言（如英语）的样本进行回放，无需依赖完整历史语料库，即可有效巩固旧知识。  

- **训练流程**：  
  - 沿用HuBERT的掩码预测损失，同时对新语言样本和回放样本进行联合优化，确保模型在适应新语言的同时不遗忘原有能力。  

- **优势总结**：  
  - 通过LoRA实现参数高效扩展，通过Soft MoE实现灵活的语言共享与特化，再辅以轻量回放策略，共同解决了多语言持续学习中的扩展成本、遗忘和跨语言干扰问题。

3)  
在 **ML-SUPERB** 基准测试中，MiLorE-SSL 在以下任务上取得了显著效果：  
- **单语自动语音识别（ASR）**：在CommonVoice和Fleurs数据集上，相较于基线模型（HuBERT Large和mHuBERT-147），在英语、普通话和粤语上均实现了更低的字符错误率（CER），例如在CommonVoice上将普通话CER从21.2%降至10.7%。  
- **语言识别（LID）**：平均准确率达到99.40%，优于对比模型。  
- **综合效果**：模型在提升新语言（普通话、粤语）性能的同时，保持了英语的强能力，且仅使用100小时回放数据就有效缓解了遗忘。
</div>

</details>

---

## Improving X-Codec-2.0 for Multi-Lingual Speech: 25 Hz Latent Rate and 24 kHz Sampling
- **Authors**: Husein Zolkepli
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.20185v1](https://arxiv.org/abs/2601.20185v1)
- **PDF**: [https://arxiv.org/pdf/2601.20185v1](https://arxiv.org/pdf/2601.20185v1)

X-Codec-2.0 在神经音频压缩与多语言语音建模中表现出色，其采用冻结的 HuBERT 特征，以 50 Hz 潜在速率和 16 kHz 采样率运行。尽管效果显著，但该配置在时间效率和音频保真度上存在局限。本研究通过引入额外的池化操作并增大解码器跳跃步长，提出一种简单而有效的改进方案。该方案将潜在速率从 50 Hz 降低至 25 Hz，同时将输出采样率从 16 kHz 提升至 24 kHz，从而在不改变核心架构的前提下，显著提升了系统效率与感知质量。在多语言 Common Voice 17 测试集上的评估显示，基于 UTMOSv2 评分，所提配置相比原始 X-Codec-2.0 基线实现了 0.29 的平均意见分提升，并在所有运行于 25 Hz 的编解码器中取得了当前最佳性能。相关源代码、模型检查点及生成对比已发布于 \href{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}{https://huggingface.co/Scicom-intl/xcodec2-25TPS-24k}。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：X-Codec-2.0 是一种基于 HuBERT 特征的多语言神经音频编解码器，在 50 Hz 潜在帧率和 16 kHz 采样率下运行。  
- **既有问题**：  
  - 50 Hz 潜在帧率限制了时间分辨率，导致高频内容模糊。  
  - 16 kHz 采样率限制了音频保真度，影响感知质量。  
  - 固定的帧率可能无法充分利用模型捕捉细粒度语音变化的能力。

2)  
- **核心方法**：通过简单的架构调整，在保持核心组件不变的情况下，同时降低潜在帧率并提高采样率。  
  - **调整跳步大小与池化**：将编码器的跳步大小从 320 样本增加到 960 样本，并在量化前引入一个平均池化层（核大小为 2，步长为 2）。这使潜在帧率从 50 Hz 降至 25 Hz。  
  - **解码器权重插值**：使用一维线性插值调整预训练解码器输出层的权重和偏置，以适应新的跳步大小，从而平滑过渡到新分辨率。  
  - **参数冻结与微调**：冻结语义编码器和编解码器编码器（直接使用预训练权重），仅微调解码器以适应新的 25 Hz 潜在帧率和 24 kHz 输出采样率。  
- **解决效果**：  
  - 潜在帧率减半，提高了时间效率，减少了生成模型所需的令牌序列长度。  
  - 采样率提升至 24 kHz，增强了高频重建能力和整体感知清晰度。  
  - 无需增加参数或训练复杂度，即实现了更高质量的音频重建。

3)  
- **评估任务**：在多语言语音重建任务上，使用 Common Voice 17 测试集（涵盖 116 种语言）进行评估。  
- **取得效果**：  
  - 在 UTMOSv2 评分上，相比原始 X-Codec-2.0 基线平均意见分数（MOS）提升 0.29。  
  - 在 25 Hz 潜在帧率的所有编解码器中，取得了最佳报告性能，尤其在荷兰语、英语、法语等代表性语言上表现一致优异。
</div>

</details>

---
