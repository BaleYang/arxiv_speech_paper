---
layout: post
title: "arXiv Daily – 2026-02-24"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-24（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-23 08:50 — 2026-02-24 08:50
- 抓取总数：9 篇 | 本页显示：9 篇（去重/过滤后）

## StyleStream: Real-Time Zero-Shot Voice Style Conversion
- **Authors**: Yisi Liu, Nicholas Lee, Gopala Anumanchipalli
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.20113v1](https://arxiv.org/abs/2602.20113v1)
- **PDF**: [https://arxiv.org/pdf/2602.20113v1](https://arxiv.org/pdf/2602.20113v1)

语音风格转换旨在将输入语音转换为与目标说话者的音色、口音和情感相匹配，其核心挑战在于将语言内容与风格特征进行解耦。现有研究虽已探索此问题，但转换质量仍有限制，且实时语音风格转换尚未实现。本文提出StyleStream，首个可流式处理的零样本语音风格转换系统，实现了最先进的性能。StyleStream包含两个组件：去风格化器（Destylizer）用于去除风格属性同时保留语言内容，以及风格化器（Stylizer）——一个基于参考语音条件化的扩散变换器（DiT），用于重新注入目标风格。通过文本监督和高度受限的信息瓶颈设计，系统实现了鲁棒的内容-风格解耦。该架构采用完全非自回归设计，以1秒的端到端延迟实现了实时语音风格转换。音频样本与实时演示见：https://berkeley-speech-group.github.io/StyleStream/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：零样本语音风格转换旨在将输入语音转换为具有目标说话人音色、口音和情感的风格，同时保留原始语言内容。核心挑战在于将语言内容与风格信息有效解耦。
- **既有方法的问题**：
  - 现有方法（如信息瓶颈、信号扰动、互信息最小化）在内容-音色解耦上取得了一定效果，但提取的特征仍与口音和情感信息纠缠。
  - 例如，CosyVoice 2的“语义标记”因码本较大（6561）仍编码了大量口音和情感信息；Vevo使用自监督量化，但缺乏明确解耦监督，导致风格泄露和内容退化。
  - 此外，现有工作未解决实时语音风格转换（涵盖音色、口音和情感）的问题，存在应用空白。

2)  
StyleStream通过**解耦器（Destylizer）**和**风格化器（Stylizer）**两个核心组件解决上述问题，实现实时零样本语音风格转换。

- **解耦器（Destylizer）**：负责从源语音中提取与风格信息解耦的纯内容特征。
  - **文本监督**：结合自动语音识别（ASR）损失进行训练，引导模型通过信息瓶颈保留语言内容，抑制风格信息（因风格对ASR预测无帮助）。
  - **紧凑码本**：采用有限标量量化（FSQ）作为信息瓶颈，码本大小限制为45（远小于CosyVoice 2的6561），形成更窄的瓶颈，强制过滤风格信息。
  - **连续量化前特征**：不直接使用离散量化标记，而是采用FSQ层前的连续表示作为内容特征。这种“软单元”在解耦和内容保留间取得更好平衡，实验表明其包含的风格信息少于Vevo和CosyVoice 2的离散标记。

- **风格化器（Stylizer）**：基于内容特征和目标参考语音，生成具有目标风格的梅尔频谱图。
  - **扩散变换器（DiT）**：采用具有频谱图修复目标的扩散变换器作为主干。其非自回归架构确保了输入输出长度一致，便于流式处理。
  - **风格编码器**：从目标语音中提取全局风格嵌入（涵盖音色、口音、情感），并通过adaLN-Zero集成到DiT中。
  - **条件流匹配训练**：使用最优传输路径的条件流匹配损失进行训练，专注于掩码区域的生成。

- **实时流式设计**：
  - 对解耦器和风格化器均采用**分块因果注意力掩码**，每个块仅关注自身及过去块，避免未来信息泄露。
  - 配合因果声码器，实现了端到端流式处理。通过固定长度目标语音和内容特征环形缓冲区管理，在高端GPU上达到约1秒的端到端延迟（处理时间小于块大小）。

3)  
- **任务**：在零样本语音风格转换任务上，对**音色、口音和情感进行联合转换**，并评估了**流式（实时）和离线**两种模式。
- **效果**：
  - **内容保真度**：离线版本词错误率（WER）最低（9.2%），显著优于之前最佳方法Vevo（17.5%），表明内容保留能力更强。流式版本（WER 15.3%）仍具竞争力。
  - **风格相似度**：在说话人相似度（S-SIM）、口音相似度（A-SIM）和情感相似度（E-SIM）的客观指标上，StyleStream均取得最佳成绩。主观平均意见得分（MOS）在说话人、口音、情感相似度上也大幅领先所有基线。
  - **实时性能**：首次实现实时语音风格转换，端到端延迟约1秒，在保持高质量的同时满足了流式处理要求。
</div>

</details>

---

## SongEcho: Towards Cover Song Generation via Instance-Adaptive Element-wise Linear Modulation
- **Authors**: Sifei Li, Yang Li, Zizhou Wang, Yuxin Zhang, Fuzhang Wu, Oliver Deussen, Tong-Yee Lee, Weiming Dong
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.19976v1](https://arxiv.org/abs/2602.19976v1)
- **PDF**: [https://arxiv.org/pdf/2602.19976v1](https://arxiv.org/pdf/2602.19976v1)

翻唱歌曲是音乐文化的重要组成部分，它在保留原曲核心旋律的同时，通过重新演绎赋予作品新的情感深度与主题侧重。尽管已有研究通过旋律条件化的文本到音乐模型探索器乐改编，但翻唱歌曲生成任务仍鲜有涉及。本研究将翻唱歌曲生成重新定义为条件生成任务，即在原始人声旋律与文本提示的共同约束下，同步生成新的人声与伴奏。为此，我们提出SongEcho系统，其核心是实例自适应逐元素线性调制框架。该框架通过改进条件注入机制与条件表示来实现可控生成：在条件注入方面，我们将特征级线性调制扩展为逐元素线性调制，以提升旋律控制的时序对齐精度；在条件表示方面，我们提出实例自适应条件优化模块，通过与生成模型隐状态的交互动态优化条件特征，实现实例自适应的条件控制。此外，针对大规模开源全曲数据集稀缺的问题，我们构建了Suno70k高质量AI歌曲数据集，并提供丰富的标注信息。在多数据集上的实验表明，本方法在仅需不足30%可训练参数的情况下，生成的翻唱歌曲质量显著优于现有方法。代码、数据集及演示示例已公开于：https://github.com/lsfhuihuiff/SongEcho_ICLR2026。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：翻唱歌曲在音乐文化中具有重要意义，但现有研究主要集中于文本到歌曲生成或单轨人声合成，缺乏对**完整翻唱歌曲生成**（同时生成人声与伴奏）的探索。  
- **既有方法问题**：现有可控音乐生成方法（如旋律控制）在条件注入机制上存在局限：  
  - **交叉注意力**机制需要额外建模时序对齐，计算冗余且间接。  
  - **逐元素加法**机制调制灵活性有限，仅为固定缩放系数的仿射变换。  
  - 条件编码**独立于生成模型的隐藏状态**，可能导致特征冲突与音频质量下降。

2)  
论文提出 **SongEcho** 框架，其核心方法是 **实例自适应的逐元素线性调制（IA-EiLM）**，包含两个关键组件以解决上述问题：  

- **逐元素线性调制（EiLM）**：  
  - 扩展了特征级线性调制（FiLM），通过线性投影直接从条件特征生成与隐藏状态维度完全匹配的调制参数（缩放因子 γ 和偏置 β）。  
  - 实现对隐藏状态的**逐元素、时序对齐的仿射变换**，公式为：\( h_i^m = \gamma_i \odot h_i + \beta_i \)。  
  - 相比交叉注意力，无需额外学习对齐；相比逐元素加法，调制更灵活。  

- **实例自适应条件优化（IACR）**：  
  - 引入门控机制，使旋律条件特征与生成模型的当前隐藏状态进行交互。  
  - 动态优化条件表示，使其适应具体生成实例，公式为：\( c_i = \tanh(h_i') \odot \tanh(m_i') \)。  
  - 避免静态条件注入导致的特征冲突，提升旋律融合的自然性与音频质量。  

- **整体框架**：  
  - 基于预训练文本到歌曲模型 ACE-Step，仅训练 IA-EiLM 模块与旋律编码器，参数高效。  
  - 将 IA-EiLM 插入每个 Transformer 块的前馈网络层前，防止旋律信息在全局注意力中被稀释。

3)  
- **任务**：在**翻唱歌曲生成**任务上评估，要求模型根据原始人声音高序列（旋律）和文本提示，同时生成新人声与和谐伴奏。  
- **效果**：  
  - 在构建的 Suno70k 数据集和 SongEval 基准上，均**超越现有最优方法**（如 Stable Audio ControlNet、MuseControlLite）。  
  - 在旋律控制指标（RPA、RCA）上显著提升，音频质量指标（FD、KL）大幅改善，CLAP 分数更高，歌词错误率（PER）更低。  
  - 主观听测中，在旋律保真度、文本遵循度、音频质量与整体偏好上均获最高分。  
  - 仅需 **4900 万可训练参数**，远少于基线（最低仅需其 3.07%）。
</div>

</details>

---

## DTT-BSR: GAN-based DTTNet with RoPE Transformer Enhancement for Music Source Restoration
- **Authors**: Shihong Tan, Haoyu Wang, Youran Ni, Yingzhao Hou, Jiayue Luo, Zipei Hu, Han Dou, Zerui Han, Ningning Pan, Yuzhu Wang, Gongping Huang
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.19825v1](https://arxiv.org/abs/2602.19825v1)
- **PDF**: [https://arxiv.org/pdf/2602.19825v1](https://arxiv.org/pdf/2602.19825v1)

音乐源修复（MSR）旨在从经过混音和母带处理的录音中恢复未经处理的音轨。该任务的核心挑战在于既要分离重叠的声源，又要重建因压缩、混响等制作效果而退化的信号。为此，我们提出DTT-BSR，一种混合生成对抗网络（GAN），其结合了用于长时域建模的旋转位置编码（RoPE）Transformer，以及用于多分辨率频谱处理的双路径频带分割循环神经网络（RNN）。该模型在ICASSP 2026 MSR挑战赛的客观评测榜上位列第三，主观评测榜上位列第四，在仅710万参数的紧凑规模下，展现出卓越的生成保真度与语义对齐能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐源修复旨在从经过混音和母带处理的录音中恢复未经处理的原始音轨。这比传统的音乐源分离更具挑战性，因为它不仅需要分离重叠的声源，还需要修复在音乐制作过程中因压缩、混响等效果而退化的信号。
- **既有方法问题**：现有方法可能在同时处理长期时间依赖关系和精细频谱特征方面存在不足，难以在一个统一的端到端框架内有效兼顾判别式分离和生成式修复。

2)  
论文提出的DTT-BSR方法是一个混合生成对抗网络，通过整合多种先进模块来解决上述问题：
- **核心架构**：以高效的DTTNet U-Net结构为骨干网络，构建了一个端到端的统一框架。
- **长期依赖建模**：在瓶颈层引入旋转位置嵌入（RoPE）Transformer模块，专门用于捕捉音频信号中的长期时间依赖关系。
- **多分辨率频谱处理**：结合双路径循环神经网络（RNN）模块，对时频特征进行细粒度提取，以处理多分辨率频谱信息。
- **训练策略**：采用复合损失函数进行训练，结合了多梅尔谱回归损失和来自EnCodec多频判别器的对抗性反馈，以提升感知质量和生成保真度。

3)  
- **任务**：在ICASSP 2026音乐源修复挑战赛的官方测试集上进行了评估，任务是从混合录音中恢复8种目标音轨（如人声、吉他、鼓等）。
- **效果**：
    - 在客观指标排行榜上获得第3名，在主观评价排行榜上获得第4名。
    - 在非人声乐器（如吉他、键盘、管弦乐）的分离与修复上表现尤为出色。
    - 模型参数量仅为710万，在保持紧凑规模的同时，展现了优异的生成保真度和语义对齐能力。
</div>

</details>

---

## Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling
- **Authors**: Yungang Yi
- **Categories**: cs.SD, cs.AI, cs.LG
- **arXiv**: [https://arxiv.org/abs/2602.19816v1](https://arxiv.org/abs/2602.19816v1)
- **PDF**: [https://arxiv.org/pdf/2602.19816v1](https://arxiv.org/pdf/2602.19816v1)

长上下文建模对于符号音乐生成至关重要，因为动机重复与发展性变化可能跨越数千个音乐事件。然而，实际作曲与演奏工作流常依赖于资源受限的设备（如电子乐器与便携计算机），使得沉重的内存与注意力计算难以部署。本文提出深度结构化音乐循环（DSMR），这是一种用于全曲符号音乐建模的循环长上下文Transformer，它通过具有分离跨片段状态的片段级循环机制，将上下文范围扩展至固定长度片段之外，并采用分层记忆时域调度策略，在深度维度上对循环键值状态进行预算分配。DSMR通过对每首完整作品进行单次从左到右的训练（类似于音乐家从头至尾体验作品的过程），同时向前传递循环跨片段状态。在此循环框架内，我们系统研究了分层时域分配如何影响优化过程、最佳检查点困惑度及计算效率。通过在各层分配不同的历史窗口长度，同时保持循环状态总预算不变，DSMR在循环注意力堆栈中构建了与深度相关的时间感受野，且未降低计算深度。我们的主要实现方案是双尺度DSMR调度策略，该策略为底层分配长历史窗口，为其余层分配统一的短窗口。在钢琴演奏数据集MAESTRO上的实验表明，双尺度DSMR为有限计算资源下基于循环注意力的全长长上下文符号音乐建模提供了一种实用的质量-效率平衡方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：符号音乐生成需要长上下文建模，以捕捉跨越数千个音符的动机重复和发展变化。然而，实际创作和表演常依赖资源受限设备（如电子乐器、便携电脑），使得高内存和注意力计算难以部署。  
- **既有方法问题**：标准Transformer自注意力随序列长度呈二次方增长，成为长音乐流的主要瓶颈。现有模型（如Transformer-XL）通常采用固定长度记忆窗口，会丢弃较早片段，或依赖固定长度截断训练，破坏了全曲的连贯性。

2)  
论文提出**深度结构化音乐循环（DSMR）**，一种用于全曲符号音乐建模的循环长上下文Transformer，通过以下方式解决上述问题：  
- **全曲循环训练**：采用Transformer-XL风格的片段级循环，但在训练时对每首作品进行单次从左到右的完整处理，携带跨片段的KV状态向前传递，使每个音符都能以整个作品的长距离因果历史为条件，避免了上下文碎片化。  
- **层间记忆视界调度**：在固定的总循环状态预算下，为不同层分配不同的历史窗口长度 `m(ℓ)`，从而在循环注意力堆栈中创建**深度依赖的时间感受野**。这允许不同层专注于不同有效上下文范围，无需增加计算深度。  
- **核心实现：双尺度DSMR调度**：将长历史窗口分配给较低层（如第1层），其余层使用统一的短窗口。这种结构先验在固定预算下诱导出多尺度感受野，较低层捕获长程依赖，较高层处理局部模式，实现了质量与效率的平衡。  
- **与纯剪枝的区别**：DSMR并非计算深度剪枝——所有层仍在每个片段内进行计算，它只是通过塑造层间感受野来结构化跨片段信息流。

3)  
- **任务**：在**MAESTRO钢琴演奏数据集**上进行全曲符号音乐建模（自回归训练）。  
- **效果**：  
  - **质量**：双尺度DSMR在验证集上取得了最佳困惑度（PPL 5.96），略优于全注意力参考模型（PPL 5.98），同时显著优于其他循环变体（如感知器AR类参考PPL 6.54）。  
  - **效率**：在匹配的循环状态预算下，DSMR的峰值GPU内存使用降至6.3 GB（比全注意力参考的15.5 GB降低59.1%），训练吞吐量提升约35.7%，到达最佳检查点的墙钟时间减少26.4%。  
  - **结论**：DSMR为资源受限下的长上下文符号音乐建模提供了一个实用的质量-效率方案。
</div>

</details>

---

## Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation
- **Authors**: Nghia Phan, Rong Jin, Gang Liu, Xiao Dong
- **Categories**: cs.SD, cs.IR, cs.LG, cs.MM
- **arXiv**: [https://arxiv.org/abs/2602.19778v1](https://arxiv.org/abs/2602.19778v1)
- **PDF**: [https://arxiv.org/pdf/2602.19778v1](https://arxiv.org/pdf/2602.19778v1)

自动和弦识别（ACR）受限于对齐和弦标签的稀缺性，因为高质量的对齐标注获取成本高昂。与此同时，开放权重的预训练模型目前比其专有训练数据更易获取。本研究提出一种两阶段训练流程，利用预训练模型结合未标注音频数据。该方法将训练解耦为两个阶段：第一阶段，使用预训练BTC模型作为教师模型，为超过1000小时多样化的未标注音频生成伪标签，并仅基于这些伪标签训练学生模型；第二阶段，当真实标注可用时，对学生模型进行持续训练，并采用选择性知识蒸馏（KD）作为正则化手段，防止第一阶段学习到的表征发生灾难性遗忘。实验中采用两种模型（BTC、2E1D）作为学生模型。第一阶段仅使用伪标签训练时，BTC学生模型达到教师模型98%以上的性能，2E1D模型在七项标准mir_eval指标上达到约96%的教师性能。第二阶段对两个学生模型进行单次训练后，BTC学生模型在所有指标上平均超越传统监督学习基线2.5%，较原始预训练教师模型提升1.55%；2E1D学生模型较传统监督学习基线平均提升3.79%，并达到与教师模型几乎相当的性能。两种模型在稀有和弦品质识别上均表现出显著提升。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **数据稀缺与标注成本**：自动和弦识别（ACR）面临高质量、对齐的和弦标注数据稀缺的问题，因为精确的音频-标签对齐需要大量人工努力，成本高昂。  
- **类别不平衡**：现有标注数据集中和弦类别分布极不均衡，模型在常见的三和弦上表现良好，但在罕见的七和弦、增减和弦上性能较差。  
- **现有方法局限**：以往的半监督方法通常需要从一开始就使用真实标签，并将伪标签数据与标注数据混合训练，限制了在标注数据完全缺失时的应用灵活性。

2)  
论文提出一个**两阶段训练流程**，利用预训练模型和大量未标注音频来解决问题：  

- **第一阶段：仅用伪标签预训练**  
  - 使用一个开源的、预训练的BTC模型作为“教师”，为超过1000小时多样化的未标注音频生成帧级伪标签。  
  - 使用这些伪标签从头训练一个“学生”模型，不依赖任何真实标注。  
  - 此阶段利用了大规模未标注数据自然覆盖的、近乎均匀的12个调性分布，避免了传统监督学习中为平衡调性而使用的、可能引入音频伪影的音高移位数据增强。

- **第二阶段：基于真实标注的持续学习与知识蒸馏正则化**  
  - 当标注数据可用时，将在第一阶段训练好的学生模型作为初始化，在真实标注数据上进行持续学习。  
  - 为**防止灾难性遗忘**第一阶段从伪标签中学到的表征，在整个第二阶段引入**选择性知识蒸馏（KD）**作为正则化器：  
    - KD损失将学生的预测“拉向”教师模型的软化概率分布。  
    - 通过一个非对称的置信度加权函数，KD主要作用于教师模型预测置信度适中的样本（这些样本通常包含有价值的决策边界信息），而过滤掉低置信度或过度自信的预测。  
    - 这使得当真实标签与教师预测冲突时（如标注噪声或对齐错误），KD能防止学生过拟合到错误标签；当二者一致时，则允许模型适应真实标注。  
  - 该方法还验证了**跨架构通用性**，不仅用于与教师同架构的BTC学生模型，也用于一个新提出的、更轻量的纯Transformer双编码器架构（2E1D）。

3)  
- **任务**：在自动和弦识别（ACR）任务上进行了评估，使用了包括Isophonics、McGill Billboard等数据集组成的标准测试集。  
- **效果**：  
  - **仅用伪标签训练（第一阶段）**：BTC学生模型达到了教师模型98%以上的性能；2E1D学生模型达到了约96%。  
  - **完整两阶段训练后**：  
    - **BTC学生模型**在全部七项`mir_eval`指标上，平均超越传统监督学习基线2.5%，超越原始教师模型1.55%。  
    - **2E1D学生模型**平均超越其监督学习基线3.79%，并达到与教师几乎相同的性能。  
  - **关键提升**：对罕见和弦品质（如减和弦、减七和弦、增和弦）的识别准确率提升尤为显著，证明了该方法有效缓解了类别不平衡问题。
</div>

</details>

---

## Continuous Telemonitoring of Heart Failure using Personalised Speech Dynamics
- **Authors**: Yue Pan, Xingyao Wang, Hanyue Zhang, Liwei Liu, Changxin Li, Gang Yang, Rong Sheng, Yili Xia, Ming Chu
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.19674v1](https://arxiv.org/abs/2602.19674v1)
- **PDF**: [https://arxiv.org/pdf/2602.19674v1](https://arxiv.org/pdf/2602.19674v1)

通过语音信号对心力衰竭进行远程监测，为长期患者管理提供了一种无创且经济高效的解决方案。然而，声学特征在个体间存在显著异质性，往往限制了传统横断面分类模型的准确性。为此，我们提出了一种纵向个体内追踪方案，旨在捕捉个体内部相对症状变化的轨迹。该框架的核心是个人化序列编码器，它将纵向语音记录转化为具有上下文感知的潜在表征。通过在每个时间戳纳入历史数据，该编码器能够实现对临床轨迹的整体评估，而非独立建模离散的就诊记录。基于225名患者队列的实验结果表明，该纵向追踪范式显著优于经典的横断面方法，在临床状态转变识别中达到了99.7%的准确率。额外随访数据进一步证实了模型的高灵敏度，验证了其在预测心力衰竭恶化方面的有效性，以及其在远程居家环境中保障患者安全的潜力。此外，本研究通过对不同语音任务设计和声学特征的全面分析，弥补了现有文献的空白。综上所述，该纵向追踪框架与个人化序列编码器架构的优越性能，验证了其可集成于长期远程监测系统的成熟性，为远程心力衰竭管理提供了可扩展的解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：心力衰竭（HF）是全球性健康问题，远程语音监测提供了一种非侵入性、低成本的长期管理方案。已有研究证实HF与语音特征（如抖动、谐噪比）存在关联。  
- **既有方法问题**：传统方法多采用**横断面建模**，即基于单次语音记录进行群体级分类。但由于个体间语音特征存在巨大异质性（如年龄、口音、共病），这类方法**准确率受限、泛化能力差**，难以捕捉患者个体内部的细微纵向变化。  

2)  
论文提出了一套名为**纵向患者内追踪（LIPT）** 的个性化框架，其核心是**个性化序列编码器（PSE）**，具体通过以下方式解决上述问题：  
- **范式转变**：从横断面群体分类转向**纵向个体内轨迹建模**。LIPT不直接对绝对语音特征分类，而是**建模患者内部不同时间点之间的相对状态变化**（如恶化或改善），从而剥离个体固有语音特性的干扰。  
- **个性化序列编码器（PSE）的设计**：  
  - **序列化处理**：PSE能处理任意长度的纵向语音记录序列，通过卷积层和循环模块（如LSTM）**聚合历史信息**，生成包含时序依赖关系的上下文感知潜在表示。  
  - **概率化表征**：将潜在表示建模为**高斯分布**，而非确定值，以更好地捕捉数据中的不确定性和个体变化模式。  
  - **预训练与重构**：通过重构损失（MSE）和分布正则化（KL散度）进行预训练，使编码器学习有信息量的语音模式，避免过拟合。  
- **特征优化策略**：  
  - 系统提取并分析了**全局特征**（如频谱、节奏、音质）和**帧级特征**（如RASTA、MFCC）。  
  - 通过**统计检验**（如t检验）筛选出与HF显著相关的特征子集（HF-voice特征集），以提升模型鲁棒性。  
- **训练机制**：在训练阶段，随机分配时间点构成正负样本对，训练模型判断状态变化方向，使模型学会独立于输入顺序的一致性判断。  

3)  
论文在以下任务上验证了效果，均基于225名HF患者的语音数据：  
- **主要任务（治疗期间状态转变识别）**：在区分失代偿期与治疗后状态的任务中，LIPT框架下的PSE模型（结合帧级RASTA与筛选后的全局特征）取得了**99.7%的准确率**，宏F1分数达99.7%，显著优于传统横断面方法（最佳约81.8%）。  
- **随访任务（恶化预警）**：在区分稳定患者与再住院患者的随访数据上，模型表现出**高敏感性（达100%）**，能有效识别所有再住院病例，但存在一定误报（假阳性）。其ROC曲线下面积（AUROC）达0.94，表明模型具有较强的判别能力。  
- **任务与特征分析**：长句计数任务因包含最全面的发音信息，性能最佳；强调浊辅音的短句任务则在敏感性与特异性间更平衡。帧级RASTA特征被证明对HF监测最有效。
</div>

</details>

---

## CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment
- **Authors**: Hanwen Liu, Saierdaer Yusuyin, Hao Huang, Zhijian Ou
- **Categories**: eess.AS, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.19574v1](https://arxiv.org/abs/2602.19574v1)
- **PDF**: [https://arxiv.org/pdf/2602.19574v1](https://arxiv.org/pdf/2602.19574v1)

基于大语言模型（LLM）的文本转语音（TTS）系统能够生成自然语音，但多数系统未针对低延迟双流合成进行设计。高质量的双流TTS依赖于准确的文本-语音对齐以及平衡合成质量与延迟的训练序列设计。现有方法通常依赖基于GMM-HMM的强制对齐工具（如MFA），这类工具流程复杂且相比神经对齐器灵活性不足；而固定比例交织文本与语音标记的策略难以有效捕捉文本-语音对齐规律。本文提出CTC-TTS系统，采用基于CTC的对齐器替代MFA，并引入基于双词单元的交织策略。我们设计了两种变体：沿序列长度进行标记拼接的CTC-TTS-L（侧重更高合成质量）和沿特征维度进行嵌入堆叠的CTC-TTS-F（侧重更低延迟）。实验表明，CTC-TTS在流式合成与零样本任务上均优于固定比例交织及基于MFA的基线方法。语音样本详见 https://ctctts.github.io/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**  
- **背景**：基于大语言模型（LLM）的文本转语音（TTS）系统能生成自然语音，但多数未针对低延迟的双流合成设计。高质量双流TTS需依赖准确的文本-语音对齐以及平衡合成质量与延迟的训练序列设计。  
- **既有问题**：  
  - 现有方法常依赖基于GMM-HMM的强制对齐工具（如MFA），其流程复杂且灵活性不如神经对齐器。  
  - 文本与语音令牌的固定比例交错难以捕捉对齐规律，增加了模型学习时序依赖的难度。  

2)  
**论文核心方法如何解决上述问题**  
本文提出CTC-TTS，通过以下创新解决对齐与序列组织问题：  
- **CTC对齐替代MFA**：  
  - 使用基于连接时序分类（CTC）的轻量对齐器，通过空白符号和维特比算法生成语音-音素对齐路径，无需精确的音素边界。  
  - 相比MFA，CTC对齐流程更简单、灵活，且能提供稳定的结构对齐，足以支持词级音素-语音块构建。  
- **双词块交错策略**：  
  - 基于词级对齐，设计“双词块”序列：当前单词音素 + 分隔符 + 下一单词音素 + 当前单词对应的语音令牌，以紧凑的前瞻平衡上下文信息与延迟。  
- **两种变体实现质量-延迟权衡**：  
  - **CTC-TTS-L**：沿序列长度维度拼接文本与语音令牌，以提升生成质量。  
  - **CTC-TTS-F**：沿特征维度堆叠嵌入，允许从第一个音素开始合成，降低首包延迟。  
- **整体框架**：  
  - 包含G2P转换、CTC对齐、交错序列构建、自回归Transformer预测及神经音频编解码器重建。训练时仅优化语音令牌和块终止符的交叉熵损失。  

3)  
**在哪些任务上取得了怎样的效果**  
- **单说话人流式合成任务**：  
  - 在VoiceAssistant400K数据集上，CTC-TTS-F相比基线LLMVox降低了WER（1.80% vs. 2.40%）和CER（1.04% vs. 1.36%），同时首包延迟更低（159ms vs. 167ms）。CTC-TTS-L进一步降低WER/CER（1.50%/0.79%），但延迟略高（210ms）。  
- **多说话人零样本任务**：  
  - **延续任务**：CTC-TTS-L在LibriSpeech上取得接近真实语音的MOS（4.33）和SMOS（4.60），显著优于基于MFA和ELLA-V序列的基线。  
  - **跨说话人任务**：在Seed-TTS测试集上，CTC-TTS-L在WER（6.33%）、CER（3.21%）和自然度（MOS 4.23）上表现最优，显示其对齐策略在跨领域场景下的泛化优势。
</div>

</details>

---

## An LLM-Enabled Frequency-Aware Flow Diffusion Model for Natural-Language-Guided Power System Scenario Generation
- **Authors**: Zhenghao Zhou, Yiyan Li, Fei Xie, Lu Wang, Bo Wang, Jiansheng Wang, Zheng Yan, Mo-Yuen Chow
- **Categories**: eess.SP, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.19522v1](https://arxiv.org/abs/2602.19522v1)
- **PDF**: [https://arxiv.org/pdf/2602.19522v1](https://arxiv.org/pdf/2602.19522v1)

多样且可控的场景生成（如风、光、负荷等）对于电力系统的鲁棒规划与运行至关重要。随着基于人工智能的场景生成方法逐渐成为主流，现有方法（如条件生成对抗网络）主要依赖固定长度的数值条件向量来控制生成结果，在用户便利性和生成灵活性方面面临挑战。本文提出了一种基于自然语言引导的场景生成框架——LLM赋能的频率感知流扩散模型（LFFD），使用户能够通过自然语言描述生成所需场景。首先，引入预训练大语言模型模块，将非结构化自然语言描述的生成请求转换为有序语义空间。其次，采用基于修正流匹配目标的流扩散模型替代标准扩散模型，以LLM输出作为模型输入，实现高效且高质量的场景生成。在模型训练过程中，引入频率感知多目标优化算法以缓解频率偏差问题。同时，设计双智能体框架用于创建文本-场景训练样本对并标准化语义评估。基于大规模光伏与负荷数据集的实验验证了所提方法的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：电力系统规划与运行需要多样可控的场景生成（如风电、光伏、负荷）。现有基于AI的方法（如条件生成对抗网络）主要依赖固定长度的数值条件向量来控制生成结果。
- **既有问题**：
  - **用户便利性差**：用户需为预定义条件向量的所有位置指定数值，即使某些位置无关或难以获取，引入噪声且不友好。
  - **生成灵活性低**：仅能通过调整条件向量值实现有限控制，损害生成结果的多样性。
  - **扩散模型效率与频率偏差**：标准去噪扩散概率模型推理速度慢，且存在频谱偏差，过度平滑高频瞬态，影响信号保真度。

2)  
论文提出 **LLM启用的频率感知流扩散模型**，通过以下核心方法解决上述问题：

- **自然语言条件生成**：
  - 引入预训练大语言模型模块，将用户用自然语言描述的需求转换为结构化条件向量，突破固定向量限制，实现灵活、用户友好的控制。

- **高效高质量的流扩散模型**：
  - 采用基于修正流匹配目标的流扩散模型，替代标准扩散模型，通过线性插值的最优传输路径实现快速生成。
  - 推理时通过求解常微分方程，以较少步数（如50步）实现高保真采样，相比扩散模型（需1000步）大幅提升效率。

- **频率感知多目标优化**：
  - 设计频谱一致性损失，在频率域衡量预测速度与目标速度的频谱幅度差异。
  - 使用时域损失和频域损失作为多目标，采用多梯度下降算法动态平衡梯度冲突，自动寻找帕累托最优解，确保模型同时捕捉场景趋势和高频瞬态。

- **文本导向的时间去噪网络**：
  - 设计U-Net架构的降噪网络，集成残差块（注入时间步条件）和跨模态注意力块（以时间特征为查询，文本嵌入为键/值），实现语义与时间特征的精准对齐。

- **双智能体框架**：
  - 标注器智能体：基于统计报告和领域知识，为未标注场景自动生成高质量文本描述，解决训练数据对稀缺问题。
  - 评判器智能体：评估生成场景与文本提示的语义对齐度，提供量化评分，实现标准化语义评估。

3)  
- **任务与效果**：
  - **场景生成质量**：在光伏和负荷数据集上，LFFD在KL散度、MMD、FD、DTW和PSDD等指标上均优于VAE、DCGAN、WGAN和DDPM等基线模型，显著提升了分布对齐和形态保真度。
  - **频谱保真度**：通过频率感知优化，将频谱密度距离大幅降低（相比最强基线降低一个数量级），有效保留了高频瞬态。
  - **文本控制有效性**：生成场景与文本提示在峰值、波动性等属性上高度对齐，平均评判智能体得分接近满分（5分制下PV为4.80，负荷为4.46）。
  - **推理效率**：相比DDPM，在达到更低FD值时实现约87倍的加速（10步 vs. 1000步），且即使在少步数下仍保持高质量生成。
</div>

</details>

---

## AuditoryHuM: Auditory Scene Label Generation and Clustering using Human-MLLM Collaboration
- **Authors**: Henry Zhong, Jörg M. Buchholz, Julian Maclaren, Simon Carlile, Richard F. Lyon
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.19409v1](https://arxiv.org/abs/2602.19409v1)
- **PDF**: [https://arxiv.org/pdf/2602.19409v1](https://arxiv.org/pdf/2602.19409v1)

音频数据的手动标注工作繁重，且难以在标签粒度与声学可分性之间取得平衡。本文提出AuditoryHuM，一种基于人机协作（人类-多模态大语言模型）的无监督听觉场景标签发现与聚类框架。该框架利用多模态大语言模型（Gemma与Qwen）为音频数据生成上下文相关的标签。为确保标签质量并减少幻觉现象，我们采用零样本学习技术（Human-CLAP）量化生成文本标签与原始音频内容之间的对齐程度，进而通过针对性的人机交互干预对对齐度最低的样本进行优化。通过引入惩罚参数以平衡类内凝聚度与主题粒度，我们采用调整后的轮廓系数将发现的标签归并为主题连贯的聚类簇。在三个多样化听觉场景数据集（ADVANCE、AHEAD-DS与TAU 2019）上的评估表明，AuditoryHuM为构建标准化分类体系提供了可扩展、低成本的解决方案，有助于训练可部署于助听器、智能家居助手等边缘设备的轻量化场景识别模型。项目页面与代码：https://github.com/Australian-Future-Hearing-Initiative

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：听觉场景识别是助听器、智能家居等设备的关键功能，通常依赖带标签的音频数据集训练深度学习模型。  
- **既有问题**：  
  - 人工标注音频数据劳动密集，难以规模化。  
  - 现有标签的粒度难以平衡：过于精细的标签（如“学校食堂中的语音”）可能缺乏声学可分性，而过于粗糙的标签（如“道路”）则实用性不足。  
  - 多模态大语言模型（MLLM）可用于自动生成标签，但存在幻觉风险，即生成与音频内容无关的描述。

2)  
论文提出 **AuditoryHuM** 框架，通过人机协作实现无监督的听觉场景标签发现与聚类，具体步骤如下：  
- **MLLM生成初始标签**：使用Gemma、Qwen等MLLM为音频样本生成上下文相关的描述性标签（如词对）。通过提示工程引导生成简洁、可解释的标签。  
- **量化标签-音频对齐度**：采用零样本学习技术（如Human-CLAP）计算生成标签与原始音频内容的CLAP分数（余弦相似度），以评估对齐质量，识别低对齐样本。  
- **靶向人工干预**：针对CLAP分数最低的样本（如底部1%），由人工进行重新标注，以纠正幻觉问题并提升标签质量。仅需少量人工审核，即可显著改善低对齐样本。  
- **标签清洗与嵌入**：对MLLM生成的标签进行清洗（如去除非字母字符、截断长句），然后使用句子转换器（Sentence Transformers）将标签映射到文本嵌入空间。  
- **基于嵌入的聚类**：在嵌入空间中使用聚类算法（如凝聚聚类）将语义相似的标签分组。通过调整轮廓分数引入惩罚参数λ，以平衡类内凝聚度与主题粒度，避免过度细分。  
- **生成复合标签**：根据每个簇的标签分布向量，再次使用MLLM生成代表整个簇的复合描述性标签，形成标准化、可解释的分类体系。  

该方法无需预定义标签集，不依赖新模型训练，可扩展至大规模音频数据，且通过人机协作确保标签质量与人类感知一致。

3)  
- **任务**：在三个听觉场景数据集（ADVANCE、AHEAD-DS、TAU 2019）上进行无监督标签发现与聚类。  
- **效果**：  
  - **标签对齐**：最先进的MLLM（Qwen 2.5 Omni 3B）生成的标签与音频内容对齐度最高（CLAP分数均值达0.49–0.61）。靶向人工干预将底部1%低对齐样本的CLAP分数显著提升（如ADVANCE数据集从0.13提升至0.30）。  
  - **聚类质量**：通过调整轮廓分数优化聚类数量，在三个数据集上分别得到152、67和116个主题凝聚的簇。t-SNE可视化显示簇内标签语义相关。  
  - **应用价值**：生成的复合标签（如“鸟鸣”、“风声”、“车辆经过”）可构成标准化分类体系，用于训练轻量级场景识别模型，适用于助听器等边缘设备。
</div>

</details>

---
