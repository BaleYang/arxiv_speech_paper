---
layout: post
title: "arXiv Daily – 2026-01-15"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-15（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-14 08:50 — 2026-01-15 08:50
- 抓取总数：8 篇 | 本页显示：8 篇（去重/过滤后）

## Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer
- **Authors**: Petros Vavaroutsos, Theodoros Palamas, Pantelis Vikatos
- **Categories**: cs.SD, cs.AI, cs.CL, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.09603v1](https://arxiv.org/abs/2601.09603v1)
- **PDF**: [https://arxiv.org/pdf/2601.09603v1](https://arxiv.org/pdf/2601.09603v1)

近年来，基础模型因其卓越性能而广受欢迎，最初主要应用于自然语言处理任务。这类模型通常包含数亿甚至数十亿参数，导致训练和部署过程资源消耗巨大、成本高昂。本文聚焦于在音乐信息检索任务中压缩基础模型规模。研究将语音识别领域首次应用的Branchformer架构与SummaryMixing模块相结合，并引入随机量化方法。为保障可复现性，我们在公开数据集上进行预训练，同时辅以规模与文献报道的私有数据集相当的专有数据集。通过构建包含多种下游音乐信息检索任务的评估框架，我们实现了稳健的性能验证。实验结果表明：相较于采用多头自注意力机制的主流模型，本架构在保持竞争力的性能表现的同时，将模型规模压缩了8.5%至12.3%。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.09603v1）
</div>

</details>

---

## Towards Realistic Synthetic Data for Automatic Drum Transcription
- **Authors**: Pierfrancesco Melucci, Paolo Merialdo, Taketo Akama
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.09520v1](https://arxiv.org/abs/2601.09520v1)
- **PDF**: [https://arxiv.org/pdf/2601.09520v1](https://arxiv.org/pdf/2601.09520v1)

深度学习模型在自动鼓点转录（ADT）领域代表了当前最高水平，但其性能依赖于大规模、成对的音频-MIDI数据集，而这类数据十分稀缺。现有使用合成数据的解决方案通常存在显著的领域差距，因为它们大多依赖缺乏声学多样性的低保真SoundFont音源库。虽然高质量的单次采样样本提供了更好的替代方案，但它们尚未以适用于训练的标准化、大规模形式存在。本文提出了一种新的ADT范式，绕过了对成对音频-MIDI训练数据的需求。我们的核心贡献是一种半监督方法，能够从未标注的音频源中自动整理出大规模且多样化的单次鼓采样样本库。随后，我们利用该样本库仅从MIDI文件合成高质量数据集，并用于训练序列到序列的转录模型。我们在ENST和MDB测试集上评估了该模型，其取得了新的最优性能，显著超越了全监督方法及先前的合成数据方案。实验复现代码已公开于：https://github.com/pier-maker92/ADT_STR

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动鼓转录（ADT）依赖大规模成对的音频-MIDI数据进行监督训练，但此类数据稀缺。  
- **既有方法问题**：现有方法常使用合成数据，但通常基于低保真度的SoundFont库，导致合成音频与真实音频存在显著的**领域差距**。此外，高质量的单次打击乐样本虽更真实，但缺乏标准化的大规模格式，且公开库存在标签模糊、命名不一致的问题，难以直接用于训练。

2)  
论文提出一种**半监督方法**，从无标注音频源自动构建大规模、多样化的单次鼓样本库，并仅用MIDI文件合成高质量训练数据，以解决数据稀缺和领域差距问题。核心方法包括：  

- **标准化乐器词汇表**：基于MIDI打击乐键位图，将47种原始音符合并为26个乐器类别，简化分类任务并保留关键打击乐元素。  
- **半监督样本库构建**：  
  - 首先手动标注少量样本作为“种子集”。  
  - 利用CLAP音频编码器提取种子样本的嵌入向量，计算每个乐器类别的**质心**。  
  - 通过计算未标注样本与各类别质心的**余弦相似度**，自动为其分配标签及置信度，从而快速扩展样本库。  
- **动态合成训练数据**：  
  - 从Lakh MIDI数据集中随机选取片段，使用构建的样本库进行音频合成。  
  - 引入**线性插值**技术，混合同一类别的两个样本以生成新样本，增加数据多样性。  
- **模型架构**：采用**编码器-解码器Transformer**，将梅尔频谱图映射为符号化的MIDI令牌序列，实现端到端的鼓转录。  

该方法**避免了成对音频-MIDI数据的需求**，通过高质量样本库缩小合成与真实数据的分布差距，并结合序列到序列模型进行专门优化。

3)  
- **任务**：在**鼓独奏（DTD）** 场景下进行自动鼓转录评估。  
- **效果**：在ENST和MDB测试集上取得了**新的最先进结果**，显著优于全监督方法和先前的合成数据方法。具体而言，在ENST上综合F1分数达0.73，在MDB上达0.79，各项乐器类别（如底鼓、军鼓）的转录性能均有提升。
</div>

</details>

---

## Analysis of the Maximum Prediction Gain of Short-Term Prediction on Sustained Speech
- **Authors**: Reemt Hinrichs, Muhamad Fadli Damara, Stephan Preihs, Jörn Ostermann
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.09461v1](https://arxiv.org/abs/2601.09461v1)
- **PDF**: [https://arxiv.org/pdf/2601.09461v1](https://arxiv.org/pdf/2601.09461v1)

信号预测广泛应用于经济预测、回声消除及数据压缩等领域，尤其在语音和音乐的预测编码中。预测编码算法通过信号预测降低数据传输或存储所需的比特率。预测增益是应用信号编码中衡量预测器质量的经典指标，它将均方预测误差与预测编码器的信号量化噪声联系起来。为评估预测模型，需要了解独立于具体模型的最大可达到预测增益。本文采用Nadaraya-Watson核回归（NWKR）和信息理论上界方法，基于新录制的持续语音/音素数据集分析了预测增益的上界。研究发现：对于清音语音，线性预测器始终能在最多0.3 dB范围内达到最大预测增益；对于浊音语音，最优单抽头预测器为线性，但从双抽头开始，最大可达到预测增益比线性预测器高出约2 dB至6 dB。同时观察到不同说话者/受试者之间存在显著差异。

所创建的数据集及相关代码可应研究需求提供。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.09461v1）
</div>

</details>

---

## Population-Aligned Audio Reproduction With LLM-Based Equalizers
- **Authors**: Ioannis Stylianou, Jon Francombe, Pablo Martinez-Nuevo, Sven Ewan Shepstone, Zheng-Hua Tan
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.09448v1](https://arxiv.org/abs/2601.09448v1)
- **PDF**: [https://arxiv.org/pdf/2601.09448v1](https://arxiv.org/pdf/2601.09448v1)

传统音频均衡调节是一个静态过程，需要手动进行繁琐调整以适应不断变化的聆听场景（如情绪、位置或社交环境）。本文提出一种基于大语言模型（LLM）的替代方案，可将自然语言文本提示映射为均衡器参数设置，从而实现对话式音响系统控制。通过利用受控听音实验收集的数据，我们的模型采用上下文学习与参数高效微调技术，能够可靠地匹配群体偏好的均衡设置。评估方法采用能捕捉用户多样化偏好的分布度量指标，结果显示：相较于随机采样和静态预设基线，该系统在分布对齐方面实现了统计学意义上的显著提升。这些结果表明，大语言模型可充当“人工均衡器”，有助于开发更易用、具备情境感知能力且达到专家水平的音频调校方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.09448v1）
</div>

</details>

---

## Speech-Hands: A Self-Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception
- **Authors**: Zhen Wan, Chao-Han Huck Yang, Jinchuan Tian, Hanrong Ye, Ankita Pasad, Szu-wei Fu, Arushi Goel, Ryo Hachiuma, Shizhe Diao, Kunal Dhawan, Sreyan Ghosh, Yusuke Hirota, Zhehuai Chen, Rafael Valle, Ehsan Hosseini Asl, Chenhui Chu, Shinji Watanabe, Yu-Chiang Frank Wang, Boris Ginsburg
- **Categories**: cs.SD, cs.AI, cs.CL, cs.MA, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.09413v1](https://arxiv.org/abs/2601.09413v1)
- **PDF**: [https://arxiv.org/pdf/2601.09413v1](https://arxiv.org/pdf/2601.09413v1)

本文提出一种语音智能体框架，其核心在于掌握一项关键的全域理解能力：判断何时应信任自身推断，何时需借助外部音频感知。研究动机源于一个重要且反直觉的发现：若直接在语音识别与外部声音理解任务上对全域模型进行微调，常会导致性能下降，因为模型易受噪声假设误导。为解决该问题，我们提出的Speech-Hands框架将问题重构为显式的自反思决策机制。这种可学习的反思基元能有效防止模型被存在缺陷的外部候选结果干扰。研究表明，该智能体行动机制可自然地从语音识别推广至复杂的多选音频推理任务。在OpenASR评测基准中，Speech-Hands在七项测试上持续超越强基线模型，词错误率降低12.1%。在音频问答决策任务中，模型取得77.37%的准确率与高F1分数，展现出跨多样音频问答数据集的鲁棒泛化能力与可靠性。通过融合感知与决策过程，本研究为实现更可靠、更具适应性的音频智能系统提供了可行路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：全模态模型旨在统一处理语音识别与音频理解任务。但研究发现，简单地将音频与外部文本假设（如ASR候选）一起微调模型，反而会导致性能下降。  
- **既有问题**：模型缺乏评估自身感知可靠性的能力，当外部假设有噪声或错误时，容易被误导，产生幻觉或过度纠正，无法在内部感知与外部建议之间做出明智的仲裁。

2)  
论文提出了 **Speech-Hands**，一个可学习的语音智能体框架，其核心是通过**显式的自反思决策**来解决上述问题。具体方法如下：  
- **统一任务建模**：将语音识别和音频问答统一为一个自反思智能体任务。给定音频和可选查询，模型首先生成内部响应，同时接收一个外部模型提供的外部响应。  
- **引入动作令牌**：模型不是隐式融合信息，而是学习一个策略，首先生成一个特殊的**动作令牌**来明确决策。令牌来自集合 {`<internal>`, `<external>`, `<rewrite>`}，分别代表：  
  - **信任自身**：当模型自身感知足够可靠时。  
  - **采纳外部**：当外部假设更优时。  
  - **重写响应**：当需要基于所有可用信息（音频、内部及外部响应）重新推理生成新答案时。  
- **监督信号构建**：  
  - **对于ASR**：基于内部转录、外部转录及生成式纠错（GER）结果与真实文本的词错误率（WER）对比，为每个训练样本分配最优的动作令牌标签。  
  - **对于音频QA**：基于内部预测、外部预测与正确答案的匹配情况分配标签。为应对外部模型的随机性，采用多数采样策略来稳定`<external>`标签的分配。  
- **训练与推理**：  
  - 训练时，将“动作令牌 + 最终目标答案”作为一个序列进行端到端优化，使模型同时学习决策和生成。  
  - 推理时，模型先预测动作令牌，再根据该决策生成最终输出。这提供了明确的、可解释的决策过程。

3)  
- **语音识别任务**：在OpenASR排行榜的七个基准测试上，Speech-Hands持续超越强基线模型，平均词错误率（WER）降低了12.1%，即使在嘈杂或对话数据集（如AMI）上也表现出优异的泛化能力。  
- **音频问答任务**：在包含生物声学、声景和复杂QA的多领域音频问答基准上，取得了77.37%的平均准确率和高F1分数，特别是在复杂QA上达到85.70%的准确率，显著优于基线模型和提示式GER方法，展现了强大的音频推理鲁棒性。
</div>

</details>

---

## SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing
- **Authors**: Ziyang Ma, Guanrou Yang, Wenxi Chen, Zhifu Gao, Yexing Du, Xiquan Li, Zhisheng Zheng, Haina Zhu, Jianheng Zhuo, Zheshu Song, Ruiyang Xu, Tiranrui Wang, Yifan Yang, Yanqiao Zhu, Zhikang Niu, Liumeng Xue, Yinghao Ma, Ruibin Yuan, Shiliang Zhang, Kai Yu, Eng Siong Chng, Xie Chen
- **Categories**: cs.SD, cs.CL, cs.MM
- **arXiv**: [https://arxiv.org/abs/2601.09385v1](https://arxiv.org/abs/2601.09385v1)
- **PDF**: [https://arxiv.org/pdf/2601.09385v1](https://arxiv.org/pdf/2601.09385v1)

近期，开源多模态大语言模型（MLLM）框架（如LLaVA）的涌现为人工智能开发者和研究者提供了便捷的起点。然而，现有MLLM框架大多以视觉为主要输入模态，对语音、音频及音乐模态的深度支持较为有限。这一现状阻碍了音频-语言模型的发展，迫使研究者耗费大量精力于代码编写与超参数调优。为此，我们推出SLAM-LLM——一个专注于语音、语言、音频及音乐处理的开源深度学习框架，旨在支持定制化MLLM的训练。SLAM-LLM提供了编码器、投影器、大语言模型及参数高效微调插件的模块化配置，并包含主流任务的详细训练与推理方案，以及基于大语言模型的自动语音识别、自动音频描述与音乐描述等高性能模型检查点。部分方案已达到或接近当前最优性能，相关技术也已获学术论文收录。我们期望SLAM-LLM能加速研究者在迭代开发、数据工程与模型训练方面的工作，并承诺通过这一开源框架持续推动音频多模态大语言模型的发展，同时呼吁社区共同贡献于基于大语言模型的语音、音频与音乐处理研究。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.09385v1）
</div>

</details>

---

## Research on Piano Timbre Transformation System Based on Diffusion Model
- **Authors**: Chun-Chieh Hsu, Tsai-Ling Hsu, Chen-Chen Yeh, Shao-Chien Lu, Cheng-Han Wu, Bing-Ze Liu, Timothy K. Shih, Yu-Cheng Lin
- **Categories**: cs.SD, cs.MM
- **arXiv**: [https://arxiv.org/abs/2601.09333v1](https://arxiv.org/abs/2601.09333v1)
- **PDF**: [https://arxiv.org/pdf/2601.09333v1](https://arxiv.org/pdf/2601.09333v1)

本文提出一种基于扩散架构的音色转换模型，旨在将各类乐器演奏的音乐精确转换为钢琴版本。该模型采用音高编码器与响度编码器提取音乐的音高与响度特征，作为扩散模型解码器的条件输入，以生成高质量的钢琴音色。案例分析结果表明，该模型在音高准确性与音色相似度方面表现优异，能够稳定处理不同音乐风格（古典、爵士、流行）与长度（从片段到完整曲目）的转换任务。尤其在处理快速变化的音符与复杂音乐结构时，模型仍能保持较高的音质与准确度，展现出良好的泛化能力。此外，该模型具备实时音乐转换的潜力，适用于现场演出与数字音乐创作工具。未来研究将着重增强对响度动态变化的处理能力，并引入更多音乐特征（如音色变化与节奏复杂度），以提升模型的适应性与表现力。我们计划探索该模型在其他音色转换任务中的应用潜力，例如人声转乐器音色或与MIDI数字钢琴的集成，进一步拓展基于扩散的音色转换模型在音乐生成领域的应用范围。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代音乐创作中，整合多种乐器录音过程复杂且耗时。使用软件音源库操作门槛高，且动态表现与音质常不理想。
- **既有方法问题**：传统方法（如RNN/LSTM、VAE、GAN）在钢琴音色转换中存在局限：
  - RNN/LSTM虽能处理时序，但对长时依赖和复杂动态捕捉有限。
  - VAE在保留音色核心特征时，生成音质的保真度不足。
  - GAN虽能生成高保真音频，但训练不稳定，且常需配对数据。

2)  
论文提出一种基于扩散模型的音色转换系统，通过以下核心设计解决上述问题：
- **条件化特征提取**：
  - 使用**Pitch Encoder**（基于CREPE基频估计器）提取音高特征，并将其离散化为索引，再通过嵌入层转换为密集向量。
  - 使用**Loudness Encoder**按ITU-R标准分段计算响度，并通过向量量化（VQ）压缩为离散索引，同样进行嵌入表示。
  - 两者作为条件输入，确保生成时保持音高准确性和动态范围。
- **扩散模型解码器**：
  - 采用**U-Net**作为扩散模型的主干，保留自注意力机制，以捕捉时序依赖和关键音乐细节。
  - 将音高和响度的嵌入向量合并，输入U-Net的下采样层，作为生成条件，引导去噪过程。
  - 该设计避免了GAN的训练不稳定问题，并比VAE生成更高质量的音频。
- **整体流程**：输入音频→音高/响度特征提取→特征嵌入→作为条件输入扩散模型→逐步去噪生成钢琴音色波形。
- **优势**：扩散模型训练稳定；条件输入确保音高保真；离散化特征降低计算负担；U-Net结构保持时序一致性。

3)  
- **任务**：将多种乐器（如小提琴、长笛、贝斯）演奏的乐曲转换为钢琴音色版本。
- **效果**：
  - 在音高和音符时长为训练数据覆盖的范围内（如C4-B6，常见节奏型），生成音色在频谱和响度上与真实钢琴高度相似，音高准确。
  - 对古典、爵士、流行等不同风格和长度的音乐，转换结果稳定。
  - 局限：在训练数据覆盖不足的低音区（如C1-C3）或罕见节奏型（如三十二分音符）上，生成音质的能量分布和动态范围出现偏差。
</div>

</details>

---

## DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion
- **Authors**: Hanlin Zhang, Daxin Tan, Dehua Tao, Xiao Chen, Haochen Tan, Yunhe Li, Yuchen Cao, Jianping Wang, Linqi Song
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.09239v1](https://arxiv.org/abs/2601.09239v1)
- **PDF**: [https://arxiv.org/pdf/2601.09239v1](https://arxiv.org/pdf/2601.09239v1)

语音分词器是离散语音大语言模型（Speech LLMs）的核心基础。现有分词器或侧重于语义编码，或将语义内容与声学风格不可分割地融合，或仅实现不完整的语义-声学解耦。为实现更优的解耦效果，本文提出DSA-Tokenizer，通过差异化优化约束显式地将语音解耦为离散的语义与声学词元。具体而言，语义词元在自动语音识别监督下捕捉语言内容，声学词元则专注于梅尔频谱图重建以编码风格信息。为消除两序列间的刚性长度约束，我们引入分层流匹配解码器以进一步提升语音生成质量。此外，采用联合重建-重组训练策略强化这种分离机制。DSA-Tokenizer通过鲁棒解耦实现高保真重建与灵活重组，为语音大语言模型的可控生成提供支持。本研究表明解耦分词将成为未来语音建模的关键范式。音频示例详见https://anonymous.4open.science/w/DSA_Tokenizer_demo/，代码与模型将在论文录用后开源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：离散语音大语言模型依赖语音分词器。现有方法主要分为三类：
  - **语义分词器**：通过自监督或ASR监督优先编码语言信息，但丢弃了音色等关键声学细节。
  - **语义-声学混合分词器**：追求高保真重建，但语义与声学信息纠缠，无法独立控制。
  - **浅层解耦分词器**：尝试在混合架构上解耦，但分离不彻底，属性混淆。
- **核心问题**：现有方法在**跨语句的语义-声学重组任务**中表现不佳，无法在保持语义源内容的同时准确继承声学源风格；且普遍存在**严格的序列长度约束**，阻碍了不同长度语音间的重组。

2)  
DSA-Tokenizer通过一个**双流解耦架构**、**基于流匹配的分层融合解码器**和**联合训练策略**系统性地解决了上述问题。

- **严格解耦的双令牌流**：
  - **语义令牌**：使用预训练的HuBERT模型提取特征，并通过有限尺度量化离散化，同时使用**连接时序分类损失**进行ASR监督，强制其仅捕获语言内容（如音素），过滤掉音色、韵律等风格信息。
  - **声学令牌**：使用SEANet风格的编码器处理梅尔频谱图，并通过FSQ离散化。其训练与解码器端到端进行，通过**流匹配损失**优化，目标是重建被语义令牌过滤掉的所有声学细节。

- **基于流匹配的分层融合解码器**：
  - 该解码器采用**扩散变换器**架构，创新性地使用**混合条件注入策略**来融合双令牌流，打破了严格的长度对齐限制。
  - **语义注入（ControlNet风格）**：将语义令牌上采样对齐后，通过一个轻量级CNN适配器处理，并直接**加到扩散过程的噪声输入**中。这确保了生成语音在时间轴上严格遵循语义内容。
  - **声学注入（交叉注意力）**：将声学令牌上采样对齐后，作为**键和值**，与经过自注意力增强的语义特征进行**交叉注意力计算**。这种方式灵活，允许模型捕获全局声学风格和细粒度细节，不受长度约束。

- **联合重建-重组训练策略**：
  - 每个训练批次以50%的概率随机采用两种模式之一，以强制属性分离并消除长度依赖：
    - **自重建模式**：使用完整的语义和声学令牌，学习重建整个梅尔频谱图。
    - **重组（上下文修复）模式**：随机掩码部分时间段的梅尔频谱图，解码器仅根据**前缀声学令牌**和**完整语义令牌**来预测被掩码区域的流匹配速度场。这模拟了重组任务，迫使模型从部分声学上下文中推断全局风格，同时严格遵守语义指导。

- **辅助损失函数**：
  - 除了核心的流匹配损失，还引入了**说话人一致性损失**，使用说话人验证任务微调的WavLM模型提取说话人嵌入，并约束聚合后的声学令牌嵌入与之对齐，以增强声学令牌对说话人音色的编码能力。

3)  
DSA-Tokenizer在多个任务上取得了显著优于基线模型的效果：
- **语音重建与重组**：在**跨语句重组任务**中表现突出，在英语和中文上均取得了最高的语音自然度（UTMOS）和说话人相似度（SIM），同时保持了最低的词错误率/字错误率，实现了语义保真与风格继承的优异平衡。在标准重建任务中也保持了竞争力。
- **解耦探测**：通过专门的ASR和说话人分类实验证明，其语义令牌几乎只包含语言信息（WER极低），声学令牌则主要编码说话人特征，**信息泄漏极少**，实现了严格的解耦。
- **基于LLM的语音克隆**：与语音大语言模型集成进行语音克隆时，在自然度、内容一致性和音色相似度上均优于对比基线（如SAC和WavTokenizer），证明了其解耦设计能有效提升LLM在声学相关任务中的性能和稳定性。
</div>

</details>

---
