---
layout: post
title: "arXiv Daily – 2025-12-04"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-04（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-03 08:50 — 2025-12-04 08:50
- 抓取总数：6 篇 | 本页显示：6 篇（去重/过滤后）

## Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning
- **Authors**: Dongchao Yang, Songxiang Liu, Disong Wang, Yuanyuan Wang, Guanglu Wan, Helen Meng
- **Categories**: cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.03783v1](https://arxiv.org/abs/2512.03783v1)
- **PDF**: [https://arxiv.org/pdf/2512.03783v1](https://arxiv.org/pdf/2512.03783v1)

近年来，Omni模型的发展已实现统一的多模态感知与生成。然而，现有系统大多仍表现出僵化的推理行为，要么对简单问题过度思考，要么在需要推理时无法有效进行。为突破这一局限，本文提出Omni-AutoThink——一种新型自适应推理框架，能够根据任务难度动态调整模型的推理深度。该框架包含两个阶段：（1）自适应监督微调阶段，通过大规模推理增强数据赋予Omni模型基础推理能力；（2）自适应强化学习阶段，基于任务复杂度与奖励反馈优化推理行为。我们进一步构建了涵盖纯文本、文本-音频、文本-视觉及文本-音频-视觉模态的综合自适应推理基准，提供用于多模态推理评估的训练与测试数据划分。实验结果表明，相较于现有基线方法，本框架显著提升了自适应推理性能。全部基准数据与代码将公开释放。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：全模态模型虽能统一感知与生成，但其推理行为通常僵化：要么对所有问题都进行推理（导致简单问题计算冗余），要么从不推理（导致复杂问题性能不佳）。  
- **既有方法问题**：现有自适应推理方法存在局限，包括依赖人工配置、使用手工数据或复杂奖励函数，且大多仅支持单一模态（如纯文本、文本-音频或文本-视觉），缺乏统一的全模态自适应推理框架与评估基准。

2)  
论文提出 **Omni-AutoThink** 框架，通过两阶段训练实现自适应推理：  
- **自适应监督微调阶段**：  
  - 使用大规模、多模态的“粗粒度”推理增强数据（含推理轨迹的样本视为“思考模式”，否则为“非思考模式”）训练基础模型，赋予其基本推理能力。  
  - 结合“细粒度”高质量数据（通过多教师模型标注难度），进一步让模型区分何时思考。  
- **自适应强化学习阶段**：  
  - 基于 GRPO 设计 **Adaptive GRPO** 算法，仅依赖准确性奖励（公式9），无需复杂奖励函数。  
  - 引入**自适应采样策略**：对每个查询，强制策略模型同时采样“思考”与“非思考”两种轨迹，确保探索平衡。  
  - 引入**拒绝策略**：对简单问题（平均奖励高于阈值）随机掩码部分轨迹，使模型聚焦于困难问题的优化。  
- **关键创新**：通过两阶段联合优化，模型学会根据问题难度动态选择推理模式——简单问题直接回答，复杂问题先推理再回答，从而实现计算效率与性能的平衡。

3)  
- **任务与效果**：在构建的 **Omni Adaptive Reasoning Benchmark** 上评估，涵盖纯文本、文本-音频、文本-视觉、文本-音频-视觉四种模态，每个模态包含五个难度等级（L1最简单，L5最难）。  
- **性能表现**：  
  - **准确性**：在文本-音频、文本-视觉、文本-音频-视觉任务上，超越 Qwen2.5-Omni-7B 和规模更大的 Qwen3-Omni-30B；在纯文本任务上接近或略低于 Qwen3-Omni-30B（因参数量较小）。  
  - **自适应行为**：模型在困难问题（L3-L5）上思考率显著更高，在简单问题（L1-L2）上思考率较低，验证了其根据难度动态调整推理深度的能力。
</div>

</details>

---

## AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning
- **Authors**: Kohei Yamamoto, Kosuke Okusa
- **Categories**: cs.SD, cs.LG, stat.ML
- **arXiv**: [https://arxiv.org/abs/2512.03637v1](https://arxiv.org/abs/2512.03637v1)
- **PDF**: [https://arxiv.org/pdf/2512.03637v1](https://arxiv.org/pdf/2512.03637v1)

基于Transformer的音频自监督学习模型常将频谱图视为图像进行处理，采用卷积分块策略并实施大幅度的时域下采样。这种做法会降低有效奈奎斯特频率并引入混叠效应，而简单的低通滤波则会损失任务相关的高频信息。本研究提出混叠感知分块嵌入方法，作为一种即插即用的分块主干网络，能够在抑制混叠的同时保留高频信息。该方法通过双边指数窗的带限复正弦核生成特征，动态针对易混叠频段，对标准分块标记进行增强。核函数的频率与衰减参数根据输入动态估计，实现并行自适应的子带分析，其输出与标准分块标记进行融合。该模块可无缝集成到掩码师生自监督学习框架中。此外，我们结合多掩码策略与对比学习目标，通过强制不同掩码模式间的一致性来提升训练稳定性。在AudioSet数据集上进行预训练后，通过在环境音等常见音频领域的多样化下游基准任务中进行微调评估。该方法在部分任务中取得了最先进的性能，其余任务中也表现出竞争力。补充的线性探测评估呈现出相同趋势，在多个基准任务中取得显著提升，其余任务亦保持强劲性能。综合分析表明，该方法能有效缓解混叠效应的影响，同时避免丢弃信息丰富的高频内容。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于Transformer的音频自监督学习模型通常将频谱图视为图像，使用卷积进行分块嵌入，并伴随大幅度的时域下采样。这会降低有效奈奎斯特频率，引入**混叠失真**。
- **既有方法问题**：  
  - 混叠会破坏学习到的表征，导致输出对微小的输入相位变化敏感，影响训练稳定性和样本效率。  
  - 简单的低通抗混叠滤波器会直接滤除高频成分，但其中可能包含对特定任务至关重要的信息，造成信息丢失。

2)  
论文提出了**混叠感知分块嵌入（AaPE）**，其核心是通过**结构化双边拉普拉斯单元（SBLU）** 动态提取并融合混叠相关特征，以解决上述问题。具体方法如下：

- **动态子带频率分析**：
  - AaPE使用一个**带限复正弦核**，并采用**双侧指数窗**进行调制，以稳定地提取易发生混叠频带的特征。
  - 核的频率和衰减参数通过一个轻量级Transformer（Lambda编码器）从输入中动态估计，实现了**自适应的、并行的子带分析**。

- **特征融合与抗混叠**：
  - 将SBLU提取的**混叠感知高频特征**与标准分块嵌入的令牌进行拼接和线性投影融合。
  - 这种方法**显式地补偿了混叠失真**，同时保留了信息丰富的高频内容，而非简单地将其滤除。

- **无缝集成与训练稳定**：
  - AaPE可作为即插即用模块，直接替换标准ViT的分块嵌入层。
  - 在自监督学习框架中，结合了**掩码师生建模**、**多掩码策略**以及**对比正则化损失**，以增强不同掩码模式间的一致性，从而稳定训练。

3)  
AaPE在多个下游音频任务上进行了微调和线性探测评估，取得了优异或具有竞争力的效果：
- **微调任务**：在AudioSet-20K（41.9% mAP）和ESC-50（97.5% 准确率）上取得了**最先进性能**；在AudioSet-2M（49.8% mAP）和Speech Commands V2（97.9% 准确率）上表现**极具竞争力**。
- **线性探测任务**：在UrbanSound8K（89.7% 准确率）和NSynth（80.5% 准确率）上取得了**最先进性能**；在CREMA-D（情感语音）上表现稍弱，表明其对周期性、准平稳音频信号（如环境声、乐器）的频率敏感特征提取更为有效。
</div>

</details>

---

## Head, posture, and full-body gestures in interactive communication
- **Authors**: Ľuboš Hládek, Bernhard U. Seeber
- **Categories**: cs.HC, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.03636v1](https://arxiv.org/abs/2512.03636v1)
- **PDF**: [https://arxiv.org/pdf/2512.03636v1](https://arxiv.org/pdf/2512.03636v1)

当面对面的交流因背景噪声或干扰性说话者而变得费力时，视觉线索的作用对沟通成功变得愈发重要。以往研究多单独考察头部或手部动作，本文则探究了在声学不利条件下全身动作的变化。我们假设，对话中背景噪声的增加会导致手部、头部、躯干和腿部等典型对话动作的频率上升。手部动作的增加应有助于说话者，而头部和躯干动作的增加则可能帮助听者。我们在虚拟声学环境中进行了一项自由双人对话实验，参与者为听力正常者（n=8）。对话动作通过新开发的典型对话行为标注系统进行描述，并对各类动作的频率进行了分析。此外，我们通过评估手语同步性来分析动作质量，基于交互耦合模型假设更高水平的背景噪声会导致同步性下降。实验结果表明，更高的噪声水平导致说话和倾听时手部动作复杂度增加、头部上下运动更明显，而倾听时的头部动作相对于说话时普遍减少，这与预期相反。同步性和峰值速度不受噪声影响，动作质量仅轻微变化。结果支持了以往关于动作频率的发现，但我们在语音-动作同步性变化方面仅找到有限证据。本研究揭示了全身的沟通模式，并说明了多模态适应沟通需求的过程。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：面对面的交流在嘈杂环境中变得费力，视觉线索（如手势、头部动作）对沟通成功至关重要。先前研究多关注头部或手部动作，但对整个身体在声学不利条件下的运动模式了解有限。
- **既有问题**：过去研究多基于结构化任务或受控语句，缺乏生态效度；且未全面考察全身运动（如躯干、腿部）在噪声环境中的适应性变化。

2)  
- **核心方法**：研究设计了一个自由对话实验，在虚拟声学环境中模拟不同噪声水平（无噪声、中、高），让正常听力参与者进行30分钟的非结构化对话。
- **全身运动分析**：开发了一套新的标注系统，对典型会话动作进行编码，涵盖：
  - **手部动作与姿势**：如复杂手势、手掌向内/外挥动等。
  - **头部动作**：如点头、摇头、上下运动等。
  - **躯干运动**：如转身、前倾/后倾等。
  - **腿部运动**：如原地踏步、交叉腿等。
- **手势-语音同步性分析**：通过分析手势峰值速度与语音基频峰值的时间差，检验噪声是否影响同步性。
- **解决思路**：通过高精度运动捕捉与虚拟现实技术，在生态效度更高的自由对话场景中，量化全身多模态行为，揭示噪声下沟通的适应性策略。

3)  
- **任务与效果**：在自由对话任务中，随着背景噪声增加：
  - **手势复杂性增加**：复杂手势显著增多，尤其在说话时。
  - **头部动作适应**：上下头部运动在说话时更明显，但听者头部动作相对减少。
  - **全身运动变化有限**：躯干前倾/后倾有增加趋势；腿部运动（如原地踏步）最常见，但不受噪声影响。
  - **同步性未显著改变**：手势-语音同步性和峰值速度未受噪声显著影响，但同步性变异性有增加趋势。
- **结论**：结果支持多模态适应性策略，噪声促使更复杂的手势以辅助沟通，但未破坏手势-语音耦合的基本同步性。
</div>

</details>

---

## State Space Models for Bioacoustics: A comparative Evaluation with Transformers
- **Authors**: Chengyu Tang, Sanjeev Baskiyar
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2512.03563v1](https://arxiv.org/abs/2512.03563v1)
- **PDF**: [https://arxiv.org/pdf/2512.03563v1](https://arxiv.org/pdf/2512.03563v1)

本研究评估了Mamba模型在生物声学领域的应用效果。我们首先通过自监督学习，基于大规模音频数据预训练了一个Mamba架构的音频大语言模型。随后在BEANS基准测试集上对BioMamba模型进行微调与评估，该数据集涵盖分类与检测等多样化生物声学任务，并将其性能与包括先进Transformer模型AVES在内的多个基线模型进行对比。实验结果表明，BioMamba在取得与AVES相当性能的同时，显著降低了显存消耗，展现了其在该领域的应用潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：生物声学领域面临标注数据稀缺的挑战，而传统Transformer模型（如AVES）虽能通过自监督预训练学习有效表征，但其自注意力机制导致二次计算复杂度，在处理长音频序列时计算资源需求高，限制了实际应用。  
- **既有方法问题**：尽管已有研究尝试优化注意力效率（如Reformer、Flash Attention），但未能从根本上克服自注意力的效率瓶颈，在资源受限的生物声学场景中实用性仍受限。

2)  
- **核心方法**：本文提出BioMamba模型，基于状态空间模型（SSM）的Mamba架构构建。其核心是通过线性复杂度的序列建模替代Transformer的二次复杂度自注意力，从而高效处理长音频序列。  
- **架构设计**：  
  - **特征提取器**：采用与wav2vec 2.0相同的CNN结构，将原始音频波形转换为帧序列特征。  
  - **编码器**：使用堆叠的Mamba层（基于Mamba2）对特征序列建模，通过选择性门控和卷积状态空间技术自适应捕获局部及长程依赖。  
- **训练策略**：  
  - **预训练**：采用类似HuBERT/AVES的自监督方法，对随机掩蔽的音频帧预测其聚类伪标签（首阶段基于MFCC特征，次阶段基于模型内部表征），使模型从未标注音频中学习通用表征。  
  - **微调**：在下游任务（分类与检测）中，为预训练模型添加任务特定的分类头，并通过监督学习进行优化，CNN特征提取器权重保持冻结。  
- **解决思路**：Mamba的线性复杂度显著降低了计算和内存开销，同时通过自监督预训练缓解数据标注不足问题，使模型在资源受限环境下仍能保持高性能。

3)  
- **任务与效果**：在BEANS基准测试的10个生物声学数据集（涵盖鸟类、哺乳动物等）上评估，任务包括物种/个体声音分类与事件检测。  
- **性能表现**：BioMamba在多数任务中达到与Transformer模型AVES相当的性能（例如在分类任务中准确率接近，在检测任务中mAP略优或持平），同时显著优于传统机器学习模型（如XGBoost、ResNet）。  
- **效率优势**：在内存效率对比中，BioMamba比AVES节省约40%的VRAM使用量，且内存增长随序列长度呈近线性趋势，验证了其在实际部署中的资源效率潜力。
</div>

</details>

---

## A Universal Harmonic Discriminator for High-quality GAN-based Vocoder
- **Authors**: Nan Xu, Zhaolong Huang, Xiao Zeng
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.03486v1](https://arxiv.org/abs/2512.03486v1)
- **PDF**: [https://arxiv.org/pdf/2512.03486v1](https://arxiv.org/pdf/2512.03486v1)

随着基于生成对抗网络（GAN）的声码器的发展，判别器作为其关键组成部分近年来得到了广泛研究。本文聚焦于改进基于时频表示的判别器。当前方法通常将短时傅里叶变换（STFT）谱图作为时频判别器的输入，但STFT谱图在不同频点具有相同的频率分辨率，这导致其性能受限，尤其在歌唱声音合成任务中表现欠佳。受此启发，我们提出一种通用谐波判别器，用于动态频率分辨率建模与谐波跟踪。具体而言，我们设计了包含可学习三角带通滤波器组的谐波滤波器，使每个频点具备灵活的带宽调节能力。此外，我们引入半谐波分量以增强低频段的细粒度谐波关系捕捉能力。在语音与歌唱数据集上的实验表明，该判别器在主客观评价指标上均展现出显著优势。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：GAN声码器中，基于时频表示的判别器是关键组件。现有方法主要依赖短时傅里叶变换（STFT）谱图作为输入。  
- **既有问题**：STFT谱图在不同频段具有相同的频率分辨率，这限制了声码器性能的提升，尤其在处理表现力丰富的歌声时，无法准确重建谐波结构。而具有动态频率分辨率的常数Q变换（CQT）表示则存在奇数谐波建模困难和时域不同步的问题。

2)  
论文提出了一种通用谐波判别器（UnivHD），其核心方法通过以下设计解决上述问题：  
- **可学习的谐波滤波器组**：  
  - 设计了一个基于三角带通滤波器的可学习谐波滤波器，其中每个频段的带宽是灵活可变的。  
  - 滤波器中心频率按谐波阶数（h）缩放，即 `h · fc`，从而构建完整的谐波空间，能够有效建模奇数谐波。  
  - 引入可学习参数γ，根据等效矩形带宽（ERB）公式动态调整带宽，实现低频段高频率分辨率（利于基频建模）和高频段宽带宽（利于快速变化的谐波跟踪）。  
- **避免时域不同步**：  
  - 以固定窗长的STFT表示作为输入，避免了CQT存在的时域不同步问题。  
- **增强低频建模**：  
  - 添加了半谐波（h=0.5）表示，以捕捉基频以下的精细谐波关系，提升低频段的能量建模能力。  
- **高效的网络架构**：  
  - 采用深度可分离卷积与普通卷积相结合的方式，分别建模语音的谐波内（intra-harmonic）和谐波间（inter-harmonic）信息，以充分利用谐波张量的三维结构。

3)  
- **任务**：在语音（LibriTTS, VCTK）和歌声（OpenSinger, M4Singer, Opencpop）数据集上，将UnivHD集成到HiFiGAN和iSTFTNET声码器中进行测试。  
- **效果**：  
  - 在主客观指标上均优于基线（原始声码器、MS-STFT和MS-SB-CQT判别器）。  
  - 显著提升了平均意见得分（MOS），在歌声任务上改善尤为明显。  
  - 可视化显示能更好地重建谐波结构并减少混叠伪影，同时能更准确地跟踪基频轮廓。
</div>

</details>

---

## A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses
- **Authors**: Maryam Maghsoudi, Mohsen Rezaeizadeh, Shihab Shamma
- **Categories**: eess.SP, cs.LG, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.03458v1](https://arxiv.org/abs/2512.03458v1)
- **PDF**: [https://arxiv.org/pdf/2512.03458v1](https://arxiv.org/pdf/2512.03458v1)

解码想象语音涉及复杂的神经过程，由于时间上的不确定性及想象响应数据集的稀缺性，其解读颇具挑战。本研究采集了训练有素的音乐家在想象与聆听音乐及诗歌刺激时的脑磁图数据，结果表明想象与感知的大脑响应均包含稳定且条件特异的信息。首先，我们采用滑动窗口岭回归模型在个体层面将想象响应映射至聆听响应，但发现该映射在跨被试时泛化能力有限。在群体层面，我们构建了一种编码器-解码器卷积神经网络，其中包含被试特异性校准层，该网络能够生成稳定且可泛化的映射。该卷积神经网络始终优于零模型，在几乎所有留出被试中，其预测聆听响应与真实响应之间的相关性均显著更高。我们的研究证明，想象神经活动可被转化为类感知响应，这为未来涉及想象语音与音乐的脑机接口应用奠定了基础。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：解码想象听觉内容（如想象音乐或言语）对脑机接口至关重要，但面临挑战。  
- **既有问题**：  
  - 想象神经信号存在时间不确定性（起始、时长、节奏多变），难以对齐。  
  - 现有方法多依赖感知数据（时间锁定、信噪比高），而想象数据信噪比低、个体差异大。  
  - 多数研究仅能对少量类别进行粗分类，无法重建连续的想象音频。  

2)  
- **核心方法**：提出一个卷积神经网络框架，包含编码器-解码器主干和轻量级个体校准层。  
- **解决思路**：  
  - **主干网络**：使用一维卷积层（核大小7）学习跨被试共享的时域映射，将想象响应转换为类似感知的响应。  
  - **校准层**：通过核大小为1的卷积对每个新被试进行空间通道重加权，适应个体差异（如头部位置、解剖结构）。  
  - **多损失函数**：结合均方误差、负Pearson相关性、时域差分和频谱损失，确保预测信号在幅度、相关性、平滑性和频域上与目标一致。  
- **优势**：  
  - 利用多被试数据学习通用时域表征，同时通过校准层灵活适应个体，解决了线性模型跨被试泛化差的问题。  
  - 无需大量标注想象数据，可将基于感知数据训练的语音解码模型应用于想象输入。  

3)  
- **任务**：将想象听觉MEG响应映射到对应的聆听响应，预测被试若实际听到声音时的神经活动。  
- **效果**：  
  - 在留一被试交叉验证中，CNN模型在多数未见被试上显著优于零模型（真实平均相关性0.0113 vs. 零模型0.0063）。  
  - 模型成功捕捉了旋律特异性结构，预测响应与同类聆听试次的相关系数显著高于异类。  
  - 为利用感知数据解码想象听觉内容提供了可行框架，支持未来脑机接口应用。
</div>

</details>

---
