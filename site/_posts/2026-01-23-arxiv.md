---
layout: post
title: "arXiv Daily – 2026-01-23"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-23（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-22 08:50 — 2026-01-23 08:50
- 抓取总数：13 篇 | 本页显示：13 篇（去重/过滤后）

## Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems
- **Authors**: Prakash Dhungana, Sayed Ahmad Salehi
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.16158v1](https://arxiv.org/abs/2601.16158v1)
- **PDF**: [https://arxiv.org/pdf/2601.16158v1](https://arxiv.org/pdf/2601.16158v1)

在资源受限的边缘设备上，部署轻量级模型的关键词检测系统常因噪声和录音条件变化导致的领域偏移而面临准确性与鲁棒性挑战。为此，我们提出一种面向持续学习的综合框架，旨在适应新领域的同时保持计算效率。该框架集成了双输入卷积神经网络，同时利用梅尔频率倒谱系数和梅尔频谱特征，并辅以多阶段去噪流程（包括离散小波变换和谱减法技术）以及模型与原型更新模块。与以往仅更新特定层的方法不同，得益于紧凑的模型架构，本方法可对完整量化模型进行更新。在运行时，通过类别原型和置信度驱动过滤选择部分输入样本，经伪标注后与回放缓冲区数据结合，用于增量模型重训练。在含噪声测试数据集上的实验结果表明，该框架在纯净数据上达到99.63%的准确率，并在多种噪声环境（即使在-10 dB信噪比下）中保持超过94%准确率的鲁棒性能。本研究证实，将高效去噪技术与基于原型的持续学习相结合，可使关键词检测模型在资源受限的动态环境中实现自主且鲁棒的运行。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.16158v1）
</div>

</details>

---

## Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization
- **Authors**: Maximos Kaliakatsos-Papakostas, Dimos Makris, Konstantinos Soiledis, Konstantinos-Theodoros Tsamis, Vassilis Katsouros, Emilios Cambouropoulos
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.16150v1](https://arxiv.org/abs/2601.16150v1)
- **PDF**: [https://arxiv.org/pdf/2601.16150v1](https://arxiv.org/pdf/2601.16150v1)

旋律和声化，即为一给定旋律生成和声伴奏的任务，在计算音乐生成领域仍是一个核心挑战。近期基于单编码器Transformer的方法将和声化构建为掩码序列建模问题，但受离散扩散启发的现有训练课程常导致旋律与和声间的（交叉）注意力较弱，从而限制了旋律线索的利用，尤其在域外情境中。本研究提出一种名为FF（全掩码到全解掩）的训练课程，该课程在训练初期保持所有和声标记被掩码若干步，随后在训练过程中逐步解掩整个序列，以强化旋律与和声的交互。我们通过多组实验轴系统评估了该方法与先前课程的对比，包括时间量化（四分音符与十六分音符）、小节级与时值条件化、旋律表示（全音域与音高类别）以及推理时的解掩策略。模型在HookTheory数据集上训练，并在域内及精选的爵士标准曲集上进行评估，采用了一套综合指标以评估和弦进行结构、和声-旋律对齐及节奏连贯性。结果表明，所提出的FF课程在几乎所有指标上均稳定优于基线方法，在域外评估中提升尤为显著，其中对新旋律线索的和声适应能力至关重要。我们还发现，在FF设置下，四分音符量化、小节标记的交织处理以及音高类别的旋律表示更具优势。本研究结果凸显了训练课程在实现有效旋律条件化中的重要性，并表明全掩码到全解掩策略为单编码器和声化提供了一种稳健的解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：旋律和声化是为给定旋律生成和声伴奏的任务，是计算音乐生成的核心挑战。  
- **既有方法的问题**：近期基于单编码器Transformer的方法将和声化视为掩码序列建模问题，但受离散扩散启发的现有训练课程（如Midpoint Doubling和Random 10%）导致旋律与和声之间的“交叉注意力”较弱。模型过度依赖和声内部的“自注意力”，未能充分利用旋律线索，尤其在域外（out-of-domain）场景下表现不佳。

2)  
- **核心方法**：论文提出名为“全掩码到全可见”（Full-to-Full, FF）的训练课程，旨在强化旋律与和声之间的交互。  
- **具体设计**：  
  - **初始阶段**：在训练早期，将所有和声令牌完全掩码，迫使模型仅依赖旋律信息建立交叉注意力路径。  
  - **渐进揭示**：随着训练步数增加，按公式逐步揭示和声令牌（从完全掩码过渡到仅保留一个掩码令牌），使模型逐渐学习结合旋律上下文与部分可见的和声信息。  
  - **训练目标**：通过这种课程设计，模型在早期强制关注旋律，后期逐步引入和声自注意力，从而平衡交叉注意力与自注意力的学习。  
- **辅助优化**：  
  - **时间量化**：采用四分音符（而非十六分音符）分辨率，以简化序列并提升效果。  
  - **结构编码**：将小节信息直接嵌入旋律与和声表示（如添加`<bar>`令牌），而非单独提供拍号条件。  
  - **旋律表示**：仅使用音级（pitch-class）钢琴卷帘表示，去除音高范围信息，以聚焦和声关联。  
- **推理灵活性**：FF课程可与多种高效推理策略（如uMD、uR10%）兼容，保持生成质量的同时减少模型调用次数。

3)  
- **任务与效果**：在HookTheory数据集（域内）和爵士标准曲数据集（域外）上评估。  
- **主要成果**：  
  - FF课程在几乎所有评估指标上均优于基线方法（MD和R10%），尤其在域外评估中提升显著。  
  - 具体优势体现在和弦进行结构（如Chord Histogram Entropy）、旋律-和声对齐（如Chord Tone to non-Chord Tone Ratio）及节奏连贯性（如Harmonic Rhythm Histogram Entropy）等方面。  
  - 结合四分音符量化、小节信息嵌入和音级旋律表示时，FF模型表现最佳。
</div>

</details>

---

## Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks
- **Authors**: Abdul Hannan, Daniele Falavigna, Shah Nawaz, Mubashir Noman, Markus Schedl, Alessio Brutti
- **Categories**: cs.SD, cs.CV
- **arXiv**: [https://arxiv.org/abs/2601.16117v1](https://arxiv.org/abs/2601.16117v1)
- **PDF**: [https://arxiv.org/pdf/2601.16117v1](https://arxiv.org/pdf/2601.16117v1)

边缘设备在资源受限且多变的条件下运行，需要能够适应可用资源限制的动态架构。为满足此类需求，通常采用层丢弃（$\mathcal{LD}$）方法，通过跳过部分网络并降低整体计算复杂度，将静态模型转换为动态模型。然而，现有的$\mathcal{LD}$方法在高低丢弃率场景下会显著影响动态模型的性能，导致性能与计算效率的权衡失衡。为此，我们提出一种基于蒸馏的层丢弃（DLD）框架，以端到端方式有效结合知识蒸馏与$\mathcal{LD}$的能力，从而为动态语音网络实现最先进的性能。通过在三个公开基准测试中使用包括Conformer和WavLM在内的知名语音识别方法进行综合实验，验证了本框架的有效性：在高丢弃率和零丢弃率场景下，词错误率分别降低$9.32\%$和$2.25\%$，同时训练时间减少$33.3\%$。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：边缘设备资源受限且多变，需要能动态调整计算量的语音网络。层丢弃（LD）是常用方法，通过跳过部分网络层来降低计算复杂度。
- **既有方法的问题**：现有LD方法（如随机丢弃）在丢弃层数较多或较少时，会导致模型性能显著下降，破坏了性能与计算效率之间的平衡。同时，这些方法难以充分利用完整模型的计算能力。

2)  
论文提出了**基于蒸馏的层丢弃（DLD）框架**，以端到端方式结合知识蒸馏（KD）与随机层丢弃（RD），核心解决思路如下：
- **框架设计**：
    - 使用一个冻结的**参考模型**（教师模型）和可动态执行的**学生模型**。
    - 学生模型编码器配备门控机制，训练时按伯努利分布随机跳过某些层；推理时可固定执行层数以适配不同资源。
- **知识迁移**：
    - 通过**KL散度损失**对齐学生模型与教师模型潜在嵌入的分布，使学生模型各“子网络”都能获得教师模型的丰富语义知识。
    - 结合**CTC损失**进行语音识别任务监督，共同优化。
- **优势**：
    - 解决了传统LD在低丢弃（或零丢弃）时性能下降的问题，因为蒸馏确保了学生模型即使使用全部层也能逼近教师性能。
    - 在高丢弃情况下，通过知识迁移保留了更强表征能力，改善了性能-计算权衡。
    - 训练效率高，所需epoch数少于传统LD方法，收敛更快。

3)  
在**自动语音识别（ASR）任务**上，使用Conformer和WavLM架构在LibriSpeech和TED-LIUM v3数据集上评估：
- **性能提升**：相比随机丢弃基线，在零丢弃情况下词错误率降低2.25%（Conformer）和4.83%（WavLM）；在高丢弃情况下也有显著改善。
- **计算效率**：在取得更好词错误率的同时，训练时间减少约33.3%，推理时可根据资源灵活调整层数，实现计算加速。
- **泛化能力**：在口语理解任务（FSC数据集）上也显示出性能提升，表明框架不限于ASR。
</div>

</details>

---

## Loose coupling of spectral and spatial models for multi-channel diarization and enhancement of meetings in dynamic environments
- **Authors**: Adrian Meise, Tobias Cord-Landwehr, Christoph Boeddeker, Marc Delcroix, Tomohiro Nakatani, Reinhold Haeb-Umbach
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.16077v1](https://arxiv.org/abs/2601.16077v1)
- **PDF**: [https://arxiv.org/pdf/2601.16077v1](https://arxiv.org/pdf/2601.16077v1)

利用麦克风阵列进行声音采集，不仅可利用频谱信息，还能结合空间信息，实现会议转录中的两项关键任务——说话人日志和信号增强。然而，当说话人移动时，空间位置与说话人之间并不存在一一对应关系。为此，本文提出一种新颖的联合空间与频谱混合模型，该模型通过概率化建模说话人与位置索引之间的关系，实现了两个子模型间的松散耦合。这一设计使得空间与频谱信息能够被共同利用，同时允许说话人在不同位置发言。在模拟说话人位置变化的LibriCSS数据集上的实验表明，该模型相较于紧密耦合的子系统取得了显著改进。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：会议转录需完成说话人日志化和语音增强任务。传统方法利用麦克风阵列的空间信息辅助处理，但面临挑战。
- **既有方法问题**：
  - 现有联合模型（如紧密耦合模型）假设说话人与空间位置存在**一一对应关系**，即一个说话人固定在一个位置。
  - 当说话人移动或在不同位置发言时，该假设失效，导致模型无法正确跟踪和分离语音。

2)  
- **核心方法**：提出一种**松散耦合的谱-空间混合模型**，通过概率依赖关系连接谱模型（vMFMM）和空间模型（cACGMM），取代原有的紧密耦合。
- **关键设计**：
  - **引入条件概率**：定义空间混合成分在给定谱混合成分下的条件概率（耦合权重 \(a_{klf}\)），允许**多个空间成分映射到同一个谱成分**（即同一说话人可对应多个位置）。
  - **分离潜变量**：谱模型使用时间层面的潜变量 \(z_{kt}^{\text{vM}}\) 表示说话人活动；空间模型使用时频层面的潜变量 \(z_{ltf}^{\text{cAC}}\) 表示位置活动。两者通过概率关系关联，而非共享同一潜变量。
  - **模型训练**：采用EM算法迭代估计模型参数（包括耦合权重），无需额外训练，可适配任意麦克风阵列配置。
- **掩模估计改进**：
  - 松散耦合模型不能直接输出说话人特定的时频掩模，因此提出启发式方法：通过计算位置到说话人的映射权重 \(\beta_{klf}\)，并结合联合后验概率，生成频率选择性的掩模用于波束成形。
- **优势**：
  - 打破“说话人-位置”一一对应假设，**支持说话人移动**。
  - 谱模型负责判断说话人是否活跃，空间模型可灵活处理不同频率下的活动状态，提升对动态环境的适应性。

3)  
- **任务与数据**：在LibriCSS数据集上进行评估，任务包括**说话人日志化、语音分离及增强**，并模拟了说话人位置变动（旋转麦克风通道模拟120°移动）。
- **效果**：
  - **静态场景**：松散耦合模型在长片段（平均75秒）上优于紧密耦合模型，cpWER绝对提升1.1%-1.7%。
  - **动态场景（说话人移动）**：松散耦合模型显著优于紧密耦合模型，cpWER绝对提升约14.9%（Oracle初始化）和14.6%（非Oracle初始化），证明其对位置变化具有强鲁棒性。
  - **局限性**：在高重叠语音（OV30、OV40）场景下性能仍会下降，且对谱初始化质量较为敏感。
</div>

</details>

---

## Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs
- **Authors**: Lalaram Arya, Mrinmoy Bhattacharjee, Adarsh C. R., S. R. Mahadeva Prasanna
- **Categories**: eess.AS, cs.HC
- **arXiv**: [https://arxiv.org/abs/2601.16023v1](https://arxiv.org/abs/2601.16023v1)
- **PDF**: [https://arxiv.org/pdf/2601.16023v1](https://arxiv.org/pdf/2601.16023v1)

直接语音到语音翻译（S2ST）因其能够将一种语言的语音翻译为另一种语言，同时减少传统级联流程中固有的错误传播和延迟，正受到越来越多的关注。然而，现有的直接S2ST系统仍面临显著挑战，包括在平行语音数据稀缺时语义-声学对齐的不稳定性、说话人身份保持困难以及多语言扩展能力有限。本文提出DS2ST-LM，一种基于多语言大语言模型（LLM）的可扩展单阶段直接S2ST框架。该架构集成了Whisper语音编码器、可学习的投影模块、Qwen2-0.5B LLM以及音色可控声码器。我们通过扩展GigaST数据集并引入高保真合成目标语音，构建了千小时级别的双语语料库GigaS2S-1000，实验表明该合成数据在一定程度上缓解了数据稀缺问题。我们研究了两种语义标记生成策略：语音衍生的S3标记与预训练LLM生成的文本衍生标记，并分析了它们对训练稳定性和语义一致性的影响。进一步评估了三种投影架构（线性、Conv1D-线性与Q-Former），发现虽然高容量投影器收敛更快，但简单的线性投影器能达到更高性能。大量实验表明，DS2ST-LM在词汇指标（BLEU、METEOR）和语义指标（BLEURT、COMET）上均优于传统级联及ST（Qwen-Audio）+ TTS基线系统，并可扩展至包括法语、西班牙语、德语、印地语、孟加拉语和乌尔都语在内的多语言对。此外，我们引入音色感知语音合成以保持说话人信息，使DS2ST-LM在说话人相似度和感知自然度上均超越现有直接S2ST系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：直接语音到语音翻译旨在减少传统级联系统（ASR+MT+TTS）的错误传播与延迟，但面临挑战。  
- **既有问题**：  
  - 平行语音数据稀缺导致语义-声学对齐不稳定。  
  - 难以保持说话人音色（身份）。  
  - 多语言扩展能力有限。  

2)  
论文提出 **DS2ST-LM**，一个基于大语言模型的单阶段直接语音到语音翻译框架，通过以下方式解决上述问题：  
- **架构设计**：  
  - 使用 **Whisper 语音编码器**提取声学特征。  
  - 通过**可学习的投影模块**将特征映射到 LLM 嵌入空间。  
  - 采用 **Qwen2–0.5B LLM** 进行跨语言语义映射，并行生成目标语义令牌和文本。  
  - 结合**音色可控声码器**，根据参考语音提示合成保留说话人音色的目标语音。  
- **数据增强**：  
  - 构建 **GigaS2S-1000** 数据集（1000小时），通过 XTTS-v2 合成高质量目标语音，缓解数据稀缺问题。  
- **关键技术探索**：  
  - **语义令牌生成**：比较从语音提取的 S3 令牌与从文本生成的 LLM 令牌，发现语音令牌性能更优，但文本令牌为无目标语音数据时提供可行替代。  
  - **投影模块比较**：评估线性、Conv1D–线性、Q-Former 三种设计，发现线性投影在保持时序对齐和泛化性能上最佳。  
- **音色保持**：  
  - 通过显式音色控制机制，将说话人嵌入与语义令牌结合，合成时保留指定说话人音色。  

3)  
- **任务与效果**：  
  - **多语言语音翻译**：在包括法语、西班牙语、德语、印地语、孟加拉语、乌尔都语等多种语言对上，DS2ST-LM 在词汇指标（BLEU、METEOR）和语义指标（BLEURT、COMET）上均优于传统级联系统和 ST + TTS 基线。  
  - **音色保持与语音质量**：在说话人相似性（SIM）和感知自然度（DNSMOS）上超越现有直接 S2ST 系统（如 Translatotron2），接近真实语音质量。  
  - **扩展性**：框架可扩展至多个语言对，尤其在资源有限的语言上表现出竞争力。
</div>

</details>

---

## A Stabilized Hybrid Active Noise Control Algorithm of GFANC and FxNLMS with Online Clustering
- **Authors**: Zhengding Luo, Haozhe Ma, Boxiang Wang, Ziyi Yang, Dongyuan Shi, Woon-Seng Gan
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.15889v1](https://arxiv.org/abs/2601.15889v1)
- **PDF**: [https://arxiv.org/pdf/2601.15889v1](https://arxiv.org/pdf/2601.15889v1)

滤波-x归一化最小均方（FxNLMS）算法在充分自适应后能够实现较低的稳态误差，但其收敛速度较慢，且存在发散风险。相比之下，生成式固定滤波器主动噪声控制（GFANC）方法响应速度快，但因其缺乏自适应性，可能导致较大的稳态误差。本文提出一种混合GFANC-FxNLMS算法，以结合两种方法的互补优势。在该混合算法中，GFANC提供帧级控制滤波器作为FxNLMS的初始化条件，而FxNLMS则以采样率进行连续自适应。若GFANC生成的滤波器存在微小波动，可能反复重新初始化FxNLMS，从而中断其自适应过程并导致系统失稳。为此，本文引入在线聚类模块，以避免不必要的重新初始化并提升系统稳定性。仿真结果表明，所提算法响应迅速、稳态误差极低且稳定性高，仅需一个预训练的宽带滤波器即可实现。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：主动噪声控制（ANC）是应对低频噪声的有效技术。传统自适应算法（如FxNLMS）虽能达到较低的稳态误差，但存在收敛慢、易发散的问题。  
- **既有方法的问题**：  
  - FxNLMS算法：收敛速度慢，且对实际系统缺陷敏感，可能导致不稳定或发散。  
  - 生成式固定滤波器ANC（GFANC）方法：响应速度快，但缺乏自适应性，可能导致较大的稳态误差。  
  - 选择性固定滤波器ANC（SFANC）方法：依赖一组预训练滤波器，当遇到与训练集差异较大的噪声时性能下降。  

2)  
- **核心方法**：提出一种结合GFANC与FxNLMS的混合算法，并引入在线聚类模块以提升稳定性。  
- **具体解决机制**：  
  - **双速率架构**：  
    - 在帧速率层面，CNN根据输入噪声帧预测权重向量，用于组合子控制滤波器以生成初始控制滤波器。  
    - 在采样速率层面，FxNLMS算法利用反馈误差信号对生成的滤波器进行连续自适应优化。  
  - **在线聚类模块**：  
    - 目的：避免因权重向量的微小波动导致FxNLMS被不必要地重新初始化，从而破坏其自适应过程并引发系统不稳定。  
    - 操作：动态地对CNN预测的权重向量进行聚类（使用欧氏距离和阈值τ）。仅当新权重向量被分配到与当前不同的聚类时，才更新用于生成滤波器的权重向量。  
    - 效果：允许容忍微小波动，同时捕捉显著变化，确保GFANC与FxNLMS之间的滤波器更新协调一致，增强系统稳定性。  
  - **优势整合**：  
    - GFANC提供快速响应和良好的滤波器初始化，克服了FxNLMS收敛慢的问题。  
    - FxNLMS的持续自适应优化降低了稳态误差，弥补了GFANC缺乏适应性的不足。  
    - 仅需一个预训练的宽带滤波器，降低了训练复杂性和资源需求。  

3)  
- **任务与效果**：在主动噪声控制任务中，通过数值仿真验证了所提算法的性能。  
  - **噪声类型**：测试包括车辆噪声、飞机噪声以及多种合成宽带噪声（如20-2000 Hz、100-1200 Hz噪声）。  
  - **效果**：  
    - 与GFANC、FxNLMS、SFANC及SFANC-FxNLMS等方法相比，所提算法实现了更优的综合性能。  
    - 具备快速响应速度（受益于GFANC的初始化）。  
    - 达到更低的稳态误差（受益于FxNLMS的持续优化）。  
    - 在线聚类模块有效提升了系统稳定性，避免了性能波动。
</div>

</details>

---

## PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation
- **Authors**: Jaekwon Im, Natalia Polouliakh, Taketo Akama
- **Categories**: cs.SD, cs.CV, cs.LG, cs.MM, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15872v1](https://arxiv.org/abs/2601.15872v1)
- **PDF**: [https://arxiv.org/pdf/2601.15872v1](https://arxiv.org/pdf/2601.15872v1)

舞蹈配乐生成旨在创作与舞蹈动作相匹配的音乐。现有方法通常依赖于从单一舞者提取的身体运动特征以及有限的舞蹈-音乐数据集，这限制了其在涉及多舞者及非人类舞者的现实场景中的性能与适用性。本文提出PF-D2M，一种基于扩散模型的通用舞蹈配乐生成方法，其通过提取舞蹈视频中的视觉特征进行建模。该模型采用渐进式训练策略，有效应对数据稀缺与泛化挑战。主客观评估均表明，PF-D2M在舞蹈-音乐对齐度与音乐质量方面达到了当前最优性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：舞蹈到音乐生成旨在根据舞蹈动作生成同步的音乐，支持编舞和数字内容创作。现有方法主要依赖从**单个舞者**提取的运动特征（如SMPL模型或2D关键点），或生成符号音乐表示（如MIDI），限制了在真实场景中的适用性。
- **既有问题**：
  - **特征局限性**：基于单舞者姿态的方法无法有效处理**多舞者**或**非人类舞者**（如动画角色）的场景，且姿态估计可能产生抖动序列。
  - **数据稀缺**：高质量舞蹈-音乐数据集（如AIST++）规模小、背景简单，导致模型过拟合、泛化能力差，且难以通过爬取扩展数据。

2)  
论文提出PF-D2M，一种基于扩散模型的通用舞蹈到音乐生成方法，通过**架构创新**和**训练策略**解决上述问题：
- **模型架构**：
  - 采用DiT（Diffusion Transformer）架构，并利用预训练的VAE进行音频压缩。
  - **视觉特征提取**：使用Synchformer从舞蹈视频中提取视觉特征，替代传统的单舞者姿态特征。这些特征通过上采样和投影后，以**拼接**和**自适应层归一化（AdaLN）调制**两种方式注入DiT，增强时空对齐能力。
  - **多条件输入**：结合文本描述（通过T5编码器）、扩散时间步嵌入和视觉特征，生成高质量音乐。
- **渐进式训练策略**：
  - **阶段0**：使用文本到音频模型（Stable Audio Open）初始化权重，保留其音频生成能力。
  - **阶段1**：在多样化视频数据集（VGGSound）上进行视频到音频对齐训练，学习跨场景的视听同步，缓解数据稀缺导致的过拟合。
  - **阶段2**：在舞蹈到音乐数据集（AIST++）、文本到音乐数据集（FMA、MoisesDB）和视频到音频数据集上进行混合微调。通过**按比例采样批次**（2:4:1）和**随机化文本提示生成**，提升音乐连贯性、结构一致性，并减少对舞蹈者所处环境声的依赖。

3)  
- **评估任务**：
  - 在**AIST++基准测试**上评估节奏对齐（BCS、CSD、BHS、HSD、F1分数）。
  - 在**野外收集数据集**上评估，涵盖单/多人类舞者、单/多非人类舞者四类场景。
- **效果**：
  - **客观评估**：PF-D2M在多数节奏对齐指标上达到SOTA（如F1分数94.3），仅BCS略低于基线，因生成更细粒度的节奏结构。
  - **主观评估**：在舞蹈-音乐对齐和音乐质量上显著优于基线（LORIS、Text-Inv），尤其在多舞者、非人类舞者及复杂视觉条件下表现鲁棒，生成音乐更具表现力和动态性。
</div>

</details>

---

## U3-xi: Pushing the Boundaries of Speaker Recognition via Incorporating Uncertainty
- **Authors**: Junjie Li, Kong Aik Lee
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.15719v1](https://arxiv.org/abs/2601.15719v1)
- **PDF**: [https://arxiv.org/pdf/2601.15719v1](https://arxiv.org/pdf/2601.15719v1)

在自动说话人验证系统中，话语级说话人嵌入通常通过对一系列帧级表征进行聚合得到。然而，在实际场景中，单个帧不仅编码了说话人相关信息，还包含多种干扰因素，导致不同帧对话语级说话人表征的贡献不均。为解决这一问题，我们提出通过估计每帧的内在不确定性并分配自适应权重，使不确定性较高的帧获得较低关注度。基于此思路，我们提出了U3-xi框架，旨在为说话人嵌入生成更可靠且可解释的不确定性估计。具体而言，我们引入了多种不确定性监督策略：首先，通过随机方差损失实现说话人级不确定性监督，将话语嵌入与其对应说话人中心点之间的距离作为不确定性学习的伪真值；其次，通过训练时将预测的不确定性注入softmax尺度，实现全局级不确定性监督，该自适应缩放机制能根据样本难度调整决策边界锐度，提供全局指导；第三，我们重构了不确定性估计模块，将Transformer编码器与多视角自注意力结合，使模型能够捕捉丰富的局部及长程时序依赖。综合实验表明，U3-xi具有模型无关性，可无缝应用于多种说话人编码器。特别地，当应用于ECAPA-TDNN时，在VoxCeleb1测试集上实现了等错误率相对降低21.1%、最小检测代价相对降低15.57%的性能提升。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动说话人验证系统通过聚合帧级表示来获取说话人嵌入。然而，实际语音帧不仅包含说话人信息，还混杂了语言内容、噪声等干扰因素，导致不同帧对最终嵌入的贡献不均。
- **既有方法问题**：现有方法（如xi-vector）的**不确定性估计模块仅由浅层线性层构成**，建模能力有限；且**训练仅依赖交叉熵损失**，缺乏对不确定性的显式监督，导致估计不可靠、鲁棒性不足。

2)  
论文提出U3-xi框架，通过以下三方面改进解决上述问题：
- **说话人级不确定性监督**：  
  - 提出**随机方差损失**，以说话人嵌入与其类中心距离作为伪真值，显式监督不确定性学习。  
  - 在余弦评分中引入不确定性，形成**不确定性感知评分机制**，根据估计方差调整帧权重。
- **全局级不确定性监督**：  
  - 将预测的不确定性注入训练中的softmax尺度参数，实现**自适应缩放**：困难样本对应较小尺度（平滑决策边界），简单样本对应较大尺度（锐化边界），从而在全局层面约束不确定性。
- **不确定性估计模块增强**：  
  - 用**Transformer编码器**替换浅层线性层，通过**多视角自注意力**同时捕获局部与长程时序依赖，提升非线性关系建模能力，得到更稳健的方差估计。

3)  
- **任务与效果**：在**VoxCeleb1**测试集上，将U3-xi应用于ECAPA-TDNN编码器，相比基线在**等错误率上相对提升21.1%**，**最小检测代价上相对提升15.57%**。  
- **模型无关性**：框架可无缝集成至不同编码器（如ResNet34、ReDimNet-B2），在**跨域数据集**（如SITW、CNCeleb）上亦能提升EER，但minDCF在部分跨域场景仍有改进空间。
</div>

</details>

---

## Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems
- **Authors**: Hengfan Zhang, Yueqian Lin, Hai Helen Li, Yiran Chen
- **Categories**: cs.SD, cs.LG, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15676v1](https://arxiv.org/abs/2601.15676v1)
- **PDF**: [https://arxiv.org/pdf/2601.15676v1](https://arxiv.org/pdf/2601.15676v1)

在边缘基础设施上部署音频语言模型时，感知深度与计算效率之间存在持续的矛盾。轻量级本地模型往往产生被动感知——即生成通用摘要而忽略多步音频推理所需的细微证据，而无差别地将任务卸载至云端则会导致不可接受的延迟、带宽成本和隐私风险。本文提出CoFi-Agent（工具增强的粗粒度到细粒度智能体），一种面向边缘服务器与网关的混合架构。该架构首先执行快速的本地感知，仅在检测到不确定性时触发条件式精细分析。CoFi-Agent首先在本地7B参数音频语言模型上进行单次前向推理，随后由云端控制器对疑难案例进行筛选，并生成轻量级执行计划以调用设备端工具（如时序重听与本地自动语音识别）。在MMAR基准测试中，CoFi-Agent将准确率从27.20%提升至53.60%，同时相比持续运行的精细分析流程实现了更优的准确率-效率平衡。总体而言，CoFi-Agent通过工具增强的条件式边缘-云端协同机制，在实际系统约束下有效弥合了感知鸿沟。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在边缘设备上部署音频-语言模型面临“感知深度”与“计算效率”的固有矛盾。  
- **既有方法问题**：  
  - **轻量本地模型**：采用单次推理范式，易产生“被动感知”，即生成通用摘要，缺乏对细微证据的捕捉，难以支持多步音频推理。  
  - **无条件云卸载**：始终将计算任务卸载至云端，导致不可接受的延迟、带宽开销和隐私风险。  
  - **静态压缩技术**：如量化和剪枝，虽能压缩模型，但对所有输入均一处理，造成计算资源浪费。

2)  
论文提出 **CoFi-Agent**，一种面向边缘服务器的混合架构，通过“本地优先、条件细化”的粗到精流程解决上述问题。  

- **核心方法流程**：  
  - **阶段0：边缘粗感知**：在本地使用轻量级7B音频-语言模型进行单次推理，生成初始答案和摘要。  
  - **自适应置信门控**：云端控制器评估初始响应的模糊性和自洽性，仅对不确定样本（约62%）触发细化路径。  
  - **阶段1：云端引导的细化规划**：云端基于接收的紧凑元数据（如时间段标记）生成轻量级调查计划，指导边缘工具执行。  
  - **阶段2：本地工具执行**：边缘设备按计划执行两种本地工具：  
    - **时序重听**：对关键音频片段进行定向推理。  
    - **本地自动语音识别**：提取文本转录，用于语义解析。  
  - **证据集成与最终裁决**：云端推理器整合所有证据生成最终答案。  

- **关键设计优势**：  
  - **条件性触发**：避免对简单样本进行不必要的工具调用，减少平均延迟和工具噪声。  
  - **数据最小化**：原始音频始终保留在本地，仅向云端传输紧凑的符号化证据（如文本转录），保护声学隐私。  
  - **动态推理**：实现“语义级联”，对多数简单样本快速响应，仅对复杂样本进行资源密集型处理。

3)  
- **评估任务**：在MMAR音频推理基准（N=1,000）上进行测试。  
- **取得效果**：  
  - **准确性显著提升**：将基线模型（单次推理）的准确率从27.20%提升至53.60%。  
  - **效率优势**：相比“始终开启”调查的基线，实现了更优的准确率-延迟权衡（见图1），平均延迟为9.62秒/样本。  
  - **工具使用分布合理**：自适应门控使38.2%的样本无需工具即可完成，其余样本按需调用单一或组合工具，优化了计算资源分配。
</div>

</details>

---

## EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning
- **Authors**: Dingdong Wang, Shujie Liu, Tianhua Zhang, Youjun Chen, Jinyu Li, Helen Meng
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.15668v1](https://arxiv.org/abs/2601.15668v1)
- **PDF**: [https://arxiv.org/pdf/2601.15668v1](https://arxiv.org/pdf/2601.15668v1)

语音中的情感信息在多模态感知中具有独特作用。然而，当前语音大语言模型与传统语音情感识别系统类似，仍将情感理解视为简单的分类问题。这导致预测的可解释性有限，同时未能充分利用大语言模型的表达与推理能力。本研究首次通过强化学习将语音情感识别重构为深度推理问题，提出EmotionThinker模型，旨在生成基于细粒度声学线索的可解释情感预测。为实现这一目标，我们首先构建了包含思维链标注与详细描述的35K规模情感推理数据集EmotionCoT-35K。其次，我们发现现有语音大语言模型对韵律特征的感知能力较弱，而韵律线索是解读情感的基础信号。为此，我们开发了韵律增强基础模型EmotionThinker-Base，并证明韵律增强能有效提升情感理解能力。最后，我们提出融合渐进式信任感知推理奖励的组相对策略优化方法。与仅依赖规则结果奖励的标准方法不同，该方法渐进引入推理奖励，通过反映推理与结果一致性的可信度权重动态调整奖励，并基于多维度标准通过奖励模型评估整体推理质量。实验表明，EmotionThinker在情感预测准确性与解释质量上均超越现有最优评估模型，推动语音情感识别向可解释多模态推理方向发展。项目页面：https://github.com/dingdongwang/EmotionThinker

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：语音情感识别（SER）是人机交互和情感计算的关键任务。当前语音大语言模型（SpeechLLMs）和传统SER系统通常将情感理解简化为分类问题。
- **问题**：
  - **可解释性不足**：仅输出离散情感标签，缺乏对“为何”做出判断的解释。
  - **能力未充分利用**：未充分发挥大语言模型的多模态表达和推理潜力。
  - **现有方法局限**：基于监督微调（SFT）的描述性方法停留在浅层描述，未能建立声学特征与情感推断之间的因果联系。

2) **论文核心方法如何解决上述问题**
本文提出 **EmotionThinker**，一个基于强化学习（RL）的可解释语音情感推理框架，通过三阶段设计解决上述问题：

- **构建高质量推理数据集**：
  - 创建 **EmotionCoT-35K** 数据集，包含约3.5万个语音-推理对，覆盖说话人特征、韵律特征（音高、语速、能量、语调、重音）和语义转录。
  - 通过自动化标注流程和精心设计的提示，利用GPT-4o生成包含逐步推理链（Chain-of-Thought）的详细解释，为模型提供细粒度监督信号。

- **增强基础模型的韵律感知能力**：
  - 发现现有SpeechLLMs韵律感知能力弱，而韵律是情感表达的核心载体。
  - 基于Qwen2.5-Omni-7B构建 **EmotionThinker-Base**，通过韵律中心的监督微调（SFT）进行增强。SFT语料整合了：
    - 词级重音感知任务。
    - 韵律属性分类任务（如音高、能量等级分类）。
    - 对比性韵律增强任务（通过信号修改学习排序模式）。
  - 实验证明，韵律增强显著提升了模型对情感相关声学线索的感知能力，为后续推理奠定基础。

- **提出创新的强化学习奖励策略（GRPO-PTR）**：
  - **问题**：标准GRPO仅依赖基于规则的结果奖励（如格式、答案准确性），无法监督开放式的中间推理过程，易导致模型“走捷径”（生成表面合理但逻辑错误的推理以获得正确答案）。
  - **解决方案**：提出 **渐进式信任感知推理奖励（GRPO-PTR）**，核心创新包括：
    - **推理奖励模型**：训练一个基于Qwen2.5-Omni-3B的奖励模型，从事实对齐、解释质量、描述完整性和流畅结构性四个维度评估推理过程的质量，输出标量推理奖励 `Rt`。
    - **信任权重（τ）**：为解决推理奖励 `Rt` 可能与任务正确性 `Ro` 不一致的问题（例如错误答案获得高推理分），引入组级信任权重 `τ`。`τ` 根据同一查询下，**正确回答组**与**错误回答组**的平均推理奖励差值动态调整。当正确组的平均推理奖励低于错误组时，`τ` 会衰减，从而抑制不可靠的推理奖励信号，确保推理与结果对齐。
    - **渐进式奖励调度**：训练初期仅使用稳定的规则奖励（格式、结果），待模型情感准确率稳定（如达到50%）后，再逐步引入推理奖励 `Rt`。这避免了早期因多个不稳定奖励信号叠加而导致的优化波动，确保了训练稳定性。
  - **整体奖励**：最终奖励是格式奖励 `Rf`、结果奖励 `Ro` 和经信任权重调制的推理奖励 `τ·Rt` 的加权和。

综上，EmotionThinker通过**数据构建**、**韵律增强**和**创新的RL奖励机制**，系统地引导模型生成**既准确又具有可解释性**的情感推理。

3) **在哪些任务上取得了怎样的效果**
- **任务**：在四个广泛使用的SER基准上评估：IEMOCAP、MELD、RAVDESS（零样本）和SAVEE（零样本）。
- **效果**：
  - **情感识别准确率**：EmotionThinker在整体平均准确率上达到 **68.89%**，超越了包括BLSP-Emo（65.41%）在内的16个开源SpeechLLMs和OmniLLMs基线模型，取得了最先进性能。
  - **推理质量**：在事实对齐、解释质量、描述完整性和流畅结构性四个维度的平均得分达到 **3.98**（5分制），显著优于所有基线模型。
  - **消融实验验证**：GRPO-PTR的关键组件（训练好的奖励模型、信任权重τ、渐进式调度）均被证明对提升准确率和推理质量有效。人类评估结果与GPT自动评估高度一致，证实了模型输出的优越性。
</div>

</details>

---

## Distributed Multichannel Active Noise Control with Asynchronous Communication
- **Authors**: Junwei Ji, Dongyuan Shi, Boxiang Wang, Ziyi Yang, Haowen Li, Woon-Seng Gan
- **Categories**: eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2601.15653v1](https://arxiv.org/abs/2601.15653v1)
- **PDF**: [https://arxiv.org/pdf/2601.15653v1](https://arxiv.org/pdf/2601.15653v1)

分布式多通道有源噪声控制（DMCANC）通过将集中式控制的计算负载分配到多个低成本节点，能够在大空间范围内实现有效的噪声抑制。然而，传统的DMCANC方法通常假设通信同步且需要频繁的数据交换，导致较高的通信开销。为提高系统效率与适应性，本研究提出一种异步通信策略：各节点执行权重约束滤波-x LMS（WCFxLMS）算法，仅在其本地降噪性能下降时独立发起通信请求。当收到请求时，其他节点将本地控制滤波器与WCFxLMS中心点之间的权重差异进行传输，随后整合这些差异以更新控制滤波器及中心点。该设计使节点能够在保持协同行为的同时实现异步运行。仿真结果表明，所提出的异步通信DMCANC（ACDMCANC）系统在显著降低通信负载的同时仍能维持有效的降噪效果，为异构网络提供了更好的可扩展性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**  
- **背景**：分布式多通道有源噪声控制（DMCANC）通过将计算负载分配到多个低成本节点，实现大空间范围的降噪。  
- **问题**：  
  - 传统DMCANC方法通常假设节点间需**同步通信**，并在每个采样点交换数据，导致**通信开销过高**。  
  - 实际硬件限制使得通信频率难以匹配系统采样率，影响性能与实用性。  
  - 现有方法（如扩散FxLMS）对非对称声学路径处理效果有限，且未充分考虑通信延迟与网络异构性。

2)  
**论文核心方法如何解决上述问题**  
本文提出**异步通信DMCANC（ACDMCANC）系统**，核心包括以下两部分：  
- **权重约束FxLMS（WCFxLMS）算法**：  
  - 在非通信阶段，每个节点独立运行WCFxLMS，其代价函数增加了一项对控制滤波器与中心点偏差的惩罚项。  
  - 该约束限制了控制滤波器的更新范围，防止因节点间声学串扰导致的失稳，确保系统在无通信时仍能保持局部降噪与稳定性。  
- **异步通信策略与混合权重差（MWD）操作**：  
  - **触发机制**：每个节点基于本地残差噪声水平（RNL）的变化自主决定是否发起通信请求，仅当性能停止改善时才请求交换信息。  
  - **信息交换**：请求时，其他节点发送其控制滤波器与中心点之间的**权重差**，该差值隐含了本地梯度更新的累积效应。  
  - **信息融合**：通过MWD操作整合所有节点的权重差，更新本地控制滤波器及中心点，从而融入全局信息以提升整体性能。  
- **整体优势**：  
  - 节点可**异步运行**，通信频率大幅降低，适应异构网络与时变条件。  
  - 结合WCFxLMS的稳定性与MWD的全局协同，在保证降噪效果的同时显著减少通信负载。

3)  
**在哪些任务上取得了怎样的效果**  
- **任务**：在**宽带噪声**（100-1000 Hz）和**真实压缩机噪声**场景下进行多通道降噪仿真。  
- **效果**：  
  - 所提ACDMCANC系统在降噪性能上接近集中式MEFxLMS和同步通信MGDFxLMS，但**通信负载显著降低**。  
  - 在宽带噪声实验中，ACDMCANC的收敛速度略慢于同步方法，但仍实现有效全局降噪，且节点通信时机呈现异步特性。  
  - 对于真实噪声，ACDMCANC在保证降噪效果的同时，展现出对异构网络更好的适应性与鲁棒性。
</div>

</details>

---

## Qwen3-TTS Technical Report
- **Authors**: Hangrui Hu, Xinfa Zhu, Ting He, Dake Guo, Bin Zhang, Xiong Wang, Zhifang Guo, Ziyue Jiang, Hongkun Hao, Zishan Guo, Xinyu Zhang, Pei Zhang, Baosong Yang, Jin Xu, Jingren Zhou, Junyang Lin
- **Categories**: cs.SD, cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15621v1](https://arxiv.org/abs/2601.15621v1)
- **PDF**: [https://arxiv.org/pdf/2601.15621v1](https://arxiv.org/pdf/2601.15621v1)

本报告介绍了Qwen3-TTS系列模型，这是一组先进的多语言、可控、鲁棒且支持流式合成的文本转语音模型。该系列支持业界领先的3秒语音克隆与基于描述的语音控制，既能生成全新音色，也能对输出语音进行细粒度调控。模型基于超过500万小时、涵盖10种语言的语音数据训练，采用双轨语言模型架构实现实时合成，并配备两种语音分词器：1）Qwen-TTS-Tokenizer-25Hz为单码本编解码器，侧重语义内容表征，可与Qwen-Audio无缝集成，并通过分块DiT实现流式波形重建；2）Qwen-TTS-Tokenizer-12Hz通过12.5Hz采样率、16层多码本设计及轻量因果卷积网络，实现了极致的比特率压缩与超低延迟流式合成，首包发射延迟低至97毫秒。大量实验表明，该系列在多项客观与主观评测基准（如多语言TTS测试集、InstructTTSEval及长语音测试集）中均达到领先性能。为促进社区研究与开发，我们已将全部分词器与模型在Apache 2.0协议下开源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：稳定、可控、拟人的语音合成被视为AGI的关键能力。当前主流方法基于离散语音分词与自回归语言建模，虽能生成高质量语音，但在实际应用中仍面临挑战。
- **问题**：
  - **可控性不足**：现有模型在通过自然语言描述进行细粒度语音属性操控方面能力有限。
  - **延迟与流式支持**：许多模型难以兼顾超低延迟的流式合成与高音质。
  - **多语言与长文本稳定性**：在多语言一致生成、跨语言音色保持以及长语音生成的稳定性方面存在不足。

2) **论文核心方法如何解决上述问题**
Qwen3-TTS通过一系列创新架构与训练策略系统性解决上述问题：
- **双轨分词器设计**：
  - **Qwen-TTS-Tokenizer-25Hz**：25 Hz单码本分词器，基于Qwen2-Audio编码器，平衡语义与声学信息，支持通过分块DiT进行流式波形重建，侧重高音质与语义丰富性。
  - **Qwen-TTS-Tokenizer-12Hz**：12.5 Hz多码本分词器，采用语义-声学解耦量化。首层码本捕获语义，后续15层RVQ精修声学细节。配合轻量因果ConvNet解码器，实现超低延迟（首包延迟低至97 ms）流式合成。
- **双轨自回归架构与训练策略**：
  - **模型架构**：基于Qwen3 LM，采用双轨表示，将文本与声学token在通道轴拼接，实现文本流式输入与语音流式输出的实时合成。针对12Hz分词器，引入多token预测（MTP）模块，分层预测所有残差码本，以捕获细节并降低延迟。
  - **三阶段预训练**：
    - **通用阶段**：使用超500万小时多语言数据建立文本到语音的单调映射。
    - **高质量阶段**：使用高质量数据进行持续预训练，减少噪声数据导致的幻觉。
    - **长上下文阶段**：将最大token长度扩展至32,768，提升处理长复杂输入的能力。
  - **后训练对齐**：采用DPO和基于规则的奖励（GSPO）进行人类偏好对齐，并引入轻量级说话人微调，提升自然度、表现力和可控性。
- **功能实现机制**：
  - **可控性**：通过ChatML格式将语音控制建模为语言建模任务，支持基于描述的语音设计与目标说话人编辑。
  - **语音克隆**：支持3秒参考音频的实时克隆，或通过上下文学习更好地保持韵律。
  - **流式效率**：12Hz分词器因其纯左上下文解码设计，无需等待未来token，显著降低首包延迟与解码时间。

3) **在哪些任务上取得了怎样的效果**
Qwen3-TTS在多项任务上达到或超越了当前最优性能：
- **零样本语音克隆**：在Seed-TTS测试集上，12Hz-1.7B模型在英文上取得最低WER（1.24），超越CosyVoice 3等基线。
- **多语言语音生成**：在10种语言的评测中，在6种语言上取得最低WER，并在所有10种语言上取得最高的说话人相似度，超越MiniMax和ElevenLabs。
- **跨语言语音生成**：在CV3-Eval跨语言基准上，尤其在中文到韩文任务上，将错误率相对CosyVoice3降低约66%（4.82 vs. 14.4），展现了卓越的跨语言泛化能力。
- **可控语音生成**：在InstructTTSEval基准上，在语音设计场景下达到开源模型最优性能；在目标说话人编辑场景下，显著超越GPT-4o-mini-tts。
- **长语音生成**：在内部长文本测试集（最长10分钟）上，25Hz-1.7B模型取得最低WER，生成流畅且无边界伪影的语音，证明了其长序列生成的稳定性。
</div>

</details>

---

## DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice
- **Authors**: Leying Zhang, Tingxiao Zhou, Haiyang Sun, Mengxiao Bi, Yanmin Qian
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15596v1](https://arxiv.org/abs/2601.15596v1)
- **PDF**: [https://arxiv.org/pdf/2601.15596v1](https://arxiv.org/pdf/2601.15596v1)

尽管现代文本转语音系统在朗读式语音合成上已实现高保真度，但其在生成自主感官经络反应语音时仍面临挑战——这是一种对放松至关重要的特殊低强度语音风格。其固有难点包括ASMR语音细微且常含非浊音的特征，以及对零样本说话人自适应的需求。本文提出DeepASMR，首个面向零样本ASMR生成的框架。我们证明仅需说话人一小段普通朗读式语音片段，即可以其音色合成高保真ASMR语音，无需目标说话人的耳语训练数据。方法上，我们首先发现离散语音单元可对ASMR风格与说话人音色实现软解耦。基于此洞见，我们设计了一个两阶段流程：采用大语言模型进行内容-风格编码，并利用流匹配声学解码器重建音色。此外，我们构建了DeepASMR-DB——一个包含670小时的英汉多说话人ASMR语音语料库，并提出融合客观指标、人工听测、基于LLM的评分及非浊音分析的新型评估方案。大量实验证实，DeepASMR能在任意音色的ASMR生成中实现最优的自然度与风格保真度，同时在常规语音合成任务中保持竞争力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代TTS系统在标准朗读语音上已实现高保真，但难以生成用于放松的ASMR（自主感官经络反应）语音。ASMR语音具有低声、气声、无浊音等细微声学特征，且需支持零样本说话人适配。  
- **既有方法问题**：现有方法主要分为三类：基于提示的大模型上下文学习、语音转换技术、以及在小规模ASMR数据上的微调。这些方法存在两大局限：  
  - 零样本泛化能力差，通常需要目标说话人的ASMR样本作为参考；  
  - 声学真实性不足，未能准确建模气声与无浊音内容的复杂交互。  

2)  
论文提出 **DeepASMR** 框架，通过两阶段流水线实现零样本ASMR生成，核心方法如下：  
- **核心洞察**：发现离散语音token（采用基于ASR目标训练的S3 tokenizer）能够对**风格**（ASMR vs. 正常）与**音色**进行“软分解”。token主要编码宏观韵律风格（如语速、停顿），同时保留少量残差音色信息。  
- **两阶段架构设计**：  
  - **第一阶段：基于LLM的文本-语义编码器**。使用Qwen2.5-0.5B初始化，以任务提示（指定目标风格）和文本为条件，自回归预测离散语义token序列。该阶段主要建模内容与ASMR风格模式。  
  - **第二阶段：基于流匹配的声学解码器**。以预测的语义token（提供内容与粗粒度韵律）和说话人提示的梅尔谱（提供细粒度音色）为条件，通过条件流匹配网络重建梅尔谱，最后经HiFi-GAN声码器输出波形。  
- **虚拟说话人池与任务提示选择**：为解决跨风格合成中“音色泄漏”问题（输出音色偏向风格参考说话人），构建了包含正常语音和ASMR语音的虚拟说话人池（各50条合成语音）。通过说话人验证系统检索与目标说话人音色最相似的候选，作为任务提示，确保LLM专注于风格建模，同时残差音色信息与目标身份一致。  
- **训练策略**：先在大规模通用TTS数据上预训练，再在混合数据集（DeepASMR-DB ASMR数据 + 正常语音数据）上微调，以保持模型稳定性，避免过度拟合气声而丧失浊音生成能力。  

3)  
- **任务与效果**：在**正常→正常（N2N）**、**ASMR→ASMR（A2A）** 的同类风格合成，以及**ASMR→正常（A2N）**、**正常→ASMR（N2A）** 的跨风格合成任务上进行了评估。  
- **关键成果**：  
  - 在核心的**零样本N2A任务**中，DeepASMR在可懂度（WER/CER）、说话人相似度（SIM）和LLM评估的风格保真度上均显著优于级联语音转换基线，且其ASMR风格分数接近真实ASMR录音。  
  - 主观听力测试表明，其生成的ASMR语音在**ASMR特定舒适度（ASMR-MOS）** 上大幅领先基线，能有效诱发放松反应。  
  - **无浊音语音分析**证实，DeepASMR在N2A任务中能生成高比例的无浊音帧（74.21%），接近真实ASMR（91.78%），而基线模型仍主要生成浊音。  
  - 在通用TTS任务上，其性能与主流模型相当，表明ASMR专精化未损害正常语音合成能力。
</div>

</details>

---
