---
layout: post
title: "arXiv Daily – 2026-01-23"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-23（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-22 08:50 — 2026-01-23 08:50
- 抓取总数：13 篇 | 本页显示：13 篇（去重/过滤后）

## Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems
- **Authors**: Prakash Dhungana, Sayed Ahmad Salehi
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.16158v1](https://arxiv.org/abs/2601.16158v1)
- **PDF**: [https://arxiv.org/pdf/2601.16158v1](https://arxiv.org/pdf/2601.16158v1)

在资源受限的边缘设备上部署轻量级模型的关键词检测系统，常因噪声和录音条件变化导致的领域偏移而面临准确性与鲁棒性挑战。为此，我们提出一种面向持续学习的综合框架，旨在适应新领域的同时保持计算效率。该框架集成了双输入卷积神经网络，同时利用梅尔频率倒谱系数和梅尔频谱特征，并辅以多阶段去噪流程（包括离散小波变换和谱减法技术），以及模型与原型更新模块。与以往仅更新特定层的方法不同，得益于紧凑的模型架构，本方法可对完整量化模型进行更新。在运行时，通过类别原型和置信度驱动过滤选择部分输入样本，经伪标注后与回放缓冲区数据结合，用于增量模型重训练。在含噪声测试数据集上的实验结果表明，该框架在纯净数据上达到99.63%的准确率，并在多种噪声环境下（即使在-10 dB信噪比条件下）保持超过94%的准确率，展现出强鲁棒性。本研究证实，将高效去噪技术与基于原型的持续学习相结合，可使关键词检测模型在资源受限的动态环境中实现自主且鲁棒的运行。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：关键词识别系统在边缘设备部署时，因环境噪声、录音条件等导致的**域偏移**，导致模型准确性显著下降（高达27%）。  
- **既有方法问题**：  
  - 现有方法通常仅更新分类器层或部分模型层，限制了模型对新域的适应能力。  
  - 先前工作多集中于类增量或任务增量学习，对**域增量学习**关注不足，且依赖复杂硬件或高信噪比环境。  
  - 模型参数量大、计算复杂，难以满足资源受限系统的实时与能效要求。

2)  
- **整体框架**：提出一个面向资源受限系统的域增量持续学习框架，集成**双阶段去噪**（小波变换与谱减法）与**基于原型的持续学习**机制。  
- **核心方法**：  
  - **轻量化模型架构**：采用双输入CNN（MFCC + LogMel特征），模型参数量仅1.64k，支持全模型量化更新，而非仅更新部分层。  
  - **运行时有效样本选择**：通过**置信度过滤**与**原型相似性**（基于MAE距离）筛选高置信、具代表性的样本，赋予伪标签后与**排练缓冲区**（含增强版本）结合，形成微批次用于增量重训练。  
  - **持续学习流程**：定期重训练量化模型，同步更新**类原型**、距离阈值及模型参数，缓解灾难性遗忘。  
  - **高效去噪管道**：前端小波去噪（Haar变换 + VisuShrink）与后端谱去噪（时频掩码归一化）结合，提升特征鲁棒性。  
- **优势**：全模型更新增强适应能力；原型机制降低计算与存储开销；双阶段去噪保障噪声环境下特征质量；整体设计兼顾精度与嵌入式部署效率。

3)  
- **任务与效果**：  
  - **干净数据**：在Google语音命令数据集上达到**99.63%** 准确率。  
  - **噪声环境**：在多种噪声场景（DWASHING、NFIELD等）及信噪比（-10 dB至10 dB）下，经持续学习后准确率均**超过94%**，在-10 dB极端噪声下仍保持稳健性能。  
  - **资源效率**：模型仅1.64k参数、0.89M FLOPs，在ARM M4等嵌入式硬件上实现低延迟与低能耗部署，优于现有方法（如ODDL、MCUNetv3）。
</div>

</details>

---

## Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization
- **Authors**: Maximos Kaliakatsos-Papakostas, Dimos Makris, Konstantinos Soiledis, Konstantinos-Theodoros Tsamis, Vassilis Katsouros, Emilios Cambouropoulos
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.16150v1](https://arxiv.org/abs/2601.16150v1)
- **PDF**: [https://arxiv.org/pdf/2601.16150v1](https://arxiv.org/pdf/2601.16150v1)

旋律和声化，即为一给定旋律生成和声伴奏的任务，在计算音乐生成领域仍是一个核心挑战。近期基于单编码器Transformer的方法将和声化构建为掩码序列建模问题，但受离散扩散启发的现有训练课程常导致旋律与和声间的（交叉）注意力较弱，从而限制了旋律线索的利用，尤其在领域外情境中。本研究提出一种名为FF（全掩码至全解掩）的训练课程，该课程在训练初期保持所有和声标记被掩码若干步，随后在训练过程中逐步解掩整个序列，以强化旋律与和声的交互。我们通过多组实验轴系统评估了该方法与先前课程的对比，包括时间量化（四分音符与十六分音符）、小节级与时值条件化、旋律表示（全音域与音高类别）以及推理时的解掩策略。模型在HookTheory数据集上训练，并在领域内及精选的爵士标准曲集上进行评估，采用了一套综合评估指标，涵盖和弦进行结构、和声-旋律对齐及节奏连贯性。结果表明，所提出的FF课程在几乎所有指标上均稳定优于基线方法，尤其在领域外评估中提升显著——此时对新旋律线索的和声适应能力至关重要。我们还发现，在FF设置下，四分音符量化、小节标记的交织处理以及音高类别的旋律表示更具优势。本研究结果凸显了训练课程在实现有效旋律条件化中的重要性，并表明全掩码至全解掩策略为单编码器和声化提供了一种稳健的解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：旋律和声化是为给定旋律生成和声伴奏的任务，是计算音乐生成的核心挑战。  
- **既有方法问题**：近期基于单编码器Transformer的方法将和声化视为掩码序列建模问题，但受离散扩散启发的现有训练课程（如Midpoint Doubling和Random 10%）导致旋律与和声间的“交叉注意力”较弱。模型过度依赖和声内部的“自注意力”，未能充分利用旋律线索，尤其在域外（如爵士乐）场景下和声适应性不足。

2)  
- **核心方法**：论文提出名为“全掩码到全可见”（Full-to-Full, FF）的训练课程，通过渐进式解掩码策略强化旋律与和声的交互。  
- **解决思路**：  
  - **初始全掩码**：训练早期保持所有和声令牌完全掩码，迫使模型仅依赖旋律信息建立交叉注意力路径。  
  - **渐进解掩码**：随着训练步数增加，按公式逐步解掩码和声令牌（从全掩码过渡到仅剩一个掩码位置），使模型逐步学习结合旋律上下文与部分可见和声进行预测。  
  - **注意力强化**：通过早期强制依赖旋律，提升模型对旋律线索的利用率，避免过度依赖和声自注意力。  
- **技术细节**：  
  - 使用单编码器Transformer架构，输入为旋律（钢琴卷表示）与部分掩码的和声序列。  
  - 训练时采用掩码语言建模损失，FF课程通过确定性调度控制可见令牌数量，区别于基线方法的随机或固定比例解掩码。  
  - 支持多种推理策略（如uMD、uR10%和序列解掩码），保持生成效率。

3)  
- **任务与效果**：在旋律和声化任务上，使用HookTheory数据集（域内）和爵士标准曲数据集（域外）进行评估。  
- **效果**：  
  - FF课程在几乎所有评估指标上优于基线方法（MD和R10%），尤其在域外数据上表现突出，显示更强的旋律适应性。  
  - 最佳配置为：四分音符量化、旋律使用音级表示、和声序列中交织小节信息。  
  - 模型在和弦进行结构、和声-旋律对齐及节奏连贯性等指标上均取得显著提升。
</div>

</details>

---

## Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks
- **Authors**: Abdul Hannan, Daniele Falavigna, Shah Nawaz, Mubashir Noman, Markus Schedl, Alessio Brutti
- **Categories**: cs.SD, cs.CV
- **arXiv**: [https://arxiv.org/abs/2601.16117v1](https://arxiv.org/abs/2601.16117v1)
- **PDF**: [https://arxiv.org/pdf/2601.16117v1](https://arxiv.org/pdf/2601.16117v1)

边缘设备在资源受限且多变的条件下运行，需要能够适应可用资源限制的动态架构。为满足此类需求，通常采用层丢弃（$\mathcal{LD}$）方法，通过跳过部分网络并降低整体计算复杂度，将静态模型转换为动态模型。然而，现有的$\mathcal{LD}$方法在高低丢弃率场景下会显著影响动态模型的性能，导致性能与计算效率的权衡失衡。为此，我们提出一种基于蒸馏的层丢弃（DLD）框架，以端到端方式有效结合知识蒸馏与$\mathcal{LD}$的能力，从而为动态语音网络实现最先进的性能。通过在三个公开基准测试中使用包括Conformer和WavLM在内的知名语音识别方法进行综合实验，验证了本框架的有效性：在高丢弃率和零丢弃率场景下，词错误率分别降低$9.32\%$和$2.25\%$，同时训练时间减少$33.3\%$。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：边缘设备资源受限且多变，需要动态网络架构。层丢弃（LD）是常用方法，通过跳过部分网络层来降低计算复杂度，将静态模型转为动态模型。
- **既有方法问题**：现有LD方法（如随机丢弃）在低丢弃率和高丢弃率场景下，动态模型的性能显著下降，破坏了性能与计算效率的权衡。例如，在极端丢弃情况下性能退化严重，且难以充分利用完整模型的计算能力。

2)  
论文提出**基于蒸馏的层丢弃（DLD）**端到端框架，通过结合知识蒸馏（KD）与随机层丢弃来解决上述问题。其核心方法如下：

- **框架设计**：
    - 使用一个**参考模型（Mref）**（完整模型）和一个**动态学生模型（MDS）**。MDS的编码器配备门控机制，根据伯努利分布（概率0.5）随机跳过某些编码器模块，实现训练时的动态深度。
    - 在推理时，可固定使用的编码器层数（nDS），以评估不同计算预算下的性能。

- **知识蒸馏与特征对齐**：
    - **核心创新**：利用参考模型所有编码器层产生的**潜在嵌入（latent embeddings）** 来监督动态学生模型的对应嵌入。通过最小化两者嵌入分布的KL散度损失（LKLD），迫使MDS学习Mref的丰富语义表示。
    - **联合训练**：总损失函数为LKLD与连接时序分类（CTC）损失（LCTC）之和，实现端到端训练。CTC损失确保ASR任务性能，而蒸馏损失确保嵌入质量。

- **解决的关键问题**：
    - **缓解性能下降**：通过蒸馏，即使MDS在训练中随机跳过层，其嵌入也能与完整模型的强表示对齐。这显著改善了在**低丢弃率（或无丢弃）** 时的性能，因为模型学会了产生与完整模型相近的高质量嵌入。
    - **提升整体权衡**：蒸馏提供了正则化效果，使动态模型在所有可能的深度（nDS值）下都能保持更优的性能，从而在计算复杂度与识别准确率（WER）之间取得更好的权衡。
    - **训练效率**：该方法只需约1/3的训练时间即可达到或超越基线模型的性能，实现了快速收敛。

3)  
在**自动语音识别（ASR）** 任务上，使用Conformer和WavLM架构在LibriSpeech和TED-LIUM v3数据集上进行了评估：
- **性能提升**：相比随机丢弃基线，DLD显著降低了词错误率（WER）。例如，对于Conformer，在无丢弃（nDS=12）时，WER相对降低达2.25%；在高丢弃（nDS=2）时，WER降低达9.32%。对于WavLM，也取得了类似的显著提升。
- **计算效率**：在保持更高精度的同时，实现了计算加速（例如，丢弃部分层后可达4.01倍加速）。
- **任务泛化**：初步实验表明，该框架可泛化至**口语理解**等下游任务，同样能提升动态模型的准确性。
</div>

</details>

---

## Loose coupling of spectral and spatial models for multi-channel diarization and enhancement of meetings in dynamic environments
- **Authors**: Adrian Meise, Tobias Cord-Landwehr, Christoph Boeddeker, Marc Delcroix, Tomohiro Nakatani, Reinhold Haeb-Umbach
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.16077v1](https://arxiv.org/abs/2601.16077v1)
- **PDF**: [https://arxiv.org/pdf/2601.16077v1](https://arxiv.org/pdf/2601.16077v1)

利用麦克风阵列进行声音采集，使得在会议转录的两个重要任务——说话人日志和信号增强中，除了频谱信息外，还能利用空间信息。然而，若说话人发生移动，则空间位置与说话人之间不存在一一对应关系。为此，本文提出一种新颖的联合空间与频谱混合模型，该模型通过概率化建模说话人与位置索引之间的关系，实现了两个子模型的松散耦合。这样既能联合利用空间与频谱信息，又允许说话人在不同位置发言。在模拟说话人位置变化的LibriCSS数据集上的实验表明，该方法相较于紧密耦合的子系统取得了显著改进。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：会议转录需完成两项关键任务：说话人日志（判断“谁在何时说话”）和语音增强（包括重叠语音分离）。传统方法利用麦克风阵列同时捕捉声谱和空间信息。
- **既有方法的问题**：现有联合模型（如紧密耦合的谱-空间混合模型）假设说话人位置与身份存在**一一对应**，即一个说话人只能固定在一个位置。这导致模型**无法处理说话人移动**的场景，因为移动会使同一说话人对应多个空间位置，造成模型失效。

2)  
论文提出一种**松散耦合的谱-空间混合模型**，以解决紧密耦合模型无法处理说话人移动的问题。其核心方法如下：

- **模型结构**：
    - 使用两个独立的潜变量：**谱模型潜变量**（基于von-Mises-Fisher混合模型，vMFMM）和**空间模型潜变量**（基于复角中心高斯混合模型，cACGMM）。
    - 二者通过**条件概率**连接：给定谱模型潜变量（表示某说话人是否活跃），空间模型潜变量（表示某个位置是否活跃）的概率分布由耦合权重 \(a_{klf}\) 描述。
    - 这允许谱模型分量数 \(K\)（说话人数）与空间模型分量数 \(L\)（位置数）不同，从而实现**多个空间位置映射到同一个说话人**。

- **关键创新**：
    - **松散耦合**：放弃了原有紧密耦合中共享单一潜变量的假设，引入了概率依赖关系 \(p(z^{\text{cAC}}_{ltf} | z^{\text{VMF}}_{kt}) = a_{klf}\)。
    - **频率选择性**：谱模型在时间帧级别判断说话人活跃性，空间模型在时频点级别判断位置活跃性，解决了紧密耦合中“同一时间帧内所有频率必须归属同一说话人”的不匹配问题。
    - **灵活建模**：同一说话人可对应多个空间分量（如因移动或墙壁反射），且模型能区分噪声分量。

- **参数估计与掩码生成**：
    - 使用EM算法迭代估计模型参数（包括耦合权重）。
    - 为进行语音分离，设计了一种**启发式掩码估计方法**：通过计算条件概率 \(\beta_{klf}\)（给定位置活跃时说话人活跃的概率），并对联合后验加权，得到每个说话人在时频点上的掩码 \(m_{ktf}\)，用于后续波束成形。

- **优势**：
    - **无需训练**：基于统计模型，可直接应用于任意麦克风阵列配置。
    - **适应动态环境**：核心突破是解耦了说话人身份与固定位置的绑定，从而能够处理说话人移动的情况。

3)  
- **任务**：在模拟会议数据的**说话人日志**和**语音增强**（最终目标为提升自动语音识别ASR性能）任务上进行评估。
- **效果**：
    - **静态场景**：在LibriCSS数据集上，使用Oracle初始化时，松散耦合模型相比紧密耦合在平均拼接最小置换词错误率（cpWER）上**提升1.1%**；在更长片段上，即使非Oracle初始化也**提升1.7%**。
    - **动态场景（模拟说话人位置变化）**：松散耦合模型表现出**极强的鲁棒性**。在说话人位置发生改变时，紧密耦合模型的性能严重恶化（cpWER从5.0%升至22.9%），而松散耦合模型仅从3.9%升至8.0%（Oracle初始化），**显著优于紧密耦合14.9%**。这证明了其处理说话人移动的有效性。
    - **局限性**：在**高重叠语音**场景下性能仍有下降，且对谱初始化的质量更为敏感。
</div>

</details>

---

## Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs
- **Authors**: Lalaram Arya, Mrinmoy Bhattacharjee, Adarsh C. R., S. R. Mahadeva Prasanna
- **Categories**: eess.AS, cs.HC
- **arXiv**: [https://arxiv.org/abs/2601.16023v1](https://arxiv.org/abs/2601.16023v1)
- **PDF**: [https://arxiv.org/pdf/2601.16023v1](https://arxiv.org/pdf/2601.16023v1)

直接语音到语音翻译（S2ST）因其能够将一种语言的语音翻译为另一种语言，同时减少传统级联流程中固有的错误传播和延迟，正受到越来越多的关注。然而，现有的直接S2ST系统仍面临显著挑战，包括在平行语音数据稀缺时语义-声学对齐的不稳定性、说话人身份保持困难以及多语言扩展能力有限。本文提出DS2ST-LM，一种基于多语言大语言模型（LLM）的可扩展单阶段直接S2ST框架。该架构集成了Whisper语音编码器、可学习的投影模块、Qwen2-0.5B LLM以及音色可控声码器。我们通过扩展GigaST数据集并加入高保真合成目标语音，构建了千小时级双语语料库GigaS2S-1000，实验表明该合成数据在一定程度上缓解了数据稀缺问题。我们研究了两种语义标记生成策略：语音衍生的S3标记和预训练LLM生成的文本衍生标记，并分析了它们对训练稳定性与语义一致性的影响。进一步评估了三种投影架构（线性、Conv1D-线性与Q-Former），发现虽然高容量投影器收敛更快，但简单的线性投影器能实现更高性能。大量实验表明，DS2ST-LM在词汇指标（BLEU、METEOR）和语义指标（BLEURT、COMET）上均优于传统级联基线及ST（Qwen-Audio）+ TTS基线，并可扩展至包括法语、西班牙语、德语、印地语、孟加拉语和乌尔都语在内的多语言对。此外，我们引入音色感知语音合成以保持说话人信息，使DS2ST-LM在说话人相似度和感知自然度上均超越现有直接S2ST系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：直接语音到语音翻译旨在减少传统级联系统（ASR-MT-TTS）的错误传播和延迟，但面临挑战。
- **既有问题**：
  - 平行语音数据稀缺导致语义-声学对齐不稳定。
  - 难以保持说话人音色（身份）。
  - 多语言扩展能力有限。

2)  
论文提出 **DS2ST-LM**，一个基于大语言模型的单阶段直接语音到语音翻译框架，通过以下方式解决上述问题：

- **架构设计**：
  - **语音编码器**：采用预训练的 Whisper 编码器提取源语音的高层声学特征。
  - **投影模块**：将语音特征映射到 LLM 的嵌入空间。实验比较了线性、Conv1D-线性 和 Q-Former 三种投影器，发现简单的线性投影器在保持时序结构的同时取得了最佳性能。
  - **LLM解码器**：使用 Qwen2–0.5B 作为核心，以自回归方式并行预测目标语言的语义语音标记和文本标记。
  - **音色可控声码器**：采用 CosyVoice，通过参考说话人提示提取的说话人嵌入进行条件化，合成高质量目标语音并保留指定音色。

- **数据增强**：
  - 构建了 **GigaS2S-1000** 数据集（1000小时英-中双语平行语音），通过 XTTS-v2 合成高质量目标语音，缓解了数据稀缺问题。

- **语义标记生成策略**：
  - 探索了两种语义标记来源：1) 从目标语音通过 S3 标记器提取的**语音驱动标记**；2) 从目标文本通过文本到标记 LLM 生成的**文本驱动标记**。实验表明语音驱动标记能带来更高的翻译准确性，但文本驱动标记为利用大量语音到文本翻译数据提供了可行替代方案。

- **训练与优化**：
  - 采用加权音频标记和文本标记的交叉熵损失进行联合训练。
  - 引入语义分组建模策略，对齐语音标记（约50 Hz）和文本标记（约3 Hz）的生成速率，降低延迟。

3)  
- **任务与效果**：在多个语言对的语音到语音翻译任务上进行了评估，包括英-中、法-英、德-英、西-英、印地-英、孟加拉-英和乌尔都-英。
- **翻译质量**：在词汇指标（BLEU, METEOR）和语义指标（BLEURT, COMET）上均超越了传统的级联基线以及 ST (Qwen-Audio) + TTS 基线。
- **音色保持与语音质量**：通过集成音色感知合成，在说话人相似度（SIM）和感知自然度（DNSMOS）上超越了之前的直接 S2ST 系统（如 Translatotron2），能够有效保留说话人身份并生成高质量的语音。
</div>

</details>

---

## A Stabilized Hybrid Active Noise Control Algorithm of GFANC and FxNLMS with Online Clustering
- **Authors**: Zhengding Luo, Haozhe Ma, Boxiang Wang, Ziyi Yang, Dongyuan Shi, Woon-Seng Gan
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.15889v1](https://arxiv.org/abs/2601.15889v1)
- **PDF**: [https://arxiv.org/pdf/2601.15889v1](https://arxiv.org/pdf/2601.15889v1)

滤波-x归一化最小均方（FxNLMS）算法在充分自适应后虽可实现较低的稳态误差，但其收敛速度较慢且存在发散风险。相比之下，生成式固定滤波器主动噪声控制（GFANC）方法响应速度快，但因其缺乏自适应性，可能导致较大的稳态误差。本文提出一种混合GFANC-FxNLMS算法，以结合两种方法的互补优势。在该混合算法中，GFANC提供帧级控制滤波器作为FxNLMS的初始化条件，而FxNLMS则以采样率进行连续自适应。若GFANC生成的滤波器存在微小波动，可能反复重新初始化FxNLMS，从而中断其自适应过程并导致系统失稳。为此，本文引入在线聚类模块以避免不必要的重新初始化，提升系统稳定性。仿真结果表明，所提算法在仅需一个预训练宽带滤波器的条件下，实现了快速响应、极低稳态误差和高稳定性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：主动噪声控制（ANC）是应对低频噪声的有效方法。传统自适应算法（如FxNLMS）收敛慢且易发散，而固定滤波器方法（如GFANC）响应快但稳态误差大。现有混合方法（如SFANC-FxNLMS）虽结合两者优势，但GFANC与FxNLMS的直接结合会因滤波器频繁重新初始化导致系统不稳定。

2)  
- **核心方法**：提出一种结合GFANC与FxNLMS的混合算法，并引入在线聚类模块以提升稳定性。  
- **双速率架构**：  
  - **帧级处理**：CNN根据噪声帧预测权重向量，在线聚类模块决定是否更新当前向量，避免微小变化触发重新初始化。  
  - **采样级处理**：FxNLMS以GFANC生成的滤波器为初值，利用误差信号连续优化滤波器系数。  
- **在线聚类机制**：  
  - 动态维护聚类中心，通过距离阈值判断新预测向量是否归属已有聚类。  
  - 仅当向量所属聚类索引变化时，才更新权重向量，从而减少不必要的滤波器重置。  
- **优势**：  
  - GFANC提供快速、良好的滤波器初始化，加速收敛。  
  - FxNLMS的持续优化降低稳态误差。  
  - 在线聚类协调两者更新，避免竞争冲突，增强系统稳定性。

3)  
- **任务与效果**：在仿真实验中，所提算法在车辆噪声和100-1200 Hz噪声等任务上进行了测试。  
- **性能表现**：  
  - 相比单独GFANC或FxNLMS，实现了更快的响应速度和更低的稳态误差。  
  - 在线聚类有效平滑了控制过程，提升了系统稳定性。  
  - 在噪声抑制水平上超越了SFANC和SFANC-FxNLMS方法。
</div>

</details>

---

## PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation
- **Authors**: Jaekwon Im, Natalia Polouliakh, Taketo Akama
- **Categories**: cs.SD, cs.CV, cs.LG, cs.MM, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15872v1](https://arxiv.org/abs/2601.15872v1)
- **PDF**: [https://arxiv.org/pdf/2601.15872v1](https://arxiv.org/pdf/2601.15872v1)

舞蹈配乐生成旨在创作与舞蹈动作相协调的音乐。现有方法通常依赖于从单一舞者提取的身体运动特征以及有限的舞蹈-音乐数据集，这限制了其在涉及多舞者及非人类舞者的真实场景中的性能与适用性。本文提出PF-D2M，一种基于扩散模型的通用舞蹈配乐生成方法，其融合了从舞蹈视频中提取的视觉特征。通过渐进式训练策略，PF-D2M有效应对了数据稀缺与泛化挑战。客观与主观评估均表明，PF-D2M在舞蹈-音乐对齐度与音乐质量方面达到了当前最优性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：舞蹈到音乐生成旨在根据舞蹈动作合成同步的音乐，应用于编舞、表演和数字内容创作。  
- **既有方法问题**：  
  - 依赖从单一人体舞者提取的运动特征（如SMPL模型或2D关键点），难以处理多舞者或非人类舞者（如动画角色）场景。  
  - 公开的高质量舞蹈-音乐数据集稀缺（如AIST++仅含60首歌曲），导致模型泛化能力差，易过拟合。  
  - 现有方法多关注粗粒度节奏特征，对细粒度舞蹈动作的同步性捕捉不足。

2)  
- **核心方法**：PF-D2M是一种基于扩散模型的通用舞蹈到音乐生成框架，通过视觉特征提取和渐进式训练策略解决上述问题。  
- **视觉特征替代姿态依赖**：  
  - 使用Synchformer视觉编码器直接从舞蹈视频中提取时空视觉特征，替代传统基于人体姿态的特征。  
  - 视觉特征通过上采样和投影后，与扩散模型的输入拼接，并通过自适应层归一化（AdaLN）注入每一层，增强模型对多舞者、非人类舞者及复杂场景的适应性。  
- **渐进式训练缓解数据稀缺**：  
  - **阶段0**：用文本到音频模型（Stable Audio Open）权重初始化，保留高质量音乐生成能力。  
  - **阶段1**：在多样化视频数据集（VGGSound）上训练，学习音频-视觉同步关系，提升对真实场景的泛化能力。  
  - **阶段2**：在舞蹈-音乐数据集（AIST++）、文本-音乐数据集（FMA、MoisesDB）和视频-音频数据集上混合微调，通过比例采样（2:4:1）平衡音乐生成与同步性，避免过拟合，并生成结构连贯、高质量的音频。  
- **技术细节**：  
  - 采用DiT架构与VAE编码器，结合文本描述（通过T5编码器）和视觉条件进行生成。  
  - 训练中使用分类器无关引导和速度预测目标，推理时采用DPM-Solver++加速生成。

3)  
- **任务与效果**：  
  - **客观评估**：在AIST++测试集上，PF-D2M在节奏对齐指标（CSD、BHS、HSD、F1）上达到最优，仅BCS略低于基线，因生成更细粒度的节奏结构（如密集hi-hat模式）。  
  - **主观评估**：在包含单/多舞者、人类/非人类舞者的真实场景数据集中，PF-D2M在舞蹈-音乐对齐性和音乐质量上显著优于基线（LORIS、Text-Inv），尤其在复杂视觉条件（非常规视角、多镜头剪辑）下表现稳健。  
  - **泛化能力**：模型成功处理多舞者、动画角色等传统方法难以应对的场景，生成音乐能突出细粒度舞蹈动作，提升表现力。
</div>

</details>

---

## U3-xi: Pushing the Boundaries of Speaker Recognition via Incorporating Uncertainty
- **Authors**: Junjie Li, Kong Aik Lee
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.15719v1](https://arxiv.org/abs/2601.15719v1)
- **PDF**: [https://arxiv.org/pdf/2601.15719v1](https://arxiv.org/pdf/2601.15719v1)

在自动说话人验证系统中，话语级说话人嵌入通常通过对一系列帧级表示进行聚合得到。然而，在实际场景中，单个帧不仅编码了说话人相关信息，还包含多种干扰因素，导致不同帧对最终话语级说话人表示的贡献不均。为解决这一问题，我们提出通过估计每帧的内在不确定性并分配自适应权重，使不确定性较高的帧获得较低关注度。基于此思路，我们提出了U3-xi框架，旨在为说话人嵌入生成更可靠且可解释的不确定性估计。具体而言，我们引入了多种不确定性监督策略：首先，通过随机方差损失实现说话人级不确定性监督，将话语嵌入与其对应说话人中心点之间的距离作为不确定性学习的伪真值；其次，通过训练时将预测的不确定性注入softmax尺度，实现全局级不确定性监督，该自适应缩放机制能根据样本难度调整决策边界的锐度，提供全局指导；第三，我们通过集成具有多视角自注意力机制的Transformer编码器重新设计了不确定性估计模块，使模型能够捕捉丰富的局部及长程时序依赖关系。综合实验表明，U3-xi具有模型无关性，可无缝应用于多种说话人编码器。特别地，当应用于ECAPA-TDNN时，在VoxCeleb1测试集上实现了等错误率相对降低21.1%、最小检测代价相对降低15.57%的性能提升。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动说话人验证系统通过聚合帧级表示来获取说话人嵌入。然而，实际语音帧不仅包含说话人信息，还混杂了语言内容、噪声等干扰因素，导致不同帧对最终嵌入的贡献不均。
- **既有方法问题**：现有方法（如xi-vector）的**不确定性估计模块仅由浅层线性层构成**，建模能力有限；且**训练仅依赖交叉熵损失**，缺乏对不确定性的显式监督，导致估计不可靠、鲁棒性不足。

2)  
论文提出U3-xi框架，通过三重改进提升不确定性估计的可靠性与可解释性：  
- **说话人级不确定性监督**：  
  - 提出随机方差损失，以**嵌入与说话人质心的距离作为伪真值**，显式监督不确定性学习。  
  - 在余弦评分中引入不确定性，形成**自适应加权评分机制**，降低高不确定性帧的权重。  
- **全局级不确定性监督**：  
  - 将**预测的不确定性注入Softmax尺度参数**，使决策边界根据样本难度动态调整：对困难样本平滑分布，对简单样本锐化分布，从而在全局层面约束不确定性。  
- **不确定性估计模块增强**：  
  - 用**Transformer编码器替换浅层线性层**，通过多视图自注意力捕获局部与长程时序依赖，提升非线性建模能力。  
- **整体贡献**：以上改进可灵活应用于不同说话人编码器，通过**显式监督与更强大的时序建模**，共同提升嵌入的判别性与鲁棒性。

3)  
- **任务与效果**：在**VoxCeleb1测试集**上，将U3-xi应用于ECAPA-TDNN编码器，相比基线在**等错误率上相对提升21.1%**，在**最小检测代价上相对提升15.57%**。  
- **跨任务验证**：框架在**多种编码器上均有效**，且在**跨域数据集**上展现出一定的泛化能力，但不确定性评分在跨域场景对最小检测代价的改进尚不稳定。
</div>

</details>

---

## Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems
- **Authors**: Hengfan Zhang, Yueqian Lin, Hai Helen Li, Yiran Chen
- **Categories**: cs.SD, cs.LG, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15676v1](https://arxiv.org/abs/2601.15676v1)
- **PDF**: [https://arxiv.org/pdf/2601.15676v1](https://arxiv.org/pdf/2601.15676v1)

在边缘基础设施上部署音频-语言模型（Audio-LLMs）时，感知深度与计算效率之间始终存在矛盾。轻量级本地模型往往产生被动感知——即生成通用摘要，却遗漏多步音频推理所需的细微证据；而无差别地将任务卸载至云端则带来难以接受的延迟、带宽成本和隐私风险。本文提出 CoFi-Agent（工具增强的从粗到精智能体），一种面向边缘服务器与网关的混合架构。该架构执行快速的本地感知，仅在检测到不确定性时触发条件式精细分析。CoFi-Agent 首先在本地 7B 参数的 Audio-LLM 上进行单次前向推理，随后由云端控制器对困难案例进行筛选，并生成轻量级执行计划，调用设备端工具（如时序重听与本地自动语音识别）。在 MMAR 基准测试中，CoFi-Agent 将准确率从 27.20% 提升至 53.60%，同时相比持续运行的精细分析流程实现了更优的准确率-效率平衡。总体而言，CoFi-Agent 通过工具增强的条件式边缘-云端协同机制，在实际系统约束下弥合了感知能力的差距。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在边缘设备上部署音频-语言模型面临“感知深度”与“计算效率”的固有矛盾。  
- **既有方法问题**：  
  - **轻量本地模型**：采用单次推理范式，对复杂音频推理（如长录音、背景噪声、时序事件）易产生“被动感知”，即生成笼统摘要而遗漏细微证据。  
  - **无条件云卸载**：始终将音频传输至云端或调用昂贵工具链（如ASR、重听），导致延迟高、带宽消耗大、声学隐私泄露风险增加。  
  - **静态压缩技术**：如量化、剪枝等方法对所有输入均一处理，无法根据查询难度动态分配计算资源，造成语义冗余。

2)  
论文提出 **CoFi-Agent**（工具增强的粗到精智能体），一种混合架构，通过条件化边缘-云协作解决上述问题：  

- **本地粗粒度感知**：在边缘设备上部署轻量级7B音频-语言模型（如Qwen2-Audio），对每个查询执行快速单次推理，生成初始答案与摘要。  
- **自适应置信门控**：云端控制器评估初始响应的模糊性与自洽性（如 hedging 语言、证据缺失、逻辑不一致），仅对不确定样本（约62%）触发细化路径，其余直接返回结果（快速路径）。  
- **云引导的细化规划**：针对不确定样本，云端仅接收紧凑的元数据（如时间段标记），并生成轻量级调查计划，指导边缘设备执行本地工具。  
- **本地工具执行**：  
  - **时序重听**：对关键时间段重新运行本地音频模型，提取细粒度音频证据。  
  - **本地ASR**：在设备端运行语音识别（如Whisper），生成文本转录，避免原始音频上传。  
- **证据集成与最终判决**：云端推理器结合初始摘要、音频证据和文本转录，生成最终答案。  
- **隐私与带宽优化**：原始音频始终保留在本地，仅传输符号化证据（如文本），支持设备端敏感信息脱敏，符合数据最小化原则。  

该方法通过 **动态推理** 替代静态压缩或无条件云卸载，实现了“语义级联”：对简单样本快速响应，仅对复杂样本触发精细化工具调查。

3)  
- **任务**：在 **MMAR 音频推理基准**（N=1,000，含多选问答）上进行评估。  
- **效果**：  
  - **准确率**：CoFi-Agent（自适应+ASR）达到 **53.60%**，较基线单次推理模型（27.20%）提升近一倍。  
  - **效率**：在相同准确率下，相比无条件全时调查基线，实现了更优的 **准确率-延迟权衡**（平均延迟 9.62 秒/样本）。  
  - **工具使用分布**：自适应门控使 **38.2%** 样本无需工具即可完成，其余样本按需调用重听（27.4%）、ASR（19.7%）或两者（14.7%），避免了不必要的计算开销。
</div>

</details>

---

## EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning
- **Authors**: Dingdong Wang, Shujie Liu, Tianhua Zhang, Youjun Chen, Jinyu Li, Helen Meng
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.15668v1](https://arxiv.org/abs/2601.15668v1)
- **PDF**: [https://arxiv.org/pdf/2601.15668v1](https://arxiv.org/pdf/2601.15668v1)

语音中的情感信息在多模态感知中具有独特作用。然而，当前语音大语言模型与传统语音情感识别系统类似，仍将情感理解视为简单的分类问题。这导致预测的可解释性有限，同时未能充分利用大语言模型的表达与推理能力。本研究首次通过强化学习将语音情感识别重构为深度推理问题，提出EmotionThinker模型，旨在生成基于细粒度声学线索的可解释情感预测。为实现这一目标，我们首先构建了EmotionCoT-35K数据集，该数据集包含思维链标注与详细描述。其次，我们发现当前语音大语言模型在韵律感知方面表现薄弱，而韵律线索是解读情感的基础信号。为此，我们开发了韵律增强基础模型EmotionThinker-Base，并证明韵律增强能提升情感理解能力。第三，我们提出结合渐进信任感知推理奖励的组相对策略优化方法。与仅依赖规则结果奖励的标准方法不同，该方法逐步引入推理奖励，通过反映推理与结果一致性的可信度权重动态调整奖励，并基于多维度标准通过奖励模型评估整体推理质量。EmotionThinker在情感准确性与解释质量上均超越先前最优评估模型，推动语音情感识别向可解释多模态推理方向发展。项目页面：https://github.com/dingdongwang/EmotionThinker

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：语音情感识别（SER）对多模态感知至关重要，但现有方法（包括语音大语言模型）大多将其视为简单的分类问题。
- **问题**：
  - **缺乏可解释性**：模型仅输出离散情感标签，无法提供“为何”做出判断的解释。
  - **未充分利用模型能力**：语音大语言模型的表达和推理潜力未被挖掘。
  - **数据与感知局限**：现有数据集缺乏细粒度声学标注用于推理监督；基础模型对韵律（如语调、重音）的感知能力弱，而韵律是情感表达的核心线索。
  - **奖励机制不足**：基于规则的强化学习奖励仅关注结果准确性，难以监督推理过程的质量，可能导致模型产生表面合理但逻辑错误的推理。

2) **论文核心方法如何解决上述问题**
论文提出 **EmotionThinker** 框架，通过三阶段方法实现可解释的情感推理：

- **阶段一：构建高质量推理数据集（EmotionCoT-35K）**
  - **数据来源**：整合多个开源情感数据集（如IEMOCAP、MELD），涵盖约200小时音频、35K个样本。
  - **自动化标注流程**：提取说话人特征（性别、年龄）、韵律特征（音高、语速、能量、重音、语调轮廓）和转录文本。
  - **生成推理链**：使用精心设计的提示词，通过GPT-4o生成包含逐步推理的思维链（Chain-of-Thought）数据，格式为 `<think>` 推理过程 `</think>` 和 `<answer>` 情感标签 `</answer>`。这为模型提供了细粒度声学线索与情感判断之间的因果关联监督。

- **阶段二：增强韵律感知的基础模型（EmotionThinker-Base）**
  - **问题识别**：实验发现现有语音大语言模型（如Qwen2.5-Omni-7B）韵律感知能力弱。
  - **解决方案**：进行以韵律为中心的监督微调（SFT）。
    - **训练数据**：整合约500小时数据，包括词级重音感知、韵律属性分类、对比性韵律增强任务以及少量EmotionCoT样本。
    - **训练目标**：联合优化音频编码器、适配器和LLM主干，使模型能准确感知音高、能量、语速等核心韵律特征，为后续推理奠定基础。

- **阶段三：基于强化学习（RL）优化推理过程（GRPO-PTR）**
  - **核心挑战**：标准GRPO仅依赖基于规则的结果奖励，无法确保中间推理过程的质量，易导致“奖励黑客”（生成正确结果但推理错误）。
  - **GRPO-PTR方案**：提出渐进式信任感知推理奖励策略。
    - **推理奖励模型**：基于Qwen2.5-Omni-3B微调，在四个维度（事实对齐、解释质量、描述完整性、流畅与结构清晰度）上评估开放式推理过程的质量。
    - **信任权重（τ）**：动态调整推理奖励的贡献。通过比较同一查询下“正确回答组”与“错误回答组”的平均推理奖励，惩罚推理奖励与结果正确性不一致的情况，抑制虚假奖励信号。
    - **渐进式奖励调度**：训练初期仅使用格式奖励和结果奖励，待模型稳定后（如准确率达50%）再引入推理奖励，避免早期优化不稳定。
  - **总体奖励**：`R = α_f * R_f（格式奖励） + α_o * R_o（结果奖励） + α_t * τ * R_t（推理奖励）`，通过多维度监督引导模型生成既准确又可解释的推理。

3) **在哪些任务上取得了怎样的效果**
- **任务**：在四个主流语音情感识别基准上评估：IEMOCAP、MELD、RAVDESS（零样本）、SAVEE（零样本）。
- **效果**：
  - **情感识别准确率**：EmotionThinker在整体平均准确率达到 **68.89%**，超越包括BLSP-Emo在内的16个开源语音大语言模型，取得最先进性能。
  - **推理质量**：在四个推理维度（事实对齐、解释质量等）的GPT自动评估和人工评估中均显著优于所有基线模型，平均得分达 **3.98**（5分制），证明了其生成细粒度、逻辑连贯且基于声学证据解释的能力。
  - **消融实验验证**：证明了韵律增强SFT、训练好的推理奖励模型、信任权重τ以及渐进式奖励调度均为提升性能的关键组件。
</div>

</details>

---

## Distributed Multichannel Active Noise Control with Asynchronous Communication
- **Authors**: Junwei Ji, Dongyuan Shi, Boxiang Wang, Ziyi Yang, Haowen Li, Woon-Seng Gan
- **Categories**: eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2601.15653v1](https://arxiv.org/abs/2601.15653v1)
- **PDF**: [https://arxiv.org/pdf/2601.15653v1](https://arxiv.org/pdf/2601.15653v1)

分布式多通道有源噪声控制（DMCANC）通过将集中式控制的计算负载分配到多个低成本节点，实现了大空间范围内的有效降噪。然而，传统DMCANC方法通常假设通信同步且需要频繁数据交换，导致较高的通信开销。为提高效率与适应性，本文提出一种异步通信策略：各节点执行权重约束滤波x LMS（WCFxLMS）算法，仅在本地降噪性能下降时独立发起通信请求。当请求触发时，其他节点将本地控制滤波器与WCFxLMS中心点之间的权重差异进行传输，随后整合这些差异以更新控制滤波器及中心点。该设计使节点能在保持协同行为的同时实现异步运行。仿真结果表明，所提出的异步通信DMCANC（ACDMCANC）系统在显著降低通信负载的同时维持了有效的降噪性能，为异构网络提供了更好的可扩展性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：分布式多通道有源噪声控制（DMCANC）通过将计算负载分配到多个低成本节点，实现大空间范围的噪声控制。  
- **既有方法问题**：  
  - 传统方法（如扩散FxLMS）通常假设节点间需**同步通信**，且每个采样点都需交换数据，导致**通信开销极高**。  
  - 实际硬件限制使得通信频率难以匹配系统采样率，影响实时应用可行性。  
  - 现有方法对非对称声学路径处理有限，且忽略通信延迟与网络异构性问题。

2)  
论文提出**异步通信DMCANC（ACDMCANC）系统**，核心方法如下：  
- **本地算法**：每个节点运行**权重约束滤波x LMS（WCFxLMS）算法**。  
  - 在成本函数中增加惩罚项，约束控制滤波器与其中心点的偏差，防止非通信阶段因声学串扰导致的失稳。  
- **异步通信触发机制**：  
  - 节点基于**局部噪声水平（RNL）** 自主判断：当平均RNL停止改善时，才发起通信请求。  
  - 其他节点收到请求后，传输**权重差异**（即本地控制滤波器与中心点的偏差），而非原始梯度数据。  
- **信息融合**：通过**混合权重差异（MWD）操作**整合来自其他节点的权重差异，更新本地控制滤波器及中心点。  
- **优势**：  
  - 节点可独立异步运行，减少通信频率，显著降低开销。  
  - WCFxLMS确保非通信阶段的稳定性，MWD实现全局信息协同，平衡性能与通信效率。

3)  
- **任务**：在**宽带噪声（100-1000 Hz）** 和**真实压缩机噪声**场景下进行多通道噪声控制。  
- **效果**：  
  - 所提ACDMCANC系统在噪声抑制性能上接近集中式MEFxLMS和同步MGDFxLMS，但**通信负载大幅降低**。  
  - 在异构网络中表现出更优的适应性与可扩展性，为实际部署提供了可行方案。
</div>

</details>

---

## Qwen3-TTS Technical Report
- **Authors**: Hangrui Hu, Xinfa Zhu, Ting He, Dake Guo, Bin Zhang, Xiong Wang, Zhifang Guo, Ziyue Jiang, Hongkun Hao, Zishan Guo, Xinyu Zhang, Pei Zhang, Baosong Yang, Jin Xu, Jingren Zhou, Junyang Lin
- **Categories**: cs.SD, cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15621v1](https://arxiv.org/abs/2601.15621v1)
- **PDF**: [https://arxiv.org/pdf/2601.15621v1](https://arxiv.org/pdf/2601.15621v1)

本报告介绍了Qwen3-TTS系列模型，这是一组先进的多语言、可控、鲁棒且支持流式合成的文本转语音模型。该系列支持业界领先的3秒语音克隆与基于描述的语音控制，既能生成全新音色，也能对输出语音进行细粒度调控。模型基于超过500万小时、涵盖10种语言的语音数据训练，采用双轨语言模型架构实现实时合成，并配备两种语音分词器：1）Qwen-TTS-Tokenizer-25Hz为单码本编码器，侧重语义内容表征，可与Qwen-Audio无缝集成，并通过分块DiT实现流式波形重建；2）Qwen-TTS-Tokenizer-12Hz通过12.5Hz频率、16层多码本设计及轻量因果卷积网络，实现极致的比特率压缩与超低延迟流式合成，首包发射延迟仅97毫秒。大量实验表明，该系列在多项客观与主观评测基准（如TTS多语言测试集、InstructTTSEval及自建长语音测试集）中均达到领先水平。为促进社区研究与开发，我们已将全部分词器与模型在Apache 2.0协议下开源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：稳定、可控、拟人的语音合成是通往AGI的关键能力。当前主流方法基于离散语音分词与自回归语言建模，虽能生成高质量语音，但在实际应用中仍面临挑战。
- **问题**：
  - **可控性不足**：现有模型对语音的细粒度属性（如韵律、风格）控制能力有限。
  - **流式合成延迟高**：实时语音生成的首包延迟较高，难以满足低延迟交互需求。
  - **多语言与跨语言泛化弱**：在零样本语音克隆、跨语言语音转换等任务中，内容一致性与说话人相似性有待提升。
  - **长语音生成不稳定**：自回归模型在生成长语音时容易出现重复、省略或韵律不连贯等问题。

2) **论文核心方法如何解决上述问题**
Qwen3-TTS通过一系列创新设计应对上述挑战：
- **双轨分词器架构**：
  - **Qwen-TTS-Tokenizer-25Hz**：25 Hz单码本分词器，融合语义与声学信息，通过分块DiT实现流式波形重建，平衡表达力与建模复杂度。
  - **Qwen-TTS-Tokenizer-12Hz**：12.5 Hz多码本分词器，采用语义-声学解耦量化，首层捕获语义，后续层细化声学细节。其纯左上下文流式编解码器设计支持超低延迟（首包延迟可低至97 ms）。
- **双轨自回归模型**：
  - 基于Qwen3 LM系列，将文本与声学token沿通道轴拼接，实现文本流式输入与音频流式输出的实时合成。
  - 针对12Hz分词器，引入多token预测（MTP）模块，一次性生成所有残差码本，在保证声学细节的同时最小化延迟。
- **三阶段训练策略**：
  - **预训练**：包含通用阶段（500万小时多语言数据）、高质量阶段（数据分层与持续预训练）和长上下文阶段（扩展上下文至32k token），分别建立基础映射、提升质量与增强长序列处理能力。
  - **后训练**：采用DPO对齐人类偏好，结合基于规则的奖励与GSPO优化，最后进行轻量级说话人微调，进一步提升自然度、可控性与跨任务稳定性。
- **可控性增强**：
  - 支持基于自然语言描述的细粒度语音控制与语音设计。
  - 引入概率激活的思维模式，提升对复杂指令的遵循能力。
  - 支持3秒语音克隆与预设高质量音色库。

3) **在哪些任务上取得了怎样的效果**
Qwen3-TTS在多项任务中达到SOTA或具有竞争力：
- **零样本语音克隆**：在Seed-TTS测试集上，12Hz-1.7B模型在英文上取得最低WER（1.24），超越CosyVoice 3等基线。
- **多语言语音生成**：在10种语言的评测中，在6种语言上取得最低WER，并在所有语言上取得最高的说话人相似性得分，显著优于MiniMax和ElevenLabs。
- **跨语言语音生成**：在中文到韩文等挑战性对上，错误率较CosyVoice3降低约66%，展现卓越的跨语言泛化能力。
- **可控语音生成**：在InstructTTSEval评测中，在语音设计场景下达到开源模型SOTA，在目标说话人编辑任务中显著优于GPT-4o-mini-tts。
- **长语音生成**：在内部长语音测试集上，25Hz-1.7B模型取得最低WER（中文1.533，英文1.571），能生成超过10分钟流畅且连贯的语音。
</div>

</details>

---

## DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice
- **Authors**: Leying Zhang, Tingxiao Zhou, Haiyang Sun, Mengxiao Bi, Yanmin Qian
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15596v1](https://arxiv.org/abs/2601.15596v1)
- **PDF**: [https://arxiv.org/pdf/2601.15596v1](https://arxiv.org/pdf/2601.15596v1)

尽管现代文本转语音系统在朗读式语音合成上已实现高保真度，但其在生成自主感官经络反应语音时仍面临挑战——这种低强度、高度专业化的语音风格对放松疗愈至关重要。其固有难点在于ASMR语音具有细微且常包含气声的特征，同时需满足零样本说话人自适应需求。本文提出DeepASMR，首个面向零样本ASMR生成的框架。我们证明仅需说话人普通朗读语音的短片段，即可以其音色合成高保真ASMR语音，无需目标说话人的气声训练数据。方法上，我们首先发现离散语音单元可对ASMR风格与说话人音色实现软解耦。基于此洞见，我们设计了一个两阶段流程：采用大语言模型进行内容-风格编码，结合流匹配声学解码器实现音色重建。此外，我们构建了DeepASMR-DB——一个包含670小时的英汉多说话人ASMR语音数据库，并提出融合客观指标、主观听测、大语言模型评分及气声分析的新型评估方案。大量实验证实，DeepASMR能在任意音色的ASMR生成中实现最优的自然度与风格保真度，同时在常规语音合成任务中保持竞争力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代TTS系统在朗读式语音上已实现高保真，但难以生成用于放松的、低强度的ASMR（自发性知觉经络反应）语音。ASMR语音具有轻柔、常为气声等细微声学特征，且需支持零样本说话人适应。  
- **既有方法问题**：现有方法主要分为三类：基于提示的大模型上下文学习、将正常语音转换为耳语的语音转换（VC）、以及在有限ASMR数据上微调。这些方法存在两大局限：  
  - **零样本泛化能力差**：大多需要目标说话人的ASMR风格参考，无法仅凭其正常语音生成ASMR。  
  - **声学真实性不足**：常将耳语简单视为“风格迁移”或“添加噪声”，未能捕捉气声与无声音素间复杂的交互作用。  

2)  
论文提出 **DeepASMR** 框架，通过以下核心方法解决上述问题：  
- **基于离散语音令牌的软分解**：研究发现，经自动语音识别（ASR）任务训练的S3令牌可在表示层面实现“风格”与“音色”的软分解。令牌主要编码内容与宏观韵律风格（如ASMR的慢速、停顿），同时保留少量残余音色信息。这为分阶段控制风格与音色提供了基础。  
- **两阶段生成流水线**：  
  - **第一阶段：基于LLM的文本到语义编码器**。使用Qwen2.5-0.5B初始化，以任务提示、目标风格和文本为输入，自回归预测离散语义令牌序列，重点建模ASMR的宏观风格模式。  
  - **第二阶段：基于流匹配的声学解码器**。以预测的语义令牌（提供内容与大体韵律）和说话人提示的梅尔谱（提供细粒度音色）为条件，通过条件流匹配网络重建梅尔谱，再经HiFi-GAN声码器生成波形。该阶段专注于从残余信息中恢复完整音色。  
- **虚拟说话人池的任务提示选择**：针对跨风格合成（如正常→ASMR）中存在的“音色泄漏”问题（输出音色偏向风格参考说话人），构建了包含50个合成语音的虚拟说话人池（分正常和ASMR风格）。通过预训练说话人验证系统检索与目标说话人音色最相似的候选作为任务提示，使LLM专注于风格变化，同时保持音色一致性。  
- **两阶段训练与数据混合**：先在20万小时内部TTS数据上预训练，再在DeepASMR-DB（ASMR数据）与Emilia（正常语音）的混合数据上微调，防止模型过度拟合气声而丧失正常语音生成能力。  

3)  
- **任务与效果**：在**正常→正常（N2N）**、**ASMR→ASMR（A2A）**、**ASMR→正常（A2N）** 及核心的**正常→ASMR（N2A）** 零样本转换任务上进行了评估。  
- **主要成果**：  
  - 在N2A任务中，DeepASMR取得了最低的词错误率（如英文6.53%），并实现了最高的ASMR风格分数（LLM评分接近+1），显著优于串联TTS+VC的基线模型。  
  - 主观听力测试显示，其生成的ASMR在整体印象和ASMR特定舒适度（引发放松感）上均接近真实录音，大幅领先基线。  
  - 在无声音语分析中，DeepASMR在N2A任务中实现了74.21%的全局无声音比例，证明其能有效生成无声音素，而基线模型仍保留较多周期性声音。  
  - 同时，在正常语音合成任务上保持竞争力，表明其专精于ASMR并未牺牲通用能力。
</div>

</details>

---
