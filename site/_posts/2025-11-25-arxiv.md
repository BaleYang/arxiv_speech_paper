---
layout: post
title: "arXiv Daily – 2025-11-25"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-11-25（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-11-24 08:50 — 2025-11-25 08:50
- 抓取总数：8 篇 | 本页显示：8 篇（去重/过滤后）

## Frequency-Invariant Beamforming in Elevation and Azimuth via Autograd and Concentric Circular Microphone Arrays
- **Authors**: Jorge Ortigoso-Narro, Jose A. Belloch, Maximo Morales-Cespedes, Maximo Cobos
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.19403v1](https://arxiv.org/abs/2511.19403v1)
- **PDF**: [https://arxiv.org/pdf/2511.19403v1](https://arxiv.org/pdf/2511.19403v1)

本研究探索将自动微分工具与同心圆麦克风阵列相结合，通过施加波束宽度与频率不变性约束，实现方位角与仰角的双轴优化。相较于仅支持单轴指向的线性阵列，二维阵列虽具备双轴调控潜力，但仰角控制仍具挑战。该方法可在宽频带内保持性能稳定的同时，实现双角度的连续优化。通过仿真实验评估多频点下的波束宽度、白噪声增益与指向性性能，并与延迟求和、改进型延迟求和、基于雅可比-安格尔展开的方法及高斯窗梯度下降法等传统及先进波束形成器进行对比。本方法展现出更优的空间选择性与更窄的主瓣宽度，尤其在低频段的仰角控制方面表现突出。实验结果验证了该方法在需要精密双轴控制的声学感知与空间音频应用中提升波束形成性能的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2511.19403v1）
</div>

</details>

---

## Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments
- **Authors**: Jorge Ortigoso-Narro, Jose A. Belloch, Adrian Amor-Martin, Sandra Roger, Maximo Cobos
- **Categories**: cs.SD, cs.AI, cs.CV
- **arXiv**: [https://arxiv.org/abs/2511.19396v1](https://arxiv.org/abs/2511.19396v1)
- **PDF**: [https://arxiv.org/pdf/2511.19396v1](https://arxiv.org/pdf/2511.19396v1)

本研究提出一种嵌入式系统，通过融合基于深度学习的物体追踪与波束成形技术，在动态环境中实现精准声源定位与定向音频采集。该系统结合单目相机深度估计与立体视觉技术，实现对运动物体的精确三维定位。采用MEMS麦克风构建的平面同心圆麦克风阵列构成紧凑高效的硬件平台，支持方位角与俯仰角的二维波束导向。实时追踪结果持续调整阵列聚焦方向，使声学响应与目标位置保持同步。通过将学习获得的空间感知能力与动态波束导向相结合，该系统在多声源或移动声源场景下仍保持稳健性能。实验评估表明其显著提升了信号干扰比，特别适用于视频会议、智能家居设备及辅助技术等应用场景。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2511.19396v1）
</div>

</details>

---

## Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation
- **Authors**: Maral Ebrahimzadeh, Gilberto Bernardes, Sebastian Stober
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2511.19342v1](https://arxiv.org/abs/2511.19342v1)
- **PDF**: [https://arxiv.org/pdf/2511.19342v1](https://arxiv.org/pdf/2511.19342v1)

当前最先进的符号音乐生成模型虽已实现卓越的输出质量，但针对和声张力等作曲特征的显式控制仍具挑战。本文提出一种创新方法，将基于音程向量分析的计算化张力模型嵌入Transformer架构。在推理阶段采用双层级束搜索策略：在音符层级，通过模型概率与多样性指标对生成候选进行重排序以保持整体质量；在乐节层级，应用基于张力的重排序机制确保生成音乐符合预期张力曲线。客观评估表明本方法能有效调控和声张力，主观听感实验证实系统输出与目标张力具有一致性。这些结果证明：通过双层级束搜索实现的显式张力控制，为AI音乐生成提供了强大直观的引导工具。实验还表明，本方法可在相同张力条件下生成多种差异化音乐演绎。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2511.19342v1）
</div>

</details>

---

## Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization
- **Authors**: Ellie L. Zhang, Duoduo Liao, Callie C. Liao
- **Categories**: cs.SD, cs.AI, eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2511.19275v1](https://arxiv.org/abs/2511.19275v1)
- **PDF**: [https://arxiv.org/pdf/2511.19275v1](https://arxiv.org/pdf/2511.19275v1)

动态可扩展的多物种鸟类声景生成始终是计算机音乐与算法声音设计领域的重大挑战。鸟类鸣声包含快速频率调制啁啾、复杂振幅包络、独特声学模式、重叠鸣叫以及动态鸟群交互，这些要素均需在三维环境中实现精确的时空控制。现有方法无论基于数字信号处理（DSP）或数据驱动，通常仅聚焦单一物种建模、静态鸣叫结构或直接基于录音合成，普遍存在噪声干扰、灵活性受限或数据需求庞大等问题。为此，我们提出一种完全由算法驱动的新型框架，通过基于DSP的啁啾生成与三维空间化技术，无需依赖录音或训练数据即可生成动态多物种鸟类声景。该方法模拟每类物种中多个独立移动的鸟类沿不同三维轨迹运动，在可扩展声景中实现可控啁啾序列、重叠合唱与真实三维运动，同时保持物种特异性声学模式。可视化界面提供鸟类轨迹、声谱图、活动时间轴与声波图谱，服务于分析与创作需求。视觉与听觉评估均表明，该系统能生成密集沉浸且具有生态启发性的声景，在计算机音乐、交互式虚拟环境与计算生物声学研究领域展现出应用潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2511.19275v1）
</div>

</details>

---

## Multidimensional Music Aesthetic Evaluation via Semantically Consistent C-Mixup Augmentation
- **Authors**: Shuyang Liu, Yuan Jin, Rui Lin, Shizhe Chen, Junyu Dai, Tao Jiang
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.18869v1](https://arxiv.org/abs/2511.18869v1)
- **PDF**: [https://arxiv.org/pdf/2511.18869v1](https://arxiv.org/pdf/2511.18869v1)

由于音乐感知具有多维度特性，评估生成歌曲的审美质量面临挑战。本文提出一种鲁棒的音乐美学评估框架，其核心包含三个创新点：(1) 采用多源多尺度特征提取技术，获取互补的片段级与曲目级音乐表征；(2)设计分层音频增强策略以扩展训练数据多样性；(3)构建融合回归损失与排序损失的混合训练目标，实现精确评分与头部作品可靠识别。在ICASSP 2026 SongEval基准测试上的实验表明，本方法在相关性指标与头部排序指标上均持续优于基线模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：随着生成式音乐模型的出现，自动音乐美学评估需求日益增长，但音乐感知具有多维度特性，现有方法难以全面捕捉关键信息。  
- **既有方法问题**：  
  - Audiobox Aesthetics采用简单Transformer结构，因无法充分提取音乐特征导致性能欠佳；  
  - SongEval虽提供高质量标注数据集，但其基础回归模型在有限标注数据下泛化能力不足。  

2)  
- **多源多尺度特征提取**：  
  - 联合预训练模型MuQ与MusicFM，提取局部片段级与全局曲目级互补特征；  
  - 引入多查询多头注意力统计池化，将变长特征转换为固定维度表示，捕捉时间、频谱等维度贡献。  
- **分层音频增强**：  
  - 数据级：通过增益扰动、噪声添加、音高/时间微调等物理变换扩充训练集；  
  - 特征级：基于核密度估计的C-Mixup策略，混合高相似度样本的特征与标签，提升低资源场景鲁棒性。  
- **混合训练目标**：  
  - 结合SmoothL1回归损失与ListMLE排序损失，平衡绝对分数预测与歌曲间相对排序能力；  
  - 通过超参数α调控损失权重，优化多维度评分与头部歌曲识别。  

3)  
- **任务与效果**：  
  - 在ICASSP 2026 SongEval基准的五个美学维度评分任务中，线性相关系数提升至91.25%，排序相关系数达90.14%；  
  - 头部歌曲识别任务中，Top-Tier准确率提升至85.65%，显著优于基线模型。
</div>

</details>

---

## PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation
- **Authors**: Huadai Liu, Kaicheng Luo, Wen Wang, Qian Chen, Peiwen Sun, Rongjie Huang, Xiangang Li, Jieping Ye, Wei Xue
- **Categories**: cs.SD, cs.CV, eess.AS, eess.IV
- **arXiv**: [https://arxiv.org/abs/2511.18833v1](https://arxiv.org/abs/2511.18833v1)
- **PDF**: [https://arxiv.org/pdf/2511.18833v1](https://arxiv.org/pdf/2511.18833v1)

视频至音频生成技术需在语义一致性、视听时序同步性、听觉美学质量与空间准确性四个感知维度实现平衡，然而现有方法因目标纠缠问题而将相互冲突的优化目标混入单一损失函数，且缺乏对人类听觉偏好的对齐。本文提出PrismAudio——首个将强化学习融入V2A生成任务并配备专业化思维链规划框架的创新方案。我们通过解耦式思维链设计，将整体推理分解为四个专项模块（语义链、时序链、美学链与空间链），每个模块均配有针对性奖励函数。这种思维链与奖励的对应关系实现了多维度强化学习优化，引导模型联合生成跨视角的优质推理，在保持可解释性的同时有效解决目标纠缠问题。为实现高效优化，我们提出Fast-GRPO算法，采用混合常微分方程-随机微分方程采样策略，较现有GRPO实施方案显著降低训练开销。同时构建AudioCanvas基准数据集，其具备更均衡的分布特性和更贴近真实场景的多样性挑战，包含300个单事件类别与501个多事件样本。实验表明，PrismAudio在VGGSound域内测试集与AudioCanvas跨域基准上，于所有四个感知维度均达到最先进性能。项目页面详见https://PrismAudio-Project.github.io。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**
   - 视频到音频生成需平衡四个关键感知维度：语义一致性、视听时序同步、美学质量与空间准确性。
   - 现有方法存在目标纠缠问题，将竞争性目标混入单一损失函数，导致模型难以权衡不同目标（例如优化语义一致性可能牺牲美学质量）。
   - 缺乏人类偏好对齐机制，模型仅依赖文本匹配，生成结果虽技术正确但感知体验不佳。
   - 基于链式思维的模型（如ThinkSound）采用单一推理路径，无法针对性处理多维度任务，易产生多模态幻觉。

2. **论文核心方法如何解决上述问题**
   - **分解式链式思维模块**：将单一推理分解为四个专用模块：
     - 语义CoT：识别音频事件及其特征。
     - 时序CoT：确定音频事件顺序。
     - 美学CoT：评估音频自然度与保真度。
     - 空间CoT：分析声音方位与距离。
   - **多维度奖励函数**：每个CoT模块对应专用奖励信号：
     - 语义奖励：通过MS-CLAP评估内容相似性。
     - 时序奖励：通过Synchformer检测同步性。
     - 美学奖励：通过Meta Audiobox Aesthetics预测主观质量。
     - 空间奖励：通过StereoCRW验证方位准确性。
   - **高效强化学习优化**：
     - 提出Fast-GRPO算法，混合使用ODE（确定性采样）与SDE（随机采样）。
     - 仅在随机时间窗口内启用SDE探索，大幅降低训练开销（如将计算复杂度从T步降至w步）。
   - **模型架构增强**：
     - 采用VideoPrism视频编码器提升复杂场景理解。
     - 使用T5-Gemma文本编码器优化结构化推理处理。

3. **在哪些任务上取得了怎样的效果**
   - **VGGSound测试集（域内评估）**：
     - 在全部四个维度上达到最优性能：语义一致性（CLAP: 0.47）、时序同步性（DeSync: 0.41）、美学质量（MOS-Q: 4.21）与空间准确性（CRW误差: 7.72）。
   - **AudioCanvas基准（域外评估）**：
     - 在复杂多事件场景中表现鲁棒，显著优于基线模型（如ThinkSound的时序误差降低50%）。
     - 人类主观评估显示生成音频在质量与一致性上均接近真实音频。
   - **效率优势**：参数更少（518M），推理速度更快（0.63秒生成9秒音频）。
</div>

</details>

---

## First Deep Learning Approach to Hammering Acoustics for Stem Stability Assessment in Total Hip Arthroplasty
- **Authors**: Dongqi Zhu, Zhuwen Xu, Youyuan Chen, Minghao Jin, Wan Zheng, Yi Zhou, Huiwu Li, Yongyun Chang, Feng Hong, Zanjing Zhai
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.18725v1](https://arxiv.org/abs/2511.18725v1)
- **PDF**: [https://arxiv.org/pdf/2511.18725v1](https://arxiv.org/pdf/2511.18725v1)

音频事件分类技术近年来在医疗应用中展现出广阔前景。在全髋关节置换术中，术中敲击声学信号为评估股骨柄初始稳定性提供了关键依据，但受股骨形态、假体尺寸及手术技术等因素影响，传统评估方法存在局限性。本研究提出首个基于深度学习的解决方案，采用TimeMIL模型处理对数梅尔频谱特征，并结合伪标注技术进行增强。在术中录音数据集上，该方法取得91.17%±2.79%的准确率，实现了可靠的股骨柄稳定性评估。对比实验表明，减少股骨柄品牌多样性可提升模型性能，但数据规模不足仍是主要瓶颈。该研究证实基于深度学习的音频事件分类可作为全髋关节置换术中稳定性评估的有效手段。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：全髋关节置换术中，股骨柄的初始稳定性对手术成功至关重要。传统方法依赖外科医生通过敲击声和触觉反馈进行主观评估，易受股骨形态、植入物尺寸及手术技术等因素影响，导致评估结果不稳定。  
- **既有问题**：经验性判断可能引发固定不足（导致早期松动）或过度撞击（引发术中骨折），现有研究多基于传统统计方法，难以处理声音信号的复杂非线性特征。  

2)  
- **方法核心**：提出首个深度学习框架，通过以下步骤解决问题：  
  - **特征提取**：将术中敲击声转换为Log-Mel谱图，模拟人耳听觉感知，并辅以MFCC、GFCC等特征增强信息表达。  
  - **模型设计**：采用TimeMIL模型，结合时间感知的多示例学习池化与Transformer自注意力机制，捕捉声音的时序依赖关系。  
  - **半监督优化**：引入伪标注技术，利用未标注数据扩展训练集，提升模型对类别边界的判别能力。  
- **优势**：  
  - 自动学习非线性特征，减少对人工设计指标的依赖；  
  - 通过伪标注缓解数据稀缺问题，提高分类鲁棒性；  
  - 降低植入物品牌多样性可优化特征分布，进一步提升性能。  

3)  
- **任务与效果**：在股骨柄稳定性分类任务中，模型基于术中敲击声数据达到**91.17% ± 2.79%**的准确率。  
- **对比实验**表明：  
  - 减少植入物品牌数量（如仅保留两种品牌）可将准确率提升至**91.24%**；  
  - 伪标注策略使多数模型性能显著改善，验证了深度学习方法在该领域的可行性。
</div>

</details>

---

## Multimodal Real-Time Anomaly Detection and Industrial Applications
- **Authors**: Aman Verma, Keshav Samdani, Mohd. Samiuddin Shafi
- **Categories**: cs.SD, cs.AI, cs.CV, cs.LG, cs.MM
- **arXiv**: [https://arxiv.org/abs/2511.18698v1](https://arxiv.org/abs/2511.18698v1)
- **PDF**: [https://arxiv.org/pdf/2511.18698v1](https://arxiv.org/pdf/2511.18698v1)

本文系统阐述了一个集成同步视频与音频处理的多模态房间监控系统，其设计思路、实现方法及迭代演进过程。该系统具备实时活动识别与异常检测能力，我们呈现了两个版本的演进：初始轻量级版本采用YOLOv8目标检测、ByteTrack目标追踪与音频频谱变换器（AST），进阶版本则融合多模型音频集成、混合目标检测、双向跨模态注意力机制及多方法异常检测模块。迭代演进显著提升了系统精度、鲁棒性及工业适用性。进阶系统集成三大音频模型（AST、Wav2Vec2与HuBERT）实现全方位音频理解，结合双目标检测器（YOLO与DETR）提升检测精度，并通过精细化融合机制强化跨模态学习能力。实验验证表明，该系统在通用监控场景与工业安全专项应用中均表现优异，在标准硬件上实现实时运行的同时保持高精度检测性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**  
   - 智能监控系统需求增长，但传统单模态方法存在明显局限：  
     - 仅依赖视觉的系统会遗漏关键音频上下文（如异常声音）  
     - 仅依赖音频的系统缺乏空间理解能力  
   - 现有多模态方法多为简单特征拼接或后期融合，难以捕捉跨模态的深层关联与时序依赖  
   - 工业场景对实时性、鲁棒性和专业化检测提出更高要求，现有系统难以兼顾速度与精度  

2. **论文核心方法如何解决上述问题**  
   - **多模型音频集成**：  
     - 融合AST（通用音频分类）、Wav2Vec2（语音识别）与HuBERT（声学场景分析）三种模型  
     - 通过线性投影层将三者768维嵌入融合为统一256维表示，兼顾全面性与效率  
   - **混合目标检测系统**：  
     - 并行使用YOLO（快速检测）与DETR（高精度检测），通过跨检测器非极大值抑制消除重复框  
     - 在工业场景中平衡实时性与检测准确性  
   - **双向跨模态注意力机制**：  
     - 使用多头部交叉注意力，使视觉与音频特征相互增强（例如视觉辅助识别碰撞声音，声音引导视觉关注事件区域）  
     - 包含4层Transformer（8头注意力、256隐藏维），支持复杂跨模态学习  
   - **多方法异常检测**：  
     - 统计方法（基于像素Z值）快速检测视觉异常  
     - 自编码器重构误差识别复杂异常模式  
     - 音频能量与频谱质心分析检测声音异常  
     - 事件分类输出识别已知异常类别（如火灾、泄漏）  
     - 加权融合多方法得分，实现全面异常识别  

3. **在哪些任务上取得了怎样的效果**  
   - **任务范围**：  
     - 通用室内活动识别（静态/移动物体分类）  
     - 工业安全监控（机械操作、烟雾、火灾、泄漏、结构损坏检测）  
   - **效果**：  
     - 在多模态场景中实现实时异常检测（标准硬件可运行）  
     - 音频分类准确性显著提升（多模型集成优于单模型）  
     - 混合检测系统在速度与精度间取得平衡，适用于高要求工业环境  
     - 成功识别多种工业异常案例（如金属碰撞、喷射操作、火灾事件）
</div>

</details>

---
