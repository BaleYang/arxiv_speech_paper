---
layout: post
title: "arXiv Daily – 2025-10-16"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-16（UTC±）窗口内更新的论文中文译文：
- 时间窗口：最近 24 小时
- 抓取总数：3 篇 | 本页显示：3 篇（去重/过滤后）

## Closing the Gap Between Text and Speech Understanding in LLMs
- **Authors**: Santiago Cuervo, Skyler Seto, Maureen de Seyssel, Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly, Zakaria Aldeneh
- **Categories**: cs.CL, cs.AI, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13632v1](http://arxiv.org/abs/2510.13632v1)
- **PDF**: [http://arxiv.org/pdf/2510.13632v1](http://arxiv.org/pdf/2510.13632v1)

大型语言模型（LLMs）可以通过适应性调整，将其文本处理能力扩展到语音输入。然而，这些经过语音适应的LLMs在语言理解任务上的表现始终低于其文本基础的对应模型，甚至低于级联管道。我们将这种表现差距称为文本-语音理解差距：即当语音适应的LLM处理口语输入时，相较于原始文本基础的LLM处理等效文本时所观察到的性能下降。近期缩小这一差距的方法要么依赖于大规模的文本语音合成，这既成本高昂又严重依赖于合成数据，要么依赖于大规模的专有语音数据集，这些数据集无法复现。因此，迫切需要更高效的数据替代方案来缩小文本-语音理解差距。在本研究中，我们分析了导致这一差距的两个因素：（i）在适应过程中遗忘文本能力，以及（ii）语音与文本之间的跨模态不对齐。基于这一分析，我们提出了SALAD——通过主动选择和跨模态蒸馏实现样本高效对齐的方法，该方法结合了跨模态蒸馏与针对性合成数据，以改善对齐并减轻遗忘。应用于3B和7B LLMs，SALAD在知识、语言理解和推理等广泛领域基准测试中，展现出与强大的开放权重模型相当的竞争性能，同时训练所需的公共语音数据量减少了一个数量级以上。

---

## Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts
  Steering Module
- **Authors**: Ruitao Feng, Bixi Zhang, Sheng Liang, Zheng Yuan
- **Categories**: cs.SD, I.2.7
- **arXiv**: [http://arxiv.org/abs/2510.13558v1](http://arxiv.org/abs/2510.13558v1)
- **PDF**: [http://arxiv.org/pdf/2510.13558v1](http://arxiv.org/pdf/2510.13558v1)

将预训练的音频编码器与大型语言模型（LLMs）对齐，为构建强大的多模态智能体提供了一条有前景且参数高效的路径。然而，现有的方法通常需要昂贵的全模型微调，或依赖于可能缺乏表现力的静态适配器。受到柏拉图表征假说的启发，我们提出了SteerMoE，这是一种新颖且模块化的音频-语言对齐框架。SteerMoE冻结了音频编码器和LLM解码器，仅训练集成在编码器层中的轻量级引导模块。该模块使用混合专家（MoE）路由器动态选择和应用学习到的引导向量，逐步将连续音频表征转换为LLM可理解的空间。通过完全在连续嵌入空间中操作，我们的方法无需对LLM的词汇进行修改，并保留其高级推理和智能体能力。我们通过在自动语音识别（ASR）、音频理解和定性函数调用任务上的实验，展示了SteerMoE在保持高度模块化和计算效率的同时，取得了强劲的性能，为开发复杂的音频-语言系统提供了一种稳健的新范式。

---

## UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity
  MoE
- **Authors**: Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang
- **Categories**: cs.SD, cs.CL
- **arXiv**: [http://arxiv.org/abs/2510.13344v1](http://arxiv.org/abs/2510.13344v1)
- **PDF**: [http://arxiv.org/pdf/2510.13344v1](http://arxiv.org/pdf/2510.13344v1)

近期统一多模态模型的进展表明，全面内容生成正朝着明确的趋势发展。然而，听觉领域仍然面临重大挑战，音乐与语音的开发往往是孤立进行的，这阻碍了通用音频合成的进展。这种分离源于固有的任务冲突和严重的数据不平衡，妨碍了真正统一的音频生成模型的发展。为了解决这一挑战，我们提出了UniMoE-Audio，这是一个在新颖的动态容量混合专家（MoE）框架下的统一语音和音乐生成模型。

在架构上，UniMoE-Audio引入了一种Top-P路由策略，以动态分配专家数量，并采用混合专家设计，包含用于领域特定知识的路由专家、用于领域无关特征的共享专家，以及用于自适应计算跳过的空专家。为了解决数据不平衡问题，我们引入了三阶段的训练课程：1）独立专家训练利用原始数据集将领域特定知识灌输到每个“原型专家”中，避免干扰；2）MoE集成与预热将这些专家整合到UniMoE-Audio架构中，使用平衡数据集的子集对门控模块和共享专家进行预热；3）协同联合训练在完全平衡的数据集上对整个模型进行端到端训练，促进跨领域的协同效应。

大量实验表明，UniMoE-Audio不仅在主要的语音和音乐生成基准上实现了最先进的性能，还展现了卓越的协同学习能力，减轻了通常在简单联合训练中出现的性能下降。我们的研究结果突显了专业化MoE架构和精心设计的训练策略在推动通用音频生成领域发展中的巨大潜力。

---
