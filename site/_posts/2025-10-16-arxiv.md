---
layout: post
title: "arXiv Daily – 2025-10-16"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-16（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-15 08:50 — 2025-10-16 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## Closing the Gap Between Text and Speech Understanding in LLMs
- **Authors**: Santiago Cuervo, Skyler Seto, Maureen de Seyssel, Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly, Zakaria Aldeneh
- **Categories**: cs.CL, cs.AI, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13632v1](http://arxiv.org/abs/2510.13632v1)
- **PDF**: [http://arxiv.org/pdf/2510.13632v1](http://arxiv.org/pdf/2510.13632v1)

<<<<<<< HEAD
大型语言模型（LLMs）可以通过适应性调整，将其文本处理能力扩展到语音输入。然而，这些经过语音适应的LLMs在语言理解任务上的表现始终低于基于文本的模型，甚至低于级联管道。我们将这种表现差距称为文本-语音理解差距：即当语音适应的LLM处理口语输入时，相较于原始基于文本的LLM处理等效文本时所观察到的性能下降。近期缩小这一差距的方法要么依赖于大规模的文本语料库语音合成，这种方法成本高且严重依赖合成数据，要么依赖于大规模的专有语音数据集，这些数据集无法复现。因此，迫切需要更为数据高效的替代方案来缩小文本-语音理解差距。在本研究中，我们分析了导致这一差距的两个因素：（i）适应过程中对文本能力的遗忘，以及（ii）语音与文本之间的跨模态不对齐。基于这一分析，我们提出了SALAD——通过主动选择和跨模态蒸馏实现样本高效对齐的方法，该方法结合了跨模态蒸馏与针对性合成数据，以改善对齐并减轻遗忘。应用于3B和7B LLMs，SALAD在知识、语言理解和推理等广泛领域基准测试中，表现出与强大的开放权重模型相当的竞争力，同时训练所需的公共语料库语音数据量减少了一个数量级以上。
=======
大型语言模型（LLM）可通过适配扩展其文本能力至语音输入。然而，这类语音适配LLM在语言理解任务中始终表现逊于纯文本模型，甚至不及级联式处理流程。我们将此现象定义为“文本-语音理解鸿沟”：即当语音适配LLM处理语音输入时，相较于原始文本模型处理等效文本时出现的性能落差。现有缩小该鸿沟的方法或依赖大规模文本语料的语音合成（成本高昂且严重依赖合成数据），或采用不可复现的大规模专有语音数据集。因此，亟需开发数据效率更高的解决方案。本研究从两个驱动因素展开分析：（i）适配过程中文本能力的遗忘现象；（ii）语音与文本的跨模态失准。基于此，我们提出SALAD框架——通过主动选择与跨模态蒸馏实现样本高效对齐，该框架结合跨模态蒸馏与定向合成数据，在缓解能力遗忘的同时提升模态对齐效果。在30亿和70亿参数LLM上的实验表明，SALAD仅使用公共语料库中数量低一个量级的语音数据进行训练，即可在知识、语言理解及推理等广域基准测试中与强开源模型保持竞争力。

<details>
<summary>详细解读</summary>

1) **研究背景与既有方法的问题**  
・大语言模型（LLM）在语音输入适配后，其语言理解能力显著低于纯文本模型及级联流水线系统，形成“文本-语音理解鸿沟”。  
・现有方法依赖大规模语音合成或私有数据集，存在成本高、依赖合成数据、不可复现等问题，缺乏数据高效的解决方案。  

2) **论文核心方法如何解决上述问题**  
・**问题根源分析**：研究指出鸿沟主要由两个因素驱动：  
  - **遗忘问题**：语音适配过程中丢失文本预训练能力；  
  - **跨模态未对齐**：语音与文本输入导致模型输出不一致。  
・**提出SALAD方法**：结合跨模态蒸馏与主动数据选择，分两阶段训练：  
  - **阶段一（蒸馏训练）**：使用自然语音数据，通过蒸馏损失（以文本LLM为教师）优化模型，减少遗忘并提升跨模态对齐；  
  - **阶段二（主动选择）**：基于模型未对齐信号，从广域文本语料中动态选择少量样本合成语音，针对性扩充训练数据，进一步降低未对齐。  
・**优势**：  
  - 蒸馏目标直接优化对齐性，且缩放性优于最大似然训练；  
  - 主动选择以极低合成成本（仅1%数据）覆盖关键缺失领域，提升样本效率。  

3) **在哪些任务上取得了怎样的效果**  
・在**广域知识、语言理解与推理任务**（如StoryCloze、MMSU、OBQA、HellaSwag、ARC-C、PIQA）上评估：  
  - SALAD-3B/7B在多数任务中超越或持平现有语音适配LLM（如GLM-4-Voice、DiVA），且**数据用量减少超一个数量级**；  
  - 与最强基线Qwen2.5-Omni性能竞争，同时显著保留文本能力（遗忘程度最低）。  
・在科学类任务（如MMSU、OBQA）中，主动选择策略带来显著提升，验证其对领域覆盖不足问题的有效性。

</details>
>>>>>>> 88f8375 (update)

---

## Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts Steering Module
- **Authors**: Ruitao Feng, Bixi Zhang, Sheng Liang, Zheng Yuan
- **Categories**: cs.SD, I.2.7
- **arXiv**: [http://arxiv.org/abs/2510.13558v1](http://arxiv.org/abs/2510.13558v1)
- **PDF**: [http://arxiv.org/pdf/2510.13558v1](http://arxiv.org/pdf/2510.13558v1)

<<<<<<< HEAD
对预训练音频编码器和大型语言模型（LLMs）进行对齐，为构建强大的多模态智能体提供了一条有前景且参数高效的路径。然而，现有方法通常需要昂贵的全模型微调，或依赖于可能缺乏表现力的静态适配器。受到柏拉图表征假说的启发，我们提出了SteerMoE，这是一种新颖的模块化音频-语言对齐框架。SteerMoE冻结了音频编码器和LLM解码器，仅训练集成在编码器层中的轻量级引导模块。该模块使用混合专家（MoE）路由器动态选择和应用学习到的引导向量，逐步将连续音频表征转换为LLM可理解的空间。通过完全在连续嵌入空间中操作，我们的方法无需对LLM的词汇进行修改，并保留其先进的推理和智能体能力。我们通过在自动语音识别（ASR）、音频理解和定性函数调用任务上的实验，证明了SteerMoE在保持高度模块化和计算效率的同时，能够实现强劲的性能，为开发复杂的音频-语言系统提供了一种稳健的新范式。
=======
将预训练音频编码器与大语言模型对齐，为构建强大多模态智能体提供了一条参数高效的可行路径。然而现有方法通常需要昂贵的全模型微调，或依赖可能缺乏表达能力的静态适配器。受柏拉图表示假说启发，我们提出SteerMoE——一种新颖的模块化音频-语言对齐框架。该框架冻结音频编码器与LLM解码器，仅训练集成在编码器各层的轻量级导向模块。该模块采用混合专家路由机制动态选择并应用学习得到的导向向量，将连续音频表征逐步转化为LLM可理解的空间。由于全程在连续嵌入空间中进行操作，本方法无需修改LLM词表，完整保留其高级推理与智能体能力。通过在语音识别、音频理解及定性函数调用任务上的实验验证，SteerMoE在保持高度模块化与计算效率的同时实现了强劲性能，为开发先进音频-语言系统提供了稳健的新范式。

<details>
<summary>详细解读</summary>

1)  
・研究背景聚焦于音频与语言模型的对齐，现有方法主要分为两类：一是端到端全模型微调，计算成本高且可能损害大语言模型的推理能力；二是参数高效微调方法，如离散化音频为词汇或使用静态适配器，前者引入量化器导致信息损失，后者表达能力有限，难以处理复杂任务。  

2)  
・**核心方法**：SteerMoE 提出一种轻量级混合专家（MoE）导向模块，插入冻结的音频编码器各层中，动态学习并应用专家导向向量，逐步将连续音频表征转化为大语言模型可理解的空间。  
・**动态路由机制**：通过共享MoE路由器根据音频内容生成门控分数，动态选择专家向量组合，并加权调整隐藏状态，增强对齐的表达能力。  
・**连续提示对齐**：最终导向的音频特征经线性投影后作为软提示输入大语言模型，避免离散化损失信息，且无需修改模型词汇表。  
・**参数高效性**：仅训练导向模块与投影层，冻结音频编码器和大语言模型，显著降低计算需求，同时保持模型模块化与组件可替换性。  

3)  
・在**自动语音识别任务**（LibriSpeech、AISHELL-2）上，SteerMoE 取得竞争性词错误率，如Whisper编码器配置下WER为5.69%，Conformer编码器可达2.42%。  
・在**音频理解任务**（Clotho-AQA）中，模型平均准确率达52.35%，优于部分大规模多模态模型，展现其复杂推理能力。  
・定性实验验证其**智能体功能调用能力**，如通过语音成功触发天气查询工具，证明大语言模型原生能力得以完整保留。

</details>
>>>>>>> 88f8375 (update)

---

## UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE
- **Authors**: Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang
- **Categories**: cs.SD, cs.CL
- **arXiv**: [http://arxiv.org/abs/2510.13344v1](http://arxiv.org/abs/2510.13344v1)
- **PDF**: [http://arxiv.org/pdf/2510.13344v1](http://arxiv.org/pdf/2510.13344v1)

<<<<<<< HEAD
近年来，统一多模态模型的进展表明，全面内容生成的趋势日益明显。然而，在听觉领域，音乐和语音的开发往往是孤立进行的，这对通用音频合成的进展构成了重大挑战。这种分离源于固有的任务冲突和严重的数据不平衡，阻碍了真正统一的音频生成模型的发展。为了解决这一挑战，我们提出了UniMoE-Audio，这是一个在新颖的动态容量专家混合（MoE）框架下的统一语音和音乐生成模型。

在架构上，UniMoE-Audio引入了一种Top-P路由策略，以实现动态专家数量的分配，并采用混合专家设计，包括用于领域特定知识的路由专家、用于领域无关特征的共享专家，以及用于自适应计算跳过的空专家。为了解决数据不平衡问题，我们提出了一个三阶段的训练课程：1）独立专家训练利用原始数据集为每个“原型专家”灌输领域特定知识，而不受干扰；2）MoE集成与预热将这些专家整合到UniMoE-Audio架构中，使用平衡数据集的子集对门控模块和共享专家进行预热；3）协同联合训练在完全平衡的数据集上对整个模型进行端到端训练，促进跨领域的协同增强。

大量实验表明，UniMoE-Audio不仅在主要的语音和音乐生成基准测试中实现了最先进的性能，还展示了卓越的协同学习能力，减轻了通常在简单联合训练中出现的性能下降。我们的研究结果突显了专业化MoE架构和精心设计的训练策略在推动通用音频生成领域发展中的巨大潜力。
=======
当前多模态统一模型的发展呈现出全面内容生成的明确趋势，但听觉领域仍面临重大挑战——音乐与语音通常被独立开发，阻碍了通用音频合成的进展。这种割裂源于任务间的固有冲突与严重的数据失衡，制约了真正统一音频生成模型的发展。为解决这一难题，我们提出UniMoE-Audio：基于新型动态容量混合专家框架的统一语音音乐生成模型。在架构层面，该模型引入Top-P路由策略实现专家数量动态分配，并采用混合专家设计：路由专家负责领域特定知识，共享专家提取跨领域特征，空专家实现自适应计算跳过。针对数据失衡问题，我们设计了三阶段训练课程：1）独立专家训练阶段利用原始数据集为每个“原型专家”注入领域知识且避免相互干扰；2）MoE集成预热阶段将专家纳入统一架构，使用平衡数据子集对门控模块和共享专家进行预热；3）协同联合训练阶段在完全平衡数据集上端到端训练全模型，促进跨领域协同增效。大量实验表明，UniMoE-Audio不仅在主流语音音乐生成基准上达到最优性能，更展现出卓越的协同学习能力，有效缓解了简单联合训练中常见的性能退化现象。本研究证实了专业化MoE架构与精细化训练策略在推进通用音频生成领域的巨大潜力。项目主页：https://mukioxun.github.io/Uni-MoE-site/home.html

<details>
<summary>详细解读</summary>

1. **研究背景与既有方法的问题**
・**背景**：多模态统一模型在视觉与语言领域进展显著，但听觉领域仍面临挑战，语音与音乐生成长期独立发展，阻碍通用音频合成的实现。  
・**问题**：  
   - **任务冲突**：语音生成注重语义清晰度与说话人身份，音乐生成侧重和声、节奏等复杂结构，二者优化目标存在冲突。  
   - **数据失衡**：高质量语音数据远多于音乐数据，导致联合训练中语音任务主导学习过程，音乐生成质量显著下降。

2. **论文核心方法如何解决上述问题**  
・**动态容量MoE架构**：  
   - **Top-P路由策略**：根据输入复杂度动态分配专家数量，替代传统固定容量的Top-K路由，实现灵活计算资源分配。  
   - **混合专家设计**：  
     - **路由专家**：处理领域特定知识（如语音或音乐）；  
     - **共享专家**：持续激活以学习跨领域通用特征；  
     - **空专家**：自适应跳过计算，提升效率。  
・**三阶段训练策略**：  
   - **独立专家预训练**：利用原始不平衡数据分别训练语音与音乐“原型专家”，注入领域知识。  
   - **MoE集成与预热**：将专家整合至统一架构，通过平衡数据子集预热路由门与共享专家，稳定训练。  
   - **协同联合训练**：在完整平衡数据集上端到端训练全模型，促进跨领域知识迁移，避免任务 dominance。  
・**综合优势**：  
   - 动态路由与混合设计缓解任务冲突，确保专家专注特定领域；  
   - 分阶段训练策略克服数据失衡，避免音乐任务因数据稀缺而性能退化。

3. **在哪些任务上取得了怎样的效果**  
・**语音合成任务**：在SeedTTS、LibriSpeech、AISHELL-3等基准测试中，取得**SOTA或竞争性表现**，如UTMOS评分达4.36（SeedTTS-EN），且仅需280K小时数据，展现高效学习能力。  
・**音乐生成任务**：在MusicCaps、V2M-bench上，**美学质量指标（PC/PQ/CE）全面领先**，语义对齐（CLAP/CLaMP3）表现优异，证明其生成音乐兼具创造力与相关性。  
・**协同增益**：相比朴素联合训练基线（Unify-Baseline），**动态MoE架构显著缓解性能下降**，在数据稀缺任务（如视频到音乐）中仍保持鲁棒性。

</details>
>>>>>>> 88f8375 (update)

---

## Towards Multimodal Query-Based Spatial Audio Source Extraction
- **Authors**: Chenxin Yu, Hao Ma, Xu Li, Xiao-Lei Zhang, Mingjie Shao, Chi Zhang, Xuelong Li
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13308v1](http://arxiv.org/abs/2510.13308v1)
- **PDF**: [http://arxiv.org/pdf/2510.13308v1](http://arxiv.org/pdf/2510.13308v1)

<<<<<<< HEAD
基于查询的音频源提取旨在从混合信号中恢复目标源，依赖于特定的查询条件。现有方法主要局限于单通道音频，未充分利用多通道录音中的空间信息。我们提出了一种基于查询的空间音频源提取框架，用于从一阶环绕声（FOA）混合信号中恢复干净的目标信号。该方法可以接受音频提示或文本提示作为条件输入，实现灵活的端到端提取。我们提出模型的核心是一个三轴变换器，能够联合建模时间、频率和空间通道的依赖关系。该模型使用对比语言-音频预训练（CLAP）嵌入，通过特征线性调制（FiLM）实现统一的音频-文本条件化。为了消除昂贵的标注并提高模型的泛化能力，我们提出了一种无标签数据处理管道，动态生成空间混合信号及其对应目标用于训练。我们的实验结果显示出高分离质量，证明了多模态条件化和三轴建模的有效性。本研究为沉浸式应用中的高保真空间音频分离建立了新的范式。

---

## Two Heads Are Better Than One: Audio-Visual Speech Error Correction with
  Dual Hypotheses
=======
基于查询的音频源提取技术旨在通过查询条件从混合音频中分离目标声源。现有方法多局限于单声道音频，未能充分利用多声道录音中的空间信息。本文提出一种基于查询的空间音频源提取框架，可从一阶Ambisonics混合信号中还原干信号目标源。该方法支持音频提示或文本提示作为条件输入，实现灵活的端端提取。模型核心采用三轴Transformer架构，联合建模时域、频域与空间通道的依赖关系，通过对比语言-音频预训练嵌入特征，利用特征线性调制实现音频与文本条件的统一适配。为降低标注成本并提升泛化能力，我们设计了无标注数据流水线，动态生成空间混合音频及对应训练目标。实验结果表明，该方法在分离质量上表现优异，验证了多模态条件机制与三轴建模的有效性。本研究为沉浸式应用中的高保真空间音频分离建立了新范式。

<details>
<summary>详细解读</summary>

1)  
・现有方法主要局限于单通道音频，未能充分利用多通道录音中的空间信息。  
・传统空间滤波或波束成形方法在强混响环境下效果不佳，且多数模型依赖特定类别训练，泛化能力有限。  
・多模态提示的研究仍集中于单通道，缺乏对时空依赖的联合建模，限制了在复杂声学环境中的适用性。  

2)  
・**核心架构**：提出BSAST框架，基于一阶Ambisonics输入，通过三轴RoPE Transformer分别建模时间、频率和空间通道的依赖关系，有效整合多维信息。  
・**多模态查询机制**：利用CLAP嵌入统一处理音频或文本查询，通过FiLM层将语义条件注入模型，支持开放域目标源提取。  
・**频带分割策略**：将频谱划分为非重叠子带，增强对频域特性的捕捉，减少跨频段干扰。  
・**无标注训练**：动态生成空间混合数据，通过扰动CLAP嵌入模拟多模态查询，无需人工标注，提升训练多样性和泛化能力。  

3)  
・在DCASE 2025任务4的官方测试集上评估，使用完整FOA通道时，音频查询的SI-SDR达7.296 dB，文本查询达4.098 dB。  
・模型在混响和复杂声学环境中能高保真分离目标源，支持多任务如语音与事件分离，并通过可视化结果验证了提取的清晰度与准确性。

</details>

---

## Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses
>>>>>>> 88f8375 (update)
- **Authors**: Sungnyun Kim, Kangwook Jang, Sungwoo Cho, Joon Son Chung, Hoirin Kim, Se-Young Yun
- **Categories**: eess.AS, cs.CL, cs.LG
- **arXiv**: [http://arxiv.org/abs/2510.13281v1](http://arxiv.org/abs/2510.13281v1)
- **PDF**: [http://arxiv.org/pdf/2510.13281v1](http://arxiv.org/pdf/2510.13281v1)

<<<<<<< HEAD
本文提出了一种新的生成错误修正（GER）框架，用于音视频语音识别（AVSR），该框架直接在语言空间中对特定模态的证据进行推理。我们的框架DualHyp使大型语言模型（LLM）能够从独立的自动语音识别（ASR）和视觉语音识别（VSR）模型中组合出独立的N-best假设。为了最大化DualHyp的有效性，我们进一步引入了RelPrompt，这是一种噪声感知的引导机制，为LLM提供基于模态的提示。RelPrompt提供了每个模态流的时间可靠性，引导模型在ASR和VSR假设之间动态切换关注点，以实现准确的修正。在各种干扰场景下，我们的框架在LRS2基准测试中相较于标准ASR基线实现了高达57.7%的错误率提升，而单流GER方法仅实现了10%的提升。为了促进在DualHyp框架内的研究，我们在https://github.com/sungnyun/dualhyp上发布了包含ASR和VSR假设的代码和数据集。

---

## MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive
  Learning and Bar-Equivariant Contact-Aware Encoding
=======
本文提出了一种视听语音识别（AVSR）中的生成式纠错新范式，该框架直接在语言空间中对多模态证据进行推理。我们提出的DualHyp框架使大语言模型能够整合来自独立自动语音识别（ASR）与视觉语音识别（VSR）模型的N-best假设列表。为提升DualHyp效能，我们进一步设计RelPrompt——一种噪声感知引导机制，通过模态锚定提示为LLM提供各模态流的时间可靠性信息，从而动态调整对ASR与VSR假设的注意力分配以实现精准纠错。在多种噪声场景下的实验表明，该框架在LRS2数据集上相较基准ASR系统实现了57.7%的错误率提升，而单流生成式纠错方法仅能获得10%的增益。为促进DualHyp框架的研究，我们已在https://github.com/sungnyun/dualhyp 开源相关代码及包含ASR/VSR假设的数据集。

<details>
<summary>详细解读</summary>

1)  
**研究背景与既有方法的问题**  
・传统生成式纠错框架依赖单一音频流，在噪声环境下性能显著下降。  
・现有视听融合方法在特征层面整合多模态信息，易受跨模态污染影响，导致错误传播。  
・单流假设生成限制了纠错潜力，无法充分利用视觉模态的互补信息（如唇动抗噪特性）。  

2)  
**论文核心方法如何解决上述问题**  
**DualHyp框架设计**  
・采用双流假设生成：分别从独立的ASR和VSR模型获取N-best假设列表，保留模态特异性。  
・在语言空间进行延迟融合：LLM直接对文本假设进行组合推理，避免特征层融合的干扰。  
・通过双流假设集合$H_{dual} = H_{asr} \cup H_{vsr}$，为LLM提供更丰富的证据源。  

**RelPrompt噪声感知引导机制**  
・引入可靠性预测器：基于CNN对音频/视频分段生成质量标签（Clean/Noisy/Mixed）。  
・将可靠性序列$m_a, m_v$作为提示输入LLM，动态指导模型关注可靠模态：  
  $\hat{y} = \arg\max_y P(y|H_{dual}, m_a, m_v; \theta_{LLM})$  
・无需词级对齐，通过时间可靠性标记实现跨模态注意力切换。  

**技术优势**  
・模块化结构支持即插即用ASR/VSR模型，避免端到端系统重训练成本。  
・语言层融合规避跨模态污染，充分发挥LLM的组合推理能力。  
・可靠性引导使模型在极端噪声条件下仍能保持稳健性能。  

3)  
**在哪些任务上取得了怎样的效果**  
・在LRS2基准测试中：  
  - 音频噪声场景下相对基线提升48.8%，视觉噪声场景提升57.7%。  
  - 在语音噪声（0dB）条件下，WER从基线25.8%降至13.2%。  
・多语言任务（MuAViC数据集）：在西班牙语、意大利语等语言上显著优于单流方法。  
・扩展性验证：使用更大规模LLM（Llama-3.2）可进一步提升至12.3% WER。

</details>

---

## MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding
>>>>>>> 88f8375 (update)
- **Authors**: Xuanchen Wang, Heng Wang, Weidong Cai
- **Categories**: cs.SD, cs.AI, cs.MM
- **arXiv**: [http://arxiv.org/abs/2510.13244v1](http://arxiv.org/abs/2510.13244v1)
- **PDF**: [http://arxiv.org/pdf/2510.13244v1](http://arxiv.org/pdf/2510.13244v1)

<<<<<<< HEAD
音乐既是一种听觉现象，也是一种具身现象，与人类运动密切相关，并通过舞蹈自然表达。然而，大多数现有的音频表示方法忽视了这一具身维度，限制了它们捕捉驱动运动的节奏和结构线索的能力。我们提出了MotionBeat，一个用于运动对齐音乐表示学习的框架。MotionBeat通过两个新提出的目标进行训练：具身对比损失（ECL），这是一种增强的InfoNCE公式，结合了节奏感知和节拍抖动的负样本，以实现细粒度的节奏区分；结构节奏对齐损失（SRAL），确保通过将音乐重音与相应的运动事件对齐来保持节奏一致性。在架构上，MotionBeat引入了小节等变相位旋转，以捕捉循环节奏模式，并采用接触引导注意力，以强调与音乐重音同步的运动事件。实验表明，MotionBeat在音乐到舞蹈生成任务中优于现有的音频编码器，并有效迁移到节拍跟踪、音乐标记、流派和乐器分类、情感识别以及音视频检索等任务。我们的项目演示页面：https://motionbeat2025.github.io/。

---

## Acoustic Teleportation via Disentangled Neural Audio Codec
  Representations
=======
音乐不仅是听觉现象，更是具身化体验，其与人体运动紧密关联并通过舞蹈自然呈现。然而现有音频表征大多忽视这一具身维度，限制了其对驱动动作的节奏与结构线索的捕捉能力。本文提出MotionBeat——一种运动对齐的音乐表征学习框架，通过两项创新目标进行训练：具身对比损失（ECL）通过融入节奏感知负样本与节拍抖动负样本的增强型InfoNCE框架实现细粒度节奏判别；结构节奏对齐损失（SRAL）通过对齐音乐重音与对应运动事件确保节奏一致性。在架构层面，MotionBeat引入小节等变性相位旋转以捕捉循环节奏模式，并采用接触引导注意力机制强化与音乐重音同步的运动事件。实验表明，MotionBeat在音乐到舞蹈生成任务中超越主流音频编码器，并能有效迁移至节拍追踪、音乐标签、流派与乐器分类、情感识别及音视频检索等任务。项目演示页面：https://motionbeat2025.github.io/。

<details>
<summary>详细解读</summary>

1)  
・研究背景：音乐与人体运动紧密相关，但现有音频表征模型（如音频-文本或音频-视觉对齐方法）忽略了音乐的具身维度，导致节奏与运动错位。  
・既有问题：这些模型依赖全局声学或语义特征，无法精细捕捉驱动舞蹈的节奏与结构线索，限制了在音乐到舞蹈生成等任务中的效果。

2)  
・**训练目标创新**：  
  - **具身对比损失（ECL）**：扩展InfoNCE，引入节奏敏感负样本（如速度感知和节拍抖动负样本），迫使模型区分细微节奏差异，增强节拍级判别能力。  
  - **结构节奏对齐损失（SRAL）**：结合节拍级（Soft-DTW对齐音频起始与运动接触）和节拍级（Earth Mover‘s Distance对齐音频重音与运动能量分布）对齐，确保全局节奏结构一致性。  
・**架构设计创新**：  
  - **节拍等变相位旋转**：对嵌入施加循环旋转变换，使表征对节拍起始点具有鲁棒性，建模节奏的周期性。  
  - **接触引导注意力**：基于接触概率加权注意力分数，强化与音乐重音同步的运动事件，增强音频-运动耦合。  
・**整体优化**：联合ECL与SRAL目标，通过轻量级编码器输出具身对齐的音乐嵌入，解决节奏-运动错位问题。

3)  
・**舞蹈生成**：在AIST++数据集上取得最优效果，表现为更高的节拍对齐分数（BAS↑）、运动多样性（Distk/g↑）和物理合理性（PFC↓）。  
・**识别任务**：在节拍追踪（GTZAN）、音乐标签（MTT）、流派与乐器分类（GTZAN/NSynth）、情感识别（Emomusic）及跨模态检索（AIST++）中均超越基线模型，证明其表征具备通用性与节奏感知优势。

</details>

---

## Acoustic Teleportation via Disentangled Neural Audio Codec Representations
>>>>>>> 88f8375 (update)
- **Authors**: Philipp Grundhuber, Mhd Modar Halimeh, Emanuël A. P. Habets
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13221v1](http://arxiv.org/abs/2510.13221v1)
- **PDF**: [http://arxiv.org/pdf/2510.13221v1](http://arxiv.org/pdf/2510.13221v1)

<<<<<<< HEAD
本文提出了一种通过在神经音频编解码器表示中解耦语音内容与声学环境特征来实现声学传送的方法。声学传送能够在保留内容和说话者身份的同时，转移语音录音的房间特征。我们基于之前的工作，采用EnCodec架构，取得了显著的客观质量提升，ScoreQ分数达到3.03，相较于先前方法的2.44。我们的训练策略包括五个任务：清晰重建、混响重建、去混响以及两种声学传送的变体。我们展示了声学嵌入的时间下采样显著降低了性能，即使是2倍下采样也导致质量的统计显著下降。学习到的声学嵌入与RT60之间表现出强相关性。通过t-SNE聚类分析有效地证明了解耦的有效性，其中声学嵌入按房间聚类，而语音嵌入则按说话者聚类。
=======
本文提出一种基于神经音频编解码器表征的声学传送方法，通过解耦语音内容与声学环境特征实现跨录音的声场特性迁移。该方法在保持语音内容与说话人身份的同时，可精准转移不同录音间的房间声学特征。我们在EnCodec架构基础上进行改进，采用包含五项任务的训练策略：纯净重建、混响重建、去混响处理以及两种声学传送变体。客观质量评估显示，本方法获得3.03分的非侵入式ScoreQ评分，较先前方法的2.44分实现显著提升。实验表明，对声学嵌入进行时间下采样会严重损害性能，即使2倍下采样也会导致质量显著下降。学习得到的声学嵌入与RT60值呈现强相关性，t-SNE聚类分析验证了解耦有效性：声学嵌入按房间聚类，而语音嵌入按说话人聚类。

<details>
<summary>详细解读</summary>

1. **研究背景与既有方法的问题**  
・传统音频编解码器依赖信号处理模块，而神经音频编解码器（NACs）虽在压缩效率和解码质量上表现更优，但其潜在表示通常混合了语音内容与声学环境信息，难以独立操控。  
・现有方法（如Omran等人）尝试在嵌入空间中分离语音与环境特征，但存在输出质量受限、可听伪影明显等问题，且对声学嵌入进行时间下采样会进一步损害性能。

2. **论文核心方法如何解决上述问题**  
・**模型架构改进**：基于EnCodec构建NAC，将128维输出分为各64维的语音嵌入和声学嵌入，分别通过独立残差向量量化（RVQ）编码，避免信息混淆。  
・**多任务训练策略**：引入五种任务提升解耦能力与泛化性：  
　- 干净重建（CR）与混响重建（RR）：确保基础重构质量；  
　- 去混响（DR）：通过置零声学嵌入实现；  
　- 声学传送（AT-SS/AT-DS）：交换不同输入的声学嵌入，验证内容与声学特征的分离效果。  
・**嵌入空间优化**：取消声学嵌入的时间下采样（原方法下采样10×），并为两类嵌入分配相等码率，显著减少信息损失。  
・**量化与性能平衡**：实验表明8个量化器即可接近最优性能，过多量化器收益有限；同时证实声学嵌入与物理参数RT60强相关，增强可解释性。

3. **在哪些任务上取得了怎样的效果**  
・**声学传送任务**：在ScoreQ非侵入式评估中达到3.03分（原方法仅2.44），成功将房间声学特性跨录音转移，同时保持语音内容与说话人身份。  
・**去混响任务**：ScoreQ得分达3.62，有效还原无混响语音。  
・**解耦验证**：t-SNE聚类显示声学嵌入按房间聚类、语音嵌入按说话人聚类，证明特征分离有效性；声学嵌入与RT60相关系数最高达0.93。

</details>
>>>>>>> 88f8375 (update)

---
