---
layout: post
title: "arXiv Daily – 2025-10-16"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-16（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-15 08:50 — 2025-10-16 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## Closing the Gap Between Text and Speech Understanding in LLMs
- **Authors**: Santiago Cuervo, Skyler Seto, Maureen de Seyssel, Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly, Zakaria Aldeneh
- **Categories**: cs.CL, cs.AI, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13632v1](http://arxiv.org/abs/2510.13632v1)
- **PDF**: [http://arxiv.org/pdf/2510.13632v1](http://arxiv.org/pdf/2510.13632v1)

大型语言模型（LLMs）可以通过适应性调整，将其文本处理能力扩展到语音输入。然而，这些经过语音适应的LLMs在语言理解任务上的表现始终低于基于文本的模型，甚至低于级联管道。我们将这种表现差距称为文本-语音理解差距：即当语音适应的LLM处理口语输入时，相较于原始基于文本的LLM处理等效文本时所观察到的性能下降。近期缩小这一差距的方法要么依赖于大规模的文本语料库语音合成，这种方法成本高且严重依赖合成数据，要么依赖于大规模的专有语音数据集，这些数据集无法复现。因此，迫切需要更为数据高效的替代方案来缩小文本-语音理解差距。在本研究中，我们分析了导致这一差距的两个因素：（i）适应过程中对文本能力的遗忘，以及（ii）语音与文本之间的跨模态不对齐。基于这一分析，我们提出了SALAD——通过主动选择和跨模态蒸馏实现样本高效对齐的方法，该方法结合了跨模态蒸馏与针对性合成数据，以改善对齐并减轻遗忘。应用于3B和7B LLMs，SALAD在知识、语言理解和推理等广泛领域基准测试中，表现出与强大的开放权重模型相当的竞争力，同时训练所需的公共语料库语音数据量减少了一个数量级以上。

---

## Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts
  Steering Module
- **Authors**: Ruitao Feng, Bixi Zhang, Sheng Liang, Zheng Yuan
- **Categories**: cs.SD, I.2.7
- **arXiv**: [http://arxiv.org/abs/2510.13558v1](http://arxiv.org/abs/2510.13558v1)
- **PDF**: [http://arxiv.org/pdf/2510.13558v1](http://arxiv.org/pdf/2510.13558v1)

对预训练音频编码器和大型语言模型（LLMs）进行对齐，为构建强大的多模态智能体提供了一条有前景且参数高效的路径。然而，现有方法通常需要昂贵的全模型微调，或依赖于可能缺乏表现力的静态适配器。受到柏拉图表征假说的启发，我们提出了SteerMoE，这是一种新颖的模块化音频-语言对齐框架。SteerMoE冻结了音频编码器和LLM解码器，仅训练集成在编码器层中的轻量级引导模块。该模块使用混合专家（MoE）路由器动态选择和应用学习到的引导向量，逐步将连续音频表征转换为LLM可理解的空间。通过完全在连续嵌入空间中操作，我们的方法无需对LLM的词汇进行修改，并保留其先进的推理和智能体能力。我们通过在自动语音识别（ASR）、音频理解和定性函数调用任务上的实验，证明了SteerMoE在保持高度模块化和计算效率的同时，能够实现强劲的性能，为开发复杂的音频-语言系统提供了一种稳健的新范式。

---

## UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity
  MoE
- **Authors**: Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang
- **Categories**: cs.SD, cs.CL
- **arXiv**: [http://arxiv.org/abs/2510.13344v1](http://arxiv.org/abs/2510.13344v1)
- **PDF**: [http://arxiv.org/pdf/2510.13344v1](http://arxiv.org/pdf/2510.13344v1)

近年来，统一多模态模型的进展表明，全面内容生成的趋势日益明显。然而，在听觉领域，音乐和语音的开发往往是孤立进行的，这对通用音频合成的进展构成了重大挑战。这种分离源于固有的任务冲突和严重的数据不平衡，阻碍了真正统一的音频生成模型的发展。为了解决这一挑战，我们提出了UniMoE-Audio，这是一个在新颖的动态容量专家混合（MoE）框架下的统一语音和音乐生成模型。

在架构上，UniMoE-Audio引入了一种Top-P路由策略，以实现动态专家数量的分配，并采用混合专家设计，包括用于领域特定知识的路由专家、用于领域无关特征的共享专家，以及用于自适应计算跳过的空专家。为了解决数据不平衡问题，我们提出了一个三阶段的训练课程：1）独立专家训练利用原始数据集为每个“原型专家”灌输领域特定知识，而不受干扰；2）MoE集成与预热将这些专家整合到UniMoE-Audio架构中，使用平衡数据集的子集对门控模块和共享专家进行预热；3）协同联合训练在完全平衡的数据集上对整个模型进行端到端训练，促进跨领域的协同增强。

大量实验表明，UniMoE-Audio不仅在主要的语音和音乐生成基准测试中实现了最先进的性能，还展示了卓越的协同学习能力，减轻了通常在简单联合训练中出现的性能下降。我们的研究结果突显了专业化MoE架构和精心设计的训练策略在推动通用音频生成领域发展中的巨大潜力。

---

## Towards Multimodal Query-Based Spatial Audio Source Extraction
- **Authors**: Chenxin Yu, Hao Ma, Xu Li, Xiao-Lei Zhang, Mingjie Shao, Chi Zhang, Xuelong Li
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13308v1](http://arxiv.org/abs/2510.13308v1)
- **PDF**: [http://arxiv.org/pdf/2510.13308v1](http://arxiv.org/pdf/2510.13308v1)

基于查询的音频源提取旨在从混合信号中恢复目标源，依赖于特定的查询条件。现有方法主要局限于单通道音频，未充分利用多通道录音中的空间信息。我们提出了一种基于查询的空间音频源提取框架，用于从一阶环绕声（FOA）混合信号中恢复干净的目标信号。该方法可以接受音频提示或文本提示作为条件输入，实现灵活的端到端提取。我们提出模型的核心是一个三轴变换器，能够联合建模时间、频率和空间通道的依赖关系。该模型使用对比语言-音频预训练（CLAP）嵌入，通过特征线性调制（FiLM）实现统一的音频-文本条件化。为了消除昂贵的标注并提高模型的泛化能力，我们提出了一种无标签数据处理管道，动态生成空间混合信号及其对应目标用于训练。我们的实验结果显示出高分离质量，证明了多模态条件化和三轴建模的有效性。本研究为沉浸式应用中的高保真空间音频分离建立了新的范式。

---

## Two Heads Are Better Than One: Audio-Visual Speech Error Correction with
  Dual Hypotheses
- **Authors**: Sungnyun Kim, Kangwook Jang, Sungwoo Cho, Joon Son Chung, Hoirin Kim, Se-Young Yun
- **Categories**: eess.AS, cs.CL, cs.LG
- **arXiv**: [http://arxiv.org/abs/2510.13281v1](http://arxiv.org/abs/2510.13281v1)
- **PDF**: [http://arxiv.org/pdf/2510.13281v1](http://arxiv.org/pdf/2510.13281v1)

本文提出了一种新的生成错误修正（GER）框架，用于音视频语音识别（AVSR），该框架直接在语言空间中对特定模态的证据进行推理。我们的框架DualHyp使大型语言模型（LLM）能够从独立的自动语音识别（ASR）和视觉语音识别（VSR）模型中组合出独立的N-best假设。为了最大化DualHyp的有效性，我们进一步引入了RelPrompt，这是一种噪声感知的引导机制，为LLM提供基于模态的提示。RelPrompt提供了每个模态流的时间可靠性，引导模型在ASR和VSR假设之间动态切换关注点，以实现准确的修正。在各种干扰场景下，我们的框架在LRS2基准测试中相较于标准ASR基线实现了高达57.7%的错误率提升，而单流GER方法仅实现了10%的提升。为了促进在DualHyp框架内的研究，我们在https://github.com/sungnyun/dualhyp上发布了包含ASR和VSR假设的代码和数据集。

---

## MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive
  Learning and Bar-Equivariant Contact-Aware Encoding
- **Authors**: Xuanchen Wang, Heng Wang, Weidong Cai
- **Categories**: cs.SD, cs.AI, cs.MM
- **arXiv**: [http://arxiv.org/abs/2510.13244v1](http://arxiv.org/abs/2510.13244v1)
- **PDF**: [http://arxiv.org/pdf/2510.13244v1](http://arxiv.org/pdf/2510.13244v1)

音乐既是一种听觉现象，也是一种具身现象，与人类运动密切相关，并通过舞蹈自然表达。然而，大多数现有的音频表示方法忽视了这一具身维度，限制了它们捕捉驱动运动的节奏和结构线索的能力。我们提出了MotionBeat，一个用于运动对齐音乐表示学习的框架。MotionBeat通过两个新提出的目标进行训练：具身对比损失（ECL），这是一种增强的InfoNCE公式，结合了节奏感知和节拍抖动的负样本，以实现细粒度的节奏区分；结构节奏对齐损失（SRAL），确保通过将音乐重音与相应的运动事件对齐来保持节奏一致性。在架构上，MotionBeat引入了小节等变相位旋转，以捕捉循环节奏模式，并采用接触引导注意力，以强调与音乐重音同步的运动事件。实验表明，MotionBeat在音乐到舞蹈生成任务中优于现有的音频编码器，并有效迁移到节拍跟踪、音乐标记、流派和乐器分类、情感识别以及音视频检索等任务。我们的项目演示页面：https://motionbeat2025.github.io/。

---

## Acoustic Teleportation via Disentangled Neural Audio Codec
  Representations
- **Authors**: Philipp Grundhuber, Mhd Modar Halimeh, Emanuël A. P. Habets
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13221v1](http://arxiv.org/abs/2510.13221v1)
- **PDF**: [http://arxiv.org/pdf/2510.13221v1](http://arxiv.org/pdf/2510.13221v1)

本文提出了一种通过在神经音频编解码器表示中解耦语音内容与声学环境特征来实现声学传送的方法。声学传送能够在保留内容和说话者身份的同时，转移语音录音的房间特征。我们基于之前的工作，采用EnCodec架构，取得了显著的客观质量提升，ScoreQ分数达到3.03，相较于先前方法的2.44。我们的训练策略包括五个任务：清晰重建、混响重建、去混响以及两种声学传送的变体。我们展示了声学嵌入的时间下采样显著降低了性能，即使是2倍下采样也导致质量的统计显著下降。学习到的声学嵌入与RT60之间表现出强相关性。通过t-SNE聚类分析有效地证明了解耦的有效性，其中声学嵌入按房间聚类，而语音嵌入则按说话者聚类。

---
