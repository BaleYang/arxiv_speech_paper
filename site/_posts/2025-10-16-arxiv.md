---
layout: post
title: "arXiv Daily – 2025-10-16"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-16（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-15 08:50 — 2025-10-16 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## Closing the Gap Between Text and Speech Understanding in LLMs
- **Authors**: Santiago Cuervo, Skyler Seto, Maureen de Seyssel, Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly, Zakaria Aldeneh
- **Categories**: cs.CL, cs.AI, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13632v1](http://arxiv.org/abs/2510.13632v1)
- **PDF**: [http://arxiv.org/pdf/2510.13632v1](http://arxiv.org/pdf/2510.13632v1)

大型语言模型（LLM）可通过适配使其文本能力扩展至语音输入。然而，这些语音适配的LLM在语言理解任务中始终表现逊于纯文本模型，甚至不及级联式处理流程。我们将这种性能落差定义为“文本-语音理解鸿沟”：即当语音适配LLM处理语音输入时，相较于原始文本模型处理等效文本时出现的性能下降。现有缩小该鸿沟的方法要么依赖大规模文本语料的语音合成（成本高昂且严重依赖合成数据），要么采用不可复现的大规模专有语音数据集。因此，亟需开发数据效率更高的解决方案。本研究从两个驱动因素展开分析：（i）适配过程中文本能力的遗忘现象；（ii）语音与文本的跨模态失准。基于此，我们提出SALAD方法——通过主动选择与跨模态蒸馏实现样本高效对齐，该方法结合跨模态蒸馏与定向合成数据，在缓解能力遗忘的同时提升模态对齐效果。在30亿和70亿参数LLM上的实验表明，SALAD仅使用公共语料库中不足十分之一的语音数据训练，即可在知识、语言理解与推理等广域基准测试中与强开源模型实现竞争性表现。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- 语音适配大语言模型在语言理解任务上表现不及纯文本模型及级联系统，存在“文本-语音理解差距”。
- 现有方法依赖大规模语音合成或专有数据集，成本高且不可复现，缺乏数据高效方案。

2) **论文核心方法如何解决上述问题**
- **SALAD方法**结合跨模态蒸馏与主动数据选择，分两阶段训练：
  - **阶段一**：在自然语音数据上使用蒸馏目标，使语音适配模型输出与文本骨干模型对齐，减少遗忘并提升跨模态一致性。
  - **阶段二**：通过主动学习算法，基于模型在文本语料各簇的未对齐程度，选择少量关键文本合成语音，针对性扩展训练数据域覆盖。
- **优势**：
  - 蒸馏目标直接优化跨模态对齐，抑制领域偏移导致的遗忘。
  - 主动选择以极低合成成本（仅1%数据）弥补自然语音数据域狭窄问题。
  - 整体方案在3B/7B模型上实现高效训练，数据用量较基线减少一个数量级。

3) **在哪些任务上取得了怎样的效果**
- 在StoryCloze、MMSU、OpenBookQA等6项广域知识推理任务上评估：
  - SALAD-3B/7B在语音输入准确率上超越多数基线，与最强模型Qwen2.5-Omni表现相当。
  - 文本-语音理解差距显著缩小（如SALAD-3B平均差距仅4.6%），且文本能力保留最佳（平均遗忘差距-0.5%）。
</div>

</details>

---

## Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts Steering Module
- **Authors**: Ruitao Feng, Bixi Zhang, Sheng Liang, Zheng Yuan
- **Categories**: cs.SD, I.2.7
- **arXiv**: [http://arxiv.org/abs/2510.13558v1](http://arxiv.org/abs/2510.13558v1)
- **PDF**: [http://arxiv.org/pdf/2510.13558v1](http://arxiv.org/pdf/2510.13558v1)

将预训练音频编码器与大语言模型（LLM）对齐，为构建强大多模态智能体提供了一条参数高效的可行路径。然而，现有方法往往需要昂贵的全模型微调，或依赖可能缺乏表达能力的静态适配器。受柏拉图表示假说启发，我们提出SteerMoE——一种模块化的音频-语言对齐框架。该框架冻结音频编码器与LLM解码器，仅训练集成在编码器各层中的轻量级导向模块。该模块通过混合专家路由机制动态选择并应用习得的导向向量，将连续音频表征逐步转化为LLM可理解的空间。由于全程在连续嵌入空间中进行操作，本方法无需修改LLM词表，完整保留其高级推理与智能体能力。通过在语音识别、音频理解及定性函数调用任务上的实验验证，SteerMoE在保持高度模块化与计算效率的同时实现了优异性能，为开发先进音频-语言系统提供了稳健的新范式。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频与语言模型对齐是构建多模态智能体的关键，现有方法主要分为两类。  
- **既有方法问题**：  
  - 端到端全模型微调计算成本高，且可能损害大语言模型的推理能力。  
  - 参数高效微调方法中，离散化音频为符号序列会引入量化器、造成信息损失；静态适配器表达能力有限，难以处理复杂任务。  

2)  
- **核心方法**：提出SteerMoE框架，通过混合专家模块动态对齐音频与语言表示。  
- **解决思路**：  
  - **动态路由机制**：在冻结的音频编码器各层插入轻量级MoE模块，根据输入内容选择专家向量，逐步调整音频表示。  
  - **连续空间操作**：避免离散化，直接映射音频特征至大语言模型嵌入空间，保留信号完整性。  
  - **模块化设计**：仅训练MoE路由、专家向量和投影层，参数效率高，支持编码器与大语言模型灵活替换。  
- **优势**：  
  - 增强对齐表达能力，优于静态适配器。  
  - 无需修改大语言模型词汇表，保持其原生推理与工具调用能力。  

3)  
- **任务与效果**：  
  - **自动语音识别**：在LibriSpeech和AISHELL-2数据集上，词错误率与字符错误率接近全微调模型（如Whisper-large-v3），验证对齐有效性。  
  - **音频理解**：在Clotho-AQA问答任务中，准确率优于部分大规模多模态模型，展现复杂推理能力。  
  - **工具调用**：定性实验表明，模型能通过语音成功触发大语言模型预定义函数，保留智能体能力。
</div>

</details>

---

## UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE
- **Authors**: Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang
- **Categories**: cs.SD, cs.CL
- **arXiv**: [http://arxiv.org/abs/2510.13344v1](http://arxiv.org/abs/2510.13344v1)
- **PDF**: [http://arxiv.org/pdf/2510.13344v1](http://arxiv.org/pdf/2510.13344v1)

当前多模态统一模型的发展呈现出全面内容生成的明确趋势，但听觉领域仍面临重大挑战：音乐与语音通常被独立开发，阻碍了通用音频合成的进展。这种割裂源于任务间的固有冲突与严重的数据失衡，制约了真正统一音频生成模型的发展。为解决该问题，我们提出UniMoE-Audio——基于新型动态容量混合专家框架的统一语音与音乐生成模型。在架构层面，我们引入Top-P路由策略实现动态专家数量分配，并设计混合专家组件：路由专家承载领域专属知识，共享专家提取跨领域特征，空置专家支持自适应计算跳过。针对数据失衡问题，我们设计了三阶段训练课程：1）独立专家预训练阶段利用原始数据集为各“原型专家”注入领域知识且避免相互干扰；2）MoE集成预热阶段将专家纳入统一架构，采用平衡数据子集对门控模块与共享专家进行预热；3）协同联合训练阶段基于完整平衡数据集端到端训练整体模型，促进跨领域协同增效。大量实验表明，UniMoE-Audio不仅在主流语音与音乐生成基准中达到最优性能，更展现出卓越的协同学习能力，有效缓解了传统联合训练中常见的性能退化现象。本研究证实了专业化MoE架构与精细化训练策略在推进通用音频生成领域的巨大潜力。项目主页：https://mukioxun.github.io/Uni-MoE-site/home.html

<details>
<summary>详细解读</summary>

<div markdown="1">
1. **研究背景与既有方法的问题**
- **任务冲突**：语音生成注重语义清晰度和说话人身份，而音乐生成关注和声、节奏等复杂结构，两者优化目标不同，导致共享模型训练时相互干扰。
- **数据不平衡**：高质量语音数据远多于音乐数据，直接联合训练会使语音任务主导学习，导致音乐生成质量显著下降。
- **现有方法局限**：传统统一音频模型（如UniAudio）采用简单联合训练，无法解决上述问题，性能受限。

2. **论文核心方法如何解决上述问题**
- **动态容量MoE架构**：
  - **Top-P路由策略**：根据输入复杂度动态分配专家数量，避免静态Top-K的资源浪费或不足。
  - **混合专家设计**：
    - 路由专家：处理领域特定知识（如语音或音乐）。
    - 共享专家：捕获跨领域通用特征。
    - 空专家：自适应跳过计算，提升效率。
- **三阶段训练课程**：
  - **独立专家训练**：利用原始不平衡数据分别训练语音和音乐“原型专家”，注入领域知识。
  - **MoE集成与预热**：将专家集成到统一架构，使用平衡数据子集预热路由器和共享专家，确保稳定性。
  - **协同联合训练**：在完整平衡数据集上端到端训练，促进跨领域知识迁移，避免任务主导。
- **综合效果**：通过架构和训练策略结合，有效缓解任务冲突与数据不平衡，实现语音和音乐生成的协同优化。

3. **在哪些任务上取得了怎样的效果**
- **语音合成任务**：
  - 在SeedTTS、LibriSpeech等基准测试中，取得最优或竞争性表现，如UTMOS评分达4.36（SOTA），WER低至1.9，数据效率显著。
- **音乐生成任务**：
  - 在Text-to-Music和Video-to-Music任务中，美学质量指标（PC、PQ、CE）全面领先，语义对齐（CLAP、CLaMP3）表现优异。
- **整体优势**：相比基线模型，UniMoE-Audio在资源稀缺任务（如V2M）上表现稳健，验证了方法在跨域统一生成中的有效性。
</div>

</details>

---

## Towards Multimodal Query-Based Spatial Audio Source Extraction
- **Authors**: Chenxin Yu, Hao Ma, Xu Li, Xiao-Lei Zhang, Mingjie Shao, Chi Zhang, Xuelong Li
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13308v1](http://arxiv.org/abs/2510.13308v1)
- **PDF**: [http://arxiv.org/pdf/2510.13308v1](http://arxiv.org/pdf/2510.13308v1)

基于查询的音频源提取技术旨在通过查询条件从混合音频中分离目标声源。现有方法多局限于单声道音频，未能充分利用多声道录音中的空间信息。本文提出一种基于查询的空间音频源提取框架，可从一阶Ambisonics混合信号中还原干信号目标源。该方法支持音频提示或文本提示作为条件输入，实现灵活的端端提取。模型核心采用三轴Transformer架构，联合建模时域、频域与空间通道的依赖关系，通过对比语言-音频预训练嵌入特征，利用特征线性调制实现音频-文本条件的统一适配。为降低标注成本并提升泛化能力，我们设计了无标注数据流水线，动态生成空间混合信号及对应训练目标。实验结果表明，该方法在分离质量上表现优异，验证了多模态条件机制与三轴建模的有效性。本研究为沉浸式应用中的高保真空间音频分离建立了新范式。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：空间音频源分离在沉浸式媒体、AR/VR等领域需求增长，需从多通道混音中恢复目标声源并利用空间线索。  
- **既有问题**：  
  - 现有方法多局限于单通道音频，未充分挖掘多通道记录中的空间信息。  
  - 传统方法依赖固定类别训练，泛化性差；空间混响强时，波束成形等技术效果受限。  
  - 多模态查询研究仍限于单通道，缺乏空间建模能力。  

2)  
- **核心方法**：提出BSAST框架，基于一阶Ambisonics输入，通过三轴Transformer联合建模时间、频率和空间通道依赖关系。  
- **关键技术**：  
  - **频带分割编码器**：将频谱划分为非重叠子带，提取频域局部特征。  
  - **多模态查询条件**：使用CLAP嵌入统一音频和文本查询，通过FiLM层动态调制特征。  
  - **三轴RoPE Transformer**：沿时间、频率和空间通道维度依次应用注意力，编码相对位置信息。  
  - **无标签训练**：动态生成空间混音数据，通过扰动CLAP嵌入模拟多模态查询，无需人工标注。  
- **解决思路**：  
  - 显式整合空间线索与频谱表示，提升混响环境下的分离鲁棒性。  
  - 支持开放域查询，突破固定类别限制；无标注管道降低数据依赖。  

3)  
- **任务**：在DCASE 2025任务4数据集上评估，涵盖多种声源、混响和噪声场景。  
- **效果**：  
  - 全FOA通道输入时，音频查询的SI-SDR达7.296 dB，文本查询达4.098 dB，显著优于单通道基线。  
  - 模型深度增加（如8层Transformer）持续提升性能，显示强扩展性。  
  - 无标签训练下，文本查询仍具竞争力，验证多模态条件泛化能力。
</div>

</details>

---

## Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses
- **Authors**: Sungnyun Kim, Kangwook Jang, Sungwoo Cho, Joon Son Chung, Hoirin Kim, Se-Young Yun
- **Categories**: eess.AS, cs.CL, cs.LG
- **arXiv**: [http://arxiv.org/abs/2510.13281v1](http://arxiv.org/abs/2510.13281v1)
- **PDF**: [http://arxiv.org/pdf/2510.13281v1](http://arxiv.org/pdf/2510.13281v1)

本文提出了一种视听语音识别（AVSR）中的生成式纠错新范式，该框架直接在语言空间中对多模态证据进行推理。我们提出的DualHyp框架使大语言模型能够整合来自独立自动语音识别（ASR）与视觉语音识别（VSR）模型的N-best假设。为最大化DualHyp效能，我们进一步提出RelPrompt——一种噪声感知引导机制，通过模态锚定提示为LLM提供各模态流的时间可靠性指导，使其能动态切换ASR与VSR假设的注意力以实现精准纠错。在多种失真场景下的实验表明，本框架在LRS2基准上相较标准ASR基线实现了57.7%的错误率提升，而单流生成式纠错方法仅获得10%提升。为促进DualHyp框架的研究，我们已在https://github.com/sungnyun/dualhyp 开源相关代码及包含ASR/VSR假设的数据集。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频-视觉语音识别（AVSR）在噪声环境下性能显著下降，传统生成式纠错（GER）方法依赖单一音频流假设，易受声学失真影响。  
- **既有方法问题**：  
  - 单流GER框架仅基于自动语音识别（ASR）的N-best假设，在低信噪比条件下纠错能力受限。  
  - 现有AVSR方法通过特征级融合（如视觉适配器或多模态编码器）整合音频和视觉信息，但易受跨模态污染，导致噪声从一个模态传播到统一表示中。  

2)  
- **核心方法**：提出DualHyp框架，通过以下机制解决上述问题：  
  - **双流假设生成**：使用独立的ASR和视觉语音识别（VSR）模型分别生成N-best假设列表，保留模态特异性信息。  
  - **语言空间融合**：大型语言模型（LLM）直接对文本假设进行组合推理，避免早期特征融合的跨模态干扰，利用其深层上下文理解能力纠正错误。  
  - **噪声感知引导（RelPrompt）**：  
    - 通过轻量预测器评估音频和视频流的时序可靠性，生成Clean/Noisy/Mixed标记序列。  
    - 将这些可靠性标记作为提示输入LLM，动态引导模型在ASR和VSR假设间切换焦点，优先依赖更可靠的模态证据。  
- **优势**：  
  - 模块化设计支持即插即用ASR/VSR模型，无需重新训练整个系统。  
  - 延迟融合策略避免模态污染，显著提升在联合噪声场景下的鲁棒性。  

3)  
- **任务与效果**：  
  - 在LRS2和LRS3基准测试中，针对多种音频-视觉联合噪声场景（如语音干扰、视觉遮挡），DualHyp + RelPrompt相比ASR基线（Whisper）降低错误率高达57.7%，而单流GER方法仅提升约10%。  
  - 在多语言AVSR任务（MuAViC数据集）中，在三种语言上超越基线，证明其跨语言泛化能力。  
  - 定性分析显示，模型能通过片段组合或主导模态优化机制有效纠正同音词和缺失片段。
</div>

</details>

---

## MotionBeat: Motion-Aligned Music Representation via Embodied Contrastive Learning and Bar-Equivariant Contact-Aware Encoding
- **Authors**: Xuanchen Wang, Heng Wang, Weidong Cai
- **Categories**: cs.SD, cs.AI, cs.MM
- **arXiv**: [http://arxiv.org/abs/2510.13244v1](http://arxiv.org/abs/2510.13244v1)
- **PDF**: [http://arxiv.org/pdf/2510.13244v1](http://arxiv.org/pdf/2510.13244v1)

音乐不仅是听觉现象，更是具身化体验，其与人体运动紧密关联，并常通过舞蹈自然呈现。然而现有音频表征大多忽略这一具身维度，导致难以捕捉驱动动作的节奏与结构线索。本文提出MotionBeat——一种运动对齐的音乐表征学习框架，通过两项创新目标进行训练：具身对比损失（ECL）在InfoNCE框架中引入节奏感知负样本与节拍抖动负样本，实现细粒度节奏判别；结构节奏对齐损失（SRAL）通过对齐音乐重音与对应动作事件确保节奏一致性。在架构层面，MotionBeat采用小节等变性相位旋转捕捉循环节奏模式，并通过接触引导注意力机制强化与音乐重音同步的动作事件。实验表明，MotionBeat在音乐驱动舞蹈生成任务上超越主流音频编码器，并能有效迁移至节拍追踪、音乐标签分类、流派与乐器识别、情感识别及音视频检索等任务。项目演示页面：https://motionbeat2025.github.io/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐与人体运动紧密相关，但现有音频表示模型（如音频-文本或音频-视觉对齐方法）忽略这一“具身”维度，导致无法有效捕捉驱动运动的节奏和结构线索。  
- **既有方法问题**：依赖全局声学或语义特征，缺乏细粒度节奏对齐能力，在音乐到舞蹈生成等任务中易出现节奏-运动失准，限制了模型在节奏敏感应用中的效果。  

2)  
- **核心方法**：MotionBeat通过新型训练目标和架构设计解决上述问题。  
  - **训练目标**：  
    - **具身对比损失（ECL）**：扩展InfoNCE，引入节奏敏感负样本（如速度感知和节拍抖动负样本），强制模型区分细粒度节奏差异。  
    - **结构节奏对齐损失（SRAL）**：结合Soft-DTW和地球移动距离，在节拍和节栏级别对齐音频起始点与运动接触事件，确保全局节奏一致性。  
  - **架构创新**：  
    - **节栏等变相位旋转**：对嵌入施加循环旋转变换，使表示对节栏起始点变化具有鲁棒性，捕捉周期性节奏模式。  
    - **接触引导注意力**：基于接触概率加权注意力分数，突出与音乐重音同步的运动事件，加强音频-运动耦合。  
- **整合机制**：总损失函数结合ECL和SRAL，通过双编码器结构提取节拍同步的音频和运动特征，最终学习到运动对齐的音乐表示。  

3)  
- **任务与效果**：  
  - **舞蹈生成**：在AIST++数据集上，生成的运动更具节奏对齐性（Beat Alignment Score提升）、物理合理性（Physical Foot Contact得分降低）和多样性（运动特征分布更广）。  
  - **识别任务**：在节拍跟踪、音乐标签、流派/乐器分类、情感识别和跨模态检索中，均超越基线模型（如wav2vec 2.0、CLAP），例如在节拍跟踪F1分数和跨模态检索Recall@1上取得最优结果。  
  - **泛化性**：证明运动对齐表示可有效迁移至多类下游任务，提升节奏和语义感知能力。
</div>

</details>

---

## Acoustic Teleportation via Disentangled Neural Audio Codec Representations
- **Authors**: Philipp Grundhuber, Mhd Modar Halimeh, Emanuël A. P. Habets
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.13221v1](http://arxiv.org/abs/2510.13221v1)
- **PDF**: [http://arxiv.org/pdf/2510.13221v1](http://arxiv.org/pdf/2510.13221v1)

本文提出一种基于神经音频编解码器表征的声学传送方法，通过解耦语音内容与声学环境特征实现跨录音的声场特性迁移。该方法在保持语音内容与说话人身份不变的前提下，将不同录音中的房间声学特性进行转移。我们在EnCodec架构基础上进行改进，客观质量评测中非侵入式ScoreQ得分达到3.03分，较原有方法的2.44分实现显著提升。训练策略包含五项任务：纯净重建、混响重建、去混响处理以及两种声学传送变体。实验表明，对声学嵌入进行时间下采样会严重损害性能，即使2倍下采样也会导致质量显著下降。学习得到的声学嵌入与RT60值呈现强相关性，t-SNE聚类分析验证了解耦有效性：声学嵌入按房间聚类，而语音嵌入按说话人聚类。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：传统音频编解码器依赖信号处理模块，而神经音频编解码器（NACs）在压缩效率和重建质量上表现更优。现有方法尝试在潜在空间中解耦语音内容与环境特征，但存在输出质量受限和可听伪影的问题。  
- **既有方法问题**：  
  - 如Omran等人的方法对声学嵌入进行时间下采样，导致信息损失和性能下降。  
  - 解耦不彻底，语音与声学特征相互干扰，影响任务如去混响和声学传输的精度。  

2)  
- **核心方法**：基于EnCodec架构，设计解耦的神经音频编解码器，将语音内容与声学环境特征分离为独立嵌入。  
  - **模型结构**：  
    - 使用128维输出，其中64维分配给语音嵌入，64维分配给声学嵌入。  
    - 通过两个独立的残差向量量化器分别量化语音和声学特征，避免信息混合。  
  - **训练策略**：  
    - 包含五类任务：干净重建、混响重建、去混响、同源声学传输和异源声学传输。  
    - 通过嵌入交换（如声学嵌入置零或跨样本替换）强制模型学习解耦表示。  
  - **改进点**：  
    - 取消声学嵌入的时间下采样，保留完整时序信息，提升重建质量。  
    - 多任务训练增强模型泛化能力，解决既有方法在复杂任务中的性能瓶颈。  

3)  
- **任务与效果**：  
  - **声学传输**：成功将房间特性（如RT60）在不同语音间转移，ScoreQ非侵入式评分达3.03（基线为2.44）。  
  - **去混响**：通过声学嵌入置零实现，ScoreQ评分达3.62，优于基线。  
  - **解耦验证**：t-SNE聚类显示声学嵌入按房间聚类，语音嵌入按说话人聚类，证明有效分离。  
  - **客观指标**：ViSQOL和ScoreQ显示模型在重建、传输任务中均显著提升质量。
</div>

</details>

---
