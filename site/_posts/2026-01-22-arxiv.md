---
layout: post
title: "arXiv Daily – 2026-01-22"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-22（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-21 08:50 — 2026-01-22 08:50
- 抓取总数：18 篇 | 本页显示：18 篇（去重/过滤后）

## WeDefense: A Toolkit to Defend Against Fake Audio
- **Authors**: Lin Zhang, Johan Rohdin, Xin Wang, Junyi Peng, Tianchi Liu, You Zhang, Hieu-Thi Luong, Shuai Wang, Chengdong Liang, Anna Silnova, Nicholas Evans
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15240v1](https://arxiv.org/abs/2601.15240v1)
- **PDF**: [https://arxiv.org/pdf/2601.15240v1](https://arxiv.org/pdf/2601.15240v1)

随着生成式人工智能的进步，合成音频已能达到与真实音频在感知上难以区分的水平。尽管这一显著进展催生了许多积极应用，但也带来了冒用身份、传播虚假信息和实施欺诈等滥用风险。尽管已有大量挑战和倡议发布了开源伪造音频检测代码，但多数仅针对特定竞赛、数据集或模型定制。目前尚缺乏一个标准化、统一的工具包，能够不仅通过通用数据库、协议和指标，还借助共享代码库，支持对各类解决方案进行公平的基准测试与比较。为此，我们提出了WeDefense——首个支持伪造音频检测与定位的开源工具包。除模型训练外，WeDefense还强调以下关键但常被忽视的组成部分：灵活的输入与数据增强、校准、分数融合、标准化评估指标，以及用于深入理解和结果分析的工具。该工具包已在https://github.com/zlin0/wedefense公开提供，并包含伪造音频检测与定位的交互式演示。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：生成式AI的进步使得合成音频能以假乱真，带来了冒用、虚假信息和欺诈等风险。  
- **既有方法的问题**：现有开源检测代码多为特定竞赛、数据集或模型定制，缺乏标准化、统一的工具包。代码结构各异，通用性和灵活性不足，且多数仅支持检测，缺乏对局部伪造音频定位的支持。现有语音处理工具包（如Kaldi、ESPnet）并非专为伪造音频设计，缺少变长输入、针对性数据增强等关键功能。

2)  
WeDefense通过以下设计解决上述问题：  
- **首个专用于伪造音频检测与定位的开源工具包**：不局限于特定数据集，易于适配，集成了主流模型并支持扩展。  
- **跨模型与数据集的统一设计**：整合了以往分散的代码配置，提供通用、可维护的结构。  
- **针对性功能设计**：  
  - 支持变长输入，避免固定长度裁剪可能丢失信息（尤其对局部伪造音频关键）。  
  - 集成专用于伪造音频检测的数据增强（如RawBoost、编解码器模拟）和声学特征（如Fbank、LFCC）。  
  - 包含多种模型（CNN、SSL基模型等）和损失函数（如OC-softmax），并兼容WeSpeaker中的说话人验证模型。  
- **支持分析与后处理**：集成校准、分数融合、嵌入可视化（UMAP）和可解释AI（Grad-CAM）工具，以深入评估和理解模型行为。  
- **模块化与部署就绪**：采用模块化接口便于扩展，支持通过TorchScript导出模型，提供预训练模型和C++部署示例。

3)  
- **任务与效果**：  
  - **伪造音频检测**：在PartialSpoof和ASVspoof5数据集上评估，SSL基模型（如SSL-MHFA）显著优于传统CNN模型。在PartialSpoof上EER最低达0.802%，与文献最佳结果相当；在ASVspoof5上EER最低为8.083%。  
  - **伪造音频定位**：在PartialSpoof上支持基于均匀分割的帧级定位，集成SSL-BAM等方法，但任务仍具挑战性（EER最低12.13%）。  
- **附加功能验证**：数据增强（如MUSAN+RIR）和校准（LR基）能提升性能；分数融合可缓解跨数据集性能下降。
</div>

</details>

---

## WavLink: Compact Audio--Text Embeddings with a Global Whisper Token
- **Authors**: Gokul Karthik Kumar, Ludovick Lepauloux, Hakim Hacid
- **Categories**: cs.SD, cs.CL, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.15118v1](https://arxiv.org/abs/2601.15118v1)
- **PDF**: [https://arxiv.org/pdf/2601.15118v1](https://arxiv.org/pdf/2601.15118v1)

Whisper已成为大型音频-语言模型中提取通用音频特征的事实编码器，通常将30秒音频片段表示为1500帧特征并投影至大语言模型。相比之下，基于CLAP的音频-文本嵌入模型主要依赖其他音频编码器（如HTS-AT、PaSST），未能有效利用Whisper。本文提出WavLink——一种紧凑型音频-文本嵌入模型，通过在Whisper编码器中引入可学习的全局标记，并与文本编码器联合训练。通过对预训练文本编码器、损失函数、训练模式及数据混合方案等设计选择进行系统研究，我们确定了能够实现最先进检索性能的配置方案。针对三种模型规模设计的两阶段训练策略，结合嵌套式监督方法，提升了模型扩展性，可在性能损失最小的情况下实现8倍压缩的嵌入表示。WavLink在AIR-Bench的多选题和零样本分类任务中也展现出竞争优势。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频-文本联合表示学习是音频理解的核心任务。现有方法存在“方法鸿沟”：音频大语言模型普遍采用Whisper作为音频编码器，但其输出为1500个帧级特征，计算和存储成本高；而音频-文本嵌入模型则主要依赖HTS-AT、PaSST等专用编码器，未能有效利用Whisper的通用音频特征能力。

2)  
论文提出 **WavLink** 模型，通过以下核心设计解决上述问题：
- **架构创新**：在Whisper编码器中引入一个可学习的全局令牌，作为内容自适应的聚合器。它将30秒音频的约1500个帧级特征聚合为单个紧凑表示，显著降低了存储和相似性搜索成本。
- **系统化设计探索**：通过消融实验，系统比较了文本编码器、损失函数、适应策略和更新范围。最终确定最佳配置为：使用CLIP文本编码器、CLIP对比损失、对音频和文本编码器进行全微调。
- **两阶段训练与可扩展性**：采用两阶段训练策略，并引入 **Matryoshka监督**。该技术训练模型同时输出多个嵌套维度的嵌入，使得嵌入维度可压缩至原始大小的1/8而性能损失极小，实现了出色的可扩展性。
- **核心优势**：该方法首次证明，基于Whisper ASR预训练特征，通过一个全局令牌即可构建高效的音频-文本嵌入，弥合了帧级音频大模型与紧凑嵌入模型之间的效率差距。

3)  
WavLink在多个任务上取得了优异效果：
- **音频-文本检索**：在AudioCaps和Clotho基准测试中，其大型模型在R@1等指标上超越所有先前的CLAP变体，提升约2-6个百分点。
- **零样本分类**：在VGGSound上达到最先进准确率；在ESC-50和US8K上表现具有竞争力。
- **多项选择问答**：在AIR-Bench基准测试中，其Base模型（8400万参数）性能与数十倍大的音频大语言模型（如Falcon3-Audio）相当，并在语音、声音和音乐分类任务上表现强劲。
- **可扩展性**：嵌入维度压缩8倍后，平均性能下降小于1个百分点，极大提升了存储和计算效率。
</div>

</details>

---

## Neural Tracking of Sustained Attention, Attention Switching, and Natural Conversation in Audiovisual Environments using Mobile EEG
- **Authors**: Johanna Wilroth, Oskar Keding, Martin A. Skoglund, Maria Sandsten, Martin Enqvist, Emina Alickovic
- **Categories**: eess.SP, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15097v1](https://arxiv.org/abs/2601.15097v1)
- **PDF**: [https://arxiv.org/pdf/2601.15097v1](https://arxiv.org/pdf/2601.15097v1)

日常交流具有动态性和多感官特性，常涉及注意力转移、重叠语音及视觉线索。然而，现有神经注意力追踪研究大多局限于高度控制的实验室环境，使用纯净且通常仅为音频的刺激材料，并要求被试持续关注单一说话者。本研究通过引入包含24名听力正常参与者的新型数据集来填补这一空白。我们采用移动脑电图（EEG）系统（44个头皮电极和20个cEEGrid电极），在视听（AV）范式下设置了三种实验条件：在双人对话环境中持续关注单一说话者、在两位说话者间切换注意力，以及包含竞争性单人说话者的非脚本化双人对话。分析方法包括时域响应函数建模、最优延迟分析、决策窗口为1.1秒至35秒的选择性注意力分类，以及对视听对话与仅含侧边音频说话者的时域响应函数比较。关键发现表明，在不同实验条件下，头皮EEG所关注的语音与忽略的语音在注意力相关的P2峰值上存在显著差异。注意力切换与持续注意条件下的性能无显著变化，说明系统对注意力转移具有鲁棒性。最优延迟分析显示，相较于单人视听刺激，对话条件下的峰值更窄，反映了多说话者处理的额外复杂性。头皮EEG数据对选择性注意的分类准确率持续高于随机水平（55%-70%），而cEEGrid数据的相关性较低，凸显了方法学进一步改进的必要性。这些结果表明，移动EEG能够可靠地追踪动态多感官听觉场景中的选择性注意力，并为未来视听范式设计和实际注意力追踪应用提供了指导。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：日常交流是动态、多感官的，涉及注意力转移、重叠语音和视觉线索。然而，现有神经注意力追踪研究大多局限于高度控制的实验室环境，使用单一、干净的音频刺激，要求持续关注单一说话者，缺乏生态效度。  
- **既有问题**：  
  - 过度依赖干净、受控的音频刺激，未反映真实多感官交流的复杂性。  
  - 对注意力动态切换（如多说话者间切换）的神经机制理解有限。  
  - 现有脑电图系统笨重，难以应用于真实场景；便携式系统（如cEEGrid）在复杂任务中的性能尚未充分验证。

2)  
- **核心方法**：本研究设计了一个新颖的视听实验范式，使用移动脑电图系统（44个头皮电极和20个cEEGrid电极），在三种条件下采集数据：  
  - **持续注意力**：在双说话者环境中持续关注单一说话者。  
  - **注意力切换**：在两名说话者之间切换注意力。  
  - **自然对话**：关注未脚本的双人对话，同时存在竞争性单说话者干扰。  
- **解决思路**：  
  - **提升生态效度**：采用自然视听刺激，包含视觉线索和重叠语音，模拟真实交流场景。  
  - **捕捉动态注意力**：通过包含切换任务和对话任务，直接研究注意力转移和多说话者处理中的神经机制。  
  - **增强实用性**：使用便携式移动脑电图，同时记录头皮和耳周电极数据，评估其在复杂环境中的适用性。  
- **分析技术**：  
  - 使用时域响应函数建模神经追踪，分析TRF波形特征（如P2峰）。  
  - 通过最优滞后分析探究神经响应的时间特性。  
  - 基于相关性的分类方法，在不同时间窗口下解码选择性注意力。  
  - 比较视听对话与单说话者刺激的TRF差异。

3)  
- **任务与效果**：  
  - **持续注意力任务**：头皮脑电图显示，注意力相关P2峰在关注与忽略语音间存在显著差异，分类准确率在1.1秒至35秒窗口下均高于随机水平（55-70%）。  
  - **注意力切换任务**：性能与持续注意力任务相当，表明方法对注意力动态切换具有鲁棒性。  
  - **自然对话任务**：成功追踪对话中的注意力，但TRF波形更宽，反映了多说话者处理的复杂性；分类准确率略低于单说话者任务，但仍显著高于随机水平。  
- **局限性**：cEEGrid电极的性能较弱，相关性较低，需进一步方法改进。
</div>

</details>

---

## Bangla Music Genre Classification Using Bidirectional LSTMS
- **Authors**: Muntakimur Rahaman, Md Mahmudul Hoque, Md Mehedi Hassain
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.15083v1](https://arxiv.org/abs/2601.15083v1)
- **PDF**: [https://arxiv.org/pdf/2601.15083v1](https://arxiv.org/pdf/2601.15083v1)

孟加拉音乐拥有丰富的音乐文化。当前，由于数字与实体格式音乐的指数级增长，音乐流派分类变得尤为重要。为提升检索效率，对音乐进行相应索引十分必要。在庞大而多样的音乐库中，自动对孟加拉音乐进行流派分类是高效定位特定作品的关键。现有的流派分类方法主要采用传统机器学习或深度学习技术。本研究引入了一个新颖的音乐数据集，涵盖十种不同的孟加拉音乐流派。针对音频分类任务，我们采用循环神经网络（RNN）架构，具体使用长短期记忆网络（LSTM）进行模型训练与分类。特征提取是音频数据处理的基础阶段，本研究利用梅尔频率倒谱系数（MFCC）将原始音频波形转换为紧凑且具有代表性的特征集。所提出的框架基于这些提取的特征实现音乐流派分类。实验结果显示分类准确率达到78%，表明该系统在优化和简化孟加拉音乐流派组织方面具有显著潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语音乐因其丰富的古典与现代风格而独具特色，但针对其流派自动分类的研究相对有限。随着数字音乐数量的激增，高效的流派分类对于音乐库索引、检索和推荐系统至关重要。  
- **既有方法的问题**：现有方法主要依赖传统机器学习（如SVM、KNN）或部分深度学习模型。这些方法往往难以充分捕捉音乐数据中复杂的时序依赖关系，且在处理孟加拉语音乐独特的声学与语言特征时效果有限。

2)  
- **核心方法**：本文提出一种基于双向长短期记忆网络（Bi-LSTM）的深度学习框架，用于孟加拉语音乐流派分类。  
- **特征提取**：使用梅尔频率倒谱系数（MFCCs）及其一阶差分（ΔMFCCs）和色度特征，将原始音频波形转换为紧凑的、具有代表性的特征序列，以有效表征音色、时序变化和谐波内容。  
- **模型架构**：  
  - 采用双向LSTM层，能够同时从前向和后向处理时序特征，从而捕捉音乐中跨越多个时间步的节奏（如塔拉循环）和旋律模式。  
  - 模型结合了LSTM的遗忘门、输入门和输出门机制，缓解了梯度消失问题，并能建模长期依赖关系。  
  - 在输出层，通过全连接层和Softmax函数生成流派概率预测。  
- **创新点**：  
  - 针对孟加拉语音乐，在特征输入中尝试整合从歌词中提取的音素特征，以结合语言特性对音乐分类的影响。  
  - 该双向结构能同时建模音乐元素的渐进演变以及歌词与器乐成分之间的双向关联，提升了分类的上下文理解能力。

3)  
- **任务**：在包含10个孟加拉语音乐流派（如Bangla嘻哈、金属、摇滚、拉里吉提、民间音乐等）的自建数据集上进行流派分类。  
- **效果**：  
  - 所提出的Bi-LSTM模型达到了78%的整体分类准确率，优于对比的多种模型（如SVM 21%，K-NN 45%，ANN 72%，LSTM 74%）。  
  - 在具体流派上表现差异较大：例如“Lalon Geet”的F1分数高达0.91，而“Folk”仅为0.04，表明模型对某些流派的区分能力仍有提升空间。
</div>

</details>

---

## VCNAC: A Variable-Channel Neural Audio Codec for Mono, Stereo, and Surround Sound
- **Authors**: Florian Grötschla, Arunasish Sen, Alessandro Lombardi, Guillermo Cámbara, Andreas Schwarz
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14960v1](https://arxiv.org/abs/2601.14960v1)
- **PDF**: [https://arxiv.org/pdf/2601.14960v1](https://arxiv.org/pdf/2601.14960v1)

本文提出VCNAC——一种可变通道神经音频编解码器。该方法采用单一编码器-解码器参数化架构，能够原生支持从单声道语音到影院级5.1声道环绕声等多种通道配置的推理。通过通道兼容性目标设计，确保多声道内容在解码为较少声道时仍保持感知质量。共享表征机制使得生成式语言模型仅需基于单一码本集合进行训练，同时支持跨模态与通道配置的推理时扩展。通过客观空间音频指标与主观听感测试评估表明，该统一方法在单声道、立体声及环绕声配置下均能保持高质量重建效果。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经音频编解码器广泛应用于语音合成与理解，但需处理单声道、立体声和5.1环绕声等多种声道配置。  
- **既有方法问题**：  
  - 现有编解码器（如EnCodec）采用固定声道架构，仅能处理预设声道数。  
  - 需为不同配置训练独立编解码器（各具独立潜在空间），或统一按最大声道数处理，导致计算效率低下。  
  - 现有多声道方法通常仅适配双声道，且无法灵活泛化至变声道需求。

2)  
论文提出VCNAC，一种可变声道神经音频编解码器，通过统一架构动态处理不同声道数，核心方法如下：  
- **可变声道处理流程**：  
  - **并行流处理**：每个输入声道通过权重共享的卷积流独立处理，仅实例化实际存在的声道，避免计算冗余。  
  - **融合与量化**：在编码器末端将各声道嵌入相加，融合为统一瓶颈表示，再经残差向量量化（RVQ）生成与声道配置无关的共享潜在空间。  
  - **分离重建**：解码时将反量化表示复制为目标声道数，通过权重共享的转置卷积重建各声道音频。  
- **跨声道注意力机制**：  
  - 在融合前与分离后引入轻量级Transformer音频自编码器块，在时间维度交错不同声道嵌入，使注意力能同时捕获时序与跨声道依赖关系。  
- **损失函数设计**：  
  - 除多尺度梅尔谱重建损失和判别器损失外，针对立体声和环绕声额外计算中/侧（mid/side）分解损失，以及遵循ITU-R标准的缩混损失，确保声道兼容性与空间特性保留。  
- **架构优势**：  
  - 单一模型参数化支持从单声道到5.1环绕声的本地推理，共享码本使生成式语言模型可在统一词汇表上训练，并支持推理时跨声道配置扩展。

3)  
- **任务与效果**：  
  - **单声道语音**（LibriTTS）：在7.9 kbit/s比特率下，PESQ（4.16）和SI-SDR（11.3 dB）优于对比编解码器（如DAC、EnCodec）。  
  - **立体声音乐**（FMA-small）：在相同比特率下，SI-SDR（9.1 dB）和梅尔谱距离（0.453）达到最佳或接近最佳性能，空间指标（ΔIPD/ΔILD）与最优方法相当。  
  - **5.1环绕声**（开源电影片段）：在7.9 kbit/s比特率下，主观MUSHRA测试显示感知质量与更高比特率的SNAC等相当，且缩混内容保持良好质量；客观指标中前声道重建质量最优，空间特性保留良好。  
- **总体**：VCNAC以约一半比特率实现了与现有先进编解码器相当的感知质量，统一支持多声道配置。
</div>

</details>

---

## Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali
- **Authors**: Nouhoum Coulibaly, Ousmane Ly, Michael Leventhal, Ousmane Goro
- **Categories**: cs.SD, cs.AI, cs.CL
- **arXiv**: [https://arxiv.org/abs/2601.14931v1](https://arxiv.org/abs/2601.14931v1)
- **PDF**: [https://arxiv.org/pdf/2601.14931v1](https://arxiv.org/pdf/2601.14931v1)

本研究探讨了生成式人工智能（Gen AI）在马里和平叙事构建与音乐遗产振兴中的潜力。研究基于马里当前的社会政治背景——社群间紧张关系与社会裂痕促使人们寻求新的象征性和解框架。我们通过实证研究探索了三个核心问题：（1）如何将Gen AI作为根植于民族语言与传统的音乐创作工具；（2）Gen AI系统在技术创新与文化真实性之间实现平衡融合的程度；（3）AI辅助的音乐协同创作如何增强社会凝聚力与文化自主性。实验结果表明，在具备文化自觉的参与式框架中，Gen AI能够成为象征性外交的催化剂，其作用在于放大而非标准化本土声音。然而，研究仍面临语言语料库的可用性、算法审查机制以及基于版权素材生成作品的伦理规范等挑战。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
研究背景是马里面临社会凝聚力下降、文化身份碎片化的挑战，传统音乐形式在联系现代受众、尤其是年轻一代方面存在局限。既有问题在于：
- 传统音乐媒介难以有效触及当代听众，对构建社会和解叙事的影响力不足。
- 缺乏能够融合文化遗产与现代创新、以产生具有本地意义的和平叙事的系统性方法。

2)  
论文通过一个参与式共创工作坊，利用生成式人工智能（Gen AI）作为核心工具，具体方法如下：
- **技术工具与框架**：选用Suno AI（根据文本提示生成音乐）、ChatGPT与GEMINI（用于文本生成与翻译），构建技术支持环境。
- **参与式工作坊设计**：招募30名来自政府、青年组织、妇女团体和传统音乐学校的多元参与者，确保多视角共同创作。
- **结构化创作流程**：
  - **初始培训**：学习Gen AI基本原理及提示工程。
  - **传统与现代融合**：设计提示词，整合马里传统乐器（如科拉琴、巴拉风、金贝鼓）与当代编曲。
  - **语言真实性**：使用班巴拉语、富拉语等民族语言创作歌词，并调整提示以尊重声调结构。
- **迭代式提示工程**：采用“流派/风格+乐器+节奏/情绪+文化参照+主题内容”的框架优化提示，通过多次生成迭代提升文化特异性。
- **文化敏感性嵌入**：强调AI工具需服务于本地文化愿景，通过控制乐器选择、音阶、语言语调等，实现技术创新与文化真实性的平衡。

该方法通过AI辅助音乐共创，将传统音乐元素与现代形式结合，并使用民族语言构建和平主题的歌词，旨在增强文化归属感、促进代际与机构间对话，从而应对社会分裂问题。

3)  
研究在以下任务上取得了效果：
- **音乐创作**：产生了12首原创作品，成功融合了传统马里乐器与现代风格（如Afrobeat-Mandingo、Reggae-sabar等混合模式），并系统使用多种民族语言。
- **社会与文化影响**：参与者报告文化归属感增强、对对话可能性信心提升；作品在“2025年全国和解周”得到推广，显示出制度认可。
- **技术赋能**：85%的参与者认为作品真实反映了马里音乐遗产；80%感到培训后能独立或辅助下进行新创作。工作坊促进了AI工具在文化创作中的有效应用。
</div>

</details>

---

## Fast-ULCNet: A fast and ultra low complexity network for single-channel speech enhancement
- **Authors**: Nicolás Arrieta Larraza, Niels de Koeijer
- **Categories**: eess.AS, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.14925v1](https://arxiv.org/abs/2601.14925v1)
- **PDF**: [https://arxiv.org/pdf/2601.14925v1](https://arxiv.org/pdf/2601.14925v1)

单通道语音增强算法常应用于资源受限的嵌入式设备中，低延迟与低复杂度设计在此类场景中尤为重要。近年来，研究者针对该问题提出了多种创新解决方案。其中，近期提出的深度学习模型ULCNet已成为该领域的先进方法之一。本文通过对ULCNet进行改进，将其GRU层替换为FastGRNN层，以降低计算延迟与复杂度。此外，本文通过实验揭示了FastGRNN在长音频信号推理过程中因内部状态漂移导致的性能衰减现象，并提出一种基于可训练互补滤波器的新方法以缓解该问题。最终得到的Fast-ULCNet模型在语音增强任务中与先进的原始ULCNet架构性能相当，同时模型规模缩减超50%，平均延迟降低34%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：单通道语音增强算法常部署于资源受限的嵌入式设备，需满足低延迟与低复杂度要求。传统信号处理方法虽满足这些要求，但深度学习模型（如ULCNet）能提供更优的语音质量。  
- **既有方法的问题**：现有低复杂度模型（如ULCNet）使用GRU层，虽已优化，但仍存在进一步降低计算开销的空间；同时，论文发现当采用更轻量的FastGRNN替换GRU时，其在长音频推理过程中会出现内部状态漂移，导致性能随时间下降。

2)  
论文通过**Fast-ULCNet**解决上述问题，其核心方法包括两个关键改进：  
- **用FastGRNN替换GRU**：  
  - FastGRNN是一种轻量级RNN，通过加权残差连接和共享权重矩阵，大幅减少参数量并提升计算效率。  
  - 直接替换后，模型参数减少一半以上，平均延迟降低34%。  
- **提出Comfi-FastGRNN以解决状态漂移**：  
  - 研究发现FastGRNN在长音频（>60秒）推理时，隐藏状态均值会随时间漂移，导致性能下降。  
  - 为此，论文设计了可训练的互补滤波器（Comfi-FastGRNN），在FastGRNN状态更新方程中引入两个可学习参数（λ和γ），分别用于补偿漂移和控制校正项权重，从而稳定长期推理性能。  
- **整体架构继承ULCNet**：  
  - 保持ULCNet的功率律压缩、通道特征重定向等模块，仅将RNN层替换为FastGRNN或Comfi-FastGRNN单元，确保结构高效性。

3)  
- **任务**：单通道语音增强，使用DNS Challenge 2020数据集进行测试。  
- **效果**：  
  - 在10秒短音频上，Fast-ULCNet与原始ULCNet性能相当（如DNSMOS指标接近）。  
  - 在90秒长音频上，Fast-ULCNet（无互补滤波器）性能显著下降；而采用Comfi-FastGRNN的Fast-ULCNet有效抑制漂移，性能与ULCNet持平。  
  - 计算效率方面，参数量减少超50%，在嵌入式设备（如Raspberry Pi）上平均延迟降低34%。
</div>

</details>

---

## Multi-Tast Transformer for Explainable Speech Deepfake Detection via Formant Modeling
- **Authors**: Viola Negroni, Luca Cuccovillo, Paolo Bestagini, Patrick Aichroth, Stefano Tubaro
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14850v1](https://arxiv.org/abs/2601.14850v1)
- **PDF**: [https://arxiv.org/pdf/2601.14850v1](https://arxiv.org/pdf/2601.14850v1)

本文提出一种用于语音深度伪造检测的多任务Transformer模型，该模型能够预测共振峰轨迹和随时间变化的浊音模式，最终将语音分类为真实或伪造，并突出显示其决策更依赖于浊音区还是清音区。基于先前的说话人-共振峰Transformer架构，我们通过改进的输入分段策略简化模型结构，重新设计解码流程，并集成内置可解释性模块。与基线模型相比，本模型在保持预测性能的同时，参数量更少、训练速度更快，且具有更优的可解释性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：AI生成语音（深度伪造）的恶意使用日益增多，对社交媒体和信息安全构成威胁。现有数据驱动的检测器虽有效，但存在两大问题：
  - **可解释性不足**：模型决策依赖的特征不透明，难以预先确定关键依据。
  - **依赖虚假线索**：研究发现，许多模型可能利用非语音特征（如无声段、背景噪声）进行判断，而非真正的语音内容。

2)  
论文提出**SFATNet-4**，一种轻量级多任务Transformer模型，通过以下设计解决上述问题：  
- **多任务学习框架**：模型同时执行三个辅助任务，迫使网络学习语音相关特征：
  - **共振峰轨迹预测**：解码器直接预测每帧的基频（F0）及前两个共振峰（F1、F2）的连续轨迹，捕捉韵律模式。
  - **清浊音区分**：另一解码器预测每帧是否为浊音（有声段），替代旧版的幅度重建任务。
  - **深度伪造检测**：合成预测器通过多头注意力池化机制为每帧分配权重，最终分类真伪。  
- **改进的输入处理**：将短时傅里叶变换的幅度与相位仅沿时间轴分割为帧级令牌（而非时频二维块），降低计算复杂度，并支持帧级可解释性分析。  
- **内置可解释性机制**：
  - 检测器的注意力权重直接标识对决策影响最大的时间帧。
  - 结合清浊音预测结果，可量化模型依赖有声段或无声段的比例，从而揭示决策依据。  
- **效率提升**：简化了解码器结构（如共振峰预测改为线性投影），参数从6470万降至4180万，训练速度提升至每轮15分钟（原需60分钟以上）。

3)  
- **任务与效果**：模型在多个语音深度伪造检测数据集上评估：
  - **领域内性能**：在ASVspoof 5（干净数据）上，等错误率（EER）降至4.41%，AUC达98.89%，优于前代SFATNet-3。
  - **跨领域泛化**：在In-the-Wild、FakeOrReal和TIMIT-TTS三个未见数据集上，平均EER为15.74%，AUC为89.40%，表现均优于基线。
  - **可解释性验证**：分析显示，模型对合成语音的决策主要依赖无声段，与已有研究指出的伪造痕迹常出现在无声区域相符。
</div>

</details>

---

## Training-Efficient Text-to-Music Generation with State-Space Modeling
- **Authors**: Wei-Jaw Lee, Fang-Chih Hsieh, Xuanjun Chen, Fang-Duo Tsai, Yi-Hsuan Yang
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.14786v1](https://arxiv.org/abs/2601.14786v1)
- **PDF**: [https://arxiv.org/pdf/2601.14786v1](https://arxiv.org/pdf/2601.14786v1)

近年来，文本到音乐生成技术取得了显著进展，能够产生高质量的音乐作品，但通常需要大量计算资源并依赖大规模专有内部数据。为提高该技术的训练可及性与开放性，亟需一种更高效、数据利用率更高的开源生成模型架构。本文通过将生成模型的可训练参数量控制在约3亿，与MusicGen-small基准模型规模相当，并将其Transformer主干网络替换为新兴的状态空间模型。具体而言，我们探索了多种适用于序列建模的SSM变体，并比较了基于SSM的单阶段设计与可分解的SSM/扩散混合双阶段设计。所有提出的模型均在完全公开的数据集上从头训练，该数据集包含457小时知识共享许可的音乐，确保了研究的完全开放性。实验结果表明：首先，SSM相比Transformer展现出更优的训练效率；其次，尽管仅使用MusicGen-small基准模型9%的浮点运算量与2%的训练数据量，我们的模型在基于MusicCaps描述的客观指标与主观听感测试中均表现出可比性能；最后，缩放实验表明，在相同训练迭代次数下，即使将模型规模缩小至四分之一，SSM仍能保持相对于Transformer基准的竞争性表现。为促进文本到音乐生成研究的普及，我们已在项目页面（https://lonian6.github.io/ssmttm/）公开处理后的文本描述、模型检查点及源代码。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前文本到音乐生成模型虽能产生高质量音频，但普遍依赖大规模私有数据和巨额计算资源，导致训练成本高昂、开放性与可复现性受限。  
- **既有问题**：主流方法（如基于Transformer的MusicGen-small）需在私有数据集上长时间训练，计算开销巨大（约10^20 FLOPs），且多数模型未使用完全公开的数据，阻碍了学术研究中的可及性与复现。

2)  
- **核心方法**：论文提出用状态空间模型替代Transformer作为生成主干，并探索了两种设计：  
  - **单阶段SSM语言模型**：基于Mamba-2和SiMBA等SSM变体构建自回归模型，通过线性复杂度序列建模提升训练效率。  
  - **两阶段SSM/扩散混合设计**：利用音频token的粗粒度到细粒度结构，第一阶段用SSM生成粗粒度语义token，第二阶段用预训练潜在扩散模型增强音频保真度。  
- **解决思路**：  
  - SSM的线性计算复杂度降低了长序列建模开销，相比Transformer的二次复杂度更高效。  
  - 通过纯公开数据集（457小时CC授权音乐）训练，确保开放性；在单GPU上限制训练步数（≤100k），降低计算门槛。  
  - 两阶段设计分解生成任务，进一步减少语言模型的序列长度负担，提升训练效率。

3)  
- **任务与效果**：  
  - **训练效率对比**：SSM模型在相同训练步数下收敛更快，Prefix SiMBA仅用100k步即达到Transformer 400k步的相似性能。  
  - **生成质量评估**：在10秒和25秒音频生成任务上，两阶段Prefix SiMBA/扩散模型在客观指标（FD、KL、CLAP）和主观听测中与MusicGen-small表现相当，但仅用其9%的FLOPs和2%的训练数据量。  
  - **模型缩放实验**：将SSM参数量缩减至原1/4（约94M）时，仍能保持竞争力，证明其参数效率优势。
</div>

</details>

---

## Test-Time Adaptation For Speech Enhancement Via Mask Polarization
- **Authors**: Tobias Raichle, Erfan Amini, Bin Yang
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14770v1](https://arxiv.org/abs/2601.14770v1)
- **PDF**: [https://arxiv.org/pdf/2601.14770v1](https://arxiv.org/pdf/2601.14770v1)

为适应未知环境，语音增强模型的测试时自适应在实际部署中至关重要，但由于对领域偏移下模型性能退化机制的理解不足，该方向仍缺乏深入探索。我们观察到，基于掩码的语音增强模型在领域偏移下会丧失置信度，表现为预测掩码趋于平坦化，从而削弱了语音保留与噪声抑制的决策能力。基于此发现，我们提出掩码极化方法——一种轻量级测试时自适应策略，通过基于Wasserstein距离的分布比较恢复掩码的双峰特性。该方法无需在已训练模型外引入额外参数，适用于资源受限的边缘部署场景。在不同领域偏移及模型架构下的实验结果表明，掩码极化能够取得高度稳定的性能提升，其效果与更为复杂的现有方法具有可比性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于深度学习的语音增强模型在训练数据之外的环境（域偏移）中性能会显著下降。实际部署需要处理多样且时变的声学环境，但通常缺乏目标域的标注数据。
- **既有方法问题**：
  - 无监督域适应方法通常需要访问源数据，存在隐私和存储问题。
  - 现有的测试时适应方法（如RemixIT、LaDen）依赖额外组件（如额外编码器或师生框架），增加了模型复杂性和参数量，不适用于资源受限的边缘部署。

2)  
论文提出了一种轻量级的测试时适应方法——掩码极化，其核心是通过恢复掩码的双峰分布来解决模型在域偏移下置信度下降的问题。具体步骤如下：

- **关键观察**：在域偏移下，基于掩码的语音增强模型预测的掩码会失去其固有的双峰特性（即值应集中在0和1附近，分别对应噪声抑制和语音保留），变得扁平化，导致分离决策不明确。

- **方法设计**：
  - **生成参考掩码**：基于谱减原理，通过估计噪声谱和模型自身增强的语音谱，计算出一个更具双峰特性的参考掩码 \( M_P \)。该参考掩码能抵消原始预测掩码 \( M \) 中不必要的衰减，使其在语音区域更接近1，在噪声区域更接近0。
  - **分布对齐**：不将 \( M_P \) 用作逐点的伪标签，而是通过计算预测掩码 \( M \) 与参考掩码 \( M_P \) 的经验分布之间的Wasserstein距离作为损失函数 \( L_W \)。这种基于分布的比较对不完美的噪声估计具有鲁棒性，同时能有效鼓励双峰结构。
  - **辅助约束**：增加一个损失项 \( L_S \) 来惩罚预测掩码中出现的负值（域偏移下的常见现象）。
  - **总损失与稳定化**：总损失为 \( L = L_W + \lambda L_S \)。采用持续权重集成来稳定适应过程，即平滑更新模型参数，防止灾难性遗忘。

- **优势**：
  - **轻量高效**：无需引入任何额外可训练参数，仅需微调模型中原有的少量参数（如归一化层和输出层）。
  - **原理驱动**：直接针对模型在域偏移下的根本失效模式（掩码置信度下降）进行修复。
  - **通用性强**：方法适用于各类基于时频掩码的语音增强模型架构。

3)  
- **任务**：在多种域偏移场景下进行语音增强，包括噪声环境变化、语音特征变化以及两者同时变化。
- **模型**：在两种代表性架构（简单的AM模型和先进的CMGAN模型）上进行了评估。
- **效果**：
  - 在感知质量（PESQ）和信号质量（如SI-SDR）等多个指标上，MPol取得了与复杂方法（LaDen、RemixIT）相当或更具竞争力的性能。
  - 特别地，MPol在PESQ指标上表现出**非常一致的性能提升**，在所有测试数据集上均有效，而其他方法的增益波动较大。
  - 在计算效率上，MPol的实时因子远优于LaDen，更适用于边缘设备部署。
</div>

</details>

---

## Inverse-Hessian Regularization for Continual Learning in ASR
- **Authors**: Steven Vander Eeckt, Hugo Van hamme
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14751v1](https://arxiv.org/abs/2601.14751v1)
- **PDF**: [https://arxiv.org/pdf/2601.14751v1](https://arxiv.org/pdf/2601.14751v1)

灾难性遗忘仍是自动语音识别（ASR）持续学习面临的主要挑战，模型需在适应新领域的同时保持对已学习场景的性能。目前已有多种ASR持续学习方法被提出，其中权重平均法——即在微调后通过合并步骤对模型进行平均——作为一种简单的无记忆策略已被证明有效。但该方法本质上是启发式的，忽略了任务内在的损失函数曲面特性，限制了适应能力。本文提出逆海森正则化方法，这是一种用于ASR持续学习的无记忆策略，将曲率信息融入模型合并步骤。在新任务微调后，通过基于前一任务的克罗内克分解逆海森近似来调整适配过程，确保模型主要沿对历史性能损害较小的方向更新，同时保持方法的轻量化。我们在两个持续学习基准测试中验证了该方法，结果表明其显著优于现有先进基线，在提升适应能力的同时有效抑制遗忘。消融实验与分析进一步证实了该方法的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动语音识别系统需持续适应新领域，但传统微调会导致灾难性遗忘，即模型在新任务上学习时，会严重遗忘旧任务的知识。
- **既有方法问题**：
  - 基于架构或重放的方法存在存储或隐私限制。
  - 权重平均等无记忆方法虽简单有效，但本质是启发式的，忽略了任务损失曲面的几何信息，导致适应性受限，且随着任务增多，新任务的权重会趋近于零，阻碍学习。

2)  
论文提出**逆Hessian正则化**，这是一种无记忆的持续学习方法，通过将曲率信息融入模型合并步骤来解决上述问题。其核心机制如下：

- **方法概述**：在新任务上微调后，获得更新参数。随后，通过一个合并步骤，利用旧任务的逆Hessian近似来调整此次参数更新，使模型主要沿着对旧任务性能不敏感的“平坦”方向移动，从而在适应新任务的同时，保持在旧任务的低损失区域内。

- **关键技术实现**：
  - **逆Hessian近似**：直接计算全参数Hessian矩阵不可行。因此，采用**Kronecker分解近似**，在层级别进行估计，这能捕捉层内参数间的相互作用，比仅考虑对角线近似更准确。
  - **轻量级设计**：仅使用**最近一个任务**的Hessian来近似所有旧任务的Hessian，这避免了存储所有历史数据的需求。逆Hessian与参数更新的乘法运算仅在微调后的合并步骤中进行一次，计算开销小。
  - **参数处理策略**：对于占参数主体的线性层，应用上述调整；对于其余参数（如卷积层、偏置），则采用简单的标量加权平均（如 `αp = 1/t`）进行合并。
  - **可调平衡**：引入超参数 `τ`，通过缩放调整后的更新幅度，灵活控制模型的稳定性（保留旧知识）与可塑性（学习新知识）之间的权衡。

- **解决既有问题**：
  - **克服启发式局限**：通过利用损失曲面的二阶曲率信息，为参数更新方向提供了理论依据，不再是简单的平均。
  - **保持适应性**：逆Hessian对更新进行**重缩放而非完全阻止**，允许在非敏感方向上进行有效学习，避免了权重平均中随着任务增长适应性下降的问题。
  - **实现无记忆**：仅需存储最近任务的Hessian近似（两个小矩阵），存储需求恒定，符合持续学习的实际约束。

3)  
- **任务**：在两个持续学习基准测试上进行了评估：
  1.  **口音适应**：在Common Voice英语数据集的五个口音（美、英、澳、印、苏格兰）上顺序学习。
  2.  **领域与口音双重适应**：从LibriSpeech模型开始，顺序适应Libri-Adapt的四个任务，涉及麦克风类型和口音的双重变化。
- **效果**：
  - **整体性能最佳**：在两项实验中，IHR的平均词错误率均显著优于所有无记忆基线方法，并且其性能接近甚至超过了需要存储历史数据的经验回放方法。
  - **有效缓解遗忘**：后向传递指标显示，IHR的遗忘程度极低（在口音适应实验中接近零），显著优于微调和其它正则化方法。
  - **保持高适应性**：IHR在新任务上的学习能力优于权重平均法，在最后一个任务上取得了更低的词错误率，实现了稳定性与可塑性的良好平衡。
</div>

</details>

---

## Unlocking Large Audio-Language Models for Interactive Language Learning
- **Authors**: Hongfu Liu, Zhouying Cui, Xiangming Gu, Ye Wang
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14744v1](https://arxiv.org/abs/2601.14744v1)
- **PDF**: [https://arxiv.org/pdf/2601.14744v1](https://arxiv.org/pdf/2601.14744v1)

在第二语言（L2）学习中，发音能力的提升仍面临挑战，尽管计算机辅助发音训练（CAPT）系统已有所发展。传统CAPT系统常提供缺乏直观性和可操作指导的反馈，限制了其有效性。音频-语言模型（ALMs）的最新进展为增强此类系统提供了潜力，能够提供更用户友好的反馈。本研究通过引入L2-Arctic-plus数据集（一个包含详细错误解释及可操作改进建议的英语数据集），探索了基于对话的发音训练中ALMs的应用。我们在该数据集上对级联ASR+LLM模型及现有ALMs进行了基准测试，重点关注发音错误检测与可操作反馈生成。为提升性能，我们进一步提出基于L2-Arctic-plus对ALMs进行指令微调。实验结果表明，经过指令微调的模型在发音错误检测和建议生成任务上，无论是客观评估还是人工评估均显著优于现有基线，凸显了所提出数据集的价值。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：第二语言（L2）发音学习具有挑战性，计算机辅助发音训练（CAPT）系统旨在提供支持。
- **传统CAPT系统问题**：其反馈通常不直观，缺乏可操作的改进指导，限制了训练效果。
- **现有技术局限**：
  - 级联的ASR+LLM框架中，ASR模型倾向于纠正发音错误，导致LLM无法从原始音频中准确检测错误。
  - 现有音频-语言模型（ALMs）在发音错误检测和生成可操作反馈方面能力有限，且未针对此教育任务进行专门优化。

2) **论文核心方法如何解决上述问题**
论文通过构建专用数据集并对ALMs进行指令微调，系统性地提升了聊天式发音训练的性能。

- **构建专用数据集L2-Arctic-plus**：
  - 基于L2-Arctic数据集扩展，为发音错误提供了详细的文本解释和可操作的改进建议标注。
  - 使用GPT-4o生成初始反馈，并经过多轮人工验证，确保了数据质量。该数据集为模型训练与评估提供了基准。

- **采用两阶段指令微调策略优化ALMs**：
  - **第一阶段：声学特征对齐**。使用大量ASR数据（如CommonVoice）训练一个可学习的投影器，将音频编码器提取的特征与LLM的文本嵌入空间对齐。此阶段仅训练投影器，使LLM能初步理解音频输入。
  - **第二阶段：任务特定指令微调**。使用在L2-Arctic数据集上构建的发音训练数据（与测试集无重叠）对模型进行微调。此阶段采用LoRA技术，同时微调投影器和LLM主干，使模型学会根据音频和文本指令，生成包含错误检测和具体建议的反馈。

- **方法优势**：
  - **端到端处理**：ALMs直接处理音频，保留了ASR转录中可能丢失的声学细节（如音素信息），优于级联框架。
  - **针对性优化**：指令微调使模型专注于“朗读”场景下的错误检测和建议生成任务，显著减少了模型“幻觉”（如输出规范文本中不存在的词）。
  - **灵活性**：方法兼容不同的音频编码器（如Whisper）和LLM主干（如Mistral、Llama），实验表明更大的音频编码器通常能带来更好的检测性能。

3) **在哪些任务上取得了怎样的效果**
论文在**聊天式发音训练**这一核心任务上进行了评估，具体包括**发音错误检测**和**反馈建议生成**两个子任务。

- **效果**：在L2-Arctic-plus测试集上，论文提出的指令微调ALMs显著超越了所有基线模型。
  - **客观指标**：在错误检测的F1分数上，相比最好的级联ASR+LLM模型和现有的GPT-4o-Audio模型，分别取得了最高134.3%和35.6%的相对提升。在建议生成的BLEU-2、ROUGE-L等文本相似度指标上也有大幅改善，且额外词比率（EWR）降至0，表明输出高度可靠。
  - **主观评估**：在人工评估和基于GPT-4o的“LLM-as-a-Judge”评估中，指令微调模型在建议相关性、用户可理解性和整体评价上均获得最高分，其生成的反馈被认定为更清晰、实用和可操作。
</div>

</details>

---

## AQAScore: Evaluating Semantic Alignment in Text-to-Audio Generation via Audio Question Answering
- **Authors**: Chun-Yi Kuan, Kai-Wei Chang, Hung-yi Lee
- **Categories**: eess.AS, cs.AI, cs.CL, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14728v1](https://arxiv.org/abs/2601.14728v1)
- **PDF**: [https://arxiv.org/pdf/2601.14728v1](https://arxiv.org/pdf/2601.14728v1)

尽管文本到音频生成在真实性和多样性方面取得了显著进展，但评估指标的发展却未能同步跟进。目前广泛采用的方法（通常基于CLAPScore等嵌入相似性度量）虽能有效衡量整体相关性，但在细粒度语义对齐和组合推理方面仍存在局限。为此，我们提出了AQAScore——一个与模型架构无关的评估框架，该框架利用音频感知大语言模型（ALLMs）的推理能力。AQAScore将评估重新定义为概率化语义验证任务：它不依赖开放式文本生成，而是通过计算针对特定语义查询给出“是”回答的精确对数概率来估计对齐程度。我们在多个基准测试中评估AQAScore，包括人工评定的相关性、成对比较和组合推理任务。实验结果表明，相较于基于相似性的指标和生成式提示基线方法，AQAScore始终与人类判断保持更高相关性，这证明了其在捕捉细微语义不一致性方面的有效性，并能随着底层ALLMs能力的提升而扩展。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本到音频生成技术发展迅速，但评估方法滞后。  
- **既有方法问题**：广泛采用的基于嵌入相似性的方法（如CLAPScore）虽能衡量整体相关性，但在细粒度语义对齐和组合推理方面存在局限，难以准确评估多声音事件、时序或属性绑定等复杂语义。

2)  
- **核心方法**：AQAScore提出一个与模型架构无关的评估框架，将评估重新定义为概率语义验证任务。  
- **具体操作**：  
  - 利用音频感知大语言模型，将音频和文本描述转化为针对性语义查询（如“该音频是否包含文本描述的声音事件？”）。  
  - 通过计算模型对“是”回答的精确对数概率，而非依赖开放式文本生成，得到连续的对齐分数。  
- **优势**：  
  - 直接验证音频是否满足文本描述的语义条件，能检测细粒度的语义不一致（如缺失事件、属性错误或时序颠倒）。  
  - 框架兼容不同ALLM骨干，且性能随ALLM能力提升而增强，避免了相似性度量或生成式提示方法的不足。

3)  
- **评估任务与效果**：  
  - **人类评分相关性**：在RELATE和PAM数据集上，AQAScore与人类评分的相关性（Pearson/Spearman/Kendall系数）均优于CLAPScore及生成式提示基线。  
  - **成对比较**：在RELATE-Pair和Baton-Pair中，AQAScore的偏好预测准确率和AUC值更高，更贴合人类偏好。  
  - **组合推理**：在CompA基准测试中，AQAScore在事件顺序和属性绑定任务上表现优于专门优化的基线（如CompA-CLAP），展示了更强的语义推理能力。
</div>

</details>

---

## NLP-Based Review for Toxic Comment Detection Tailored to the Chinese Cyberspace
- **Authors**: Ruixing Ren, Junhui Zhao, Xiaoke Sun, Qiuping Li
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14721v1](https://arxiv.org/abs/2601.14721v1)
- **PDF**: [https://arxiv.org/pdf/2601.14721v1](https://arxiv.org/pdf/2601.14721v1)

随着移动互联网的深度融合与社交平台的广泛普及，中文网络空间中的用户生成内容呈现爆发式增长。其中，有害评论的泛滥对个体心理健康、社区氛围及社会信任构成了严峻挑战。由于中文网络语言具有强语境依赖性、文化特异性及快速演化性，有害表达常通过谐音、隐喻等复杂形式传递，给传统检测方法带来显著局限。本文聚焦于中文网络空间中有害评论的自然语言处理检测这一核心议题，系统梳理并批判性分析了该领域的研究进展与关键挑战。首先界定了中文有害评论的内涵与特征，并剖析了其所依赖的平台生态与传播机制；继而全面评述了现有公开数据集的构建方法及其局限性，提出了一种新颖的细粒度、可扩展的有害评论定义与分类框架，以及相应的数据标注与质量评估策略；系统总结了检测模型从传统方法到深度学习的演进路径，特别强调了可解释性在模型设计中的重要性；最后深入探讨了当前研究面临的开放性挑战，并对未来研究方向提出了前瞻性建议。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：随着移动互联网与社交平台的深度融合，中文网络空间用户生成内容激增，其中包含大量有害评论，对个人心理健康、社区氛围和社会信任构成严重威胁。  
- **既有方法的问题**：传统检测方法面临显著局限，主要源于中文网络语言的强语境依赖性、文化特异性及快速演变特性。有害表达常通过谐音、隐喻等复杂形式传递，导致基于规则、词典或浅层统计机器学习的方法难以有效识别隐晦、动态的毒性内容。

2)  
- **提出细粒度、可扩展的定义与分类框架**：论文构建了一个包含攻击类型、强度、目标与意图的多维度分级标注体系（如表IV所示），为毒性评论提供了清晰、统一的操作化定义，超越了传统的二元粗分类。  
- **设计高效的人机协同标注流程**：针对细粒度标注成本高、主观性强的问题，提出了一个闭环工作流（图3）。该流程结合规则/词典初筛、大语言模型预标注、人工重点审核与仲裁，并利用标注数据持续优化专用辅助模型，实现了大规模高质量标注的效率提升。  
- **强调数据质量与时效性验证**：主张从内部一致性（如分维度计算Kappa系数）、时间动态性（如词汇新颖性分析）、平台生态代表性（如评估数据源分布偏差）和对抗鲁棒性（使用扰动测试集评估）四个维度综合评估数据集，确保其能支撑模型开发。  
- **系统梳理技术演进并强调可解释性**：论文总结了从规则方法、统计机器学习、深度学习、预训练模型到大语言模型的技术发展路径（表VI）。特别强调了模型可解释性的重要性，分析了不同技术路线（如LIME、注意力可视化、积分梯度、思维链提示）在提供决策依据、实现结果追溯与优化中的关键作用。

3)  
- **任务与效果**：本论文是一篇综述，未提出单一的新模型，因此未在特定任务上报告如准确率、F1值等量化效果。  
- **贡献与影响**：其核心贡献在于系统性地梳理和批判性分析了中文毒性评论检测领域的研究进展与关键挑战，特别是在概念定义、数据集构建方法论和技术演进路径方面提供了清晰的路线图与前瞻性建议，旨在为构建更适应中文网络空间复杂性的检测系统奠定坚实的文献基础与理论框架。
</div>

</details>

---

## Triage knowledge distillation for speaker verification
- **Authors**: Ju-ho Kim, Youngmoon Jung, Joon-Young Yang, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14699v1](https://arxiv.org/abs/2601.14699v1)
- **PDF**: [https://arxiv.org/pdf/2601.14699v1](https://arxiv.org/pdf/2601.14699v1)

在资源受限设备上部署说话人验证系统仍面临高容量模型计算成本的挑战，知识蒸馏（KD）为此提供了解决方案。传统KD方法将目标类置信度与非目标类结构信息耦合在KL散度项中，限制了关系信息的传递。解耦KD虽将这两类信号分离为目标项与非目标项，却对非目标类进行均等处理，且在大规模类别场景中易受低概率长尾类别的影响。本文提出分诊式知识蒸馏（TRKD），该方案通过“评估-优先级-聚焦”三阶段操作实现蒸馏优化。TRKD引入累积概率阈值τ评估样本难度，将教师后验概率划分为三组：目标类、高概率非目标混淆集及背景集。为优先传递信息量高的信号，TRKD蒸馏混淆集的条件分布并舍弃背景集，同时传递表征样本难度与类间混淆关系的三值质量分布（目标/混淆/背景）。最后，TRKD通过基于τ的课程学习机制聚焦训练：初始阶段采用较大τ值传递广泛非目标上下文信息，随后逐步降低τ以收缩混淆集，使监督信号集中于最易混淆的类别。在VoxCeleb1数据集上进行的同构与异构师生模型组合实验中，TRKD始终优于近期KD变体，并在所有测试协议中取得了最低的等错误率。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在资源受限设备上部署说话人验证系统面临高容量模型计算成本高的挑战，知识蒸馏是常用解决方案。  
- **既有方法问题**：  
  - 经典知识蒸馏将目标类置信度与非目标类结构信息耦合在单一KL散度项中，限制了关系信息的传递。  
  - 解耦知识蒸馏虽分离了目标与非目标信号，但仍平等对待所有非目标类，在大规模分类中易受低概率长尾类干扰，导致监督信号被稀释。  

2)  
论文提出**Triage知识蒸馏**，其核心方法遵循“评估-优先-聚焦”三步原则，具体如下：  

- **评估**：引入累积概率阈值τ，根据教师后验将类别动态划分为三组：  
  - **目标类**：真实类别。  
  - **混淆集**：累积概率超过τ的最小非目标类子集，包含高概率易混淆类。  
  - **背景集**：剩余低概率非目标类。  

- **优先**：  
  - 仅对齐学生与教师在**混淆集**上的条件分布，传递细粒度类间关系。  
  - 同时通过**三质量项**（目标/混淆/背景）传递样本难度与类间混淆信息。  
  - **丢弃背景集**的精细对齐，避免长尾噪声干扰优化。  

- **聚焦**：  
  - 采用基于τ的课程学习策略：训练初期τ较大，混淆集较宽，传递广泛非目标上下文；  
  - 随训练进行，τ逐渐减小，混淆集收缩，监督逐渐聚焦于最难区分的易混淆类。  
  - 该策略稳定了早期优化，并在后期强化决策边界。  

**整体目标函数**结合了三质量KL散度项与混淆集条件KL散度项，丢弃背景集项，并与标准分类损失共同优化。  

3)  
- **任务**：在VoxCeleb1数据集的标准评测协议（原始、扩展、困难版本）上进行说话人验证。  
- **效果**：  
  - 在多种同构与异构教师-学生模型配对中，TRKD均一致优于现有知识蒸馏方法（如经典KD、DKD、GKD及嵌入级方法）。  
  - 在所有18组评估中取得了最低的等错误率，平均相对学生基线（无蒸馏）提升18.7%。  
  - 消融实验证实，三质量划分与传递、混淆集对齐及τ课程调度均为性能提升的关键因素。
</div>

</details>

---

## Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch
- **Authors**: Kanami Imamura, Tomohiko Nakamura, Kohei Yatabe, Hiroshi Saruwatari
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14684v1](https://arxiv.org/abs/2601.14684v1)
- **PDF**: [https://arxiv.org/pdf/2601.14684v1](https://arxiv.org/pdf/2601.14684v1)

基于深度神经网络的音频处理方法通常在单一采样频率下进行训练。为处理未经训练的采样频率，通常采用信号重采样技术，但这种方法可能导致性能下降，尤其在输入采样频率低于训练采样频率时更为明显。本文通过两个假设探讨性能下降的原因：(i) 上采样引入的高频成分缺失；(ii) 高频成分的存在比其精确表征更为重要。为验证这些假设，我们比较了传统重采样与三种替代方案：后重采样噪声添加（向重采样信号添加高斯噪声）、噪声核重采样（通过高斯噪声扰动插值核以丰富高频成分）以及可训练核重采样（通过训练自适应调整插值核）。音乐源分离实验表明，噪声核与可训练核重采样能够缓解传统重采样导致的性能下降。我们进一步证明噪声核重采样在不同模型中均具有有效性，凸显其作为一种简单而实用的解决方案的潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于深度神经网络的音频处理方法通常在单一采样频率下训练。当处理未经训练的采样频率时，通常对输入信号进行重采样，但这会导致性能下降，尤其是在输入采样频率低于训练采样频率时。
- **既有方法的问题**：现有研究仅关注特定模型和条件，缺乏对性能下降机制的系统性分析。常规重采样方法（如加窗sinc插值）在升采样时，会将输入奈奎斯特频率以上的高频分量设为零，导致高频成分缺失，与训练数据分布不匹配，从而引发性能退化。

2)  
论文通过两个假设探究性能下降的原因，并提出了三种重采样方法进行验证和解决：
- **假设一**：升采样导致的高频成分缺失是性能下降的主因。
- **假设二**：高频成分的“存在”比其“频谱内容”对分离性能更重要。

**提出的三种方法**：
- **后重采样噪声添加**：在常规重采样后的信号上直接添加高斯噪声，以补充高频能量。但实验表明，该方法因噪声与信号无关，未能有效缓解性能下降。
- **噪声核重采样**：在常规的加窗sinc插值核上添加高斯噪声。这使得重采样信号在高频带具有更大的幅度，且生成的高频成分与输入信号的低频成分相关。该方法简单有效，能持续缓解性能下降。
- **可训练核重采样**：使用多层感知机参数化插值核，并在保持预训练分离模型冻结的情况下，仅训练核网络。该方法通过端到端学习，使重采样过程适应模型，同样能有效减轻性能退化。

**核心解决机制**：
- **噪声核**与**可训练核**方法都能在重采样信号中引入高于输入奈奎斯特频率的成分，支持了假设一。
- 这两种方法引入的高频成分与输入信号相关，而直接加噪方法因引入无关噪声而失败，这支持了假设二，即高频成分的存在及其与输入的相关性比其具体频谱内容更重要。

3)  
- **任务**：在音乐源分离任务上进行了实验，使用了MUSDB18-HQ数据集，分离目标包括人声、贝斯、鼓和其他乐器。
- **效果**：
    - **噪声核重采样**和**可训练核重采样**在输入采样频率（如8 kHz）低于训练频率（44.1 kHz）时，有效缓解了常规重采样带来的性能下降，对人声等源的分离效果提升显著。
    - **噪声核重采样**在多种分离模型（如Conv-TasNet, BSRNN, Mel-RoFormer, HT-Demucs）上均表现出有效性，且在不发生性能下降的情况下也不会损害性能，具有通用性和实用性。
</div>

</details>

---

## READ-Net: Clarifying Emotional Ambiguity via Adaptive Feature Recalibration for Audio-Visual Depression Detection
- **Authors**: Chenglizhao Chen, Boze Li, Mengke Song, Dehao Feng, Xinyu Liu, Shanchen Pang, Jufeng Yang, Hui Yu
- **Categories**: cs.CV, cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14651v1](https://arxiv.org/abs/2601.14651v1)
- **PDF**: [https://arxiv.org/pdf/2601.14651v1](https://arxiv.org/pdf/2601.14651v1)

抑郁症是一种严重的全球性心理健康问题，会损害日常功能与整体生活质量。尽管当前视听融合方法在自动抑郁检测方面有所进展，但忽略情感线索的方法往往难以捕捉隐藏在情感表达中的细微抑郁信号。反之，引入情感信息的方法又常在特征表示中将短暂的情绪表达与稳定的抑郁症状相混淆，这一现象被称为**情感模糊性**，进而导致检测误差。为解决这一关键问题，我们提出READ-Net——首个通过自适应特征重校准机制来显式化解情感模糊性的视听抑郁检测框架。该机制的核心思想是动态调整情感特征的权重，以增强与抑郁相关的信号。READ-Net并非简单忽略或机械融合情感信息，而是创新性地识别并保留情感特征中与抑郁相关的线索，同时自适应地滤除无关的情感噪声。这一重校准策略显著提升了特征表示的清晰度，有效缓解了长期存在的情感干扰问题。此外，READ-Net能够便捷地集成到现有框架中以提升性能。在三个公开数据集上的大量实验表明，READ-Net优于当前最优方法，在准确率与F1分数上分别平均提升4.55%和1.26%，证明了其对情感干扰的鲁棒性，并推动了视听抑郁检测技术的进步。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：抑郁症是全球严重的心理健康问题，传统临床诊断依赖主观量表，存在时效性差、主观性强等局限。基于音视频的自动检测方法虽取得进展，但面临关键挑战。
- **既有方法问题**：
  - **情感无关方法**：忽略情感线索，无法捕捉隐藏在情感表达中的抑郁信号，导致特征建模不完整。
  - **情感感知方法**：直接融合情感与抑郁特征，混淆了短暂情绪波动（如访谈中的瞬间悲伤）与稳定的抑郁症状（如长期情感麻木），产生“情感模糊性”，降低检测准确性与泛化能力。

2)  
论文提出 **READ-Net** 框架，其核心是通过 **自适应特征重校准（AFR）** 机制解决情感模糊性问题。AFR 动态调整情感特征的权重，以增强抑郁相关信号，而非简单忽略或融合情感信息。具体通过三个模块实现：

- **分层特征分离（HFS）**：
  - 首先从多模态数据中分离出抑郁特征、噪声特征和情感特征。
  - 进而将情感特征细分为与抑郁相关的情感线索和与非抑郁相关的情感波动。
  - 通过互信息最大化等目标，确保抑郁相关特征被突出，无关情感噪声被抑制。

- **双重一致性正则化（DCR）**：
  - 构建动态图模型，分别对抑郁特征、噪声特征、情感特征以及细分的两类情感特征进行图卷积更新。
  - 通过图正则化和信息流控制损失，增强同类特征内部的一致性，并减少不同类特征间的相关性，从而提升特征的判别性和鲁棒性。

- **非对称蒸馏（AD）**：
  - 采用“教师-学生”网络结构，从精炼后的抑郁相关情感特征中蒸馏出紧凑、信息丰富的表示。
  - 通过非对称融合（情感特征单向增强抑郁特征），将蒸馏后的情感线索与抑郁特征融合，形成最终用于分类的稳健表示。

整个机制使 READ-Net 能够自适应地区分并保留情感表达中与抑郁病理相关的线索（如情感钝化），同时过滤掉由情境引发的短暂情绪噪声，从而澄清特征表示，有效缓解情感干扰。

3)  
READ-Net 在三个公开的音频-视觉抑郁症检测数据集上进行了评估：
- **任务**：音频-视觉多模态抑郁症检测（二分类：抑郁/正常）。
- **效果**：
  - 在 **LMVD**、**D-vlog** 和 **DAIC-WOZ** 数据集上，其性能均超越现有最优方法。
  - 平均准确率提升 **4.55%**，平均 F1 分数提升 **1.26%**。
  - 专门的“情感模糊性”测试表明，模型在面对不同程度的情感干扰时，保持了更高的鲁棒性和更低的误分类率。
  - 此外，其模块化设计可作为即插即用组件，有效提升现有多种基线模型的性能。
</div>

</details>

---

## Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models
- **Authors**: Wenda Zhang, Hongyu Jin, Siyi Wang, Zhiqiang Wei, Ting Dang
- **Categories**: eess.AS, cs.AI, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14620v1](https://arxiv.org/abs/2601.14620v1)
- **PDF**: [https://arxiv.org/pdf/2601.14620v1](https://arxiv.org/pdf/2601.14620v1)

语音情感识别模型通常采用单一类别标签，这忽视了人类情感固有的模糊性。模糊情感识别通过将情感表示为概率分布来解决此问题，但由于从稀疏人工标注推断出的真实分布可靠性不足，其进展受到限制。本文探讨大型音频-语言模型是否能够通过生成高质量合成标注来缓解标注瓶颈。我们提出一种利用音频-语言模型创建合成感知代理的框架，通过增强人工标注来提升真实分布的可靠性。我们通过统计分析这些代理与人类分布的一致性进行验证，并利用增强后的情感分布对音频-语言模型进行微调以评估其效果。此外，为解决类别不平衡问题并实现无偏评估，我们提出了DiME-Aug——一种分布感知的多模态情感增强策略。在IEMOCAP和MSP-Podcast数据集上的实验表明，合成标注能有效改善情感分布，尤其在标注一致性较高的低模糊度区域效果显著。然而，对于人类标注分歧较大的高模糊度情感，其改善效果有限。本研究首次证明音频-语言模型能够缓解模糊情感识别中的标注稀缺问题，同时指出需要开发更先进的提示或生成策略来处理高模糊度案例。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音情感识别传统上使用单一类别标签，忽略了人类情感固有的模糊性。模糊情感识别将情感建模为概率分布，但进展受限于从稀疏人工标注推断出的“真实”分布不可靠。
- **既有方法问题**：现有方法依赖每段语音仅3-5个标注者，导致分布估计粗糙，无法充分捕捉情感模糊性。这限制了模型学习真实模糊性的能力，损害了其性能和泛化能力。

2)  
论文提出一个三模块框架来解决标注稀疏和类别不平衡问题：
- **合成感知代理**：利用大型音频-语言模型生成合成标注，以扩充稀疏的人工标注。
    - 通过调整温度参数和修改提示指令，模拟不同标注者的视角，为每段语音生成大量单标签合成标注。
    - 将这些合成标注与原始人工标注结合，共同推断出更丰富、更可靠的情感概率分布。
- **DiME-Aug**：一种分布感知的多模态数据增强策略，旨在解决数据集中常见的类别不平衡问题。
    - 识别少数类别样本，在特征空间中为其寻找最近邻样本。
    - 通过加权混合音频信号和情感分布来生成新的增强样本，同时保持文本转录的连贯性，从而创建更平衡的训练集。
- **ALM微调**：使用扩充后的情感分布对ALM进行微调，以预测情感分布。
    - 采用Jensen-Shannon散度作为损失函数，直接优化预测分布与目标分布之间的相似性。
- **核心解决思路**：通过ALM生成大量合成标注来逼近真实的情感分布，并结合数据增强技术优化训练过程，从而缓解标注稀疏性和类别不平衡对模糊情感识别模型性能的限制。

3)  
- **任务**：在模糊情感识别任务上评估方法效果，主要使用IEMOCAP和MSP-Podcast两个语音情感数据集。
- **效果**：
    - **分布近似**：合成标注生成的情感分布在加入约6-10个标注后能与人工分布较好对齐。
    - **性能提升**：在自然、模糊性较低的MSP-Podcast数据集上，结合合成与人工标注进行训练，模型性能优于仅使用人工标注。特别是在应用DiME-Aug数据增强后，效果更佳。
    - **局限性**：对于高度模糊的情感（如IEMOCAP中部分样本），合成标注的补充效益减弱，人工标注仍不可或缺。合成标注单独使用效果最差。
</div>

</details>

---
