---
layout: post
title: "arXiv Daily – 2026-01-22"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-22（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-21 08:50 — 2026-01-22 08:50
- 抓取总数：18 篇 | 本页显示：18 篇（去重/过滤后）

## WeDefense: A Toolkit to Defend Against Fake Audio
- **Authors**: Lin Zhang, Johan Rohdin, Xin Wang, Junyi Peng, Tianchi Liu, You Zhang, Hieu-Thi Luong, Shuai Wang, Chengdong Liang, Anna Silnova, Nicholas Evans
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15240v1](https://arxiv.org/abs/2601.15240v1)
- **PDF**: [https://arxiv.org/pdf/2601.15240v1](https://arxiv.org/pdf/2601.15240v1)

随着生成式人工智能的进步，合成音频已能达到与真实音频在感知上难以区分的水平。尽管这一显著进展催生了许多积极应用，但也带来了冒用身份、传播虚假信息和实施欺诈等滥用风险。尽管已有大量挑战和倡议发布了开源伪造音频检测代码，但多数仅针对特定竞赛、数据集或模型定制。目前尚缺乏一个标准化、统一的工具包，能够不仅通过通用数据库、协议和指标，还借助共享代码库来支持对各类解决方案进行公平的基准测试与比较。为此，我们提出了WeDefense——首个同时支持伪造音频检测与定位的开源工具包。除模型训练外，WeDefense还强调以下关键但常被忽视的组成部分：灵活的输入与数据增强、校准、分数融合、标准化评估指标，以及用于深入理解和结果分析的工具。该工具包已在https://github.com/zlin0/wedefense公开提供，并包含伪造音频检测与定位的交互式演示。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.15240v1）
</div>

</details>

---

## WavLink: Compact Audio--Text Embeddings with a Global Whisper Token
- **Authors**: Gokul Karthik Kumar, Ludovick Lepauloux, Hakim Hacid
- **Categories**: cs.SD, cs.CL, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.15118v1](https://arxiv.org/abs/2601.15118v1)
- **PDF**: [https://arxiv.org/pdf/2601.15118v1](https://arxiv.org/pdf/2601.15118v1)

Whisper已成为大型音频-语言模型中提取通用音频特征的事实标准编码器，通常将30秒音频片段表示为1500帧特征并投影至大语言模型。相比之下，基于CLAP的音频-文本嵌入模型主要依赖其他音频编码器（如HTS-AT、PaSST），未能有效利用Whisper。本文提出WavLink——一种紧凑型音频-文本嵌入模型，通过为Whisper编码器添加可学习的全局标记，并与文本编码器联合训练。通过对预训练文本编码器、损失函数、训练模式及数据混合方案等设计选择进行系统研究，我们确定了能实现最先进检索性能的配置方案。针对三种模型规模设计的两阶段训练策略，结合套娃式监督方法，提升了模型扩展性，可在性能损失极小的情况下实现8倍压缩的嵌入表示。WavLink在AIR-Bench的多选题测试和零样本分类任务中也展现出竞争优势。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频-文本联合表示学习是音频理解的核心任务。现有方法存在分野：
  - **音频大语言模型**：普遍采用Whisper作为音频编码器，将30秒音频编码为约1500个帧级特征，计算和存储开销大。
  - **音频-文本嵌入模型**：如CLAP系列，主要依赖HTS-AT、PaSST等专用编码器，未能有效利用Whisper的通用音频特征。

2)  
论文提出WavLink模型，通过以下核心方法解决上述问题：

- **架构设计**：
  - 以Whisper为音频编码器，但引入一个**可学习的全局令牌**，将其附加到输入序列中。
  - 经过Whisper Transformer编码后，仅使用该全局令牌的最终状态作为整个音频片段的**单一紧凑表示**，替代原有的1500个帧级特征。
  - 文本编码器采用CLIP或ModernBERT，音频和文本表示通过轻量级线性投影器映射到共享嵌入空间。

- **系统化设计探索**：
  - 通过消融实验，系统比较了文本编码器（CLIP vs. ModernBERT）、损失函数（CLIP对比损失 vs. SigLIP）、适应策略（仅投影器、LoRA、全微调）和更新范围（仅音频塔、双塔联合）共24种配置。
  - 确定了**最佳配置**：使用CLIP文本编码器、CLIP对比损失、对编码器进行全微调、并联合更新音频和文本塔。

- **训练与扩展策略**：
  - 采用**两阶段训练**：第一阶段在大规模合成数据集（Auto-ACD, AudioSetCaps）上训练；第二阶段在高质量人工标注数据集（AudioCaps, Clotho）上微调。
  - 引入**Matryoshka监督**：在训练时同时对完整维度及其多个子维度（如768, 384, 192, 96）计算对比损失，使单个模型能输出多种尺度的嵌套嵌入，实现嵌入大小的可扩展性。

- **核心优势**：
  - **高效性**：将音频表示为单一向量，极大减少了存储和相似性搜索成本。
  - **通用性**：成功将Whisper（原用于语音识别）的特征适配到通用音频-文本对齐任务。
  - **可扩展性**：Matryoshka训练使嵌入维度可压缩至1/8而性能损失极小。

3)  
WavLink在多个任务上取得了优异效果：
- **音频-文本检索**：在AudioCaps和Clotho基准测试中，其大型模型在R@1等指标上超越所有CLAP基线模型约2-6个百分点。
- **零样本分类**：在VGGSound上达到最优准确率；在ESC-50和US8K上表现具有竞争力。
- **多项选择问答**：在AIR-Bench基准测试中，其Base模型（8400万参数）准确率达42.0%，优于LAION-CLAP，并与参数量大43-100倍的音频大语言模型（如Falcon3-Audio）性能相当。
- **嵌入可压缩性**：嵌入维度压缩至1/8（如从768维到96维）后，平均性能下降小于1个百分点，实现了8倍的存储和计算节省。
</div>

</details>

---

## Neural Tracking of Sustained Attention, Attention Switching, and Natural Conversation in Audiovisual Environments using Mobile EEG
- **Authors**: Johanna Wilroth, Oskar Keding, Martin A. Skoglund, Maria Sandsten, Martin Enqvist, Emina Alickovic
- **Categories**: eess.SP, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.15097v1](https://arxiv.org/abs/2601.15097v1)
- **PDF**: [https://arxiv.org/pdf/2601.15097v1](https://arxiv.org/pdf/2601.15097v1)

日常交流具有动态性和多感官特性，常涉及注意力的转移、重叠的语音及视觉线索。然而，现有神经注意力追踪研究大多局限于高度控制的实验室环境，使用纯净且通常仅为音频的刺激材料，并要求被试持续关注单一说话者。本研究通过引入包含24名听力正常参与者的新型数据集来填补这一空白。我们采用移动脑电图（EEG）系统（44个头皮电极和20个cEEGrid电极），在视听（AV）范式下设置了三种实验条件：在双人对话环境中持续关注单一说话者、在两位说话者之间切换注意力，以及参与无脚本的双人对话并同时存在竞争性单人说话者干扰。分析方法包括时序响应函数（TRF）建模、最优延迟分析、决策窗口为1.1秒至35秒的选择性注意力分类，以及对视听对话与仅含音频的干扰说话者TRF的比较。主要发现表明，头皮EEG在不同条件下，被关注与忽略语音的注意力相关P2峰值存在显著差异。注意力切换与持续注意条件下的性能无显著变化，说明系统对注意力转移具有鲁棒性。最优延迟分析显示，相较于单人视听刺激，对话条件下的峰值更窄，反映了多说话者处理的额外复杂性。头皮EEG的选择性注意力分类准确率持续高于随机水平（55%-70%），而cEEGrid数据的相关性较低，凸显了方法学进一步改进的必要性。这些结果表明，移动EEG能够可靠地追踪动态多感官听觉场景中的选择性注意力，并为未来视听范式设计和实际注意力追踪应用提供了指导。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.15097v1）
</div>

</details>

---

## Bangla Music Genre Classification Using Bidirectional LSTMS
- **Authors**: Muntakimur Rahaman, Md Mahmudul Hoque, Md Mehedi Hassain
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.15083v1](https://arxiv.org/abs/2601.15083v1)
- **PDF**: [https://arxiv.org/pdf/2601.15083v1](https://arxiv.org/pdf/2601.15083v1)

孟加拉音乐拥有丰富的音乐文化。当前，由于数字与实体格式音乐的指数级增长，音乐流派分类变得尤为重要。为提升检索效率，对音乐进行相应索引十分必要。对孟加拉音乐进行自动流派分类，有助于在庞大而多样的音乐库中高效定位特定曲目。现有的流派分类方法主要采用传统机器学习或深度学习技术。本研究引入了一个新颖的音乐数据集，涵盖十种不同的孟加拉音乐流派。针对音频分类任务，我们采用循环神经网络（RNN）架构，具体使用长短期记忆网络（LSTM）进行模型训练与分类。特征提取是音频数据处理的基础阶段，本研究利用梅尔频率倒谱系数（MFCC）将原始音频波形转换为紧凑且具有代表性的特征集。所提出的框架基于这些提取的特征实现音乐流派分类。实验结果显示分类准确率达到78%，表明该系统在优化和简化孟加拉音乐流派组织方面具有显著潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语音乐因其丰富的古典与现代风格而独具特色，但针对其流派自动分类的研究相对有限。随着数字音乐数量的激增，高效的流派分类对于音乐库索引、检索和推荐系统至关重要。
- **既有方法的问题**：现有方法主要依赖传统机器学习算法（如SVM、KNN）结合MFCC等声学特征，但这些方法难以捕捉音乐数据中复杂的时序依赖和内在关系，导致分类性能受限。

2)  
- **核心方法**：本文提出了一种基于双向长短期记忆网络（Bi-LSTM）的深度学习框架，用于孟加拉语音乐流派分类。
- **特征提取**：使用梅尔频率倒谱系数（MFCC）及其一阶差分（ΔMFCC）和色度特征，将原始音频波形转换为具有代表性的特征序列，以有效表征音色、时序变化和谐波内容。
- **模型架构**：
  - 采用Bi-LSTM层，能够同时从前向和后向处理时序特征，从而捕捉音乐中跨越多个时间步的节奏（如塔拉循环）和旋律模式。
  - 模型结合了批量归一化和全连接层，并通过LSTM的门控机制缓解梯度消失问题，增强对长序列依赖的建模能力。
  - 在输入中整合了孟加拉语语音特征（如通过预训练音素模型提取的歌词特征），以利用语言特性对音乐特征的影响。
- **训练与优化**：使用交叉熵损失函数进行端到端训练，使模型能够从孟加拉语音乐特有的声学模式中自动学习判别性特征。

3)  
- **任务与效果**：在包含10个孟加拉语音乐流派（如Bangla嘻哈、金属、摇滚、拉宾德拉歌曲、民间音乐等）的自建数据集上进行了流派分类实验。
- **主要结果**：
  - 所提出的Bi-LSTM模型达到了78%的整体分类准确率，显著优于对比的传统方法（如SVM准确率21%，KNN准确率45%）及其他深度学习模型（如ANN 72%，LSTM 74%）。
  - 在具体流派上，模型对Lalon Geet和Polli Geeti表现优异（F1分数分别为0.91和0.86），但在民间音乐和金属等流派上分类效果仍有提升空间。
</div>

</details>

---

## VCNAC: A Variable-Channel Neural Audio Codec for Mono, Stereo, and Surround Sound
- **Authors**: Florian Grötschla, Arunasish Sen, Alessandro Lombardi, Guillermo Cámbara, Andreas Schwarz
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14960v1](https://arxiv.org/abs/2601.14960v1)
- **PDF**: [https://arxiv.org/pdf/2601.14960v1](https://arxiv.org/pdf/2601.14960v1)

本文提出VCNAC——一种可变通道神经音频编解码器。该方法采用单一编码器-解码器参数化架构，能够原生支持从单声道语音到影院级5.1声道环绕声等多种通道配置的推理。通过通道兼容性目标设计，确保多声道内容在解码为较少声道时仍保持感知质量。共享表征机制使得生成式语言模型仅需基于单一码本集合进行训练，同时支持跨模态与通道配置的推理时扩展能力。通过客观空间音频指标评估与主观听感测试表明，该统一架构在单声道、立体声及环绕声配置下均能保持卓越的重建质量。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经音频编解码器在语音合成等应用中需处理单声道、立体声和环绕声等多种声道配置。现有方法存在显著局限。
- **既有问题**：
  - **架构僵化**：如EnCodec等主流编解码器采用固定声道架构，只能处理预设数量的声道。
  - **效率低下**：为支持多声道，通常需为每种配置单独训练模型（导致潜在空间不统一），或始终以最大声道数处理所有音频（造成计算资源浪费）。
  - **扩展性不足**：现有多声道方法通常仅适配至多两个声道，且多通过简单修改输入输出层来实现，无法灵活泛化至不同的声道需求。

2)  
论文提出的VCNAC（可变声道神经音频编解码器）通过以下核心方法解决上述问题：

- **可变声道处理流程**：
  - **并行通道流**：每个输入声道通过参数共享的独立卷积流进行处理，仅实例化实际存在的声道，避免了填充无用声道带来的计算开销。
  - **融合与量化**：在编码器特定深度，将各通道流的嵌入表示按时间位置相加，融合为一个统一的瓶颈表示，再进行残差向量量化（RVQ）。这创造了一个**独立于输入声道配置的统一潜在空间**。
  - **分离与重建**：解码时，将反量化后的表示复制并分离为目标数量的输出流，再通过参数共享的转置卷积重建各声道音频。

- **跨通道注意力机制**：
  - 在融合前和分离后，引入轻量化的Transformer音频自编码器（TAAE）块。
  - 注意力机制在时间上交错的通道序列上操作，能同时捕获**时序依赖和跨通道依赖**，提升了多声道信息的交互与重建质量。

- **统一的训练与损失设计**：
  - **共享码本**：所有声道配置使用同一组码本，实现了统一的离散表示，便于后续生成式语言模型在单一词汇表上训练。
  - **声道兼容性目标**：通过计算中/侧（mid/side）声道和标准下混（遵循ITU-R BS.775-4）的损失，确保多声道内容在解码为更少声道（如环绕声下混为立体声）时仍能保持感知质量。
  - **混合批次训练**：支持在一个批次中混合不同声道数量的样本，并通过通道掩码处理，高效地联合优化。

3)  
VCNAC在多项任务上取得了优于或媲美现有方法的性能，同时比特率更低（7.9 kbit/s）：
- **单声道语音**（LibriTTS）：在PESQ和SI-SDR指标上均优于对比的神经编解码器（如DAC、EnCodec）及传统编解码器Opus。
- **立体声音乐**（FMA-small）：在SI-SDR、SI-SNR及梅尔谱距离等指标上表现最佳，空间音频指标（ΔIPD, ΔILD）与最优方法相当。
- **5.1环绕声**（开源电影片段）：在客观指标上，前声道重建质量最优，空间特性保持良好；主观MUSHRA听力测试表明，其在音乐、前后声道及下混内容上均获得高感知质量评分，在几乎一半比特率下达到了与先进编解码器SNAC相当的感知质量。
</div>

</details>

---

## Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali
- **Authors**: Nouhoum Coulibaly, Ousmane Ly, Michael Leventhal, Ousmane Goro
- **Categories**: cs.SD, cs.AI, cs.CL
- **arXiv**: [https://arxiv.org/abs/2601.14931v1](https://arxiv.org/abs/2601.14931v1)
- **PDF**: [https://arxiv.org/pdf/2601.14931v1](https://arxiv.org/pdf/2601.14931v1)

本研究探讨了生成式人工智能（Gen AI）在构建和平叙事与振兴马里音乐遗产方面的潜力。研究基于马里当前的政治社会背景——社群间紧张关系与社会裂痕促使人们寻求新的象征性和解框架。通过实证方法，本研究聚焦三个核心问题：（1）如何将生成式人工智能作为基于民族语言与传统的音乐创作工具；（2）生成式人工智能系统能在多大程度上实现技术创新与文化真实性之间的平衡融合；（3）AI辅助的音乐协同创作如何增强社会凝聚力与文化自主性。实验结果表明，在具备文化自觉的参与式框架下，生成式人工智能能够成为象征性外交的催化剂，其作用在于放大而非同化本土声音。然而，研究也指出该领域仍面临语言语料库稀缺、算法审查机制以及基于版权素材的生成伦理等挑战。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
研究背景是马里面临社会凝聚力下降、文化身份碎片化的挑战，传统音乐形式在联系现代受众、尤其是年轻一代方面存在局限。既有问题在于：
- 传统音乐媒介难以有效触及当代听众或承载新的和解叙事。
- 缺乏能够融合文化传承与现代创新、以构建包容性和平叙事的有效工具。

2)  
论文通过一个参与式共创工作坊，探索生成式人工智能（Gen AI）作为文化敏感的工具来解决问题。核心方法包括：
- **技术框架与工具**：选用Suno AI（根据文本提示生成音乐）、ChatGPT与GEMINI（用于文本生成与翻译），构成技术支持。
- **参与式工作坊设计**：招募30名来自政府、青年组织、妇女团体和传统音乐界的多元参与者，进行为期三天的共创。
- **结构化创作流程**：
  - **分阶段培训**：从AI工具使用、提示工程，到融合传统乐器（如科拉琴、巴拉风、金贝鼓）与现代编曲，再到使用民族语言（如班巴拉语、富拉语等）创作歌词。
  - **迭代式提示工程**：引导参与者使用结构化提示框架（如【风格+乐器+节奏+文化参考+主题】），通过多次尝试（平均每首作品8-12次）细化提示，以生成文化上地道的音乐。
  - **文化融合策略**：强调乐器选择、多语言歌词、和解词汇与地理意象的使用，确保作品既根植传统又具有现代吸引力。
- **数据收集与分析**：通过作品主题分析、半结构化访谈和调查，评估文化真实性、社会凝聚力影响和技术适用性。

该方法通过**将AI置于文化敏感的参与式框架中**，使AI成为服务于本地文化愿景的工具，而非标准化输出的引擎，从而在技术创新与文化真实性之间取得平衡，并促进社会对话。

3)  
研究在以下任务上取得了效果：
- **音乐创作与文化遗产振兴**：成功生成12首融合传统与现代的原创音乐作品，形成了如“非洲节拍-曼丁哥模式”等混合架构，参与者认为这些作品（85%）真实反映了马里音乐遗产。
- **社会凝聚力与叙事构建**：工作坊促进了跨代际、跨机构的对话，强化了参与者的文化归属感；作品在国家和解周（SENARE 2025）上获得推广，显示出社会与制度层面的共鸣。
- **技术赋能与文化主权**：参与者（80%）表示在培训后能独立或借助少量帮助进行创作，体现了技术赋能；AI被用作放大本地声音、增强文化主权的工具。
</div>

</details>

---

## Fast-ULCNet: A fast and ultra low complexity network for single-channel speech enhancement
- **Authors**: Nicolás Arrieta Larraza, Niels de Koeijer
- **Categories**: eess.AS, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.14925v1](https://arxiv.org/abs/2601.14925v1)
- **PDF**: [https://arxiv.org/pdf/2601.14925v1](https://arxiv.org/pdf/2601.14925v1)

单通道语音增强算法常应用于资源受限的嵌入式设备中，低延迟与低复杂度设计在此类场景中尤为重要。近年来，研究者针对该问题提出了多种创新解决方案。其中，近期提出的深度学习模型ULCNet已成为该领域的先进方法之一。本文通过将ULCNet中的GRU层替换为FastGRNN，提出一种改进方案以降低计算延迟与复杂度。此外，本文通过实验揭示了FastGRNN在长音频信号推理过程中因内部状态漂移导致的性能衰减现象，并提出一种基于可训练互补滤波器的新方法以缓解该问题。最终得到的Fast-ULCNet模型在语音增强任务中与先进的原始ULCNet架构性能相当，同时模型规模缩减超50%，平均延迟降低34%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：单通道语音增强算法常部署于资源受限的嵌入式设备，需满足低延迟与低复杂度要求。传统信号处理方法虽满足这些要求，但深度学习模型（如ULCNet）能提供更优的音频质量。  
- **既有方法的问题**：现有ULCNet等模型使用GRU作为循环层，虽已降低复杂度，但在进一步减少计算延迟和参数数量方面仍有优化空间。此外，论文发现轻量级RNN变体FastGRNN在长音频推理时存在内部状态漂移问题，导致性能随时间下降。

2)  
论文核心方法**Fast-ULCNet**通过以下改进解决上述问题：  
- **架构替换**：将ULCNet中的GRU层替换为更轻量的FastGRNN单元，显著减少参数数量和计算量。FastGRNN通过加权残差连接和共享权重矩阵，在保持性能的同时降低复杂度。  
- **解决状态漂移**：针对FastGRNN在长序列推理时出现的状态漂移问题，提出**Comfi-FastGRNN**。该变体引入可训练互补滤波器，通过公式 \( h_t^{\text{comfi}} = \gamma h_t + (1-\gamma)\lambda \) 对隐藏状态进行校正，其中 \(\gamma\) 和 \(\lambda\) 为可学习参数，以抑制状态累积并保持稳定性。  
- **整体设计**：沿用了ULCNet的通道特征重定向、幂律压缩等模块，结合双向频率轴FastGRNN和子带时序FastGRNN块进行特征建模，最终通过复数比率掩码重构增强语音。

3)  
- **任务**：在单通道语音增强任务上，使用DNS Challenge数据集进行测试。  
- **效果**：  
  - 在10秒短音频上，Fast-ULCNet与原始ULCNet性能相当（如DNSMOS分数接近）。  
  - 在90秒长音频上，Fast-ULCNet（无互补滤波器）性能显著下降，而Comfi-FastGRNN版本能有效维持性能，与ULCNet持平。  
  - 计算效率：参数量减少超一半（从0.685M降至0.338M），平均延迟降低34%，在嵌入式设备（如树莓派）上实时因子（RTF）显著提升。
</div>

</details>

---

## Multi-Tast Transformer for Explainable Speech Deepfake Detection via Formant Modeling
- **Authors**: Viola Negroni, Luca Cuccovillo, Paolo Bestagini, Patrick Aichroth, Stefano Tubaro
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14850v1](https://arxiv.org/abs/2601.14850v1)
- **PDF**: [https://arxiv.org/pdf/2601.14850v1](https://arxiv.org/pdf/2601.14850v1)

本文提出一种用于语音深度伪造检测的多任务Transformer模型，该模型能够预测共振峰轨迹和随时间变化的浊音模式，最终将语音分类为真实或伪造，并揭示其决策更依赖于浊音区还是清音区。基于先前的说话人-共振峰Transformer架构，我们通过改进的输入分段策略简化模型结构，重新设计解码流程，并集成内置可解释性模块。与基线模型相比，本模型在保持预测性能的同时，参数量更少、训练速度更快，且具有更优的可解释性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：AI生成语音（深度伪造）的恶意使用日益增多，对社交媒体和信息安全构成威胁。现有数据驱动的检测器虽有效，但存在两大问题：
  - **可解释性不足**：模型决策依赖的特征不透明，难以预先确定关键依据。
  - **依赖虚假线索**：许多模型可能利用非语音特征（如无声段、背景噪声）进行判断，而非真实语音内容。

2)  
论文提出**SFATNet-4**，一种轻量级多任务Transformer模型，通过以下设计解决上述问题：
- **改进输入分割**：将语音频谱图仅沿时间轴分割为单帧令牌（而非时频二维块），降低计算复杂度，并支持帧级可解释性分析。
- **多任务解码器设计**：
  - **共振峰解码器**：直接预测每帧的基频（F0）及前两个共振峰（F1、F2）轨迹，学习语音韵律模式。
  - **清浊音解码器**：替代原有的幅度重建任务，预测每帧为浊音（有声）或清音（无声），增强语音结构感知。
  - **合成预测器**：用于深度伪造检测，引入多头池化机制为每帧分配注意力权重，突出影响决策的关键时段。
- **内置可解释性机制**：
  - 通过合成预测器的帧级注意力权重，可可视化哪些时间段对分类贡献最大。
  - 结合清浊音解码器的输出，可分析模型决策更依赖浊音还是清音区域，从而揭示其依据的语音特征类型。
- **效率提升**：相比前代SFATNet-3，参数从6470万降至4180万，单轮训练时间从60分钟缩短至15分钟，且无需牺牲性能。

3)  
- **任务与效果**：模型在语音深度伪造检测任务上进行了评估。
  - **性能**：在ASVspoof 5数据集上，等错误率（EER）为4.41%，AUC为98.89%，优于前代模型（EER 8.85%，AUC 96.69%）。
  - **泛化能力**：在三个未见数据集（In-the-Wild、FakeOrReal、TIMIT-TTS）上平均EER为15.74%，AUC为89.40%，显示出较好的跨域鲁棒性。
  - **可解释性分析**：模型对合成语音的决策高度依赖清音区域，表明合成伪影多存在于无声段，与先前研究一致，提供了决策依据的直观解释。
</div>

</details>

---

## Training-Efficient Text-to-Music Generation with State-Space Modeling
- **Authors**: Wei-Jaw Lee, Fang-Chih Hsieh, Xuanjun Chen, Fang-Duo Tsai, Yi-Hsuan Yang
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.14786v1](https://arxiv.org/abs/2601.14786v1)
- **PDF**: [https://arxiv.org/pdf/2601.14786v1](https://arxiv.org/pdf/2601.14786v1)

近期文本到音乐生成（TTM）领域取得了高质量成果，但通常以大量计算资源与大型专有内部数据为代价。为提升TTM训练的经济性与开放性，需要一种更高效、数据利用率更高的开源生成模型架构。本文通过将生成模型的可训练参数量约束至与MusicGen-small基准（约3亿参数）相当，并将其Transformer主干替换为新兴的状态空间模型（SSM）来实现这一目标。具体而言，我们探索了多种适用于序列建模的SSM变体，并比较了基于SSM的单阶段设计与可分解的SSM/扩散混合双阶段设计。所有提出的模型均在完全公开的数据集（包含457小时CC授权音乐）上从头训练，确保完全开放性。实验发现可归纳为三点：首先，SSM相比Transformer展现出更优的训练效率；其次，尽管仅使用MusicGen-small基准9%的FLOPs与2%的训练数据量，我们的模型在基于MusicCaps描述的客观指标与主观听感测试中均取得可比性能；最后，缩放实验表明，在相同训练预算（以迭代次数计）下，即使模型尺寸缩小至四分之一，SSM仍能保持相对于Transformer基线的竞争性表现。为促进TTM研究的普及，已处理的文本描述、模型检查点及源代码均通过项目页面在GitHub开源：https://lonian6.github.io/ssmttm/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前文本到音乐生成模型虽能产生高质量音频，但普遍依赖大规模私有数据和巨额计算资源，导致训练成本高昂，且模型的开源性与可复现性受限。  
- **既有问题**：主流方法（如基于Transformer的MusicGen-small）需在私有数据集上长时间训练，计算开销巨大（如约10^20 FLOPs），且仅少数模型使用公开数据，阻碍了研究的开放与普及。

2)  
- **核心方法**：论文提出用状态空间模型替代Transformer作为生成主干，并探索了两种设计：  
  - **单阶段SSM语言模型**：包括Prefix Mamba、Prefix SiMBA等变体，利用SSM的线性计算复杂度提升长序列建模效率。  
  - **两阶段混合设计**：首阶段用SSM生成粗粒度音频token（捕获语义信息），第二阶段用预训练扩散模型生成细粒度token以提升音质。  
- **解决思路**：  
  - **训练效率**：SSM的线性复杂度（相比Transformer的二次复杂度）减少了计算开销，在相同训练步数下收敛更快。  
  - **数据效率**：仅使用457小时公开CC授权数据（仅为基准模型数据的2%），通过两阶段分解降低建模难度。  
  - **开放性**：全程使用公开数据集与代码，确保可复现性。

3)  
- **任务与效果**：  
  - **音频生成质量**：在10秒和25秒音频生成任务上，两阶段SSM/扩散模型在客观指标（FD、KL、CLAP）与主观听测中均达到与MusicGen-small相当的性能。  
  - **训练效率**：仅用9%的FLOPs和2%的数据量，在单GPU上训练100k步即可获得竞争性结果。  
  - **模型缩放**：将SSM参数量缩减至原型的1/4（约94M参数）时，仍能保持较好的生成质量，体现了参数效率优势。
</div>

</details>

---

## Test-Time Adaptation For Speech Enhancement Via Mask Polarization
- **Authors**: Tobias Raichle, Erfan Amini, Bin Yang
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14770v1](https://arxiv.org/abs/2601.14770v1)
- **PDF**: [https://arxiv.org/pdf/2601.14770v1](https://arxiv.org/pdf/2601.14770v1)

在语音增强模型的实际部署中，适应未知环境至关重要，然而由于对语音增强模型在领域偏移下性能下降机制的理解不足，针对语音增强的测试时自适应研究仍较为缺乏。我们观察到，基于掩码的语音增强模型在领域偏移下会失去置信度，其预测的掩码趋于平坦化，从而削弱了语音保留与噪声抑制的决策能力。基于这一发现，我们提出掩码极化方法，这是一种轻量级的测试时自适应技术，通过基于Wasserstein距离的分布比较来恢复掩码的双峰特性。该方法无需在已训练模型之外引入额外参数，适用于资源受限的边缘部署场景。在不同领域偏移和模型架构下的实验结果表明，掩码极化方法能够取得非常稳定的性能提升，其效果与更为复杂的现有方法相当。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于深度学习的语音增强模型在训练数据之外的环境（域偏移）中性能会显著下降。实际部署需要模型能适应多样且时变的声学环境，而获取带标签的目标域数据通常不现实。
- **既有方法的问题**：
  - 无监督域适应方法通常需要访问源数据，存在隐私和存储问题。
  - 现有的测试时适应方法（如RemixIT、LaDen）需要引入额外的编码器或师生框架，增加了模型复杂性和参数量，不适用于资源受限的边缘部署。

2)  
论文提出了一种轻量级的测试时适应方法——掩码极化，其核心是通过恢复掩码的双峰分布来解决模型在域偏移下置信度下降的问题。具体方法如下：

- **关键观察**：在源域中，基于掩码的语音增强模型会预测出具有明显双峰分布（值接近0或1）的掩码，能有效分离语音和噪声。但在域偏移下，预测的掩码分布变得平坦（值集中在中间范围），表明模型失去了分离决策的信心，导致增强质量下降。

- **掩码极化方法**：
  - **生成参考掩码**：首先，通过一个简化的噪声谱估计（基于信号功率最低的时间帧）来计算一个参考掩码 \(M_P\)。该参考掩码利用了模型自身的增强输出和估计的噪声，其理论推导表明，无论在语音主导还是噪声主导的时频单元中，\(M_P\) 都比原始预测掩码 \(M\) 更倾向于极值（0或1），从而呈现出更理想的双峰特性。
  - **分布对齐**：不将 \(M_P\) 作为逐点的伪标签，而是通过计算预测掩码 \(M\) 与参考掩码 \(M_P\) 的经验分布之间的Wasserstein距离作为损失函数 \(L_W\)。这种基于分布的比较对不完美的噪声估计具有鲁棒性，同时能鼓励掩码恢复双峰结构。
  - **辅助约束**：增加一个损失项 \(L_S\)，惩罚预测掩码中出现的负值（域偏移下的常见现象）。
  - **总损失与稳定化**：总损失为 \(L = L_W + \lambda L_S\)。采用持续权重集成来稳定适应过程，即每一步更新后的模型参数都与源模型参数进行加权平均。

- **方法优势**：
  - **轻量高效**：无需在已训练模型之外添加任何组件或参数，仅需适应少量参数（如归一化层和输出层）。
  - **原理驱动**：直接针对域偏移下模型失效的根本原因（掩码置信度丧失）进行优化。
  - **适用性广**：方法适用于广泛的时频掩码类语音增强架构。

3)  
- **测试任务**：在多种域偏移场景下进行语音增强测试，包括噪声环境变化、语音内容变化以及两者同时变化。使用了9个不同的目标域数据集进行评估。
- **评估模型**：在两个代表性架构上验证：简单的幅度掩码模型和先进的CMGAN模型。
- **取得效果**：
  - 在多个感知质量（如PESQ、CSIG）和信号级指标上，MPol取得了与复杂方法（LaDen、RemixIT）相竞争甚至更优的性能。
  - 性能提升非常一致，在所有测试数据集上均观察到增益，而对比方法的性能波动较大。
  - 在计算开销上，MPol的实时因子远低于LaDen，更适用于边缘设备部署。
</div>

</details>

---

## Inverse-Hessian Regularization for Continual Learning in ASR
- **Authors**: Steven Vander Eeckt, Hugo Van hamme
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14751v1](https://arxiv.org/abs/2601.14751v1)
- **PDF**: [https://arxiv.org/pdf/2601.14751v1](https://arxiv.org/pdf/2601.14751v1)

灾难性遗忘仍是自动语音识别（ASR）持续学习面临的核心挑战，模型需在适应新领域的同时保持对已学习场景的性能。针对ASR的持续学习已提出多种方法，近期研究表明，权重平均——即在微调后通过合并步骤对模型进行平均——作为一种简单的无记忆策略具有显著效果。然而该方法本质上是启发式的，未考虑任务底层损失曲面的特性，限制了适应能力。本文提出逆海森正则化方法，这是一种用于ASR持续学习的无记忆策略，将曲率信息融入模型合并步骤。在新任务微调后，通过基于前一任务的克罗内克分解逆海森近似来调整适应过程，确保模型主要沿对历史性能损害较小的方向更新，同时保持方法的轻量化特性。我们在两个持续学习基准测试中验证了该方法，结果表明其显著优于现有先进基线，在提升适应能力的同时有效抑制遗忘。消融实验与进一步分析也证实了该方法的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动语音识别系统需持续适应新领域，但传统微调会导致灾难性遗忘，即模型在新任务上学习时，会严重遗忘旧任务的知识。
- **既有方法问题**：
  - 基于架构扩展或重放的方法受限于存储或隐私。
  - 无记忆的权重平均方法虽简单有效，但本质是启发式的，忽略了任务损失曲面的几何信息，导致随着任务增加，新任务权重趋近于零，适应性受限。

2)  
论文提出**逆Hessian正则化**，一种无记忆的持续学习方法，通过将曲率信息融入模型合并步骤来解决上述问题。其核心机制如下：

- **方法原理**：在微调新任务后，不直接平均新旧模型参数，而是利用旧任务损失函数的逆Hessian近似来调整参数更新方向。逆Hessian的主特征向量对应旧任务损失曲面较平坦的方向，沿这些方向更新对旧任务性能影响较小。

- **关键技术实现**：
  - **Kronecker分解近似**：为避免计算全Hessian矩阵的高开销，采用Kronecker分解的块对角近似，逐层估计线性层的逆Hessian。这既考虑了层内参数间的相互作用，又保持了计算轻量。
  - **轻量级合并步骤**：逆Hessian-向量乘法仅在微调后的合并步骤执行一次，无需在训练过程中持续计算，降低了计算负担。
  - **存储高效**：仅保留最近一个任务的逆Hessian近似（每层存储两个小矩阵），存储需求不随任务数量增长。
  - **参数更新策略**：对线性层，使用调整后的更新规则；对其他参数（如卷积层、偏置），采用简单的标量加权平均。

- **解决既有问题**：
  - 通过曲率感知的更新方向调整，模型能更智能地平衡新旧任务，避免权重平均中均匀妥协的启发式局限。
  - 保持了无记忆优势，同时提升了模型在新任务上的适应能力，并显著减轻遗忘。

3)  
- **任务与效果**：在两个持续学习基准测试上评估：
  - **实验1**：在Common Voice英语数据集的五种口音序列上，IHR取得了最佳平均词错误率，遗忘几乎为零，且在所有任务上均优于初始模型。
  - **实验2**：在LibriSpeech与Libri-Adapt的跨麦克风和口音域适应任务上，IHR在无记忆方法中表现最优，显著降低了平均词错误率，虽遗忘略高于实验1，但仍远低于基线。
- **整体表现**：IHR在保持高适应性的同时，显著减少了灾难性遗忘，其性能甚至可与需要存储历史数据的经验回放方法媲美。
</div>

</details>

---

## Unlocking Large Audio-Language Models for Interactive Language Learning
- **Authors**: Hongfu Liu, Zhouying Cui, Xiangming Gu, Ye Wang
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14744v1](https://arxiv.org/abs/2601.14744v1)
- **PDF**: [https://arxiv.org/pdf/2601.14744v1](https://arxiv.org/pdf/2601.14744v1)

在第二语言（L2）学习中，发音能力的提升仍面临挑战，尽管计算机辅助发音训练（CAPT）系统已有所发展。传统CAPT系统常提供缺乏直观性和可操作性的反馈，限制了其有效性。音频-语言模型（ALMs）的最新进展为改进此类系统提供了可能，能够提供更友好的用户反馈。本研究通过引入L2-Arctic-plus数据集（包含详细错误解释及可操作的改进建议的英语发音数据集），探索了基于对话的发音训练中ALMs的应用。我们在该数据集上对级联ASR+LLM模型及现有ALMs进行了基准测试，重点关注发音错误检测与可操作性反馈生成。为进一步提升性能，我们提出基于L2-Arctic-plus对ALMs进行指令微调。实验结果表明，经过指令微调的模型在发音错误检测和建议生成任务上，无论是客观指标还是人工评估均显著优于现有基线，凸显了所提数据集的价值。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：第二语言（L2）发音学习具有挑战性，计算机辅助发音训练（CAPT）系统旨在提供支持。
- **传统CAPT系统的问题**：其反馈通常不够直观，缺乏可操作的改进指导，限制了训练效果。
- **现有技术方案的局限**：
  - 级联的ASR+LLM框架中，ASR模型倾向于纠正发音错误，导致LLM无法从原始音频中准确检测错误。
  - 现有的音频-语言模型（ALMs）在此复杂任务上表现不佳，缺乏针对性的训练数据。

2) **论文核心方法如何解决上述问题**
论文通过构建专用数据集并对ALMs进行指令微调，系统性地提升了聊天式发音训练的效果。

- **构建专用数据集L2-Arctic-plus**：
  - 基于L2-Arctic数据集扩展，为每个样本添加了基于文本的错误解释和可操作的改进建议标注。
  - 标注过程采用“从粗到精”的两阶段法：先利用GPT-4o生成初始反馈，再由人工验证和修正，确保质量。

- **对ALMs进行两阶段指令微调**：
  - **第一阶段：声学特征对齐**。使用大量ASR数据（如CommonVoice）训练一个可学习的投影器，将音频编码器的特征与LLM的文本嵌入空间对齐。此阶段仅训练投影器，使模型初步理解音频。
  - **第二阶段：任务特定指令微调**。使用在L2-Arctic数据集上构建的发音训练数据（与测试集L2-Arctic-plus无重叠）继续微调模型。此阶段采用LoRA技术同时微调投影器和LLM主干，使模型学会根据音频输入生成包含错误检测和具体建议的文本反馈。

- **方法优势**：
  - **端到端处理**：避免了ASR转录造成的信息丢失，ALMs能直接利用音频中的声学细节。
  - **针对性训练**：指令微调使模型专注于“检测错误”和“生成建议”这一特定任务。
  - **缓解幻觉**：任务特定训练有效减少了模型在检测中输出规范文本外单词的情况。

3) **在哪些任务上取得了怎样的效果**
论文在**聊天式发音训练**这一核心任务上评估效果，具体包括**发音错误检测**和**反馈建议生成**两个子任务。

- **客观指标显著提升**：在L2-Arctic-plus测试集上，指令微调后的ALMs在错误检测的F1分数上，相比最好的级联ASR+LLM基线提升了134.3%，相比现有最好的ALM（GPT-4o-Audio）提升了35.6%。在建议生成的BLEU-2、ROUGE-L等文本相似度指标上也有大幅改善。
- **人类评估领先**：在建议相关性、用户可理解性和整体评价三个维度上，指令微调模型均显著优于所有基线模型（包括GPT-4o-Audio）。
- **LLM即法官评估胜出**：使用GPT-4o作为评估者进行参考引导的评分和成对比较，指令微调模型获得了最高平均分，并在与基线的对比中取得了很高的胜率。
</div>

</details>

---

## AQAScore: Evaluating Semantic Alignment in Text-to-Audio Generation via Audio Question Answering
- **Authors**: Chun-Yi Kuan, Kai-Wei Chang, Hung-yi Lee
- **Categories**: eess.AS, cs.AI, cs.CL, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14728v1](https://arxiv.org/abs/2601.14728v1)
- **PDF**: [https://arxiv.org/pdf/2601.14728v1](https://arxiv.org/pdf/2601.14728v1)

尽管文本到音频生成在真实性和多样性方面取得了显著进展，但评估指标的发展却未能同步跟进。目前广泛采用的方法（通常基于CLAPScore等嵌入相似性度量）虽能有效衡量整体相关性，但在细粒度语义对齐和组合推理方面仍存在局限。为此，我们提出了AQAScore——一个与模型架构无关的评估框架，它利用音频感知大语言模型（ALLMs）的推理能力。AQAScore将评估重新定义为概率化语义验证任务：不依赖开放式文本生成，而是通过计算针对特定语义查询给出“是”答案的精确对数概率来估计对齐程度。我们在多个基准测试中评估AQAScore，包括人工评定的相关性、成对比较和组合推理任务。实验结果表明，相较于基于相似性的指标和生成式提示基线方法，AQAScore始终与人类判断保持更高相关性，证明了其在捕捉细微语义不一致性方面的有效性，并能随着底层ALLMs能力的提升而扩展。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本到音频生成技术发展迅速，但评估方法滞后。  
- **既有方法问题**：广泛采用的基于嵌入相似性的方法（如CLAPScore）虽能衡量整体相关性，但在细粒度语义对齐和组合推理方面存在局限，难以准确评估多声音事件、时序或属性绑定等复杂语义。

2)  
- **核心方法**：AQAScore提出一个与主干模型无关的评估框架，将评估重新定义为概率语义验证任务。  
- **具体解决方式**：  
  - **利用音频感知大语言模型**：借助ALLMs的音频问答能力，将音频-文本对齐问题转化为“音频是否包含文本描述的声音事件？”的二元问答。  
  - **概率化评估**：通过计算模型对“是”回答的精确对数概率，获得连续的对齐分数，而非依赖开放式文本生成或全局嵌入相似性。  
  - **细粒度检测**：这种方法能捕捉缺失或错误的语义元素，有效识别时序颠倒、属性绑定错误等细微不一致。  
  - **可扩展性**：框架兼容不同ALLM主干，其评估效果随ALLM能力提升而增强。

3)  
- **评估任务与效果**：  
  - **人类评分相关性**：在RELATE和PAM基准上，AQAScore与人类评分的相关性（皮尔逊、斯皮尔曼系数）显著高于CLAPScore及生成式提示基线。  
  - **成对比较**：在RELATE-Pair和Baton-Pair数据集上，其预测人类偏好的准确率和AUC值均优于基线。  
  - **组合推理**：在CompA基准（事件顺序和属性绑定任务）上，AQAScore表现优于专门优化的CompA-CLAP等方法，展示了更强的语义推理能力。
</div>

</details>

---

## NLP-Based Review for Toxic Comment Detection Tailored to the Chinese Cyberspace
- **Authors**: Ruixing Ren, Junhui Zhao, Xiaoke Sun, Qiuping Li
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14721v1](https://arxiv.org/abs/2601.14721v1)
- **PDF**: [https://arxiv.org/pdf/2601.14721v1](https://arxiv.org/pdf/2601.14721v1)

随着移动互联网的深度融合与社交平台的广泛普及，中文网络空间中的用户生成内容呈现爆发式增长。其中，有害评论的泛滥对个体心理健康、社区氛围及社会信任构成了严峻挑战。由于中文网络语言具有强语境依赖性、文化特异性及快速演化等特点，有害表达常通过谐音、隐喻等复杂形式传递，给传统检测方法带来显著局限。本文聚焦于中文网络空间基于自然语言处理的有害评论检测这一核心议题，系统梳理并批判性分析了该领域的研究进展与关键挑战。首先界定了中文有害评论的内涵与特征，并剖析了其所依赖的平台生态与传播机制；继而全面综述了现有公开数据集的构建方法及其局限性，提出了一种新颖的细粒度、可扩展的有害评论定义与分类框架，以及相应的数据标注与质量评估策略。系统总结了检测模型从传统方法到深度学习的演进路径，特别强调了可解释性在模型设计中的重要性。最后深入探讨了当前研究面临的开放性挑战，并对未来研究方向提出了前瞻性建议。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：随着移动互联网与社交平台的深度融合，中文网络空间用户生成内容爆炸式增长，其中有害评论的泛滥对个人心理健康、社区氛围和社会信任构成严峻挑战。  
- **既有方法的问题**：传统检测方法面临显著局限，因为中文网络语言具有强语境依赖性、文化特异性和快速演化性。有害表达常通过谐音、隐喻等复杂形式传递，使得基于规则、词典或传统机器学习的方法难以有效识别隐晦、动态且高度依赖平台和语境的有害内容。  

2)  
- **提出细粒度定义与分类框架**：论文构建了一个包含攻击类型、强度、目标和意图四个维度的细粒度有害评论定义与分级体系（如表IV所示），为数据标注和模型理解提供了统一、清晰的概念基础。该框架超越了简单的二元分类，能更精确地刻画有害内容的丰富梯度特征。  
- **倡导人机协同的高效数据构建方法**：针对标注成本高、主观性强的问题，论文提出了一个动态增强的闭环人机协同标注工作流（如图3）。该流程结合规则/词典过滤、大语言模型预标注、人工重点审核与仲裁，并利用生成式模型进行定向数据增强，旨在以低成本实现大规模、高质量标注数据的可持续生产。  
- **系统梳理技术演进并强调可解释性**：论文系统回顾了从规则方法、统计机器学习、深度学习到预训练模型及大语言模型的技术发展路径（如表VI）。特别强调了模型可解释性的重要性，总结了从规则透明、LIME等归因方法、注意力可视化到集成梯度以及大语言模型思维链提示等多种解释技术，旨在提升检测系统的可靠性和决策透明度。  
- **注重平台生态适配性**：论文深入分析了中文主流平台（如微博、小红书、B站等）的生态多样性及其对有害评论表现形式的影响（如表II），指出检测系统需适应不同平台的社区文化和话语体系。提出的分级框架包含了基于平台语境的结果校准步骤，以提升模型的跨平台适用性。  

3)  
- **任务与效果**：本论文是一篇综述，未提出单一的新模型，因此不报告具体的模型性能指标。其核心贡献在于系统性地梳理和批判性分析了中文有害评论检测领域的研究进展。  
- **效果体现在**：为构建更适应中文网络空间复杂性的检测系统提供了清晰的路线图和坚实的文献基础，具体包括：明确了有害评论的概念体系与平台生态；提出了改进数据集构建（细粒度标注、人机协同）和质量评估的策略；总结了模型技术演进与可解释性分析方法；并指出了当前研究面临的开放性挑战（如对同质语境的过度依赖、多模态信息利用不足、数据噪声影响等），为未来研究方向提供了前瞻性建议。
</div>

</details>

---

## Triage knowledge distillation for speaker verification
- **Authors**: Ju-ho Kim, Youngmoon Jung, Joon-Young Yang, Jaeyoung Roh, Chang Woo Han, Hoon-Young Cho
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.14699v1](https://arxiv.org/abs/2601.14699v1)
- **PDF**: [https://arxiv.org/pdf/2601.14699v1](https://arxiv.org/pdf/2601.14699v1)

在资源受限设备上部署说话人验证系统仍面临高容量模型计算成本的挑战，知识蒸馏（KD）为此提供了解决方案。传统KD方法将目标类置信度与非目标类结构信息耦合在KL散度项中，限制了关系信息的传递。解耦KD虽将这两类信号分离为目标项与非目标项，却对非目标类进行均等处理，且在大规模类别场景中易受低概率长尾类别的影响。本文提出分诊式知识蒸馏（TRKD），该方案通过“评估-优先级划分-聚焦”三阶段实现蒸馏过程。TRKD引入累积概率阈值τ评估每个样本的难度，并将教师后验概率划分为三组：目标类、高概率非目标混淆集和背景集。为优先传递信息量高的信号，TRKD蒸馏混淆集的条件分布并舍弃背景集，同时传递反映样本难度与类间混淆关系的三值质量分布（目标/混淆/背景）。最后，TRKD通过基于τ的课程学习机制聚焦训练：初始阶段采用较大τ值传递广泛非目标上下文信息，随后逐步减小τ以收缩混淆集范围，使监督聚焦于最易混淆的类别。在VoxCeleb1数据集上进行的同构与异构师生模型组合实验中，TRKD始终优于近期KD变体，并在所有测试协议中取得了最低的等错误率。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在资源受限设备上部署说话人验证系统面临高容量模型计算成本高的挑战，知识蒸馏是常用解决方案。  
- **既有方法问题**：  
  - 经典知识蒸馏将目标类置信度与非目标类结构信息耦合在单一KL散度项中，限制了关系信息的传递。  
  - 解耦知识蒸馏虽分离了目标与非目标信号，但仍平等对待所有非目标类，且在大规模分类中易受低概率长尾类干扰，导致监督信号被稀释。

2)  
论文提出**Triage知识蒸馏**，其核心方法遵循“评估-优先-聚焦”三步原则，具体如下：  
- **评估**：引入累积概率阈值τ，根据教师后验将类别划分为三组：  
  - 目标类  
  - 高概率非目标混淆集（累积概率超过τ的最小类别子集）  
  - 低概率背景集  
- **优先**：  
  - 对齐学生与教师在混淆集上的条件分布，传递细粒度类别关系。  
  - 通过三质量项（目标/混淆/背景）传递样本难度和类间混淆信息。  
  - 丢弃背景集的细粒度对齐，避免长尾噪声干扰。  
- **聚焦**：采用基于τ的课程学习策略：  
  - 训练初期使用较大τ，传递广泛的非目标上下文信息。  
  - 逐步减小τ以收缩混淆集，使监督集中在最易混淆的类别上，从而稳定优化并锐化决策边界。

3)  
- **任务**：在VoxCeleb1数据集的原版、扩展版和困难版三个协议上进行说话人验证实验。  
- **效果**：  
  - TRKD在18组教师-学生配对评估中均取得最低等错误率，平均相对学生基线提升18.7%。  
  - 在异构和同构模型配对中均一致优于现有知识蒸馏方法，显著缩小了教师与学生之间的性能差距。
</div>

</details>

---

## Dissecting Performance Degradation in Audio Source Separation under Sampling Frequency Mismatch
- **Authors**: Kanami Imamura, Tomohiko Nakamura, Kohei Yatabe, Hiroshi Saruwatari
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14684v1](https://arxiv.org/abs/2601.14684v1)
- **PDF**: [https://arxiv.org/pdf/2601.14684v1](https://arxiv.org/pdf/2601.14684v1)

基于深度神经网络的音频处理方法通常在单一采样频率下进行训练。为处理未经训练的采样频率，信号重采样被普遍采用，但该方法可能导致性能下降，尤其在输入采样频率低于训练采样频率时更为明显。本文通过两项假设探究性能下降的原因：(i) 上采样过程缺失高频分量；(ii) 高频分量的存在比其精确表征更为重要。为验证假设，我们将传统重采样与三种替代方案进行比较：后重采样噪声添加（向重采样信号添加高斯噪声）、噪声核重采样（通过高斯噪声扰动插值核以增强高频分量）以及可训练核重采样（通过训练自适应调整插值核）。音乐源分离实验表明，噪声核与可训练核重采样能缓解传统重采样导致的性能下降。我们进一步证明噪声核重采样在不同模型中均具有有效性，凸显其作为一种简洁而实用的解决方案的潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于深度神经网络的音频处理方法通常在单一采样频率下训练。当处理未经训练的采样频率时，通常会对输入信号进行重采样，但这可能导致性能下降，尤其是在输入采样频率低于训练采样频率时。现有研究多关注特定模型和条件，缺乏系统性分析，导致性能下降的机制尚不明确。

2)  
论文通过两个假设探究性能下降的原因，并提出了三种重采样方法进行验证和解决。

- **假设与验证**：
    - **假设一**：上采样导致的高频成分缺失是性能下降的原因。当输入采样频率较低时，上采样后高于输入奈奎斯特频率的成分被置零，这与训练数据分布不匹配。
    - **假设二**：高频成分的“存在”比其“具体频谱内容”对分离性能更重要。许多分离模型（如Conv-TasNet、BSRNN）对高频成分的分辨率较粗，但需要其存在以维持性能。

- **提出的方法**：
    - **后重采样噪声添加**：在常规重采样后的信号上直接添加高斯噪声，以补充高频能量。实验表明，该方法未能有效缓解性能下降，因为添加的噪声与信号无关，可能干扰原有成分。
    - **噪声核重采样**：在用于插值的窗函数sinc核上添加高斯噪声。这使得重采样后的信号在高频带（高于输入奈奎斯特频率）具有与输入信号相关的能量。该方法简单有效，能持续缓解性能下降。
    - **可训练核重采样**：使用深度神经网络（MLP）参数化插值核，并在保持预训练分离模型冻结的情况下，仅训练该核网络，使其适应模型以减少失配。该方法也能有效缓解性能下降。

- **核心解决机制**：噪声核与可训练核方法通过引入与输入信号相关的高频成分（而非无关噪声），满足了分离模型对高频成分“存在”的需求，从而解决了因采样频率失配导致的性能退化问题。实验支持了上述两个假设。

3)  
- **任务**：在音乐源分离任务上进行了评估，使用了MUSDB18-HQ数据集，分离目标包括人声、贝斯、鼓和其他乐器。
- **效果**：
    - **噪声核重采样**：在输入采样频率（如8 kHz）低于训练频率（44.1 kHz）时，有效缓解了常规重采样带来的性能下降（例如，对于BSRNN模型的人声分离，SDR提升约2.6 dB），并且在多种分离模型（如Conv-TasNet、BSRNN、Mel-RoFormer、HT-Demucs）上均表现出有效性。
    - **可训练核重采样**：同样能有效缓解性能下降，效果与噪声核方法相当。
    - **通用性与安全性**：噪声核重采样在常规重采样未导致性能下降的模型上使用时，也不会损害性能，表明其具有普适性和实用性。
</div>

</details>

---

## READ-Net: Clarifying Emotional Ambiguity via Adaptive Feature Recalibration for Audio-Visual Depression Detection
- **Authors**: Chenglizhao Chen, Boze Li, Mengke Song, Dehao Feng, Xinyu Liu, Shanchen Pang, Jufeng Yang, Hui Yu
- **Categories**: cs.CV, cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14651v1](https://arxiv.org/abs/2601.14651v1)
- **PDF**: [https://arxiv.org/pdf/2601.14651v1](https://arxiv.org/pdf/2601.14651v1)

抑郁症是一种严重的全球性心理健康问题，会损害日常功能与整体生活质量。尽管当前视听融合方法已提升了自动抑郁检测的性能，但忽略情感线索的方法往往难以捕捉隐藏在情感表达中的细微抑郁信号。反之，引入情感信息的方法又常在特征表示中将短暂的情绪表达与稳定的抑郁症状相混淆，这一现象被称为**情感模糊性**，进而导致检测误差。为解决这一关键问题，我们提出了READ-Net，这是首个通过自适应特征重校准（AFR）机制来显式解决情感模糊性的视听抑郁检测框架。AFR的核心思想是动态调整情感特征的权重，以增强与抑郁相关的信号。READ-Net并非简单地忽略或直接融合情感信息，而是创新性地识别并保留情感特征中与抑郁相关的线索，同时自适应地滤除无关的情感噪声。这一重校准策略显著提升了特征表示的清晰度，有效缓解了情感干扰这一长期存在的挑战。此外，READ-Net能够轻松集成到现有框架中以提升性能。在三个公开数据集上的大量实验表明，READ-Net优于当前最先进方法，在准确率上平均提升4.55%，F1分数平均提升1.26%，证明了其对情感干扰的鲁棒性，并推动了视听抑郁检测的进步。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：抑郁症是全球严重的心理健康问题，传统临床诊断依赖主观量表，存在时效性低、主观性强等局限。基于视听模态的自动检测方法受到关注，但现有方法在处理情感线索时存在不足。
- **既有方法的问题**：
  - **情感无关方法**：忽略情感线索，可能遗漏抑郁相关的情绪表达（如情感钝化），导致特征建模不完整。
  - **情感感知方法**：直接融合情感与抑郁特征，易将短暂情绪波动（如访谈中的瞬间悲伤）与稳定的抑郁症状混淆，产生“情感模糊性”，干扰检测准确性。

2)  
论文提出 **READ-Net** 框架，通过 **自适应特征重校准（AFR）** 解决情感模糊性问题。其核心方法包括三个模块：
- **分层特征分离（HFS）**：
  - 从原始多模态特征中分离出抑郁特征、噪声特征和情感特征。
  - 进一步将情感特征细分为抑郁相关情感特征和非抑郁相关情感特征，避免短暂情绪干扰。
- **双重一致性正则化（DCR）**：
  - 通过动态图构建与正则化，增强同类特征（如抑郁特征）内部一致性，抑制跨类特征（如情感与噪声）之间的相关性。
  - 使用信息流控制损失，提升特征判别性，确保抑郁相关情感特征与抑郁特征对齐。
- **非对称蒸馏（AD）**：
  - 采用“教师-学生”网络，从情感特征中蒸馏出抑郁相关线索，并单向融合到抑郁特征中，避免噪声反向传播。
  - 通过注意力机制自适应调整权重，保留与抑郁相关的情绪信号（如持久性情感钝化），过滤情境性情绪噪声。

整体上，AFR 机制动态评估情感特征与抑郁的相关性，实现特征表示的澄清与增强，从而区分稳定抑郁症状与短暂情绪波动。

3)  
- **任务与数据集**：在三个公开数据集上进行评估：**LMVD**（视频访谈）、**D-vlog**（网络视频日志）和 **DAIC-WOZ**（临床访谈）。
- **效果**：
  - 在 LMVD 上准确率达到 77.59%，F1分数 77.47%，显著优于现有最佳方法（平均准确率提升 4.55%，F1提升 1.26%）。
  - 在 D-vlog 和 DAIC-WOZ 上也取得最优或竞争性结果，尤其在处理高情感模糊性场景中表现稳健。
  - 作为即插即用模块，能提升现有模型（如 ViT、DepMamba）的性能，平均准确率提升 1.48%。
</div>

</details>

---

## Scaling Ambiguity: Augmenting Human Annotation in Speech Emotion Recognition with Audio-Language Models
- **Authors**: Wenda Zhang, Hongyu Jin, Siyi Wang, Zhiqiang Wei, Ting Dang
- **Categories**: eess.AS, cs.AI, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.14620v1](https://arxiv.org/abs/2601.14620v1)
- **PDF**: [https://arxiv.org/pdf/2601.14620v1](https://arxiv.org/pdf/2601.14620v1)

语音情感识别模型通常采用单一类别标签，这忽视了人类情感固有的模糊性。模糊情感识别通过将情感表示为概率分布来解决此问题，但其进展受限于从稀疏人工标注推断出的不可靠真实分布。本文探讨大型音频-语言模型能否通过生成高质量合成标注来缓解标注瓶颈。我们提出一种利用音频-语言模型创建合成感知代理的框架，通过增强人工标注来提升真实分布的可靠性。我们通过统计分析这些代理与人类分布的一致性进行验证，并利用增强后的情感分布对音频-语言模型进行微调以评估其效果。此外，为解决类别不平衡问题并实现无偏评估，我们提出分布感知多模态情感增强策略DiME-Aug。在IEMOCAP和MSP-Podcast数据集上的实验表明，合成标注能有效改善情感分布，尤其在标注一致性较高的低模糊度区域效果显著。然而，对于人类标注分歧较大的高模糊度情感，其改善效果有限。本研究首次证明音频-语言模型能够缓解模糊情感识别中的标注稀缺问题，同时指出需要开发更先进的提示或生成策略来处理高模糊度案例。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音情感识别传统上使用单一类别标签，忽略了人类情感固有的模糊性。模糊情感识别将情感建模为概率分布，但进展受限于从稀疏人工标注推断出的不可靠真实分布。
- **既有问题**：现有方法依赖每段语音仅3-5个标注者，导致分布估计粗糙，无法充分捕捉情感模糊性，限制了模型学习真实模糊性和泛化能力。

2)  
论文提出一个三模块框架，利用音频-语言模型生成合成标注以增强稀疏人工标注，从而解决标注瓶颈和分布不可靠问题。

- **合成感知代理**：
    - 使用ALM（如Gemini 2.5-Pro）生成合成情感标注。通过设计详细提示（分析音频和文本）并引入控制变量（如调整温度参数、修改提示以模拟不同标注者视角），为每段语音生成大量单标签合成标注。
    - 将这些合成标注与原始稀疏人工标注结合，视为从底层情感分布中抽取的样本，共同推断出增强的情感分布。

- **DiME-Aug（分布感知多模态情感增强）**：
    - 为解决多模态情感数据集中常见的类别不平衡问题，提出一种数据增强策略。
    - 基于特征空间中的最近邻搜索，对少数类样本的音频信号进行混合（mixup），并相应插值其真实情感分布以生成新样本的分布。文本模态则继承主导原始话语的转录本，确保多模态一致性。
    - 此策略创造了平衡的训练集，支持对合成标注影响的可靠、无偏评估。

- **ALM微调**：
    - 使用增强后的情感分布微调ALM骨干网络（如Qwen2-Audio）。
    - 添加一个分布预测头，并使用Jensen-Shannon散度损失直接优化预测分布与目标分布之间的相似性。
    - 通过比较在不同标注源（仅人工、仅合成、混合）上微调后的模型性能，评估合成标注的效用。

3)  
- **任务**：在模糊情感识别任务上评估方法，使用IEMOCAP和MSP-Podcast数据集，聚焦愤怒、快乐、悲伤、中性四类情感。
- **效果**：
    - **分布近似**：合成标注生成的情感分布能与人工分布较好对齐（JS散度降低），约6-10个合成标注即可达到饱和。
    - **下游性能**：在MSP-Podcast（自然语音、模糊性较低）上，混合标注取得了最佳性能（JS散度最低，BC系数最高），优于仅用人工标注。在IEMOCAP（表演语音、模糊性较高）上，人工标注仍表现最佳，合成标注在高度模糊区域补充效果有限。
    - **数据增强**：DiME-Aug有效缓解了类别不平衡，在大多数配置下提升了模型性能。
</div>

</details>

---
