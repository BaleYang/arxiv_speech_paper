---
layout: post
title: "arXiv Daily – 2026-02-26"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-26（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-25 08:50 — 2026-02-26 08:50
- 抓取总数：5 篇 | 本页显示：5 篇（去重/过滤后）

## TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition
- **Authors**: Cheng-Yeh Yang, Chien-Chun Wang, Li-Wei Chen, Hung-Shin Lee, Hsin-Min Wang, Berlin Chen
- **Categories**: eess.AS, cs.AI, cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.22039v1](https://arxiv.org/abs/2602.22039v1)
- **PDF**: [https://arxiv.org/pdf/2602.22039v1](https://arxiv.org/pdf/2602.22039v1)

低资源自动语音识别（ASR）仍面临重大挑战，主要源于许多语言缺乏充足的转录数据。尽管电视剧和网络视频中存在大量口语内容，但以台湾闽南语为例，其转录文本往往稀缺，且现有字幕大多仅提供普通话版本。为弥补这一不足，我们提出TG-ASR——一种面向台湾闽南语剧集语音识别的翻译引导ASR框架，该框架利用多语言翻译嵌入来提升低资源环境下的识别性能。其核心是并行门控交叉注意力机制，该机制能自适应地将多种辅助语言的嵌入向量整合至ASR解码器中，在实现强健跨语言语义引导的同时，确保优化稳定性并最小化语言间干扰。为支持持续研究，我们构建了YT-THDC语料库，包含30小时台湾闽南语剧集语音、对齐的普通话字幕及人工校验的闽南语转录文本。综合实验与分析确定了最能有效提升ASR性能的辅助语言，实现了字错误率相对降低14.77%，证明了翻译引导学习在实际应用中对资源匮乏语言的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：许多地区性及濒危语言（如台湾闽南语）的语音转录数据稀缺，导致其自动语音识别（ASR）发展受限。尽管存在大量影视剧等口语内容，但字幕多为普通话，缺乏本族语转录。  
- **既有方法问题**：传统低资源ASR方法（如数据增强、多语言联合训练、从资源丰富语言迁移）在语音数据极少且与预训练语言差异大时效果有限；针对台湾闽南语，现有方法尝试利用普通话字幕进行对齐，但未能充分利用多语言翻译文本的语义信息。

2)  
论文提出 **TG-ASR** 框架，通过 **并行门控交叉注意力（PGCA）** 机制解决上述问题，具体如下：  
- **核心架构**：基于 Whisper 模型，采用两阶段训练。第一阶段微调整个 Whisper 模型（编码器与解码器）；第二阶段冻结编码器与第一阶段解码器参数，仅更新新增的 PGCA 层。  
- **多语言嵌入提取**：使用 SeamlessM4T 将语音转录翻译成多种辅助语言（如普通话、英语、西班牙语等），再通过预训练的多语言 BERT 提取翻译嵌入。  
- **PGCA 机制**：  
  - **并行交叉注意力**：在解码器每个模块前端引入多个独立的交叉注意力分支，分别处理不同辅助语言的翻译嵌入，使模型能同时关注多种语言信息。  
  - **门控控制**：每个分支配备可学习的 tanh 门控参数，动态调节各语言嵌入的贡献度，促进有益信号，抑制噪声或干扰。  
  - **自适应融合**：加权后的多语言嵌入与解码器输入相加，再经前馈网络处理，从而在解码过程中注入跨语言语义指导。  
- **优势**：PGCA 实现了多语言信息的自适应融合，在保持训练稳定性的同时，最小化语言间干扰，显著提升了低资源语言 ASR 的准确性。

3)  
- **任务**：台湾闽南语电视剧语音识别（低资源场景）。  
- **效果**：在自建的 **YT-THDC 数据集**（30小时台湾闽南语电视剧语音，含普通话字幕和人工校验转录）上评估：  
  - 使用 **普通话与西班牙语** 作为辅助语言时效果最佳，相比无辅助语言的基线模型，**字错误率相对降低 14.77%**。  
  - 多语言组合（普通话+西班牙语）优于单语言辅助，证明 PGCA 能有效利用互补的跨语言信息。  
  - 消融实验验证了 PGCA 各组件（如门控机制、并行注意力结构）的必要性。
</div>

</details>

---

## EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs
- **Authors**: Wenjie Tian, Zhixian Zhao, Jingbin Hu, Huakang Chen, Haohe Liu, Binshen Mu, Lei Xie
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.21900v1](https://arxiv.org/abs/2602.21900v1)
- **PDF**: [https://arxiv.org/pdf/2602.21900v1](https://arxiv.org/pdf/2602.21900v1)

全模态大语言模型（Omni-LLMs）的发展革新了人机交互，实现了统一的视听感知与语音响应。然而，现有全模态大语言模型在复杂现实场景中表现欠佳，常导致理解流于表面及情感回应与语境失配。这一问题因其“思考者-表达者”架构而进一步加剧：该架构通过隐状态间接连接，致使情感细节大量流失。本研究提出EmoOmni——一个面向多模态情感对话的精准理解与表达统一框架。其核心在于引入情感思维链（E-CoT），该机制强制模型从细粒度多模态感知逐步推理至文本响应。此外，我们显式地将E-CoT作为高层情感指令来引导表达模块，从而实现精准的情感表达。为支撑模型构建，我们建立了EmoOmniPipe管道以获取真实场景的标注对话数据，并创建基准测试集EmoOmniEval，以系统评估多模态情感对话任务。实验表明，在采用相同表达模块的条件下，EmoOmni-7B模型取得了与Qwen3Omni-30B-A3B-Thinking相当的性能表现。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：全模态大语言模型（Omni-LLMs）旨在实现统一的多模态感知与语音交互，但在复杂真实场景中面临挑战。  
- **既有问题**：  
  - 模型对音频、视觉等多模态线索的理解往往流于表面，尤其在信号冲突时（如欢快语调伴随皱眉表情）易产生错误意图推断。  
  - 主流“思考者-说话者”架构仅通过隐藏状态隐式传递情感信息，导致情感细节丢失，生成语音虽语义正确但情感失准。  
  - 缺乏高质量、细粒度标注的真实世界多模态对话数据，且现有评估基准多关注任务正确性，忽视交互情境中的情感智能。

2)  
论文提出 **EmoOmni** 框架，通过显式建模“感知-推理-表达”因果链解决上述问题：  
- **情感思维链（E-CoT）机制**：  
  - 将推理过程结构化，依次进行：1）细粒度多模态情感感知；2）用户意图分析；3）响应策略规划；4）文本响应生成。  
  - E-CoT 作为高层情感指令显式指导说话者模块，确保情感细节在传递中不被稀释，实现语义与情感的对齐。  
- **两阶段训练策略**：  
  - 第一阶段专注**感知基础**，使用多模态情感理解数据优化模型对音频、视觉线索的细粒度感知能力。  
  - 第二阶段进行**联合推理调优**，激活完整因果链，学习从感知到生成的复杂依赖关系。  
- **数据与评估体系**：  
  - 构建 **EmoOmniPipe** 数据流水线，从电影、电视剧中提取并标注情感丰富的真实对话数据，解决数据稀缺问题。  
  - 建立 **EmoOmniEval** 基准，从视频到语音、视频到文本、指令跟随三个维度系统评估模型的情感理解与表达能力。

3)  
- **任务与效果**：在构建的 **EmoOmniEval** 基准（包含 MELD 和 ch-sims-v2 测试集）上，EmoOmni-7B 与参数量大得多的 Qwen3Omni-30B-A3B-Thinking 模型在相同说话者条件下取得了相当的性能。  
- **具体表现**：在视频到语音的响应情感策略（VS-RES）和内容相关性（VS-RC）等关键指标上显著优于同规模模型，并在文本情感分析（VT-EA）等任务上展现出强大能力，证明了 E-CoT 与真实世界数据能有效补偿模型规模，提升多模态情感对话中的理解与表达质量。
</div>

</details>

---

## UniWhisper: Efficient Continual Multi-task Training for Robust Universal Audio Representation
- **Authors**: Yuxuan Chen, Peize He, Haoyuan Xu, Junzi Zhang
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.21772v1](https://arxiv.org/abs/2602.21772v1)
- **PDF**: [https://arxiv.org/pdf/2602.21772v1](https://arxiv.org/pdf/2602.21772v1)

通用音频表征应能在单一编码器中同时捕获细粒度语音线索与环境声、音乐的高层语义。现有编码器往往在单一领域表现优异，而在其他领域性能下降。本文提出UniWhisper——一种高效的持续多任务训练框架，将异构音频任务统一转化为指令-应答格式。该方法支持标准的下一个词元训练，无需任务特定的输出头或损失函数。我们使用38千小时的公开音频数据进行训练，并通过浅层MLP探针和k近邻（kNN）方法在涵盖语音、环境声和音乐的20项任务上评估编码器性能。实验表明：UniWhisper在MLP探针评估中达到0.81的归一化加权平均值，在kNN评估中达到0.61，显著优于Whisper的0.64和0.46，同时保持了优异的语音处理能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：通用音频表征旨在用一个编码器同时处理语音、环境声和音乐。现有方法存在**领域不平衡**问题：语音专用编码器（如Whisper）缺乏对非语音场景的语义理解，而通用音频模型（如CLAP）又难以保留语音所需的细粒度时序线索。  
- **既有方法的问题**：为扩大覆盖范围，现有方案要么通过大规模持续训练（成本高），要么采用多编码器融合（如SALMONN），但这会引入**音频令牌冗余**、增加计算开销，并需要额外的对齐数据与协调机制。

2)  
论文提出 **UniWhisper**，一个高效的持续多任务训练框架，其核心方法通过以下设计解决上述问题：  

- **统一的指令-答案格式**：  
  - 将异构音频任务（如语音识别、音频描述、音频问答）全部转化为统一的提示词（含控制标签和自然语言指令）与文本答案格式。  
  - 所有任务共享相同的训练和解码接口，无需为不同任务设计专用头部或损失函数，简化了训练流程。  

- **单编码器与紧凑解码器**：  
  - 采用单一音频编码器（基于Whisper Large-v3初始化），避免多编码器特征拼接带来的令牌冗余和上下文窗口压力。  
  - 将原始Whisper解码器替换为预训练语言模型（Qwen3 0.6B），并通过轻量适配器将编码器输出映射到解码器隐藏维度。该解码器提供强大的语言先验，能更好地对齐指令跟随目标，加速收敛。  

- **高效的持续多任务训练**：  
  - 在38k小时的公开音频数据上，通过混合多领域任务数据进行统一格式的持续训练。  
  - 仅更新编码器和投影适配器，冻结预训练语言模型解码器，以标准下一词元预测损失（仅对答案词元）进行优化，显著降低了训练数据需求和计算成本。  

该方法在**统一监督信号**下，增强了编码器对非语音语义的捕获能力，同时保留了语音任务的细粒度线索，实现了跨领域的高效平衡。

3)  
在涵盖语音、环境声和音乐的**20个任务**上进行评估：  
- **评估协议**：使用浅层MLP探针和非参数kNN分类，计算归一化加权平均分数。  
- **主要效果**：UniWhisper在MLP探针上达到**0.81**，在kNN上达到**0.61**，显著优于Whisper（0.64和0.46）及其他基线模型（如HuBERT、WavLM、BEATs、CLAP）。  
- **跨领域表现**：在非语音任务（如音频标注、标签分类）上提升明显，同时在语音任务（如说话人识别、语音识别）上保持强大性能，未出现明显的灾难性遗忘。
</div>

</details>

---

## Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization
- **Authors**: MD. Sagor Chowdhury, Adiba Fairooz Chowdhury
- **Categories**: cs.CL, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.21741v1](https://arxiv.org/abs/2602.21741v1)
- **PDF**: [https://arxiv.org/pdf/2602.21741v1](https://arxiv.org/pdf/2602.21741v1)

本文介绍了我们为Kaggle平台DL Sprint 4.0竞赛提交的孟加拉语长语音自动识别与说话人日志系统的端到端解决方案。孟加拉语在这两项任务中均面临显著挑战：音素体系庞大、方言差异显著、常与英语发生语码混合，且大规模标注语料相对稀缺。在语音识别方面，我们通过结合经BengaliAI微调的Whisper medium模型、采用Demucs进行人声分离的源分离技术、基于静音边界的分块处理以及精细调整的生成超参数，最终在私有测试集上取得了0.37738的词错误率，在公开测试集上取得了0.36137的词错误率。在说话人日志任务中，我们将pyannote.audio流程中的默认分割模型替换为经孟加拉语微调的变体，并配合wespeaker-voxceleb-resnet34-LM嵌入向量与基于质心的聚合聚类方法，在私有测试集上实现了0.27671的日志错误率，在公开测试集上实现了0.20936的日志错误率。实验表明，针对分割模块的领域自适应微调、人声源分离技术以及基于自然静音感知的分块策略，是提升低资源孟加拉语语音处理性能最具影响力的三项设计选择。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语作为全球第七大语言，其长时语音处理面临资源匮乏的挑战。现有数据集多为短时、清晰的朗读语音，而现实场景（如电视剧、脱口秀）包含背景音乐、多说话人重叠、方言变异和英-孟语码混合，现有系统难以应对。
- **既有方法问题**：  
  - 通用多语言ASR模型（如SeamlessM4T）对孟加拉语长时音频识别效果差。  
  - 固定长度分块策略易在词中切断，引入边界错误。  
  - 说话人日志系统（如pyannote）的预训练分割模型对孟加拉语韵律和停顿模式不敏感，导致高错误率。  
  - 缺乏针对音乐背景和方言变异的鲁棒性处理。

2)  
**ASR方面**：  
- **模型选择**：通过定性评估选定`bengaliai-asr_whisper-medium`作为骨干，该模型针对孟加拉语广播和对话语音进行了微调。  
- **音频预处理**：使用Demucs htdemucs进行人声分离，降低背景音乐干扰；通过频谱通量启发式方法选择性应用，避免对纯语音段引入失真。  
- **分块策略**：基于静默边界（librosa检测）的自适应分块，累积至20-30秒并在静默处切分，相比固定窗口减少词边界错误。  
- **解码优化**：采用束搜索（beam=5）与采样结合，设置轻度重复惩罚（rep_penalty=0.8），避免过度抑制孟加拉语中合法的词汇重复。  
- **后处理**：仅进行Unicode标准化等轻量处理，禁用方言归一化（因其增加错误）。

**说话人日志方面**：  
- **分割模型微调**：在孟加拉语对话数据上微调`pyannote/segmentation-3.0`模型，使其适应语言特定的韵律和停顿，此举将DER降低近20个百分点。  
- **说话人嵌入替换**：将默认ECAPA嵌入后端替换为`wespeaker-voxceleb-resnet34-LM`，增强说话人区分能力。  
- **聚类优化**：采用基于质心的层次聚类（τ=0.65，最小簇大小=20），平衡过分割和说话人混淆。  
- **数据修复**：通过正则表达式修复约20%的训练标注文件，为微调提供额外监督数据。

3)  
- **自动语音识别（ASR）**：在DL Sprint 4.0测试集上取得最佳私有WER为0.37738（公开WER为0.36137），相比基线模型（WER约0.6877）显著提升。关键改进源于人声分离、静默分块和解码调优。  
- **说话人日志**：取得最佳私有DER为0.27671（公开DER为0.20936），较基准（DER约0.4008）绝对提升12.41个百分点。核心贡献是分割模型微调，解决了语音活动检测的瓶颈问题。  
- **整体效果**：系统在长时、多说话人、带背景音乐的孟加拉语音频上表现出鲁棒性，为低资源语言语音处理提供了可复用的方法框架。
</div>

</details>

---

## A Knowledge-Driven Approach to Music Segmentation, Music Source Separation and Cinematic Audio Source Separation
- **Authors**: Chun-wei Ho, Sabato Marco Siniscalchi, Kai Li, Chin-Hui Lee
- **Categories**: eess.AS, cs.AI, cs.LG, eess.SP
- **arXiv**: [https://arxiv.org/abs/2602.21476v1](https://arxiv.org/abs/2602.21476v1)
- **PDF**: [https://arxiv.org/pdf/2602.21476v1](https://arxiv.org/pdf/2602.21476v1)

本文提出一种基于知识的模型化方法，用于将音频分割为单一类别与混合类别片段，并应用于音源分离任务。其中“知识”指与数据相关的信息（如乐谱），“模型”指可用于音频分割与识别的工具（如隐马尔可夫模型）。与传统学习方法通常依赖带有已标注片段类别及其边界的数据来指导学习过程不同，本框架不依赖于任何预分割的训练数据，而是直接从输入音频及其相关知识源中自主学习，自主构建所需模型。在仿真数据上的评估表明，基于乐谱引导的学习方法在音乐分割与分离任务中取得了优异效果。在电影音轨数据上进行影视音频源分离测试也显示，利用声音类别知识能够获得优于不使用此类信息的纯数据驱动技术的分离效果。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐分割旨在检测音频流中乐器、节奏等有意义事件的边界，对音乐源分离等下游任务至关重要。传统方法依赖信号处理（如能量滤波、自相似性矩阵）或需要大量带标注边界的数据进行监督学习。
- **既有问题**：现有数据驱动方法通常需要预先分割的训练数据，标注成本高；且在缺乏标注的现实场景中，模型难以获取准确的单乐器片段用于训练，影响后续分离性能。

2)  
- **核心方法**：提出一种知识驱动的模型化框架，利用外部知识（如乐谱）指导分割与分离，无需预分割训练数据。
    - **基于HMM的分割与对齐**：使用隐马尔可夫模型对连续音频进行强制对齐，乐谱作为知识源提供乐器序列信息，从而自动检测单乐器片段及其边界。
    - **知识驱动的音乐源分离训练**：利用对齐得到的单乐器片段，通过随机缩放、裁剪和混合构建“伪混合”数据，用于训练分离模型（如BSRNN、HTDemucs）。支持从头训练或基于预训练模型微调。
    - **分段信息辅助的影视音频源分离**：将帧级别的声源活动指示（如语音、音乐、音效）作为额外输入，投影为低维嵌入后与频谱特征拼接，输入分离器（如SepReformer），以提升复杂混合场景下的分离精度。

3)  
- **音乐分割**：在模拟数据集上，基于乐谱的HMM强制对齐平均边界误差为9.03帧（每帧10毫秒），显著优于无乐谱的HMM识别（准确率94%但易混淆钢琴与混合片段）。
- **音乐源分离**：在Slakh2100数据上，知识驱动训练（即使数据量较小）优于传统预训练，BSRNN的SDR提升约2.8 dB；微调后性能进一步改善。
- **影视音频源分离**：在DNR-nonverbal数据集上，加入声源活动知识的SepReformer取得SOTA效果，语音、音乐、音效的SDR分别达11.03 dB、5.12 dB、6.67 dB，较无知识版本显著提升。
</div>

</details>

---
