---
layout: post
title: "arXiv Daily – 2026-02-26"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-26（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-25 08:50 — 2026-02-26 08:50
- 抓取总数：5 篇 | 本页显示：5 篇（去重/过滤后）

## TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition
- **Authors**: Cheng-Yeh Yang, Chien-Chun Wang, Li-Wei Chen, Hung-Shin Lee, Hsin-Min Wang, Berlin Chen
- **Categories**: eess.AS, cs.AI, cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.22039v1](https://arxiv.org/abs/2602.22039v1)
- **PDF**: [https://arxiv.org/pdf/2602.22039v1](https://arxiv.org/pdf/2602.22039v1)

低资源自动语音识别（ASR）仍面临严峻挑战，主要源于许多语言缺乏充足的转录数据。尽管电视剧和网络视频中存在大量口语内容，但以台湾闽南语为例，其转录文本往往稀缺，且现有字幕大多仅提供普通话版本。为弥补这一不足，我们提出面向台湾闽南语剧集语音识别的TG-ASR框架——一种利用多语言翻译嵌入增强低资源环境下识别性能的翻译引导式ASR系统。该框架核心为并行门控交叉注意力机制，可自适应地将多种辅助语言的嵌入向量融合至ASR解码器中。该机制在实现强健跨语言语义引导的同时，确保了优化过程的稳定性，并最大限度降低语言间干扰。为支持持续研究，我们构建了YT-THDC语料库，包含30小时台湾闽南语剧集语音、对齐的普通话字幕及人工校验的闽南语转录文本。综合实验与分析确定了最能提升ASR性能的辅助语言组合，实现了字错误率相对降低14.77%，验证了翻译引导学习在实际应用中对资源匮乏语言的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：许多地区性及濒危语言（如台湾闽南语）的语音转录数据稀缺，导致低资源自动语音识别发展受限。尽管存在大量闽南语影视内容，但字幕多为普通话，缺乏对应的闽南语文本。  
- **既有方法问题**：传统方法（如数据增强、多语言联合训练、从高资源语言迁移）在语音数据极少且与预训练语言差异大时效果有限；现有针对闽南语的方法仅依赖初始ASR系统对齐普通话字幕，未能充分利用多语言语义信息。

2)  
论文提出**TG-ASR框架**，核心是**并行门控交叉注意力机制**，以解决低资源ASR中语义指导不足的问题：  
- **框架设计**：基于Whisper模型，采用两阶段训练。第一阶段微调整个Whisper模型；第二阶段冻结编码器及第一阶段解码器参数，仅更新新增的PGCA层。  
- **多语言嵌入提取**：使用SeamlessM4T将语音转录翻译成多种辅助语言（如普通话、英语、西班牙语等），再通过多语言BERT提取翻译嵌入。  
- **PGCA机制**：  
  - **并行结构**：在解码器每个模块前端引入多个独立的交叉注意力分支，分别处理不同辅助语言的翻译嵌入，使模型能同时关注多语言信息。  
  - **门控控制**：每个分支配备可学习的tanh门控参数，动态调节各语言嵌入的贡献权重，抑制噪声或低相关性的输入，提升训练稳定性。  
  - **自适应融合**：加权后的多语言嵌入与解码器输入相加，再经前馈网络处理，从而在解码过程中注入细粒度的跨语言语义指导。  
- **优势**：PGCA实现了多语言信息的自适应集成，既强化了有效的语义信号，又最小化了语言间干扰，显著提升了低资源场景下的识别鲁棒性。

3)  
- **任务**：台湾闽南语电视剧语音识别（低资源场景）。  
- **效果**：在自建的YT-THDC语料库（30小时）上评估，以字符错误率为指标：  
  - 使用单一最佳辅助语言（普通话）时，CER相对降低11.42%。  
  - 结合多语言（普通话+西班牙语）时，取得最佳效果，CER相对降低**14.77%**。  
- **其他验证**：PGCA机制在消融实验中优于序列注意力、共享注意力等变体；门控模块能有效区分并促进高相关性语言（如普通话），抑制低相关性语言（如英语）。
</div>

</details>

---

## EmoOmni: Bridging Emotional Understanding and Expression in Omni-Modal LLMs
- **Authors**: Wenjie Tian, Zhixian Zhao, Jingbin Hu, Huakang Chen, Haohe Liu, Binshen Mu, Lei Xie
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.21900v1](https://arxiv.org/abs/2602.21900v1)
- **PDF**: [https://arxiv.org/pdf/2602.21900v1](https://arxiv.org/pdf/2602.21900v1)

全模态大语言模型（Omni-LLMs）的发展革新了人机交互，实现了统一的视听感知与语音响应。然而，现有全模态大语言模型在复杂现实场景中表现欠佳，常导致理解流于表面及情感回应与语境失配。这一问题因其“思考者-表达者”架构而进一步加剧：该架构通过隐状态间接连接，致使情感细节大量流失。本研究提出EmoOmni——一个面向多模态情感对话的精准理解与表达统一框架。其核心在于引入情感思维链（E-CoT），强制模型执行从细粒度多模态感知到文本响应的推理过程。同时，我们显式地将E-CoT作为高层情感指令来引导表达者，从而实现精准的情感表达。为支撑模型构建，我们创建了EmoOmniPipe以获取真实场景的标注对话数据，并建立基准测试集EmoOmniEval，以系统评估多模态情感对话任务。实验表明，在采用相同表达者的情况下，EmoOmni-7B模型取得了与Qwen3Omni-30B-A3B-Thinking相当的性能表现。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：全模态大语言模型（Omni-LLMs）旨在实现统一的多模态感知与语音交互，但在复杂真实场景中面临挑战。  
- **既有问题**：  
  - 模型对音频、视觉等多模态线索的理解往往停留在表层，难以处理隐含或冲突的情感信号（如欢快语调伴随皱眉表情），导致意图推断错误。  
  - 主流“思考者-说话者”（Thinker-Talker）架构通过隐藏状态隐式传递情感信息，造成情感细节丢失，生成的语音响应虽语义正确但情感失准。  
  - 缺乏高质量、细粒度标注的真实世界多模态对话数据，且现有评估基准多关注任务正确性，忽视交互情境中的情感智能。

2)  
论文提出 **EmoOmni** 框架，通过显式建模“感知-推理-表达”因果链来解决上述问题，核心方法包括：  
- **情感思维链（E-CoT）机制**：  
  - 将推理过程结构化，依次进行：1）**细粒度多模态情感感知**（描述音频、视觉可观测线索）；2）**用户意图分析**（推断深层动机与心理状态）；3）**响应策略规划**（制定高层情感回应策略）；4）**文本响应生成**。  
  - E-CoT 作为显式情感指令，直接指导说话者模块，确保情感细节在传递中不被稀释。  
- **两阶段训练策略**：  
  - **第一阶段（感知奠基）**：使用多模态情感理解数据，专门优化模型对情感线索的感知能力。  
  - **第二阶段（联合推理调优）**：引入多模态情感对话数据，端到端训练整个因果链，强化从感知到生成的依赖关系。  
- **指令引导的语音生成**：  
  - 说话者模块（EmoOmni-Talker）接收 E-CoT 输出的策略指令，通过轻量级语言模型将其转化为具体声学控制信号（如“以 playful 语调…”），实现语音的情感与语义对齐。  
- **数据与评估体系**：  
  - 构建 **EmoOmniPipe** 数据流水线，从影视剧中提取、清洗并细粒度标注多模态对话数据。  
  - 建立 **EmoOmniEval** 基准，从视频到语音、视频到文本、指令跟随三个维度系统评估情感理解与表达。

3)  
- **任务与效果**：在构建的 **EmoOmniEval** 基准（包含 MELD 英文与 ch-sims-v2 中文测试集）上评估多模态情感对话任务。  
- **主要结果**：  
  - 在视频到语音（VS）评估中，EmoOmni-7B 在响应情感策略（VS-RES）与内容相关性（VS-RC）上优于同规模模型，并与参数量大得多的 **Qwen3Omni-30B-A3B-Thinking** 性能相当。  
  - 在文本推理（VT）维度，其情感分析（VT-EA）与响应策略（VT-RES）得分接近满分，显著优于无显式推理机制的基线模型。  
  - 消融实验证实 E-CoT、两阶段训练及真实世界数据均为关键提升因素。
</div>

</details>

---

## UniWhisper: Efficient Continual Multi-task Training for Robust Universal Audio Representation
- **Authors**: Yuxuan Chen, Peize He, Haoyuan Xu, Junzi Zhang
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.21772v1](https://arxiv.org/abs/2602.21772v1)
- **PDF**: [https://arxiv.org/pdf/2602.21772v1](https://arxiv.org/pdf/2602.21772v1)

通用音频表征应能在单一编码器中同时捕捉细粒度语音线索与环境声、音乐的高层语义。现有编码器往往在单一领域表现优异，而在其他领域性能下降。本文提出UniWhisper——一种高效的持续多任务训练框架，将异构音频任务统一转化为指令-应答格式。该方法支持标准的下一个词元训练，无需任务特定头部或损失函数。我们在38千小时公开音频数据上进行训练，并通过浅层MLP探针和k近邻算法在涵盖语音、环境声与音乐的20项任务上评估编码器性能。实验表明：UniWhisper在MLP探针和kNN评估中分别达到0.81和0.61的归一化加权平均值，显著优于Whisper的0.64和0.46，同时保持了优异的语音处理能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：通用音频表征旨在用一个编码器同时处理语音、环境声和音乐。现有方法存在**领域不平衡**问题：
  - 语音专用编码器（如Whisper、WavLM）缺乏对非语音场景的语义覆盖。
  - 通用音频模型（如BEATs、CLAP）难以保留语音所需的细粒度时序线索。
- **既有方法的问题**：
  - 为扩大覆盖范围，常需在多样数据集上持续训练，导致**计算成本高、数据需求大**。
  - 多编码器融合系统（如SALMONN）需协调不同分辨率和表示空间，并引入**音频令牌冗余**，占用大语言模型的有限上下文窗口。

2)  
论文提出 **UniWhisper**，一个高效的持续多任务训练框架，其核心方法通过以下设计解决上述问题：

- **统一指令-答案格式**：
  - 将异构音频任务（如ASR、音频描述、音频问答）全部转换为统一的提示（Prompt）和答案（Answer）文本格式。
  - 所有任务共享相同的训练和解码接口（标准的下一个词元预测），**无需任务特定的输出头或损失函数**。

- **单编码器与高效解码器设计**：
  - 采用**单一音频编码器**（基于Whisper Large-v3初始化），避免多编码器特征拼接带来的令牌冗余和架构复杂性。
  - 识别出原始Whisper解码器在指令式对齐下效率低下，将其替换为**紧凑的预训练语言模型**（Qwen3 0.6B），并通过轻量级适配器连接。
  - 该解码器提供强大的语言先验，能更好地匹配指令跟随目标，**加速收敛**，并能在更小的训练数据量（38k小时）下实现高效适配。

- **持续多任务训练流程**：
  - 在混合的公开音频数据集上进行持续训练，所有数据通过统一模板转换为指令格式后混合。
  - 训练时只更新编码器和投影适配器，**冻结预训练的语言模型解码器**，优化目标仅为答案词元的交叉熵损失。
  - 这种方法从源头消除了音频令牌冗余，并通过统一的监督信号，在**单一编码器**中同时增强了非语音语义和保留了语音能力。

3)  
在涵盖语音、环境声和音乐的 **20个任务** 上进行了评估：
- **主要效果**：UniWhisper在**归一化加权平均**指标上显著超越基线模型。
  - 使用浅层MLP探测时达到 **0.81**（Whisper为0.64）。
  - 使用k近邻（kNN）评估时达到 **0.61**（Whisper为0.46）。
- **具体表现**：
  - 在依赖全局语义的**非语音任务**（如音频标注、标签分类）上提升最大。
  - 同时**保持了强大的语音任务性能**（如说话人识别、语音识别），未出现明显的灾难性遗忘。
  - 整体上实现了比领域专用模型更**均衡的跨领域性能**。
</div>

</details>

---

## Robust Long-Form Bangla Speech Processing: Automatic Speech Recognition and Speaker Diarization
- **Authors**: MD. Sagor Chowdhury, Adiba Fairooz Chowdhury
- **Categories**: cs.CL, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.21741v1](https://arxiv.org/abs/2602.21741v1)
- **PDF**: [https://arxiv.org/pdf/2602.21741v1](https://arxiv.org/pdf/2602.21741v1)

本文介绍了我们为Kaggle平台DL Sprint 4.0竞赛提交的孟加拉语长语音自动识别与说话人日志系统的端到端解决方案。孟加拉语在两项任务中均面临显著挑战：音素体系庞大、方言差异显著、频繁夹杂英语混码现象，且大规模标注语料相对稀缺。在语音识别方面，我们通过融合孟加拉语优化的Whisper medium模型、基于Demucs的声源分离技术、静默边界分块策略及精细调整的生成超参数，实现了最佳私有词错误率0.37738与公开词错误率0.36137。在说话人日志任务中，我们将pyannote.audio流程中的默认分割模型替换为孟加拉语优化版本，结合wespeaker-voxceleb-resnet34-LM嵌入向量与基于质心的层次聚类方法，取得了最佳私有日志错误率0.27671与公开日志错误率0.20936。实验表明，针对分割模块的领域自适应微调、人声声源分离以及基于自然静默的分块策略，是提升低资源孟加拉语语音处理性能的三个最关键设计要素。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语作为全球第七大语言，在长语音处理领域资源匮乏。现有数据集多为短语音、朗读式，而真实场景（如电视剧、脱口秀）包含长音频、背景音乐、多说话人重叠、方言变异和英-孟语码混合，对现有系统构成挑战。
- **既有方法问题**：  
  - 通用多语言ASR模型（如SeamlessM4T）在孟加拉语上表现不佳（WER > 60%）。  
  - 固定长度分块策略易在词中切断音频，引入边界错误。  
  - 预训练说话人日志系统（如pyannote）未针对孟加拉语进行适配，语音活动检测（VAD）和分割模型在方言和对话模式上泛化能力差。

2)  
**ASR方面**：  
- **模型选择**：通过定性评测，选定针对孟加拉语微调的`bengaliai-asr_whisper-medium`作为骨干模型。  
- **音频预处理**：使用Demucs htdemucs进行人声分离，减少背景音乐干扰；通过频谱通量启发式方法选择性应用，避免对纯语音段引入伪影。  
- **分块策略**：采用基于静默边界的分块（librosa检测，阈值top_db=25），累积至20-30秒并在静默处切分，相比固定窗口减少约83%的词边界错误。  
- **解码调优**：采用束搜索（beam=5）与采样结合，设置轻度重复惩罚（rep_penalty=0.8），避免过度抑制孟加拉语中合法的词汇重复。  
- **后处理**：仅进行Unicode标准化等轻量处理，禁用方言归一化（因其增加WER）。

**说话人日志方面**：  
- **分割模型微调**：在孟加拉语对话数据上微调`pyannote/segmentation-3.0`模型，使其适应语言特有的韵律和停顿模式，这是最关键的一步（降低DER近20个百分点）。  
- **嵌入模型替换**：将默认ECAPA-TDNN嵌入后端替换为`wespeaker-voxceleb-resnet34-LM`，提升说话人区分能力。  
- **聚类优化**：采用基于质心的层次聚类（阈值τ=0.65，最小簇大小=20），平衡过分割和混淆错误。  
- **数据修复**：通过正则表达式修复约20%的训练标注文件，增加微调监督数据。

3)  
- **自动语音识别（ASR）**：在DL Sprint 4.0测试集上取得最佳**私有WER为37.74%**（公开WER为36.14%），显著优于基线模型（如SeamlessM4T的WER > 60%）。关键改进源于人声分离、静默分块和解码策略。  
- **说话人日志（Diarization）**：取得最佳**公开DER为20.94%**（私有DER为27.67%），相比Bengali-Loop基线（DER 40.08%）绝对提升约12.4个百分点。核心贡献是分割模型微调和嵌入模型优化。
</div>

</details>

---

## A Knowledge-Driven Approach to Music Segmentation, Music Source Separation and Cinematic Audio Source Separation
- **Authors**: Chun-wei Ho, Sabato Marco Siniscalchi, Kai Li, Chin-Hui Lee
- **Categories**: eess.AS, cs.AI, cs.LG, eess.SP
- **arXiv**: [https://arxiv.org/abs/2602.21476v1](https://arxiv.org/abs/2602.21476v1)
- **PDF**: [https://arxiv.org/pdf/2602.21476v1](https://arxiv.org/pdf/2602.21476v1)

本文提出一种基于知识的模型化方法，用于将音频分割为单一类别与混合类别片段，并应用于音源分离任务。其中“知识”指与数据相关的信息（如乐谱），“模型”指可用于音频分割与识别的工具（如隐马尔可夫模型）。与传统学习方法通常依赖带有预设类别标签及边界标注的数据来指导学习不同，本框架无需任何预分割的训练数据，而是直接从输入音频及其关联知识源中自主学习并构建所需模型。在仿真数据上的评估表明，基于乐谱引导的学习能取得优异的音乐分割与分离效果。在电影音轨数据上进行影视音频源分离测试也显示，利用声音类别知识能够取得优于不使用此类信息的纯数据驱动方法的分离结果。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐分割旨在检测音频流中乐器、节奏等有意义事件的时间边界，是音乐源分离等下游任务的关键步骤。传统方法依赖信号处理（如能量滤波、自相似性矩阵）或数据驱动的深度学习模型。
- **既有问题**：传统监督学习严重依赖人工标注的片段边界作为训练数据，这在现实场景中往往难以获取。此外，纯数据驱动方法在缺乏明确结构信息（如乐谱）时，分割和分离的准确性可能受限。

2)  
论文提出了一种**知识驱动、基于模型**的框架，核心是利用外部“知识”替代对标注数据的依赖，并采用隐马尔可夫模型（HMM）等工具进行自主建模。具体解决方案如下：
- **知识驱动的音乐分割**：
    - 将**乐谱**（如MIDI文件）作为“知识”源，其包含的乐器、音符序列等信息可指导分割。
    - 采用**HMM进行强制对齐**：类似语音识别中的强制对齐技术，利用乐谱信息将连续音频与乐谱中的音符或乐器序列对齐，从而自动确定单乐器片段、混合片段及静音段的边界，无需任何预分割的训练数据。
- **知识驱动的音乐源分离**：
    - 利用上述分割得到的**单乐器片段**构建训练数据：通过强制对齐提取纯净的单乐器片段，将其随机缩放、裁剪并混合，生成“伪混合”音频。
    - 使用这些伪混合音频及其对应的纯净片段，训练源分离模型（如BSRNN、HTDemucs）。此方法可在**无独立分离音轨**的情况下实现模型训练或微调。
- **知识驱动的电影音频源分离**：
    - 将**声音类别活动信息**（如语音、音乐、音效在每一帧是否活跃）作为辅助输入知识。
    - 如图1所示，该系统将混合音频的时频特征与经过投影的帧级声音活动指示符拼接，共同输入分离器（如RNN、Transformer），从而引导模型更准确地将能量分配至对应源。

3)  
- **音乐分割任务**：在Slakh2100模拟数据上，基于乐谱的HMM强制对齐实现了平均9.03帧（每帧10ms）的边界检测误差，能有效提取单乐器片段。
- **音乐源分离任务**：在相同数据集上，知识驱动训练（无论是从头训练还是微调）的BSRNN和HTDemucs模型，其信号失真比均显著优于仅使用大量数据预训练的基线模型。
- **电影音频源分离任务**：在DNR-nonverbal数据集上，融入声音活动信息的SepReformer模型取得了当前最优的分离效果，在语音、音乐和音效上的SDR均大幅超过纯数据驱动的基准模型。
</div>

</details>

---
