---
layout: post
title: "arXiv Daily – 2025-10-30"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-30（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-29 08:50 — 2025-10-30 08:50
- 抓取总数：10 篇 | 本页显示：10 篇（去重/过滤后）

## Efficient Vocal Source Separation Through Windowed Sink Attention
- **Authors**: Christodoulos Benetatos, Yongyi Zang, Randal Leistikow
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.25745v1](http://arxiv.org/abs/2510.25745v1)
- **PDF**: [http://arxiv.org/pdf/2510.25745v1](http://arxiv.org/pdf/2510.25745v1)

当前主流的人声分离模型（如Mel-Band-Roformer）采用全时序自注意力机制，要求每个时间帧与所有其他帧进行交互，导致计算复杂度随音频输入长度呈平方级增长，这促使研究者采用分块与加窗策略。通过对预训练人声分离模型的分析，我们发现其时序注意力模式呈现高度局部化特征。基于该发现，我们采用小尺度时序注意力窗口与注意力汇聚机制，将全注意力替换为加窗汇聚注意力（WSA）。实验表明，基于原模型检查点进行微调后，新模型在保持92%原始信号失真比性能的同时，将浮点运算量降低至原模型的1/44.5。相关代码与模型检查点已基于MIT许可发布于https://github.com/smulelabs/windowed-roformer。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：人声分离是音乐信息检索的核心任务，现有方法如Mel-Band-Roformer依赖全时序自注意力机制，导致计算成本随输入音频长度呈二次方增长。  
- **既有问题**：全注意力机制存在冗余，分析发现时序注意力高度局部化，但模型仍为全局交互支付高昂计算代价，限制了在长序列或边缘设备上的部署效率。  

2)  
- **核心方法**：提出窗口化汇注意力（WSA），通过局部窗口和全局汇令牌替换全注意力：  
  - **局部窗口**：限制每个时间帧仅关注相邻帧（窗口大小W=10），覆盖约100毫秒感受野，直接建模观测到的局部依赖。  
  - **汇令牌**：添加8个全局令牌，与所有帧双向交互，捕获原模型中的分散全局信息，避免纯局部注意力的信息损失。  
- **优化实现**：基于PyTorch FlexAttention编译稀疏模式，避免全矩阵计算，将复杂度从O(N²)降至O(N×(W+S))。  
- **训练策略**：通过知识蒸馏将原模型作为教师，学生模型（WSA变体）结合重构损失和掩码蒸馏损失进行微调，仅需20万步即可恢复性能。  

3)  
- **任务与效果**：在MUSDB18HQ人声分离任务上评估：  
  - **性能保留**：SDR达11.17 dB（原模型12.12 dB），保留92%性能，主观听感无显著差异。  
  - **效率提升**：注意力计算量减少44.5倍，支持长序列训练与边缘设备部署。  
  - **局限性**：无声段分离质量略有下降，非目标源干扰指标（Bleedless）降低。
</div>

</details>

---

## Binaspect -- A Python Library for Binaural Audio Analysis, Visualization & Feature Generation
- **Authors**: Dan Barry, Davoud Shariat Panah, Alessandro Ragano, Jan Skoglund, Andrew Hines
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.25714v1](http://arxiv.org/abs/2510.25714v1)
- **PDF**: [http://arxiv.org/pdf/2510.25714v1](http://arxiv.org/pdf/2510.25714v1)

本文介绍Binaspect——一个用于双耳音频分析、可视化及特征生成的开源Python工具库。该库通过计算改进的耳间时间差与声级差谱图，将时频单元聚类为稳定的时域-方位直方图，从而生成可解释的“方位映射图”。该方法能呈现多路活跃声源形成的独立方位聚类，而音质损伤则表现为分布展宽、扩散或偏移。值得注意的是，Binaspect无需头相关传输函数先验知识，可直接对音频进行盲处理。这种可视化技术有助于研究人员和工程师观察编解码器与渲染器设计等下游处理对双耳听觉线索的损伤效应。我们在码率阶梯、高阶 Ambisonic 渲染及矢量基幅度平移定位等场景中验证了该工具，均清晰呈现出音质损伤现象。除诊断功能外，生成的表征数据可导出为结构化特征，适用于质量评估、空间音频分类等双耳任务的机器学习模型训练。Binaspect采用开源协议发布，完整复现脚本详见：https://github.com/QxLabIreland/Binaspect。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：空间音频在虚拟现实等领域应用广泛，但现有工具分散于不同MATLAB和Python库中，缺乏专门用于双耳音频特征可视化与生成的集成库。  
- **既有方法问题**：传统双耳分析依赖未优化的ITD/ILD表示，存在无界或对数尺度问题，且缺乏直观的方位聚类可视化，难以直接诊断编码器或渲染器导致的线索退化。  

2.  
- **核心方法**：Binaspect通过计算改进的ITD和ILR（有界耳间电平比）谱图，并将时频单元聚类为稳定的时间-方位直方图，生成可解释的“方位图”。  
- **关键步骤**：  
  - 有界ILR谱图：将原始ILR映射到[-1,1]区间，提供稳定的线性增益表示。  
  - ITD谱图：通过相位差计算时间延迟，并限制频率范围以避免混叠。  
  - 直方图生成：对ILR和ITD值进行能量加权分箱，形成时间-方位分布。  
- **优势**：  
  - 无需先验头部模型，直接处理音频。  
  - 多源显示为清晰方位簇，退化表现为分布扩散或偏移。  
  - 支持机器学习特征导出，兼容其他时频表示。  

3.  
- **任务与效果**：  
  - **高阶混响分析**：对比HOA与FOA渲染，清晰显示FOA中ITD/ILR线索退化（如方位偏移3.5°）。  
  - **编解码压缩**：在Opus码率测试中，低码率（32k）导致双耳线索分散，轨迹异常。  
  - **下混分析**：VBAP虚拟定位导致5.1布局中90°声源方位偏移（平均11.6°），直方图直观揭示空间图像失真。
</div>

</details>

---

## Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models
- **Authors**: Harm Lameris, Shree Harsha Bokkahalli Satish, Joakim Gustafson, Éva Székely
- **Categories**: eess.AS, cs.AI, cs.CL
- **arXiv**: [http://arxiv.org/abs/2510.25577v1](http://arxiv.org/abs/2510.25577v1)
- **PDF**: [http://arxiv.org/pdf/2510.25577v1](http://arxiv.org/pdf/2510.25577v1)

语音基础模型（SFMs）的最新进展实现了对原始音频中口语的直接处理，无需借助中间文本表征。这一能力使SFMs得以接触并可能响应输入语音信号中丰富的副语言信息变异。发声质量作为副语言变异中尚未被充分探索的维度，涵盖了嘎裂声与气声等发声类型。已知这些发声类型会影响听者对语音中情感状态、立场及社会意义的推断。现有语音理解基准主要依赖多项选择题问答模式，此类方法易出现失效，因而难以可靠捕捉副语言特征影响模型行为的细微方式。本文通过开放式生成任务与语音情感识别对SFMs进行探测，评估模型行为在不同发声输入中是否保持一致。我们提出包含合成化发声质量调整的新型平行数据集，专用于评估SFMs对嘎裂声与气声的响应。本研究首次系统检验了SFMs对语音感知中此类非词汇特征的敏感性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音基础模型（SFMs）能直接从原始音频处理口语，捕捉丰富的副语言信息，但现有评估方法主要依赖多项选择题回答（MCQA）框架。  
- **既有问题**：MCQA限制模型输出，无法可靠反映副语言特征（如音质）对模型行为的影响；音质作为副语言变体的关键维度（如气声、嘎裂声），其社会与情感含义在SFM评估中尚未系统探索。  

2)  
- **核心方法**：提出VQ-Bench评估框架，通过合成平行语音数据（包含模态、气声、嘎裂声及末端嘎裂声四种音质）测试SFMs对音质变化的敏感性。  
  - **数据构建**：基于Buckeye和VCTK语料库，使用F5-TTS系统合成语音，并通过VoiceQualityVC工具调整声门源参数以控制音质。  
  - **任务设计**：  
    - **开放式生成任务**：涵盖治疗建议、职业咨询、面试筛选和故事生成四类场景，评估模型响应如何受音质影响。  
    - **语音情感识别（SER）**：使用微调的Wav2Vec 2.0模型，分析音质对情感分类的干扰。  
  - **评估机制**：采用LLM法官对生成内容多维度评分（如行动性、领导力），并结合统计模型（如CLMM）分析音质与性别的影响。  

3)  
- **任务与效果**：  
  - **开放式生成**：音质显著改变模型响应，例如气声和末端嘎裂声在职业建议中提升STEM倾向评分，而嘎裂声更倾向关怀类建议；在面试任务中，非模态音质普遍降低评分，女性语音更易受负面偏见。  
  - **语音情感识别**：气声增加平静与中性情感预测，嘎裂声减少恐惧与快乐预测；女性语音更易被归类为恐惧或惊讶。  
  - **整体结论**：SFMs对音质变化敏感，其行为与人类感知偏见一致，凸显了在招聘、治疗等应用中评估副语言偏差的必要性。
</div>

</details>

---

## PitchFlower: A flow-based neural audio codec with pitch controllability
- **Authors**: Diego Torres, Axel Roebel, Nicolas Obin
- **Categories**: eess.AS, cs.LG
- **arXiv**: [http://arxiv.org/abs/2510.25566v1](http://arxiv.org/abs/2510.25566v1)
- **PDF**: [http://arxiv.org/pdf/2510.25566v1](http://arxiv.org/pdf/2510.25566v1)

本文提出PitchFlower——一种具备显式音高可控性的基于流的神经音频编解码器。该方法通过简单扰动实现特征解耦：在训练过程中对基频轨迹进行平坦化与随机偏移处理，同时将真实基频作为条件输入。向量量化瓶颈层有效阻隔音高信息复原，基于流的解码器则生成高质量音频。实验表明，PitchFlower在实现比WORLD更精确音高控制的同时显著提升音频质量，并在保持与SiFiGAN相当音质的前提下增强可控性。该框架为解耦其他语音特征提供了简洁可扩展的实现路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音属性操控是重要研究方向，传统方法如WORLD基于源-滤波器模型分解语音参数，但修改音高时忽略参数间相互作用，导致音频不自然且存在伪影。  
- **既有方法问题**：  
  - 深度学习方法（如SiFiGAN）仍依赖源-滤波器假设，或需额外损失函数与超参数，训练不稳定。  
  - 现有神经音频编解码器（如FACodec）无法实现精确音高控制，PeriodCodec虽支持音高控制但训练复杂。  

2)  
- **核心方法**：PitchFlower采用基于扰动的解耦策略，结合流模型与向量量化瓶颈，实现端到端训练。  
  - **扰动机制**：训练时对输入音频的F0轮廓进行平坦化与随机偏移，强制模型依赖真实F0条件重建信号。  
  - **瓶颈设计**：向量量化模块阻止潜在表示包含音高信息，确保音高仅通过条件信号控制。  
  - **流解码器**：通过条件流匹配损失学习分布，从扰动输入中生成高质量音频，补偿信息损失。  
- **优势**：  
  - 仅需单一生成式损失，简化训练流程。  
  - 无需对抗训练或复杂蒸馏策略，平衡控制力与质量。  

3)  
- **任务与效果**：在LibriTTS数据集上评估音高控制与音频质量。  
  - **音高控制精度**：F0均方根误差显著低于WORLD，优于SiFiGAN。  
  - **音频质量**：UTMOS评分与SiFiGAN相当，主观评价显示深度学习方法均优于WORLD。  
  - **局限性**：说话人相似度略低于SiFiGAN，因扰动步骤引入WORLD伪影；音高有效范围受限（60–700 Hz）。
</div>

</details>

---

## Controlling Contrastive Self-Supervised Learning with Knowledge-Driven Multiple Hypothesis: Application to Beat Tracking
- **Authors**: Antonin Gagnere, Slim Essid, Geoffroy Peeters
- **Categories**: cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.25560v1](http://arxiv.org/abs/2510.25560v1)
- **PDF**: [http://arxiv.org/pdf/2510.25560v1](http://arxiv.org/pdf/2510.25560v1)

在机器学习任务中，数据歧义与问题约束可能导致存在多种同样合理的结果。以节拍与强拍追踪为例，不同听者可能采用相异的节奏解读方式，且这些解读均不必然有误。为此，我们提出一种基于对比自监督的预训练方法，通过数据中可能存在的多组正样本假设进行学习。该模型通过基于知识的评分函数筛选最合理的假设，学习与不同假设相兼容的表征。在标注数据上的微调实验表明，本方法在标准基准测试中优于现有方案，尤其展现了将领域知识与多假设选择融入音乐表征学习的优势。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- 音乐节拍跟踪任务存在固有的模糊性，不同听众可能对同一音乐片段给出多种合理的节拍解释，而传统方法难以有效处理这种多义性。
- 现有自监督学习方法（如对比学习）通常依赖单一假设定义正负样本对，无法适应任务中可能存在的多种合理输出，限制了模型对多样化解释的建模能力。

2) **论文核心方法如何解决上述问题**
- 提出知识驱动的多假设学习框架（KD-MHL），通过引入多个基于领域知识的假设来定义对比学习中的正负样本采样策略：
  - 使用多个投影头对应不同节拍假设（如不同节拍比率和相位组合），每个假设定义特定的锚点、正样本和负样本采样方式。
  - 设计基于音频特征的评分函数，评估每个假设与输入数据的兼容性，优先选择最合理的假设进行训练。
  - 采用Top-n胜者通吃策略（如3-WTA）动态选择多个兼容假设，使模型能够同时学习与多种合理解释兼容的表征。
- 在音乐节奏分析任务中，利用主导局部脉冲函数生成节拍候选，通过假设池覆盖不同的节拍模式和相位偏移，有效捕捉音乐中的多义性。

3) **在哪些任务上取得了怎样的效果**
- 在节拍和强拍跟踪任务上，使用标准数据集评估：
  - 在GTZAN数据集上，3-WTA预训练结合BCE-DBN微调取得最佳效果（平均指标83.1），比基线提升约2%。
  - 跨数据集验证中，自训练变体在多个基准（如Ballroom、Harmonix）达到最优性能，平均指标达89.3，显著超越现有方法。
</div>

</details>

---

## Separating peripheral and higher-level effects on speech intelligibility using a hearing loss simulator and an objective intelligibility measure
- **Authors**: Toshio Irino, Ayako Yamamoto, Fuki Miyazaki
- **Categories**: eess.AS, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.25235v1](http://arxiv.org/abs/2510.25235v1)
- **PDF**: [http://arxiv.org/pdf/2510.25235v1](http://arxiv.org/pdf/2510.25235v1)

本文提出一种新方法，用于区分外周性听力损失（HL）与高级认知处理对言语清晰度（SI）的影响。在前期研究中，我们曾对14名老年听者（OA）进行言语清晰度实验，测试材料包括经过理想比率掩蔽（IRM）增强技术处理的噪声中言语信号及原始未处理信号。本研究则针对15名年轻正常听力听者（YNH）开展实验，采用WHIS听力损失模拟器处理过的模拟HL声音，该模拟器精确复现了前期研究中某特定OA听者的听力水平。实验结果显示：该目标OA的SI得分高于YNH听者平均水平，表明其高级认知处理机制可能优于普通YNH听者。为探究其他OA听者的特性，我们采用GESI客观清晰度度量进行SI预测。首先验证了GESI能够较精准地预测YNH与OA听者的SI得分；继而基于YNH实验估计的参数对14名OA听者的SI得分进行预测。结果表明：部分OA听者的SI得分高于YNH平均水平，而一名OA听者得分偏低。这些SI得分差异可能反映了高级认知处理效率的个体差异。研究证明WHIS模拟器与GESI度量可突破听力水平限制，实现YNH与OA听者的对照实验，从而为个体化研究OA听者的高级认知处理机制提供有效途径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：老年人口增加导致年龄相关听力损失（HL）患者增多，HL损害言语交流并降低生活质量。现有助听器虽能补偿外周HL，但效果有限，且无法区分外周HL与中枢及更高级处理过程的影响。  
- **既有方法问题**：  
  - 先前研究多关注年轻正常听力者（YNH）或老年正常听力者，但缺乏对HL老年人的直接比较。  
  - 个体HL差异大，难以分离外周HL与高级过程（如认知、词汇访问）的独立作用。  
  - 现有方法忽略个体HL老年人的特性，阻碍个性化助听设备开发。  

2)  
- **核心方法**：结合听力损失模拟器（WHIS）和客观可懂度指标（GESI），分离外周HL与高级过程的影响。  
  - **WHIS模拟外周HL**：基于伽马啁啾滤波器组（GCFB）合成模拟HL声音，重现特定老年人的听力水平，使YNH受试者体验HL条件，从而直接比较其与老年人的言语可懂度（SI）差异。  
  - **GESI预测SI**：  
    - GESI是一种自下而上的模型，利用GCFB和调制滤波器组计算客观SI分数，不包含高级过程模型。  
    - 通过YNH实验确定S型函数参数（a、b），并用于预测老年人SI分数。  
    - 比较预测分数与主观SI分数，差异归因于高级过程（如认知能力）的效率变化。  
  - **优势**：  
    - WHIS提供高质量HL模拟，减少感知失真。  
    - GESI跨听力水平预测SI，支持个体化分析。  
    - 方法允许对比YNH与老年人，无需匹配听力水平。  

3)  
- **任务与效果**：  
  - **SI实验**：在噪声中言语识别任务中，WHIS模拟的HL条件下，目标老年人（OA#7）的SI分数高于YNH平均值，表明其高级过程更有效。  
  - **GESI预测**：  
    - 准确预测YNH和老年人的SI分数（RMSE与先前研究相当）。  
    - 使用YNH参数预测14名老年人SI，发现多数人分数高于YNH平均水平，仅一人较低，揭示个体高级过程效率差异。  
  - **整体贡献**：方法成功分离外周HL与高级过程影响，为个性化助听策略提供基础。
</div>

</details>

---

## Studies for : A Human-AI Co-Creative Sound Artwork Using a Real-time Multi-channel Sound Generation Model
- **Authors**: Chihiro Nagashima, Akira Takahashi, Zhi Zhong, Shusuke Takahashi, Yuki Mitsufuji
- **Categories**: cs.SD, cs.AI
- **arXiv**: [http://arxiv.org/abs/2510.25228v1](http://arxiv.org/abs/2510.25228v1)
- **PDF**: [http://arxiv.org/pdf/2510.25228v1](http://arxiv.org/pdf/2510.25228v1)

本文通过开发生成式声音装置《Studies for》（与声音艺术家Evala合作完成），探索了AI技术融入艺术创作流程的实践。该装置采用轻量化高品质声音生成模型SpecMaskGIT，实时生成并播放八声道音频，在三个月展期内构建沉浸式听觉体验。作品基于“新型档案”理念，旨在保存艺术家风格的同时，通过持续生成新声音元素突破其既往作品边界。通过使用Evala逾200小时历史声音艺术作品数据集训练AI模型，实现了这种前瞻性艺术保存方案。

本研究针对AI艺术协同创作的核心需求，重点阐释了三方面价值：（1）融合艺术家反馈的必要性；（2）基于艺术家历史作品构建数据集；（3）确保生成结果兼具意外性与新颖性。在《Studies for》中，模型设计既延续了艺术家的创作特质，又生成前所未闻的新声音，完美实现了“新型档案”概念。我们提出人机协同创作框架，将声音生成AI模型有效融入声音艺术创作流程，为突破艺术家物理存在局限的声音艺术创作与存档开辟了新路径。演示页面：https://sony.github.io/studies-for/

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声音艺术（尤其是场地特定作品）因依赖空间和表演情境，传统存档方式难以捕捉其本质，导致作品在艺术家去世后难以再现。  
- **既有方法问题**：  
  - 传统存档无法保留作品的临场感与空间特性；  
  - 早期声音生成模型计算量大，无法支持实时生成与艺术家快速迭代反馈；  
  - 生成结果易沦为艺术家过往作品的简单拼接，缺乏新颖性。  

2)  
- **核心方法**：采用轻量级高质量声音生成模型SpecMaskGIT，结合艺术家的历史作品数据集与多模态条件控制，实现实时多通道生成。  
- **解决路径**：  
  - **模型轻量化与优化**：  
    - 将Transformer块数从24减至12，参数量降至8927万，替换HiFi-GAN为更快的Vocos声码器；  
    - 支持48kHz高采样率，在双NVIDIA RTX 4080设备上实现八通道实时生成。  
  - **艺术身份保留**：  
    - 使用艺术家Evala逾200小时作品作为独家训练集，确保输出风格一致性；  
    - 通过快速生成-反馈循环，让艺术家直接参与模型调优。  
  - **生成新颖性控制**：  
    - 引入双条件机制：音频输入（标志性声音）与文本提示（过往作品标题），通过CLAP模型融合语义引导；  
    - 扩展Classifier-Free Guidance平衡文本与音频条件，避免输出沦为历史素材拼贴。  
  - **持续生成设计**：  
    - 采用音频外绘策略，每10秒片段重叠5秒，保障三个月展览中声音连贯无中断。  

3)  
- **任务与效果**：  
  - **生成式声音装置**：在东京NTT InterCommunication Center实现三个月实时八通道声音生成，吸引超2万名观众，营造沉浸式听觉环境；  
  - **艺术存档创新**：模型学习艺术家风格后持续生成新声音，实现“新形式存档”概念，突破传统存档限制；  
  - **人机协作验证**：通过轻量模型与多条件输入，平衡了风格保持与生成新颖性，为AI辅助艺术创作提供实践范例。
</div>

</details>

---

## State Space and Self-Attention Collaborative Network with Feature Aggregation for DOA Estimation
- **Authors**: Qi You, Qinghua Huang, Yi-Cheng Lin
- **Categories**: eess.SP, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.25193v1](http://arxiv.org/abs/2510.25193v1)
- **PDF**: [http://arxiv.org/pdf/2510.25193v1](http://arxiv.org/pdf/2510.25193v1)

声源到达方向（DOA）的精确估计因声学特性在时频维度上的持续变化而极具挑战。在此类场景中，精准定位依赖于有效聚合相关特征并建模时序依赖关系的能力。时序建模领域始终面临模型性能与计算效率难以兼顾的难题。为此，我们提出FA-Stateformer——一种融合特征聚合的态空间与自注意力协同网络。该网络首先通过特征聚合模块增强时域与频域的信息特征，继而采用受压缩激励机制启发的轻量化Conformer架构，通过压缩前馈层来降低冗余与参数量。同时引入时序偏移机制，在保持紧凑卷积核的前提下扩展感受野。为强化序列建模能力，网络集成双向Mamba模块，实现基于态空间的双向时序依赖高效表征。剩余自注意力层与Mamba模块协同构建建模框架，在表征能力与计算效率间取得平衡。大量实验表明，FA-Stateformer相较传统架构具有更优的性能与效率表现。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声源到达方向（DOA）估计在复杂声学环境中面临挑战，传统方法（如MUSIC、SRP-PHAT）在混响、噪声或多源动态场景中性能显著下降。  
- **既有问题**：  
  - 传统方法依赖理想物理假设，对动态声源和复杂环境适应性差。  
  - 深度学习模型（如RNN、Conformer）难以平衡计算效率与长序列建模能力。  
  - Mamba类模型虽高效捕获长程依赖，但对局部动态变化不敏感，影响DOA估计精度。  

2)  
- **特征聚合模块**：  
  - 使用时频分离的注意力机制，通过条状卷积分别提取时间和频域关键特征，避免无关维度干扰，增强定位相关特征。  
- **轻量化SEConformer设计**：  
  - 压缩前馈网络层，减少参数冗余；引入时间移位卷积，在不增加核大小的情况下扩展感受野，提升长序列建模效率。  
- **双向Mamba+模块**：  
  - 结合前向与反向状态空间建模，通过选择性遗忘门动态整合历史信息，弥补标准Mamba对双向上下文建模的不足。  
- **协同框架Stateformer**：  
  - 将自注意力层与Mamba块结合，在保持线性计算复杂度的同时，兼顾全局依赖与局部动态特征，实现高效精准的序列建模。  

3)  
- **任务与效果**：  
  - 在模拟和真实数据集（如LOCATA）上评估，涵盖单/多声源、静态/动态场景。  
  - 在移动双声源任务中，Acc5达71.8%，MAE降至2.7°，显著优于对比模型（如IPDnet、ConBiMamba）。  
  - 在低信噪比（-10dB）和高混响（RT60=0.8s）条件下仍保持最优稳定性，计算量仅为IPDnet的1/10，兼顾高精度与低资源消耗。
</div>

</details>

---

## Retaining Mixture Representations for Domain Generalized Anomalous Sound Detection
- **Authors**: Phurich Saengthong, Tomoya Nishida, Kota Dohi, Natsuo Yamashita, Yohei Kawaguchi
- **Categories**: eess.AS, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.25182v1](http://arxiv.org/abs/2510.25182v1)
- **PDF**: [http://arxiv.org/pdf/2510.25182v1](http://arxiv.org/pdf/2510.25182v1)

在开放环境下的异常声音检测需对分布偏移具备鲁棒性，例如应对未知低信噪比的机器与噪声混合输入。当前最优系统通过适配的音频编码器提取嵌入表征，并采用最近邻搜索进行异常检测，但在含噪机器声音上的微调常呈现去噪优化倾向，这会抑制噪声信息并降低系统在失配混合或不一致标注条件下的泛化能力。基于冻结自监督学习编码器的免训练系统虽能规避该问题并展现优异的首样本泛化性能，但当混合声音嵌入偏离纯净源嵌入时，其检测性能会出现显著下降。本研究提出采用“保留而非去噪”策略增强自监督学习骨干网络，以更完整保留混合声源信息。该方法通过多标签音频分类损失与混合对齐损失相结合，使学生模型的混合嵌入向纯净样本和噪声输入的教师模型凸嵌入空间对齐。在稳态、非稳态及失配噪声子集上的受控实验表明，该方法能有效提升分布偏移下的鲁棒性，缩小与理想混合表征之间的性能差距。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：异常声音检测在真实环境中需应对分布偏移，如未知信噪比下的机器与噪声混合。现有方法依赖音频编码器提取嵌入，通过最近邻搜索检测异常。  
- **既有问题**：  
  - 基于微调的方法在含噪数据上训练时，隐式去噪会抑制噪声信息，导致在混合条件不匹配或标签不一致时泛化能力下降。  
  - 免训练的SSL编码器虽能避免微调偏差，但在低信噪比下混合嵌入偏离干净源嵌入，性能显著降低。  

2)  
- **核心方法**：提出“保留而非去噪”策略，通过联合优化多标签音频分类损失和混合对齐损失，提升SSL骨干网络对混合声音的表征能力。  
  - **多标签音频分类**：对混合输入同时标注机器和噪声类别，使用二元交叉熵损失训练学生编码器，保留多源信息。  
  - **混合对齐损失**：冻结教师编码器分别提取干净机器和噪声的嵌入，通过凸组合生成目标嵌入；学生编码器对混合输入提取嵌入，通过均方误差损失对齐教师目标。  
- **解决思路**：  
  - 避免传统方法因去噪导致的信息丢失，直接保留混合中的关键成分。  
  - 通过教师-学生框架稳定嵌入空间，提升对分布偏移的鲁棒性。  
- **技术细节**：采用信噪比控制的混合策略，确保振幅-功率关系符合实际条件；固定权重λ=0.5平衡机器与噪声嵌入的贡献。  

3)  
- **任务与效果**：在包含静态噪声、非静态噪声及不匹配噪声的评估子集上测试：  
  - 在低信噪比（-10~0 dB）条件下，相比基线BEATs迭代3模型，性能提升显著（Hmean分数提高4.1）。  
  - 在公开基准DCASE2023T2和DCASE2025T2上，方法在高层网络层保持性能，优于去噪基线，证实其对多样噪声条件的适应性。  
- **关键成果**：缩小了实际混合嵌入与理想混合表征的差距，在分布偏移下实现了更稳定的异常检测。
</div>

</details>

---

## Joint Analysis of Acoustic Scenes and Sound Events Based on Semi-Supervised Training of Sound Events With Partial Labels
- **Authors**: Keisuke Imoto
- **Categories**: cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.25075v1](http://arxiv.org/abs/2510.25075v1)
- **PDF**: [http://arxiv.org/pdf/2510.25075v1](http://arxiv.org/pdf/2510.25075v1)

【中文摘要】
标注声音事件的时间边界需要大量人力，限制了强监督学习在音频检测中的扩展性。为降低标注成本，仅使用片段级标签的弱监督学习已被广泛采用。作为一种替代方案，部分标签学习提供了一种经济高效的标注策略——通过提供候选标签集合而非精确的弱标注来实现标注简化。然而，音频分析中的部分标签学习仍亟待探索。基于声学场景可为声音事件候选集构建提供上下文信息的发现，本研究利用声学场景信息构建声音事件的部分标签。基于此思路，我们提出一种多任务学习框架，通过声音事件的部分标签联合实现声学场景分类与声音事件检测。尽管弱监督与部分标签学习能降低标注成本，但由于缺乏精确事件集及其时间标注，其检测性能往往受限。为在标注成本与检测性能间取得更好平衡，我们进一步探索了同时利用强标签与部分标签的半监督框架。此外，为实现标签优化并提升模型训练效果，针对所提出的部分标签方法，我们设计了一种基于自蒸馏的标签精细化策略。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：环境声音分析中的声学场景分类（ASC）和声音事件检测（SED）依赖大规模标注数据，但声音事件的时间边界标注成本高昂，限制了强监督学习的扩展性。  
- **既有方法问题**：弱监督学习仅使用片段级标签，虽降低标注成本，但缺乏精确事件集和时序信息，导致检测性能下降；部分标签学习在音频分析中尚未充分探索，现有方法多针对图像分类，未考虑SED中多标签共存的问题。  

2.  
- **核心方法**：提出基于多任务学习（MTL）的半监督框架，联合执行ASC和SED，利用声学场景信息构建声音事件的部分标签集，以减少标注成本。  
- **解决方案细节**：  
  - **部分标签生成**：通过大型语言模型（如ChatGPT）根据声学场景标签自动生成可能的声音事件候选集，替代人工弱标注。  
  - **半监督训练**：结合少量强标签数据和大量部分标签数据，通过自蒸馏技术细化部分标签，生成伪强标签用于模型再训练。  
  - **网络结构**：共享层提取声学特征，分支网络分别处理ASC和SED任务；损失函数整合场景分类的交叉熵和事件检测的二元交叉熵，支持强/弱标签切换。  
- **优势**：部分标签通过场景上下文约束候选事件集，自蒸馏缓解标签噪声，在保证性能的同时显著降低标注需求。  

3.  
- **任务与效果**：在TUT Acoustic Scenes 2016/2017和TUT Sound Events 2016/2017数据集上评估：  
  - **ASC任务**：微/宏F分数达92.12%/92.58%，优于传统方法，部分标签提供的场景信息增强了分类性能。  
  - **SED任务**：微/宏F分数为51.77%/21.51%（段基）和23.96%/14.87%（交并比基），与使用弱标签的半监督方法相当，尤其在长时事件（如鸟鸣、风扇）检测中接近强监督性能。  
- **局限性**：短时事件（如餐具声）检测性能略有下降，因自蒸馏可能引入确认偏差。
</div>

</details>

---
