---
layout: post
title: "arXiv Daily – 2026-02-27"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-27（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-26 08:50 — 2026-02-27 08:50
- 抓取总数：10 篇 | 本页显示：10 篇（去重/过滤后）

## Align-Consistency: Improving Non-autoregressive and Semi-supervised ASR with Consistency Regularization
- **Authors**: Wanting Huang, Weiran Wang
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23171v1](https://arxiv.org/abs/2602.23171v1)
- **PDF**: [https://arxiv.org/pdf/2602.23171v1](https://arxiv.org/pdf/2602.23171v1)

一致性正则化（CR）通过确保预测在输入扰动下保持稳定，提升了连接时序分类（CTC）的鲁棒性与准确性。本文提出对齐一致性（Align-Consistency），作为CR的一种扩展，专为对齐优化（Align-Refine）模型设计——该模型是一种非自回归（non-AR）模型，通过迭代优化帧级假设实现识别。该方法在保持并行推理速度的同时，显著提升了识别性能。对齐一致性的有效性在两种场景中得到验证：首先，在全监督场景下，结果表明将CR同时应用于基础CTC模型及其后续优化步骤至关重要，且非自回归解码与CR带来的精度提升具有叠加效应；其次，在半监督语音识别中，我们利用快速非自回归解码在未标注数据上生成在线伪标签，进一步优化监督模型，从而取得显著性能提升。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：端到端自动语音识别（ASR）模型依赖大量标注数据，在低资源条件下性能显著下降。一致性正则化（CR）虽能提升CTC模型的鲁棒性，但其在其他模型（如非自回归模型）中的应用尚不充分。  
- **既有问题**：  
  - 非自回归解码（如Align-Refine）虽能并行推理加速，但缺乏针对输入扰动的稳定性约束。  
  - 半监督ASR中，伪标签生成通常基于较弱的基础模型（如CTC），限制了性能提升空间。

2)  
论文提出 **Align-Consistency** 方法，将一致性正则化（CR）扩展到基于对齐迭代优化的非自回归模型Align-Refine中，具体通过以下机制解决问题：  
- **核心设计**：  
  - 对同一干净输入施加两次不同的SpecAugment扰动，生成两个增强版本 \(\tilde{x}_1\) 和 \(\tilde{x}_2\)。  
  - 在Align-Refine的 **基础CTC模块**（第0步）和 **所有迭代优化步骤**（第1至S步）上均施加CR损失，强制两个增强输入对应的帧级后验分布保持一致。  
  - CR损失采用对称KL散度计算，无需真实标签，因此可同时用于有监督和无监督数据。  
- **监督训练**：总损失结合非自回归损失（\(L_{nar}\)）与CR损失，权重 \(\lambda_0\) 和 \(\lambda_1\) 分别控制基础CTC和优化步骤的正则化强度。实验表明同时对两者施加CR（\(\lambda_0=0.2, \lambda_1=0.2\)）效果最佳。  
- **半监督扩展**：  
  - 使用Align-Consistency模型（而非基础CTC）**在线生成更准确的伪标签**（取自最后优化步骤的输出）。  
  - 在无监督数据上同样计算Align-Consistency损失（含CR项），提升模型对噪声伪标签的鲁棒性。  
- **关键优势**：  
  - CR增强了模型对输入扰动的稳定性，提升泛化能力。  
  - 非自回归解码提供高效推理，而CR进一步优化各步骤预测一致性，两者增益相互叠加。  
  - 半监督中，高质量伪标签与CR正则化协同作用，大幅降低对标注数据的依赖。

3)  
在LibriSpeech数据集上的实验结果：  
- **完全监督任务**：  
  - 在低资源（LS-100）和高资源（LS-960）设置下，Align-Consistency显著降低词错误率（WER）。例如，LS-100上test-clean/test-other WER从CR-CTC的12.2/26.7降至10.0/22.9；LS-960上从4.3/9.9降至3.3/7.4。  
- **半监督任务**：  
  - 基于LS-100模型，加入960小时无标注数据后，WER进一步降至4.3/9.6；额外加入6000小时数据后降至3.8/9.1。  
  - 基于LS-960模型，加入6000小时无标注数据后，WER从3.3/7.4降至2.5/5.7。  
- **结论**：该方法在监督和半监督ASR中均取得显著性能提升，同时保持非自回归解码的高效性。
</div>

</details>

---

## A Directional-Derivative-Constrained Method for Continuously Steerable Differential Beamformers with Uniform Circular Arrays
- **Authors**: Tiantian Xiong, Yongyi Deng, Kunlong Zhao, Jilu Jin, Xueqin Luo, Gongping Huang, Jingdong Chen, Jacob Benesty
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23119v1](https://arxiv.org/abs/2602.23119v1)
- **PDF**: [https://arxiv.org/pdf/2602.23119v1](https://arxiv.org/pdf/2602.23119v1)

差分麦克风阵列凭借其高空间指向性与紧凑的阵列结构，为远场声信号采集提供了极具前景的解决方案。其核心设计难点在于实现能够连续转向、并能增强来自任意方向目标信号的差分波束形成器。本文针对圆形阵列的差分波束形成器设计展开研究，提出了一种融合方向导数约束的新框架。通过将波束方向图在期望转向方向的一阶导数约束为零，并为高阶导数赋予适当数值，该波束形成器能够确保在目标方向获得最大响应，并提供充分的波束转向能力。该方法不仅提升了转向灵活性，还实现了更直观、更稳健的波束方向图设计。仿真结果表明，所提方法能够生成连续可转向的波束方向图。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：差分麦克风阵列因其高指向性和紧凑结构，在远场声信号采集中具有优势。核心挑战在于设计能连续转向、增强任意方向目标信号的差分波束形成器。
- **既有方法问题**：
  - 传统方法（如零点约束法）仅约束目标方向增益和零点位置，未考虑阵列对称性，导致波束主瓣可能偏离目标方向，且波束图存在增益大于1的区域，可能放大噪声。
  - 对称零点约束法虽能实现全向转向，但缺乏理论严谨性和直观解释性。
  - 级数展开法需要完整的期望指向性模式先验知识，实践中往往难以获得。

2)  
论文提出了一种基于**方向导数约束**的新框架，以解决传统方法在连续转向和波束对准上的不足。其核心方法如下：

- **核心思想**：为确保波束形成器在目标方向θs达到最大响应，直接约束波束图在该方向的一阶导数为零，这保证了θs是一个局部极值点。结合无失真约束（单位增益），即可确保该点为最大值。
- **方法构建**：
  - 对于N阶差分波束形成器，构建约束矩阵DC(ω)。该矩阵整合了三种约束：
    1.  **无失真约束**：`h^H d_{θs} = 1`，确保目标信号无失真通过。
    2.  **方向导数约束**：约束波束图在θs处的一阶导数 `[d_{θs}^H]^{(1)} h = 0`，确保主瓣对准。对于更高阶导数，奇数阶可约束为零，偶数阶可约束为负值以影响波束形状。
    3.  **零点约束**：在指定的干扰方向`θ_{N,n}`施加零点约束 `d_{θ_{N,n}}^H h = 0`。
  - 这些约束共同构成一个线性方程组 `DC(ω) h(ω) = i_β`。
- **求解与优势**：
  - 通过求解上述方程组（例如最大化白噪声增益）可得波束形成器权重向量。
  - 该方法从数学上直接保证了波束主瓣在目标方向取得最大值，实现了**连续且精确的转向**。
  - 它提供了比对称零点约束法更**直观和严谨的理论基础**，同时避免了对完整期望波束图先验知识的依赖。

3)  
论文在**均匀圆形阵列的差分波束形成任务**上验证了所提方法（DMA-DerivCon）的效果：
- **波束形成性能**：仿真表明，该方法能生成主瓣精确对准任意指定转向方向、并在预定位置形成深度零点的波束图，实现了连续、准确的波束转向与置零。
- **指标对比**：与DMA-Null、DMA-SymNull和DMA-SeriesExp等方法相比，DMA-DerivCon在一阶和二阶差分波束形成中，能在整个频带内保持**更高且更平滑的指向性因子**，同时维持**可比的白噪声增益**，在指向性与噪声鲁棒性间取得了良好平衡。
</div>

</details>

---

## Make It Hard to Hear, Easy to Learn: Long-Form Bengali ASR and Speaker Diarization via Extreme Augmentation and Perfect Alignment
- **Authors**: Sanjid Hasan, Risalat Labib, A H M Fuad, Bayazid Hasan
- **Categories**: cs.SD, cs.AI, cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23070v1](https://arxiv.org/abs/2602.23070v1)
- **PDF**: [https://arxiv.org/pdf/2602.23070v1](https://arxiv.org/pdf/2602.23070v1)

尽管孟加拉语自动语音识别（ASR）已取得显著进展，但长时音频处理与鲁棒的说话人日志化仍是关键的研究空白。为应对该语言在联合ASR与日志化任务上严重缺乏资源的问题，我们推出了Lipi-Ghor-882——一个包含882小时的多说话人孟加拉语数据集。本文详细介绍了我们在DL Sprint 4.0竞赛中的提交方案，系统评估了多种适用于长时孟加拉语音频的架构与方法。在ASR方面，我们发现原始数据扩增效果有限；相反，采用精准对齐的标注数据配合合成声学退化（噪声与混响）进行定向微调，成为唯一最高效的策略。而在说话人日志化任务中，我们发现当前全球开源先进模型（如Diarizen）在此复杂数据集上表现意外欠佳。大量模型重训练带来的改进微乎其微；通过基于启发式规则对基线模型输出进行策略性后处理，反而成为提升精度的主要驱动力。最终，本研究构建了一个高度优化的双通道处理流程，实现了约0.019的实时因子，为低资源长时语音处理建立了具有实践指导意义且经实证验证的基准。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语自动语音识别（ASR）虽已取得进展，但处理长音频和鲁棒的说话人日志（SD）仍是关键挑战。  
- **既有问题**：  
  - 缺乏大规模、时间对齐的多说话人对话数据集，导致联合ASR与SD资源严重不足。  
  - 现有预训练模型（如Moonshine、Wav2Vec2）在长音频处理中面临速度与准确性的权衡，且通用开源SOTA模型（如Diarizen）在该语言复杂数据上表现不佳。  
  - 传统方法（如参数高效微调、上下文偏置、集成学习）对性能提升有限，甚至导致模型退化。

2)  
- **ASR核心方法**：  
  - **推理优化**：采用CTranslate2格式转换与faster-whisper并行处理，将Whisper-Medium推理时间从4小时降至26分钟（RTF≈0.019），并手动调整VAD参数避免语音截断。  
  - **数据策略**：放弃单纯扩增数据，转为在**小规模完美对齐数据**上微调，并**主动添加噪声与混响**（20%音频人工失真），迫使模型学习深层语音特征而非声学记忆，显著提升识别鲁棒性。  
- **SD核心方法**：  
  - **放弃模型重训练**：发现对Pyannote等模型进行微调（包括使用模糊音频训练）对DER改进无效。  
  - **启发式后处理**：基于Pyannote Community-1输出，设计严格后处理算法：  
    - 强制说话人间隔（θ_gap=0.17s），合并同一说话人微片段（θ_merge=3.79s）。  
    - 过滤短片段（<0.75s）与总时长不足的说话人（<9.0s），并消除重叠。  
- **资源贡献**：构建并开源**Lipi-Ghor-882数据集**（882小时多说话人孟加拉语音频），提供时间对齐标注，支撑方法验证。

3)  
- **ASR任务**：在22小时隐藏测试集上，优化后的Whisper-Medium模型取得最佳词错误率（WER≈0.3107），相比基线（WER≈0.4444）显著提升，同时保持高效推理（RTF≈0.019）。  
- **说话人日志任务**：通过后处理策略，在相同测试集上将DER降至最低约0.2028（公开榜），优于直接使用Diarizen等SOTA模型（DER>0.2789）。  
- **整体效果**：双管道实现高精度与低延迟，为低资源长音频处理提供了实证基准。
</div>

</details>

---

## TADA: A Generative Framework for Speech Modeling via Text-Acoustic Dual Alignment
- **Authors**: Trung Dang, Sharath Rao, Ananya Gupta, Christopher Gagne, Panagiotis Tzirakis, Alice Baird, Jakub Piotr Cłapa, Peter Chin, Alan Cowen
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.23068v1](https://arxiv.org/abs/2602.23068v1)
- **PDF**: [https://arxiv.org/pdf/2602.23068v1](https://arxiv.org/pdf/2602.23068v1)

现代文本转语音系统日益采用大语言模型架构，以实现可扩展、高保真、零样本的语音生成。然而，这些系统通常依赖固定帧率的声学标记化方案，导致生成的语音序列长度远超过对应文本，且两者存在异步性。这不仅降低了计算效率，还容易引发TTS系统的内容幻觉问题，并加剧口语建模中的模态差异。本文提出一种新颖的标记化方案，可在连续声学特征与文本标记间建立一一对应的同步关系，从而在大语言模型中实现统一的单流建模。实验表明，这种同步标记在保持高保真音频重建能力的同时，能够通过配备流匹配头的大语言模型在隐空间中进行有效建模。此外，该方案支持在上下文环境中无缝切换语音模态，实现纯文本引导——通过融合纯文本模式与文本-语音模式的逻辑输出，灵活弥合纯文本大语言模型的智能差距。实验结果显示，本方法在显著降低推理成本的同时，取得了与前沿TTS及口语建模系统相当的性能，几乎完全消除了内容幻觉现象，并有效保持了语言完整性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**  
- **背景**：基于大语言模型（LLM）的现代TTS系统虽能实现零样本生成，但通常依赖固定帧率的声学分词，导致语音序列远长于文本序列。  
- **问题**：  
  - **计算效率低**：序列长度差异导致Transformer上下文窗口需求剧增，计算复杂度呈二次增长，拖慢训练与推理。  
  - **模态鸿沟**：文本与语音序列异步，需复杂交错或分层建模，阻碍单流统一建模。  
  - **幻觉问题**：长度失配易引发TTS内容幻觉（如跳词、插入无关内容），降低系统可靠性。  

2)  
**论文核心方法如何解决上述问题**  
本文提出 **TADA** 框架，通过 **文本-声学双对齐** 实现同步分词与统一建模，具体如下：  

- **同步分词方案**：  
  - 使用CTC与Viterbi解码，建立文本单元与音频帧的 **一对一单调映射**，将声学特征压缩为与文本令牌对齐的潜在向量。  
  - 编码器通过局部注意力机制，将变长音频段聚合为同步潜在表示，解码器则基于对齐位置重建高保真音频。  
  - 该方案将语音建模帧率降至2–3 fps，大幅缩短序列长度。  

- **统一自回归建模**：  
  - 在LLM中，文本与声学特征以 **单流同步方式** 融合（而非交错），文本令牌与声学特征在序列中并行对齐。  
  - 通过 **流匹配头** 联合预测声学特征与每令牌的时长，实现端到端生成。  
  - 结构上支持 **纯文本推理** 与 **文本-语音多模态推理** 的无缝切换。  

- **模态鸿沟缓解**：  
  - 提出 **语音自由引导**：通过混合纯文本模式与多模态模式的logits，调节超参数 λSFG，使语言性能接近纯文本基线，同时保留利用语音上下文的能力。  
  - **在线拒绝采样**：基于说话人嵌入的一致性检测，实时剔除低质量生成样本，提升输出稳定性。  

- **关键优势**：  
  - **计算高效**：同步分词极大压缩序列长度，降低LLM上下文负担，提升训练与推理吞吐量。  
  - **消除幻觉**：一对一对齐提供强归纳偏置，从根本上避免时序错位与内容幻觉。  
  - **灵活扩展**：支持固定帧率编码非转录音频，保持高重建质量。  

3)  
**在哪些任务上取得了怎样的效果**  
- **语音克隆**：在SeedTTS-Eval、LibriTTS等基准上，CER与SIM指标媲美SOTA系统（如Index-TTS2、VibeVoice），且 **内容幻觉率为零**（基线模型存在数十个幻觉样本）。  
- **长时表达性生成**：在EARS数据集上，结合文本自由引导与在线拒绝采样，说话人相似度接近最优，自然度主观评分位列第二。  
- **推理效率**：帧率降至2–3 fps，RTF显著低于固定帧率系统（如VibeVoice），内存增长随输出时长增加而更平缓。  
- **口语语言建模**：在Seamless Interaction、Spoken StoryCloze等基准上，文本-语音模式的困惑度接近纯文本LLM，且通过SFG在多模态任务中超越部分7B参数基线。
</div>

</details>

---

## Scattering Transform for Auditory Attention Decoding
- **Authors**: René Pallenberg, Fabrice Katzberg, Alfred Mertins, Marco Maass
- **Categories**: eess.SP, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23003v1](https://arxiv.org/abs/2602.23003v1)
- **PDF**: [https://arxiv.org/pdf/2602.23003v1](https://arxiv.org/pdf/2602.23003v1)

随着人口结构变化，助听器的使用将在未来几年持续增长。新一代助听器仍需解决的关键难题是鸡尾酒会问题。基于脑电图的听觉注意力解码技术为此提供了一种潜在解决方案，近年来已成为多项研究的焦点，这些研究大多采用相似的预处理方法。本研究为寻求性能突破，提出使用散射变换作为传统预处理方法的替代方案。我们对比了两层散射变换与常规滤波器组、同步压缩短时傅里叶变换及通用预处理方法的效果。为验证性能，在鲁汶大学（KUL）和丹麦技术大学（DTU）提供的两个常用数据集上，针对不同分类任务比较了传统方法与新提出的预处理方法。分类模型涵盖经典与新兴神经网络架构，包括卷积神经网络、长短期记忆网络以及最新的基于Transformer/图结构的模型。通过多种评估策略的对比，重点考察了对训练集未出现说话人的分类能力。实验表明，两层散射变换能显著提升针对特定受试者的分类性能，在KUL数据集上表现尤为突出。然而在DTU数据集上，该优势仅体现在部分模型或采用更丰富训练数据（如十倍交叉验证）的场景中。这些发现证明散射变换能够提取额外的有效信息。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于脑电图的听觉注意解码是解决助听设备“鸡尾酒会问题”的关键技术。现有方法大多依赖相同的预处理流程，即通过滤波器组提取音频包络并对EEG进行带通滤波。
- **既有问题**：传统预处理存在两个主要局限：一是音频包络提取过程丢弃了高阶时间-频率结构（如分层幅度调制）；二是该方法在短决策窗口下表现脆弱，而实时助听应用恰恰需要短窗口下的鲁棒性能。

2)  
- **核心方法**：论文提出使用**散射变换**作为替代预处理方法。散射变换通过级联复小波卷积、模运算和低通平均，构建具有局部平移不变性和对小时间形变稳定的分层时频表示。
- **解决既有问题**：
    - **保留高阶信息**：与传统滤波器组仅捕获一阶频谱内容不同，散射变换的第二层系数能明确捕获“调制的调制”，即嵌套的包络动态和更精细的频谱能量分布，这提供了对区分听觉事件至关重要的二阶时间结构信息。
    - **增强短窗口鲁棒性**：散射变换的数学特性（如Lipschitz稳定性）使其对信号的小形变不敏感，这有助于解决传统相关方法在短决策窗口下的脆弱性问题，更符合实时应用的低延迟要求。
    - **引入强归纳偏置**：散射变换使用数学设计的滤波器核，类比于学习到的卷积滤波器但参数更少。这减少了模型的可学习参数量，有助于在每被试数据有限的AAD任务中缓解过拟合。
    - **兼容性与评估**：该方法与多种神经网络架构（CNN、LSTM、Transformer/图网络）兼容。论文采用严谨的评估策略（如Dietterich‘s 5×2交叉验证）来系统比较其性能。

3)  
- **任务与效果**：论文在KU Leuven和DTU两个公开数据集上，针对多种分类任务评估了散射变换预处理的效果。
    - **被试内条件**：在KUL数据集上，散射变换显著提升了所有测试模型在**被试相关条件**下的性能，平均准确率最高可达0.92。在更具挑战性的**说话人未知**评估中，散射变换也带来了显著改进。
    - **跨数据集表现**：在DTU数据集上，散射变换的优势因模型而异，仅在GCANet-NoEn和LSTM-X等部分模型上，或当提供更多训练数据（如10折交叉验证）时，才表现出明显提升。这表明散射变换能够提取额外的相关信息，但其效益受数据集特性（如注意力切换频率、声学环境异质性）和训练数据量的影响。
</div>

</details>

---

## A Holistic Framework for Robust Bangla ASR and Speaker Diarization with Optimized VAD and CTC Alignment
- **Authors**: Zarif Ishmam, Zarif Mahir, Shafnan Wasif, Md. Ishtiak Moin
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.22935v1](https://arxiv.org/abs/2602.22935v1)
- **PDF**: [https://arxiv.org/pdf/2602.22935v1](https://arxiv.org/pdf/2602.22935v1)

尽管孟加拉语是全球使用最广泛的语言之一，但在自然语言处理领域仍属于低资源语言。当前主流的孟加拉语自动语音识别与说话人日志系统在处理超过30-60秒的长音频时面临挑战。本文提出一个专为长时孟加拉语内容设计的鲁棒性框架，通过结合预训练模型与针对DL Sprint 4.0竞赛设计的新型优化流程实现性能提升。该框架采用语音活动检测优化技术，并通过基于强制词对齐的连接时序分类分割方法，确保长时音频的时间精度与转写完整性。此外，我们运用了多种微调技术，并采用数据增强与噪声消除方法进行预处理。通过弥补复杂多说话人场景下的性能差距，本研究为实际应用中的长时孟加拉语语音处理提供了可扩展的解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语是全球使用广泛的低资源语言，其自动语音识别和说话人日志系统在处理超过30-60秒的长音频时面临挑战。  
- **既有问题**：  
  - 主流模型通常在短片段（<30秒）上训练，处理长音频时易出现“幻觉”、灾难性遗忘或时间对齐丢失，导致词错误率飙升。  
  - 说话人日志在长音频、多说话人环境中因缺乏标注数据而性能下降。  
  - 现有方法难以在保持语音连续性的同时进行精确分段。

2)  
论文提出一个针对长音频孟加拉语内容的整体框架，通过以下核心方法解决上述问题：  
- **音频预处理与数据增强**：  
  - 采用源分离技术进行降噪，提升音频质量。  
  - 在训练中应用动态增益调整和音频增强，以增强模型在不同录音环境中的鲁棒性。  
- **优化的语音活动检测**：  
  - 引入精细化的VAD层，准确识别并过滤非语音段，减少后续处理的干扰。  
- **CTC对齐与分段**：  
  - 利用基于CTC的强制词级对齐，为每个单词提取精确的时间戳。  
  - 根据时间戳将长音频智能切分为小于30秒的片段，确保单词边界不被破坏，同时适配Whisper等模型的输入限制。  
- **集成化说话人日志**：  
  - 设计三阶段课程学习管道：  
    - 阶段一：在原始音频上微调基线模型，适应孟加拉语语音特征。  
    - 阶段二：使用Demucs进行人声分离，在纯净人声上进一步微调，提升边界检测和说话人区分能力。  
    - 阶段三：引入动态增益增强，防止模型对纯净音频过拟合，增强鲁棒性。  
- **模型微调**：  
  - 使用CTC对齐后的数据集对Whisper模型进行微调，优化词错误率。

3)  
- **自动语音识别任务**：在孟加拉语公共和私有测试集上评估，微调后的Tugstugi模型词错误率最低（公共集WER 0.21988，私有集WER 0.23585），显著优于零样本和其他基准模型。  
- **说话人日志任务**：采用“微调+Demucs优化+数据增强”策略在公共集上取得最低DER（0.21460），表明该方法能有效提升长音频多说话人环境下的日志精度。  
- **效率**：通过双GPU并行推理，将ASR推理时间缩短至约2小时。
</div>

</details>

---

## Same Words, Different Judgments: Modality Effects on Preference Alignment
- **Authors**: Aaron Broukhim, Nadir Weibel, Eshin Jolly
- **Categories**: cs.SD, cs.AI, cs.HC
- **arXiv**: [https://arxiv.org/abs/2602.22710v1](https://arxiv.org/abs/2602.22710v1)
- **PDF**: [https://arxiv.org/pdf/2602.22710v1](https://arxiv.org/pdf/2602.22710v1)

基于偏好的强化学习（PbRL）是当前将人工智能系统与人类偏好对齐的主流框架，但其在语音领域的应用仍待深入探索。本研究通过一项受控跨模态实验，对比了人类与合成偏好标注在文本和音频两种模态下的表现，对100组语义内容完全相同的提示进行了评估。结果表明，音频偏好标注的可靠性不亚于文本，在约9名标注者参与时，评分者间一致性达到良好水平（ICC(2,k) $\approx$ .80）——这是偏好标注文献中首次基于ICC对任一模态进行可靠性量化。然而，模态显著影响了人们的评判方式：音频标注者表现出更严格的决策阈值、更弱的长度偏差，且更倾向于以用户为中心的评估标准，导致跨模态一致性接近随机水平。合成评分进一步与人类判断保持一致，并能预测评分者间一致性，这支持了其在筛选模糊样本对或完全替代人工标注方面的应用潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于偏好的强化学习是AI对齐的主流方法，但针对语音模态的研究不足。现有研究集中于文本，存在已知偏见（如长度偏好），且文本标注过程存在不可控变量（如注意力分配）。  
- **既有方法问题**：语音偏好标注研究稀缺，常将文本数据集通过TTS转换为音频，并假设文本偏好可直接迁移至语音，但跨模态偏好的一致性未知，且缺乏对标注可靠性的系统量化。  

2)  
- **核心方法**：本研究设计了一项受控的跨模态实验，使用PRISM文本数据集，通过TTS生成语义内容相同的音频，分别收集人类对文本和音频的偏好评分（1-100连续评分），并引入合成评分（GPT-4o生成）。  
- **解决既有问题**：  
  - **评估音频可靠性**：首次在偏好标注文献中使用组内相关系数量化可靠性，发现音频与文本在聚合约9名评分者时均达到良好一致性（ICC ≈ .80），证明音频数据同样可靠。  
  - **揭示模态效应**：通过比较跨模态评分，发现偏好判断模式存在差异：音频评分者决策阈值更窄、长度偏见更弱、更关注用户需求（如“帮助”），而文本评分者更关注内容细节。跨模态一致性接近随机水平，表明文本偏好不能直接代理音频偏好。  
  - **探索合成评分效用**：合成评分与人类判断一致，并能预测人类评分者间的一致性，可用于筛选模糊样本或替代部分人工标注，降低成本。  

3)  
- **任务与效果**：  
  - **偏好标注可靠性评估**：在文本和音频模态上，首次量化了达到特定可靠性所需评分者数量（约9人可达良好一致性）。  
  - **跨模态偏好分析**：揭示了模态对判断标准、长度敏感性和顺序效应（近因偏见）的影响，为设计模态特定的对齐管道提供了依据。  
  - **合成评分应用**：在文本和音频任务上，合成评分能有效识别易/难样本，并与人类评分对齐，支持其在RLAIF中作为增强或替代方案。
</div>

</details>

---

## Deepfake Word Detection by Next-token Prediction using Fine-tuned Whisper
- **Authors**: Hoan My Tran, Xin Wang, Wanying Ge, Xuechen Liu, Junichi Yamagishi
- **Categories**: eess.AS, cs.CL
- **arXiv**: [https://arxiv.org/abs/2602.22658v1](https://arxiv.org/abs/2602.22658v1)
- **PDF**: [https://arxiv.org/pdf/2602.22658v1](https://arxiv.org/pdf/2602.22658v1)

深度伪造语音可通过将真实语音中的部分词语替换为由语音生成模型合成的语义不同词语来伪造。针对此类伪造，虽然可开发专用的合成词检测器，但本研究探索了一种更经济高效的方法：通过微调预训练的Whisper模型，使其在通过下一词预测进行语音转写的同时检测合成词。我们进一步研究了使用部分声码化语音作为微调数据，以降低数据收集成本。实验表明，在领域内测试数据上，微调后的Whisper模型在合成词检测错误率和转写错误率上均表现优异。在领域外测试数据（包含未见过的语音生成模型产生的合成词）上，该模型与基于ResNet的专用检测模型性能相当；但整体性能下降表明仍需提升其泛化能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：深度伪造语音可通过替换真实语音中的词语来伪造。现有方法需开发专门的检测模型，流程复杂且成本高。
- **既有问题**：传统检测方法需要完整的数据收集、模型设计、训练和调优流程，计算和存储开销大。同时，检测语音中合成部分（而非整句）的任务更为复杂，现有方案通常基于二元检测器进行扩展，不够高效。

2)  
- **核心思路**：通过微调预训练的Whisper模型，使其在转录语音的同时，通过下一个词元预测任务来检测合成词。该方法无需改变模型架构或训练算法，成本低。
- **具体方法**：
  - **任务集成**：在训练数据的文本序列中，为合成词添加特殊标记（如`<TOF>`和`<EOF>`），将合成词检测任务融入Whisper的转录任务中。推理时，模型输出中位于这对标记之间的词即被判定为合成词。
  - **数据准备**：为降低数据收集成本，使用声码器对真实语音片段进行重合成来模拟合成词，构建微调数据。这避免了依赖多样化的语音生成模型。
- **优势**：避免了开发独立检测器的成本，仅需对现有ASR模型进行微调，同时保持了转录准确性。

3)  
- **任务与效果**：
  - **领域内测试**：在训练与测试数据领域匹配时（如使用声码器或TTS系统数据），微调后的Whisper在合成词检测上错误率低（如FAR 7.22%， FRR 0.52%），与专用ResNet检测模型性能相当，且转录错误率（WER）未下降。
  - **跨领域测试**：当测试数据来自不同领域或使用未见过的合成方法时（如YouTube或工作室录音），检测性能出现不同程度下降，表明模型泛化能力有待提升。
</div>

</details>

---

## Relating the Neural Representations of Vocalized, Mimed, and Imagined Speech
- **Authors**: Maryam Maghsoudi, Rupesh Chillale, Shihab A. Shamma
- **Categories**: cs.SD, eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2602.22597v1](https://arxiv.org/abs/2602.22597v1)
- **PDF**: [https://arxiv.org/pdf/2602.22597v1](https://arxiv.org/pdf/2602.22597v1)

本研究利用公开的立体定向脑电图数据，探究了发声、默读及想象语音的神经表征间的关系。以往研究多集中于单独解码各条件下的语音响应，而本文则通过为每种条件训练线性频谱图重建模型，并评估其在跨条件下的泛化能力，以探索不同条件间响应的关联性。实验表明，基于单一条件训练的线性解码器通常能成功迁移至其他条件，这暗示了语音表征的共享性。通过基于排序的分析方法，我们在刺激层面评估了这种共性，证明在条件内及跨条件下，刺激特异性结构均得以保持。最后，我们对比了线性模型与非线性神经网络的重建效果：两者均表现出跨条件迁移能力，但线性模型在刺激层面的区分度上表现更优。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：听觉神经科学与脑机接口（BCI）致力于从神经活动中解码语音特征。以往研究多集中于**单独解码**听语音、发声语音、默读或想象语音的神经响应。
- **既有问题**：多数工作仅在**单一条件内**进行解码，未能系统探究**不同语音产生模式**（发声、默读、想象）之间神经表征的**关系与共享性**。此外，解码想象语音因缺乏外部时间线索而更具挑战。

2)  
- **核心方法**：本研究使用公开的立体脑电图（sEEG）数据集，包含同一被试在**发声、默读、想象**三种条件下重复说100个中文句子的记录。核心分析围绕**线性解码器**展开，具体步骤如下：
    - **条件内与跨条件训练与测试**：为每种条件分别训练线性解码器，以从时滞神经响应中重建语音频谱图。随后，将在一个条件下训练的解码器**直接应用于另外两种条件**的测试数据，评估其跨条件泛化能力。
    - **表征相似性分析**：通过比较跨条件重建性能（计算重建频谱图包络与目标包络的线性相关系数），推断不同语音模式之间神经表征的**共享程度**。性能显著高于随机基线即表明存在共享表征。
    - **刺激特异性判别力评估**：为补充相关性分析，进行了**基于排序的分析**。计算每个重建包络与所有候选目标包络的相关系数，检查正确句子出现在前k个匹配中的概率，并通过计算曲线下面积（AUC）来量化**句子水平的判别力**，评估重建是否保留了刺激特有的结构。
    - **与非线性模型的对比**：同时训练了一个**非线性神经网络解码器**（卷积层+循环层）进行相同的跨条件评估，以比较不同架构在重建性能与刺激特异性保留上的差异。
- **解决思路**：该方法通过**系统性的跨条件解码器迁移测试**，直接检验了不同语音产生模式之间神经表征的**重叠与层级关系**。线性模型的可解释性有助于从几何角度（子空间投影）理解这种共享性，而排序分析则确保了评估不仅关注整体相似性，也关注**刺激个体信息的保留**。

3)  
- **任务与效果**：在**从sEEG解码语音频谱图**的任务上，研究取得了以下效果：
    - **成功实现跨条件解码**：线性与非线性解码器在**所有跨条件组合**（如用默读数据训练，测试于发声数据）中，重建性能均**显著高于随机水平**，证实了不同语音模式间存在共享的神经表征。
    - **揭示了层级表征关系**：解码器迁移模式支持**发声、默读、想象语音的神经表征呈层级嵌套**的假设。例如，默读训练的解码器对默读和发声数据表现相似，表明共享发音相关结构；而想象训练的解码器对想象和默读数据表现相似，表明共享计划相关结构。
    - **保留了刺激特异性信息**：排序分析显示，重建信号在**句子水平上具有高于随机水平的判别力**（AUC > 0），表明重建保留了刺激特有的结构。**线性模型在刺激特异性保留方面优于非线性模型**，尽管后者整体重建相关性更高。
</div>

</details>

---

## Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing
- **Authors**: An-Ci Peng, Kuan-Tang Huang, Tien-Hong Lo, Hung-Shin Lee, Hsin-Min Wang, Berlin Chen
- **Categories**: cs.CL, cs.AI, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.22522v1](https://arxiv.org/abs/2602.22522v1)
- **PDF**: [https://arxiv.org/pdf/2602.22522v1](https://arxiv.org/pdf/2602.22522v1)

台湾客家语作为一种资源匮乏且濒危的语言，在自动语音识别（ASR）领域面临显著挑战，包括较高的方言变异性以及存在汉字与拼音两种不同的书写系统。传统ASR模型在此类场景中常遇困难，因其倾向于在音系和词汇维度上将核心语言内容与方言特有的变异混为一谈。为应对这些挑战，我们提出了一种基于循环神经网络转换器（RNN-T）的统一框架。该框架的核心是引入方言感知建模策略，旨在将方言“风格”与语言“内容”解耦，从而增强模型学习鲁棒且泛化性强的表征能力。此外，框架采用参数高效的预测网络，同时对汉字与拼音的ASR任务进行建模。实验表明，这些任务间形成了强大的协同效应，跨书写系统的目标函数可作为相互正则化器以提升主要ASR任务的性能。在HAT语料库上的实验结果显示，我们的模型在汉字与拼音ASR任务上分别实现了57.00%和40.41%的相对错误率降低。据我们所知，这是首次系统性地探究客家方言变异对ASR的影响，也是首个能够联合处理这些任务的单一模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：台湾客家语是一种低资源、濒危语言，其自动语音识别面临两大挑战：方言变异性高（如四县、海陆等方言在音韵和词汇上差异显著），以及存在汉字和拼音两种书写系统。  
- **既有方法问题**：传统ASR模型在处理此类语言时，共享编码器容易将核心语言内容与方言特异性特征混淆，导致表示学习效率低下。现有方法多针对高资源语言设计，例如为不同方言训练独立预测网络或在解码器中添加方言ID，但这些策略在低资源场景下因数据不足而效果有限，且未能解决编码器中的表示混淆问题。

2)  
论文提出一个基于RNN-T的统一框架，通过方言感知建模和多任务学习协同解决上述问题。  

- **方言感知建模**：旨在解耦方言“风格”与语言“内容”。  
    - **方言信息集成**：将方言ID映射为连续嵌入向量，与编码器输出的声学表示拼接，使联合网络能依据方言调整预测。  
    - **辅助方言分类器**：在编码器输出端添加分类头，通过辅助损失函数引导编码器学习更具判别性的方言特征。  
    - **方言条件化**：在解码端，创新性地提出**词符交错条件化**策略，即在目标序列中每个词符后插入方言ID，为预测网络提供持续的方言上下文，使其充当方言条件语言模型，动态适应不同方言的词汇和音韵映射。  

- **多任务学习框架**：使用单一共享编码器，但为汉字和拼音任务配备轻量级、参数高效的独立预测网络与联合网络。  
    - **协同效应**：拼音任务强调声学精确性，汉字任务强调高层语义上下文，两者形成互补目标，共同正则化共享编码器，使其学习到既精确又富含上下文信息的表示。  

- **整体优势**：该方法在编码器和解码器端同时注入方言知识，并利用多任务学习的正则化作用，从而在低资源条件下实现更鲁棒和泛化的表示学习。

3)  
- **任务与效果**：在台湾客家语HAT语料库上，针对汉字和拼音ASR任务进行评测。  
- **主要成果**：最终模型（结合ADC、DII和TIC策略）相比单任务基线，在汉字ASR上实现了**57.00%** 的相对错误率降低，在拼音ASR上实现了**40.41%** 的相对错误率降低。  
- **其他贡献**：这是首个系统研究客家方言变异对ASR影响的工作，并首次实现了单一模型联合处理汉字与拼音识别任务。
</div>

</details>

---
