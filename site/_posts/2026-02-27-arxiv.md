---
layout: post
title: "arXiv Daily – 2026-02-27"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-27（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-26 08:50 — 2026-02-27 08:50
- 抓取总数：12 篇 | 本页显示：12 篇（去重/过滤后）

## SemanticVocoder: Bridging Audio Generation and Audio Understanding via Semantic Latents
- **Authors**: Zeyu Xie, Chenxing Li, Qiao Jin, Xuenan Xu, Guanrou Yang, Wenfu Wang, Mengyue Wu, Dong Yu, Yuexian Zou
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.23333v1](https://arxiv.org/abs/2602.23333v1)
- **PDF**: [https://arxiv.org/pdf/2602.23333v1](https://arxiv.org/pdf/2602.23333v1)

近期音频生成模型通常依赖变分自编码器（VAE），并在其潜在空间中进行生成。尽管VAE在压缩与重建方面表现优异，但其潜在表示本质上编码的是低层次声学细节，而非具有语义区分性的信息，这导致事件语义纠缠不清，增加了生成模型的训练难度。为解决这些问题，我们摒弃了VAE的声学潜在表示，引入了语义编码器潜在表示，进而提出了SemanticVocoder——一种能够直接从语义潜在表示合成波形的生成式声码器。基于SemanticVocoder构建的文本到音频生成模型，在AudioCaps测试集上取得了12.823的弗雷歇距离和1.709的弗雷歇音频距离，这表明所引入的语义潜在表示相比声学VAE潜在表示具有更优的区分能力。除了提升生成性能外，该模型也为在共享语义空间中统一音频理解与生成任务提供了有价值的探索。生成样本可在 https://zeyuxie29.github.io/SemanticVocoder/ 获取。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本到音频生成的主流方法采用两阶段架构，首阶段使用变分自编码器压缩音频为声学潜在表示。  
- **既有问题**：VAE的声学潜在表示侧重于保留细粒度声学细节，导致语义判别性弱、事件语义纠缠，且其低维瓶颈限制了语义信息捕获，这增加了第二阶段生成模型将文本映射到复杂低层声学特征的难度。

2)  
论文提出**SemanticVocoder**，核心方法是摒弃VAE声学潜在表示，改用语义潜在表示，并基于流匹配直接合成波形，从而解决上述问题。具体包括：  
- **使用语义潜在表示**：采用预训练的MAE编码器提取高维语义潜在表示，其专注于高层语义内容（如“狗叫”），而非低层声学细节，具有更强的语义解缠和判别性结构，更利于生成建模。  
- **采用生成式流匹配范式**：设计基于流匹配的生成式声码器，直接以语义潜在表示为条件合成波形，避免了传统VAE重建目标与生成目标不匹配的问题，缓解了从语义潜在表示重建波形时的失真。  
- **实现模块解耦与即插即用**：以语义潜在表示为锚点，文本到潜在表示的生成模型与SemanticVocoder可独立训练，两者相互独立，具备即插即用能力，打破了传统VAE与第二阶段模型必须顺序训练的依赖。

3)  
- **文本到音频生成任务**：在AudioCaps测试集上，取得了最佳性能，Fréchet Distance为12.823，Fréchet Audio Distance为1.709，表明生成音频的分布最接近真实音频分布。  
- **音频理解任务**：在HEAR基准测试（包括事件检测、分类等任务）上，语义潜在表示相比基线VAE声学潜在表示展现出显著更强的判别性能，例如在DCASE2016任务上达到93.690的准确率。  
- **音频重建任务**：尽管主要为生成优化，其重建性能与专注于重建的基线模型相当，显示了语义表示的灵活性。
</div>

</details>

---

## A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations
- **Authors**: Soumya Dutta, Smruthi Balaji, Sriram Ganapathy
- **Categories**: cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23300v1](https://arxiv.org/abs/2602.23300v1)
- **PDF**: [https://arxiv.org/pdf/2602.23300v1](https://arxiv.org/pdf/2602.23300v1)

对话中的情感识别面临独特挑战，需同时捕捉多轮对话的时序动态并有效融合多模态信息。本文提出基于语音-文本专家混合的情感识别模型（MiSTER-E），该模块化专家混合框架旨在解耦情感识别中的两大核心问题：模态特定上下文建模与多模态信息融合。MiSTER-E利用针对语音和文本分别微调的大语言模型生成丰富的语句级嵌入表示，并通过卷积-循环上下文建模层进行增强。系统通过可学习的门控机制动态加权整合三个专家（纯语音、纯文本及跨模态专家）的预测结果。为促进跨模态表征的一致性与对齐，我们引入了配对语音-文本表征的监督对比损失，以及基于KL散度的专家预测正则化方法。值得注意的是，MiSTER-E在所有处理阶段均不依赖说话人身份信息。在IEMOCAP、MELD和MOSI三个基准数据集上的实验表明，本方法分别取得了70.9%、69.5%和87.9%的加权F1分数，优于多种语音-文本情感识别基线系统。我们还通过多组消融实验验证了所提方法中各组件的贡献。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：对话中的情感识别（ERC）需建模多轮对话的时序动态并融合多模态信息。现有方法面临两大核心挑战：  
- **既有问题**：多数系统采用单一架构，将**时序上下文建模**与**跨模态信息融合**这两个本质不同的任务耦合在一起。这种耦合在训练数据稀缺时易导致过拟合，限制了模型的泛化能力。此外，当不同模态性能差异显著时，现有融合方法难以超越最佳单模态系统。

2)  
论文提出 **MiSTER-E**，一个模块化的专家混合框架，通过以下方式解决上述问题：  
- **架构解耦**：  
  - 使用三个独立优化的专家分支：**纯语音专家**、**纯文本专家**和**多模态专家**。  
  - 前两者通过时序初始网络和循环层捕获模态特定的对话上下文，后者通过交叉注意力和自注意力层融合语音与文本特征。  
- **决策级融合**：  
  - 引入**学习型门控机制**，在Softmax归一化前动态加权三个专家的输出（logits）。这种logit级融合允许模型根据每句话自适应选择专家，有效处理模态不平衡（例如当某一模态明显更可靠时）。  
- **辅助训练目标**：  
  - 使用**监督对比损失**对齐配对语音-文本表征，增强跨模态一致性。  
  - 引入**基于KL散度的一致性正则化**，鼓励专家预测之间的一致性，稳定专家专业化。  
- **特征提取**：  
  - 分别微调大型语言模型（LLaMA-3.1-8B）和语音大语言模型（SALMONN-7B）作为文本和语音编码器，获得丰富的语句级嵌入。

3)  
在三个基准数据集上取得了最先进的效果：  
- **IEMOCAP**：加权F1分数达 **70.9%**。  
- **MELD**：加权F1分数达 **69.5%**。  
- **MOSI**（二分类情感）：加权F1分数达 **87.9%**。  
- 均优于多个基线语音-文本ERC系统，且**不依赖说话人身份信息**。
</div>

</details>

---

## Align-Consistency: Improving Non-autoregressive and Semi-supervised ASR with Consistency Regularization
- **Authors**: Wanting Huang, Weiran Wang
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23171v1](https://arxiv.org/abs/2602.23171v1)
- **PDF**: [https://arxiv.org/pdf/2602.23171v1](https://arxiv.org/pdf/2602.23171v1)

一致性正则化（CR）通过确保预测在输入扰动下保持稳定，提升了连接时序分类（CTC）的鲁棒性与准确性。本文提出对齐一致性（Align-Consistency），作为CR的扩展方法，专为对齐优化（Align-Refine）模型设计——该模型是一种非自回归模型，可对帧级假设进行迭代优化。该方法在保持并行推理速度的同时，显著提升了识别性能。对齐一致性的有效性在两种场景中得到验证：首先，在全监督场景下，结果表明将CR同时应用于基础CTC模型及其后续优化步骤至关重要，且非自回归解码与CR带来的精度提升具有叠加效应；其次，在半监督语音识别中，我们采用快速非自回归解码为无标注数据生成在线伪标签，并利用这些标签进一步优化监督模型，从而取得显著性能提升。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：端到端自动语音识别（ASR）模型依赖大量标注数据，在低资源条件下性能显著下降。一致性正则化（CR）虽能提升CTC模型的鲁棒性，但其在其他模型（如非自回归模型）中的应用研究不足。
- **既有方法问题**：
  - 自回归模型解码效率低，非自回归模型（如Align-Refine）虽能并行推理，但性能仍有提升空间。
  - 半监督ASR中，伪标签质量受限于基础模型（如CTC）的准确性。
  - CR此前主要应用于CTC，未扩展到包含多步细化的非自回归框架。

2)  
论文提出 **Align-Consistency** 方法，将一致性正则化（CR）集成到基于对齐迭代细化的非自回归模型（Align-Refine）中，并在半监督设置中应用。具体解决方式如下：

- **核心机制**：
  - 模型基于Align-Refine，包含一个CTC模块和一个Transformer解码器，进行多步帧级假设细化。
  - 对同一干净输入应用两次数据增强（SpecAugment），生成两个增强版本。
  - 在**所有步骤**（包括基础CTC和每个细化步骤）上施加CR损失，强制两个增强版本对应的帧级后验分布保持一致，使用对称KL散度计算损失。
  - 最终的监督学习损失是标准非自回归损失与各步骤CR损失的加权和。

- **解决监督学习中的问题**：
  - CR应用于整个细化流程，而不仅仅是CTC，使模型对输入扰动更鲁棒，提升了非自回归解码的准确性。
  - 实验表明，对基础CTC和细化步骤同时应用CR能获得最佳效果，两者改进具有互补性。

- **解决半监督学习中的问题**：
  - 采用**在线自训练**：使用训练中的模型（而非固定种子模型）为无标签数据动态生成伪标签。
  - **更优的伪标签源**：使用Align-Consistency模型最后一步细化的输出作为伪标签，其准确率高于基础CTC的输出。
  - **无标签数据上的CR**：即使在无标签数据上，也计算CR损失，这有助于模型学习更稳定的表示，对噪声伪标签具有鲁棒性。
  - 半监督目标函数结合了有标签数据和无标签数据上的Align-Consistency损失。

3)  
在LibriSpeech数据集上的两个任务中取得了显著效果提升：
- **完全监督ASR**：在低资源（LS-100）和高资源（LS-960）设置下，相比CR-CTC，词错率（WER）显著降低。例如，在LS-100上，test-clean/other的WER从12.2/26.7降至10.0/22.9。
- **半监督ASR**：在自训练中利用无标签数据（LS-960或LibriLight），取得了大幅WER下降。例如，从LS-100初始模型出发，增加960小时无标签数据后，WER从10.0/22.9降至4.3/9.6；再增加6000小时数据，进一步降至3.8/9.1。结果表明该方法数据效率高，且性能优于或媲美使用更多无标签数据的基线模型。
</div>

</details>

---

## A Directional-Derivative-Constrained Method for Continuously Steerable Differential Beamformers with Uniform Circular Arrays
- **Authors**: Tiantian Xiong, Yongyi Deng, Kunlong Zhao, Jilu Jin, Xueqin Luo, Gongping Huang, Jingdong Chen, Jacob Benesty
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23119v1](https://arxiv.org/abs/2602.23119v1)
- **PDF**: [https://arxiv.org/pdf/2602.23119v1](https://arxiv.org/pdf/2602.23119v1)

差分麦克风阵列凭借其高空间指向性与紧凑的阵列结构，为远场声信号采集提供了极具前景的解决方案。其核心挑战在于设计能够连续转向、并能增强来自任意方向目标信号的差分波束形成器。本文针对圆形阵列的差分波束形成器设计展开研究，提出了一种融合方向导数约束的新框架。通过将波束方向图在期望转向方向的一阶导数约束为零，并为高阶导数赋予适当数值，确保波束形成器在目标方向获得最大响应，并提供充分的波束转向能力。该方法不仅提升了转向灵活性，还实现了更直观、更稳健的波束方向图设计。仿真结果表明，所提方法能够生成连续可调的波束方向图。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：差分麦克风阵列因其高指向性和紧凑结构，在远场声信号采集中具有优势。核心挑战在于设计能连续转向、增强任意方向目标信号的差分波束形成器。
- **既有方法问题**：
  - 传统线性阵列转向能力有限，通常仅在端射方向性能最优。
  - 针对均匀圆形阵列的现有方法（如对称约束法）通常只能转向至有限固定方向，缺乏灵活性。
  - 级数展开法虽能全向转向，但需已知完整的期望指向性图，实际中往往不可得。
  - 基于对称零点约束的方法虽能实现全向转向，但缺乏理论严谨性和直观可解释性。

2)  
论文提出了一种基于**方向导数约束**的新框架，以解决传统方法在转向灵活性、理论严谨性和设计直观性方面的不足。其核心方法如下：

- **核心思想**：为确保波束形成器在期望的转向方向 $\theta_s$ 达到最大响应（即主瓣准确对准），直接对波束图在该方向的方向导数施加约束。
- **关键约束**：
  - **一阶导数约束**：强制波束图在 $\theta_s$ 处的一阶方向导数为零（$\partial B[h(\omega), \theta] / \partial \theta |_{\theta=\theta_s} = 0$）。这确保了 $\theta_s$ 是一个驻点（极值点）。
  - **高阶导数赋值**：对高阶（特别是偶数阶）导数赋予合适的值（如负值），以控制波束形状（如主瓣宽度）。
  - **无失真约束**：同时保留传统的无失真约束（$h^H(\omega)d_{\theta_s}(\omega) = 1$），确保目标信号无失真通过。
  - **零点约束**：根据差分波束形成器的阶数 $N$，在指定方向施加 $N$ 个零点约束以抑制干扰。

- **方法实现**：
  - 将无失真约束、零点约束以及各阶方向导数约束组合成一个线性方程组：$D_C(\omega) h(\omega) = i_\beta$。
  - 通过求解该方程组（例如以最大化白噪声增益为目标）得到波束形成器权重向量 $h(\omega)$。
- **方法优势**：
  - **确保准确转向**：一阶导数为零与无失真约束共同保证了主瓣最大值出现在期望方向，解决了传统零点约束法可能出现的最大响应方向偏离问题。
  - **实现连续转向**：通过理论推导的导数约束，波束可以连续、准确地转向任意方位角。
  - **更具直观性**：导数约束直接对应于波束图的数学特性（极值、曲率），使设计过程更直观、可解释。
  - **理论严谨**：提供了明确的理论依据，弥补了对称零点约束法在理论清晰度上的不足。

3)  
- **任务**：在均匀圆形阵列上设计一阶和二阶连续可转向的差分波束形成器。
- **效果**：
  - **准确转向**：仿真表明，该方法生成的主瓣能准确对准任意指定的期望方向（如20°, 50°, 120°, 240°），并在预定位置形成深零点。
  - **性能均衡**：与现有方法（DMA-Null, DMA-SymNull, DMA-SeriesExp）相比，所提方法（DMA-DerivCon）在整个频带内保持了更平滑且较高的指向性因子，同时白噪声增益与其他方法相当。
  - **验证有效性**：该方法成功应用于一阶和二阶波束形成器设计，验证了其对于实现准确零点控制和连续波束转向的有效性与鲁棒性。
</div>

</details>

---

## Make It Hard to Hear, Easy to Learn: Long-Form Bengali ASR and Speaker Diarization via Extreme Augmentation and Perfect Alignment
- **Authors**: Sanjid Hasan, Risalat Labib, A H M Fuad, Bayazid Hasan
- **Categories**: cs.SD, cs.AI, cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23070v1](https://arxiv.org/abs/2602.23070v1)
- **PDF**: [https://arxiv.org/pdf/2602.23070v1](https://arxiv.org/pdf/2602.23070v1)

尽管孟加拉语自动语音识别（ASR）已取得显著进展，但长时音频处理与鲁棒的说话人日志化仍是关键的研究空白。为缓解该语言在联合ASR与日志化任务上资源严重匮乏的问题，我们推出了Lipi-Ghor-882——一个包含882小时的多说话人孟加拉语数据集。本文详细介绍了我们在DL Sprint 4.0竞赛中的提交方案，系统评估了多种适用于长时孟加拉语语音处理的架构与方法。在ASR方面，我们发现原始数据扩增效果有限；相反，采用精准对齐的标注数据配合合成声学退化（噪声与混响）进行针对性微调，成为最有效的单一策略。而在说话人日志化任务中，我们发现当前主流的开源模型（如Diarizen）在这一复杂数据集上表现显著不佳。大量模型重训练带来的改进微乎其微；通过基于启发式规则对基线模型输出进行策略性后处理，反而成为提升精度的主要驱动力。最终，本研究构建了一套高度优化的双通道处理流程，实现了约0.019的实时因子，为低资源长时语音处理建立了具有实证支撑的实用基准。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语自动语音识别（ASR）虽已取得进展，但处理长音频和鲁棒的说话人日志（SD）仍是关键挑战。  
- **既有问题**：  
  - 缺乏大规模、时间对齐的多说话人对话数据集，导致联合ASR与SD资源严重不足。  
  - 现有预训练模型在长音频上存在速度与准确性的权衡：快速模型（如Moonshine）准确率低，高精度模型（如Whisper）推理耗时过长。  
  - 说话人日志任务中，开源SOTA模型（如Diarizen）在该数据集上表现意外不佳，且模型重训练收效甚微。

2)  
- **ASR解决方案**：  
  - **推理优化**：采用`faster-whisper`与`CTranslate2`格式转换，结合双T4 GPU并行处理与Silero VAD分块，将Whisper-Medium推理时间从4小时降至26分钟（RTF≈0.019）。  
  - **训练策略**：放弃传统数据扩增与模型集成，转向**极端数据增强**：在小规模完美对齐的数据子集上，对20%音频添加合成噪声与混响，迫使模型学习深层语音特征而非声学记忆，显著提升准确率。  
  - **失败尝试**：参数高效微调（PEFT）、上下文偏置、音频去噪（Demucs）及ROVER集成均未带来稳定增益或引入不可接受的开销。  

- **说话人日志解决方案**：  
  - **模型选择**：放弃重训练SOTA模型，采用Pyannote Community-1作为基础模型。  
  - **后处理算法**：设计启发式后处理流程，核心包括：  
    - 强制说话人间隔（≥0.17秒）并消除重叠；  
    - 合并同一说话人的微片段（间隔<3.79秒）；  
    - 过滤短片段（<0.75秒）与总时长过短的说话人（<9.0秒）。  
  - **关键发现**：对于该任务，算法后处理比模型架构改进或数据增强更有效。

3)  
- **ASR任务**：在22小时测试集上，优化后的Whisper-Medium模型取得最佳词错误率（WER）：公开集0.30842，私有集0.31070，同时保持RTF≈0.019的高效推理。  
- **说话人日志任务**：通过Pyannote Community-1结合严格后处理，在公开集上取得0.24522的DER，私有集上为0.26640，优于其他测试模型（如Diarizen）。  
- **整体贡献**：构建了882小时孟加拉语多说话人数据集Lipi-Ghor-882，并为低资源长音频处理建立了高效双管道基准。
</div>

</details>

---

## TADA: A Generative Framework for Speech Modeling via Text-Acoustic Dual Alignment
- **Authors**: Trung Dang, Sharath Rao, Ananya Gupta, Christopher Gagne, Panagiotis Tzirakis, Alice Baird, Jakub Piotr Cłapa, Peter Chin, Alan Cowen
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.23068v1](https://arxiv.org/abs/2602.23068v1)
- **PDF**: [https://arxiv.org/pdf/2602.23068v1](https://arxiv.org/pdf/2602.23068v1)

现代文本转语音系统日益采用大语言模型架构，以实现可扩展、高保真、零样本的语音生成。然而，这些系统通常依赖固定帧率的声学标记化方案，导致生成的语音序列长度远超对应文本且存在时序异步问题。这不仅降低了计算效率，还容易引发TTS系统的内容幻觉，并加剧口语建模中的模态差异。本文提出一种新颖的标记化方案，可在连续声学特征与文本标记间建立一对一同步关系，从而在大语言模型中实现统一的单流建模。实验表明，这种同步标记既能保持高质量音频重建，又能在潜空间内通过配备流匹配头的大语言模型进行有效建模。此外，该方案支持在上下文环境中无缝切换语音模态，实现纯文本引导——通过融合纯文本模式与文本-语音模式的逻辑值，灵活弥合纯文本大语言模型的智能差距。实验结果显示，本方法在显著降低推理成本的同时，取得了与前沿TTS及口语建模系统相当的性能，几乎完全消除了内容幻觉并保持了语言完整性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代基于大语言模型的TTS系统通常依赖固定帧率的声学分词，导致语音序列远长于文本序列且异步。  
- **既有问题**：  
  - **计算低效**：序列长度差异导致Transformer上下文窗口需求剧增，计算复杂度呈二次增长，降低训练和推理效率。  
  - **模态鸿沟**：文本与语音的异步表示加剧了语音语言建模中的模态差距，并常引发内容幻觉（如跳词、插入无关内容）。  
  - **结构复杂**：现有方法需通过语义分词、层次化建模或交错策略处理多模态，无法实现真正的单流统一建模。

2)  
论文提出 **TADA** 框架，通过 **文本-声学双对齐** 解决上述问题：  
- **同步分词方案**：  
  - 使用基于CTC的**对齐器**（Wav2Vec2架构）建立文本分词与音频帧的**一对一单调映射**，将声学特征压缩为与文本分词对齐的潜在向量。  
  - **编码器-解码器架构**（基于VAE）：编码器通过局部注意力机制将变长音频段聚合为同步潜在向量；解码器根据对齐位置重建高保真音频。  
  - 该方案将语音建模帧率降至2–3 fps，大幅缩短序列长度。  

- **统一自回归建模**：  
  - 在LLM中，将文本分词与对齐的声学特征（通过K位置偏移）融合为**单流序列**，进行联合自回归预测。  
  - 使用**流匹配头**预测声学嵌入及前后空白帧数，实现端到端的语音生成。  
  - 单流设计最大化音频上下文信息密度，相同token预算下可处理音频时长提升近10倍。  

- **模态鸿沟缓解**：  
  - **语音自由引导**：通过混合纯文本模式与文本-语音模式的logits，调节超参数λSFG，使语言性能接近纯文本推理，同时保留利用语音上下文的能力。  
  - **在线拒绝采样**：基于说话人嵌入的一致性检测，实时拒绝低质量生成样本，提升稳定性。  

- **关键优势**：  
  - 消除内容幻觉（实验显示零幻觉样本），保持语言完整性。  
  - 大幅降低推理成本（RTF仅0.09），支持长序列高效生成。

3)  
- **语音克隆任务**：  
  - 在SeedTTS-Eval和LibriTTSR-Eval数据集上，CER与SIM指标与先进系统（如XTTS-v2、Index-TTS2）相当，且**实现零内容幻觉**。  
  - 长表达语音生成（EARS数据集）中，结合文本自由引导与在线拒绝采样，说话人相似度接近最优模型。  

- **语音语言建模任务**：  
  - 在Seamless Interaction数据集上，困惑度接近或优于同规模纯文本模型。  
  - 在Spoken StoryCloze任务中，应用SFG后准确率接近纯文本模式，部分任务甚至超越。  

- **效率表现**：  
  - 推理速度显著提升（RTF降低至0.09），内存占用随输出时长增长更具扩展性。
</div>

</details>

---

## Scattering Transform for Auditory Attention Decoding
- **Authors**: René Pallenberg, Fabrice Katzberg, Alfred Mertins, Marco Maass
- **Categories**: eess.SP, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.23003v1](https://arxiv.org/abs/2602.23003v1)
- **PDF**: [https://arxiv.org/pdf/2602.23003v1](https://arxiv.org/pdf/2602.23003v1)

随着人口结构变化，助听器的使用将在未来几年持续增长。新一代助听器仍需解决的一个开放问题是鸡尾酒会问题。基于脑电图的听觉注意解码是一种可能的解决方案，近年来已成为多项研究的主题，这些研究大多采用相同的预处理方法。本研究为寻求性能突破，提出使用散射变换作为这些预处理方法的替代方案。我们将双层散射变换与常规滤波器组、同步压缩短时傅里叶变换及常用预处理方法进行了对比。为验证性能，在鲁汶大学（KUL）和丹麦技术大学（DTU）提供的两个常用数据集上，针对不同分类任务比较了传统预处理方法与所提出的方法。分类模型涵盖了成熟的神经网络模型（如CNN、LSTM）以及新兴的基于Transformer/图结构的模型。通过多种评估策略的对比，重点考察了对训练集未出现说话人的分类任务。结果表明，双层散射变换在受试者相关条件下能显著提升性能，尤其在KUL数据集上表现突出。然而在DTU数据集上，该优势仅体现在部分模型中，或是在提供更多训练数据（如10折交叉验证）的情况下。这证明散射变换能够提取额外的有效信息。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于脑电图的听觉注意解码是解决助听设备“鸡尾酒会问题”的关键技术。现有方法大多依赖相同的预处理流程。
- **既有方法问题**：
  - 传统预处理通过滤波器组提取音频包络，压缩了信号，丢弃了可能具有诊断价值的层次化幅度调制和跨频交互等时频结构信息。
  - 这种表示在短决策窗口下（实时助听所需）尤其脆弱，性能受限。

2)  
- **核心方法**：论文提出使用**散射变换**作为传统预处理方法的替代方案。
- **解决思路与机制**：
  - **提取高阶调制信息**：散射变换通过级联复小波卷积、模运算和低通平均，构建**局部平移不变且对微小时间形变稳定**的表示。其第二层系数能捕获“调制的调制”，显式编码嵌套的包络动态和更精细的频谱能量分布，这弥补了传统方法丢失高阶时频结构的缺陷。
  - **增强短窗口鲁棒性**：散射变换的Lipschitz稳定性和能量守恒特性，使其在短时间窗口下比基于相关性的方法更稳健，更符合实时AAD的延迟约束。
  - **引入强归纳偏置**：散射变换使用数学设计的滤波器核，类比于学习到的卷积滤波器但参数更少。这**大幅减少了可学习参数量**，有助于在每被试数据有限的AAD任务中缓解过拟合。
  - **灵活兼容多种模型**：该方法作为预处理步骤，可与多种神经网络架构（如CNN、LSTM、Transformer/图网络）无缝集成，并在不同评估策略下进行验证。

3)  
- **任务与效果**：
  - 在**KU Leuven数据集**上，散射变换在**被试内（subject-wise）** 和**说话人未知（speaker-wise）** 的分类任务中表现优异，相比基线方法带来显著性能提升，例如LSTM-2模型在说话人未知任务中准确率最高达0.88。
  - 在**DTU数据集**上，散射变换的优势仅在部分模型（如GCANet-NoEn、LSTM-X）或提供更多训练数据（如10折交叉验证）时显现，突显了其数据效率。
  - 总体而言，散射变换能提取额外相关信息，尤其在**被试相关条件**下能显著改进性能，但其**跨被试（cross-subject）** 泛化能力仍有待提升。
</div>

</details>

---

## A Holistic Framework for Robust Bangla ASR and Speaker Diarization with Optimized VAD and CTC Alignment
- **Authors**: Zarif Ishmam, Zarif Mahir, Shafnan Wasif, Md. Ishtiak Moin
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.22935v1](https://arxiv.org/abs/2602.22935v1)
- **PDF**: [https://arxiv.org/pdf/2602.22935v1](https://arxiv.org/pdf/2602.22935v1)

尽管孟加拉语是全球使用最广泛的语言之一，但在自然语言处理领域仍属于低资源语言。当前主流的孟加拉语自动语音识别与说话人日志系统在处理超过30-60秒的长音频时面临显著挑战。本文提出一个专为长时孟加拉语内容设计的鲁棒性框架，通过结合预训练模型与针对DL Sprint 4.0竞赛设计的新型优化流程实现突破。该框架采用语音活动检测优化与基于强制词对齐的连接时序分类分割技术，确保长时音频的时间精度与转写完整性。此外，我们通过多种微调技术及数据增强与降噪预处理方法提升系统性能。本研究通过弥合复杂多说话人场景下的性能差距，为现实场景中的长时孟加拉语语音应用提供了可扩展的解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语是全球第七大母语，但在自然语言处理领域被视为低资源语言。主流的自动语音识别和说话人日志系统在处理超过30-60秒的长音频时面临挑战。
- **既有问题**：
  - 现有模型通常在短片段（<30秒）上训练和评估，难以处理播客、司法程序等长音频流。
  - 长音频处理中，标准模型易出现“幻觉”、灾难性遗忘或时间对齐丢失，导致词错误率显著上升。
  - 说话人日志在长音频、多说话人环境中因缺乏标注数据而变得复杂。

2)  
论文提出一个针对长音频孟加拉语内容的专用框架，通过以下核心方法解决上述问题：

- **音频预处理与噪声去除**：采用先进的源分离技术，提升音频质量。
- **动态数据增强**：在训练中应用可变增益调整和动态音频增强，提高模型在不同真实录音环境中的鲁棒性。
- **优化的语音活动检测**：实施精细化的VAD层，在转录前准确识别并过滤非语音段，减少冗余处理。
- **CTC对齐与分割**：
  - 利用基于强制词对齐的CTC分割，提取每个词的精确起止时间戳。
  - 根据时间戳将连续音频智能分割为小于30秒的片段，确保不切断词语边界，同时适配Whisper等模型的输入长度限制。
  - 此方法保持了语音的上下文连续性，解决了长序列导致的模型性能下降问题。
- **集成化说话人日志**：
  - 设计了一个三阶段课程学习管道，逐步优化说话人日志模型。
  - **阶段一（基础适应）**：在原始含噪音频上微调基线模型，使其适应孟加拉语的语音特征和重叠对话模式。
  - **阶段二（纯净精炼）**：使用Demucs进行人声分离，在纯净人声音频上继续微调，使模型专注于边界检测和说话人区分。
  - **阶段三（鲁棒性与增强）**：对纯净音频施加动态增益扰动等数据增强，防止模型对“过于干净”的音频过拟合，提升在多变环境下的泛化能力。

该框架通过精细化的分割对齐和渐进式的模型优化，共同解决了长音频处理中的转录准确性下降和说话人日志复杂性问题。

3)  
- **自动语音识别任务**：在孟加拉语公共和私有测试集上评估。微调后的Tugstugi模型取得了最佳效果，公共测试集词错误率为21.99%，私有测试集为23.59%，显著优于其他零样本或微调的基线模型。
- **说话人日志任务**：评估了不同训练策略的日志错误率。结合Demucs精炼和数据增强的策略在公共测试集上取得了最低的DER（21.46%）。该方法有效提升了模型在复杂多说话人环境中的性能。
</div>

</details>

---

## Same Words, Different Judgments: Modality Effects on Preference Alignment
- **Authors**: Aaron Broukhim, Nadir Weibel, Eshin Jolly
- **Categories**: cs.SD, cs.AI, cs.HC
- **arXiv**: [https://arxiv.org/abs/2602.22710v1](https://arxiv.org/abs/2602.22710v1)
- **PDF**: [https://arxiv.org/pdf/2602.22710v1](https://arxiv.org/pdf/2602.22710v1)

基于偏好的强化学习（PbRL）是使人工智能系统与人类偏好对齐的主流框架，但其在语音领域的应用仍待深入探索。本研究通过一项受控跨模态实验，对比了人类与合成偏好标注在文本和音频两种模态下对相同语义内容（涵盖100个提示）的评估结果。实验表明，音频偏好标注的可靠性不亚于文本，当评分者数量达到约9人时，评分者间一致性达到良好水平（ICC(2,k) ≈ 0.80）——这是偏好标注文献中首次基于ICC对任一模态进行的可靠性量化。然而，模态显著影响了人们的评判方式：音频评分者表现出更严格的决策阈值、更弱的长度偏见，且更倾向于以用户为中心的评估标准，导致跨模态一致性接近随机水平。合成评分结果进一步与人类判断保持一致，并能预测评分者间一致性，这支持了其在实际应用中的双重作用：既可用于筛选模糊样本对，也可作为人类标注的完整替代方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于偏好的强化学习是AI对齐的主流方法，但现有研究集中于文本模态，对语音模态的偏好对齐探索不足。  
- **既有方法问题**：  
  - 文本偏好标注存在已知偏差，如长度偏差（偏好更长回复）和位置偏差。  
  - 文本标注过程无法控制注意力分配和阅读顺序等变量。  
  - 语音偏好研究常依赖文本转语音生成的音频，并假设文本偏好可直接迁移至语音，但跨模态偏好的一致性未知。

2)  
- **核心方法**：本研究通过一项受控的跨模态实验，直接比较人类与合成标注在文本和音频模态下对相同语义内容的偏好判断。  
- **解决上述问题的具体方式**：  
  - **评估音频偏好可靠性**：首次在偏好标注文献中使用组内相关系数量化可靠性，发现约9名标注者即可达到良好一致性，证明音频与文本偏好数据在聚合层面同样可靠。  
  - **揭示模态特异性影响**：  
    - 发现音频标注者的决策阈值更窄、对长度偏差更不敏感，且其评价理由更侧重于用户需求（而非文本标注者关注的内容细节）。  
    - 跨模态偏好一致性接近随机水平，且偏好差异因提示词而异，表明文本偏好不能直接替代音频偏好。  
  - **利用合成评分辅助标注**：  
    - 合成评分与人类判断高度一致，并能预测人类标注者间的一致性。  
    - 合成评分可用于筛选模糊样本或完全替代部分人工标注，从而在不牺牲质量的前提下降低标注成本。

3)  
- **任务与效果**：  
  - 在**语音与文本模态的偏好标注任务**中，量化了达到可靠标注所需的最小标注者数量（约9人），为实际数据收集提供了依据。  
  - 在**跨模态偏好对齐分析**中，揭示了模态对判断标准的根本性影响，证明直接使用文本偏好数据训练语音对齐模型存在风险。  
  - 在**合成评分应用**上，验证了其可用于高效筛选样本或替代人工，为降低对齐成本提供了可行方案。
</div>

</details>

---

## Deepfake Word Detection by Next-token Prediction using Fine-tuned Whisper
- **Authors**: Hoan My Tran, Xin Wang, Wanying Ge, Xuechen Liu, Junichi Yamagishi
- **Categories**: eess.AS, cs.CL
- **arXiv**: [https://arxiv.org/abs/2602.22658v1](https://arxiv.org/abs/2602.22658v1)
- **PDF**: [https://arxiv.org/pdf/2602.22658v1](https://arxiv.org/pdf/2602.22658v1)

深度伪造语音可通过将真实语音中的部分词语替换为由语音生成模型合成的语义不同词语来伪造。本研究探索一种高效方法：通过微调预训练的Whisper模型，使其在通过下一词预测进行语音转写的同时检测合成词语。我们进一步研究使用部分声码化语音作为微调数据，以降低数据收集成本。实验表明，在领域内测试数据上，微调后的Whisper模型在合成词检测错误率和转写错误率方面均表现优异。在采用未见过的语音生成模型合成词语的领域外测试数据上，该模型与基于ResNet的专用检测模型性能相当；但整体性能下降表明仍需提升其泛化能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：深度伪造语音检测已从整句检测发展到需定位句中伪造部分（如替换个别词）。现有方法通常需专门设计序列分类模型，流程复杂且成本高。
- **既有问题**：开发专用检测器需完整的数据收集、模型设计、训练和调优流程，增加计算和存储开销。缺乏高效、低成本的方法来同时进行语音识别和伪造词定位。

2)  
- **核心思路**：通过微调预训练的Whisper模型，使其在语音转文本（ASR）的同时，通过下一词预测（next-token prediction）检测合成词。该方法将检测任务无缝集成到现有ASR流程中，无需改变模型架构或训练算法。
- **具体方法**：
  - **数据标注**：在训练数据的文本标注中，为每个合成词前后插入特殊标记（如`<TOF>`和`<EOF>`），将合成词检测转化为预测这些标记的任务。
  - **微调过程**：使用包含标记的文本序列对Whisper进行微调，模型学习在转录时预测标记位置，从而识别合成词。
  - **数据成本优化**：为降低真实合成数据收集成本，采用声码器（vocoder）对部分真实语音进行重合成，模拟合成词，构建训练数据。
- **优势**：避免了开发独立检测器的成本，仅需微调现有ASR模型，且保持了原有的转录准确性。

3)  
- **任务与效果**：
  - **领域内测试**：在训练与测试数据域匹配时（如均来自有声书），微调后的Whisper在合成词检测上错误率（FAR/FRR）与专用ResNet模型相当，同时转录词错误率（WER）显著降低。
  - **跨领域测试**：当测试数据来自不同领域（如YouTube或录音室）或使用未见过的合成模型时，检测性能出现下降，与ResNet模型表现相近但均未达到可靠水平。
  - **数据效率**：使用声码器模拟的合成词进行训练，在领域内测试中表现良好，证明了该数据生成方法的有效性。
</div>

</details>

---

## Relating the Neural Representations of Vocalized, Mimed, and Imagined Speech
- **Authors**: Maryam Maghsoudi, Rupesh Chillale, Shihab A. Shamma
- **Categories**: cs.SD, eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2602.22597v1](https://arxiv.org/abs/2602.22597v1)
- **PDF**: [https://arxiv.org/pdf/2602.22597v1](https://arxiv.org/pdf/2602.22597v1)

本研究利用公开的立体定向脑电图数据，探究了发声、默读及想象语音的神经表征间的关系。以往研究多集中于单独解码各条件下的语音响应，而本文则通过为每种条件训练线性频谱图重建模型，并评估其在跨条件下的泛化能力，以探索不同条件间响应的关联性。实验表明，基于单一条件训练的线性解码器通常能成功迁移至其他条件，这暗示了语音表征的共享性。通过基于排序的分析方法，我们在刺激层面评估了这种共性，证明在条件内及跨条件下，刺激特异性结构均得以保持。最后，我们对比了线性模型与非线性神经网络的重建效果：两者均表现出跨条件迁移能力，但线性模型在刺激层面的区分度上表现更优。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：听觉神经科学与脑机接口（BCI）致力于从大脑活动中解码语音特征。以往研究多集中于**单独解码**听语音或发声语音的神经响应，而对**无声发音**（mimed）和**想象发音**（imagined）这两种内部语音过程的神经表征关注较少。  
- **既有方法的问题**：多数研究仅在**单一条件**（如仅发声、仅想象）下训练和评估解码器，未能系统探究不同语音产生模式（发声、无声、想象）之间神经表征的**相似性与可迁移性**。这限制了对内部语音处理层次的理解，也影响了BCI在无法发声人群中的应用潜力。

2)  
- **核心方法**：本研究利用公开的立体脑电图（sEEG）数据集，包含同一被试在**发声、无声和想象**三种条件下重复说相同句子的记录。核心分析包括：  
  - **线性解码器训练**：为每个条件分别训练线性模型，将时间延迟的sEEG信号映射到语音谱图。  
  - **跨条件泛化评估**：将在一个条件下训练的解码器直接应用于另外两个条件的测试数据，通过计算重建谱图与目标谱图的**线性相关性**来评估性能。  
  - **句子级可区分性分析**：采用**排序分析**，计算重建信号与所有候选目标信号的相关性，通过“前k匹配”曲线和曲线下面积（AUC）量化**句子特异性信息**的保留程度。  
  - **非线性模型对比**：使用基于卷积和循环层的神经网络进行相同分析，比较线性与非线性架构在跨条件迁移和句子区分能力上的差异。  
- **如何解决问题**：  
  - 通过**跨条件评估**，直接检验解码器在不同语音模式间的泛化能力，从而推断神经表征的共享程度。  
  - 提出**分层表征假设**：假设神经活动可分解为规划（XP）、发音（XA）和感觉反馈（XS）成分。想象语音主要包含XP，无声语音包含XP+XA，发声语音包含XP+XA+XS。线性解码器的跨条件性能可通过这些成分在解码子空间上的投影来解释。  
  - 结合**相关性重建**与**排序分析**，不仅评估整体重建质量，还量化了刺激特异性结构的保留情况，弥补了单一指标不足。  
  - 通过对比线性和非线性模型，揭示了**线性模型在保持句子特异性结构方面的优势**，尽管非线性模型重建相关性更高。

3)  
- **任务与效果**：  
  - **跨条件语音谱图重建**：线性解码器在跨条件测试中均显著优于随机模型（p≪0.001），表明三种语音模式共享神经表征。具体而言：  
    - 用无声语音训练的解码器在**发声和无声数据**上表现相似，优于想象数据。  
    - 用想象语音训练的解码器在**发声数据**上表现最佳，在无声和想象数据上表现相当。  
  - **句子级区分任务**：排序分析显示，所有跨条件测试的AUC均高于随机水平，证实重建保留了句子特异性信息。**发声数据的区分度最高**（AUC=0.32），且线性模型在相关性相近时比非线性模型具有**更好的句子区分能力**。  
  - **模型对比**：非线性模型在重建相关性上更高，但线性模型在**刺激特异性结构保留**上更优，强调了可解释性的重要性。
</div>

</details>

---

## Efficient Dialect-Aware Modeling and Conditioning for Low-Resource Taiwanese Hakka Speech Processing
- **Authors**: An-Ci Peng, Kuan-Tang Huang, Tien-Hong Lo, Hung-Shin Lee, Hsin-Min Wang, Berlin Chen
- **Categories**: cs.CL, cs.AI, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.22522v1](https://arxiv.org/abs/2602.22522v1)
- **PDF**: [https://arxiv.org/pdf/2602.22522v1](https://arxiv.org/pdf/2602.22522v1)

台湾客家语作为一种资源匮乏且濒危的语言，在自动语音识别（ASR）领域面临显著挑战，包括较高的方言变异性以及存在两种不同的书写系统（汉字与拼音）。传统ASR模型在此类场景中常遇困难，因其倾向于将核心语言内容与方言在音系和词汇层面的特异性变化混为一谈。为应对这些挑战，我们提出一个基于循环神经网络传感器（RNN-T）的统一框架。该框架的核心是引入方言感知建模策略，旨在将方言“风格”与语言“内容”解耦，从而增强模型学习鲁棒且泛化性表征的能力。此外，框架采用参数高效的预测网络，以同步建模汉字与拼音的ASR任务。实验表明，这些任务间形成了强大的协同效应，跨书写目标可作为相互正则化器以提升主要ASR任务的性能。在HAT语料库上的实验显示，我们的模型在汉字与拼音ASR任务上分别实现了57.00%与40.41%的相对错误率降低。据我们所知，这是首次系统探究客家方言变异性对ASR的影响，也是首个能够联合处理这些任务的单一模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
研究背景与既有方法的问题：
- **背景**：台湾客家语是一种低资源、濒危语言，具有显著的方言变异性，并存在汉字和拼音两种书写系统。这为自动语音识别带来了巨大挑战。
- **既有方法的问题**：
  - 传统ASR模型在低资源、多方言场景下，其共享编码器倾向于将核心语言内容与方言特有的声学和词汇变异**混淆**，导致学习效率低下。
  - 现有处理方言变异的方法（如为每个方言训练独立的预测网络，或在解码器中加入方言ID）通常针对高资源语言设计，在数据稀缺时难以训练出鲁棒的专用组件，且大多忽略了编码器内部的表征混淆问题。

2)  
论文核心方法通过一个基于RNN-T的统一框架，从两个层面系统性地解决了上述问题：

- **方言感知建模**：旨在编码器层面将方言“风格”与语言“内容”解耦。
  - **辅助方言分类器**：在编码器输出上添加一个分类头，通过辅助损失函数迫使编码器学习能区分方言的声学特征。
  - **方言信息集成**：将方言标签映射为连续嵌入向量，并与编码器的声学表征拼接，使联合网络能根据特定方言调整其预测。

- **方言条件化**：在解码器（预测网络）层面，通过修改目标序列，将方言ID作为条件信号。
  - 提出了三种策略，其中**词间交错条件化**效果最佳。该方法在输出序列的每个语言标记后都插入方言ID，为解码器的每一步预测提供持续、密集的方言上下文引导，使其能动态适应不同方言的词汇选择和语音-字符映射。

- **多任务学习框架**：使用一个共享编码器和参数高效的预测网络，同时处理汉字和拼音两种ASR任务。
  - 拼音任务强调声学精确性，汉字任务强调高层语义上下文。两者形成互补目标，共同正则化共享编码器，使其学习到既声学精确又具备语言上下文感知的鲁棒表征。

3)  
在以下任务上取得的效果：
- **任务**：在台湾客家语（涵盖海陆、四县、南四县三种方言）的自动语音识别任务上，分别评估汉字和拼音的识别性能。
- **效果**：
  - 与单任务基线相比，最终模型（集成ADC、DII和TIC策略）在**汉字识别**上实现了**57.00%** 的相对错误率降低（字错误率）。
  - 在**拼音识别**上实现了**40.41%** 的相对错误率降低（音节错误率）。
  - 该模型是首个能联合处理客家语多书写系统、并系统应对方言变异的单一模型。
</div>

</details>

---
