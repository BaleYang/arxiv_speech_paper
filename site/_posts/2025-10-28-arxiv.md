---
layout: post
title: "arXiv Daily – 2025-10-28"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-28（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-27 08:50 — 2025-10-28 08:50
- 抓取总数：12 篇 | 本页显示：12 篇（去重/过滤后）

## ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models
- **Authors**: Bohan Li, Wenbin Huang, Yuhang Qiu, Yiwei Guo, Hankun Wang, Zhihan Li, Jing Peng, Ziyang Ma, Xie Chen, Kai Yu
- **Categories**: cs.SD, cs.CL, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.23558v1](http://arxiv.org/abs/2510.23558v1)
- **PDF**: [http://arxiv.org/pdf/2510.23558v1](http://arxiv.org/pdf/2510.23558v1)

大型音频语言模型通过融合声学感知与大语言模型能力，实现了对音频多维度信息的提取与理解，已引发学界与工业界的广泛关注。然而现有模型对指令表述方式表现出高度敏感性，这既影响指令遵循率，也制约任务性能表现。目前尚缺乏系统性评估此类敏感度的基准工具。本研究提出ISA-Bench动态评估基准，从指令描述、输出格式与任务组合三个维度量化LALMs的指令敏感性。通过对近期开源与商用LALMs的测评，我们在受控指令变体下系统评估了模型的指令遵循度与任务准确率。实验表明，即便是最先进的LALMs仍存在显著的指令敏感现象，导致基础音频理解任务性能下降。为缓解该问题，我们在精心构建的复杂指令变体数据集上对Qwen2-Audio进行微调，显著提升了模型指令遵循能力。但此举也引发不可忽视的灾难性遗忘现象：模型在适应新指令风格时部分原有任务能力出现退化。本基准为评估和改进LALMs指令敏感性提供了标准化依据，凸显了实际应用场景中对指令鲁棒性音频理解能力的迫切需求。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：大型音频语言模型（LALMs）结合音频感知与语言模型，用于通用音频理解任务。  
- **既有问题**：现有LALMs对指令表述高度敏感，影响指令遵循率和任务性能；当前基准（如Speech-IFEval）仅评估单一维度，缺乏对性能鲁棒性和复合任务的系统性评测。  

2)  
- **核心方法**：提出ISA-Bench动态基准，从三个维度评估指令敏感性：  
  - **D维度**：指令文本描述（如标点、语义复杂度、语法错误）。  
  - **F维度**：输出格式要求（如大小写、前缀后缀、JSON格式）。  
  - **N维度**：复合任务数量（如组合ASR、情感识别等子任务）。  
- **评估策略**：  
  - 使用合规感知指标（如MetricIF），仅对格式合规输出计算任务性能。  
  - 通过LLM生成多样化指令变体，覆盖真实部署中的未知指令。  
- **缓解实验**：对Qwen2-Audio进行监督微调（SFT），使用复杂指令变体数据提升指令遵循能力，但发现可能引发灾难性遗忘。  

3)  
- **评测任务**：涵盖5项原子任务（ASR、语音翻译、情感识别、性别识别、音频描述）和复合任务（组合2-3项子任务）。  
- **效果**：  
  - 主流LALMs（如Gemini-2.5-Pro、DeSTA2.5-Audio）在部分任务中表现良好，但均存在显著指令敏感性，尤其在JSON格式和复合任务中性能下降。  
  - SFT可提升指令遵循率（如F维度提升56%），但可能导致原子任务能力退化。
</div>

</details>

---

## SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity
- **Authors**: Hanke Xie, Haopeng Lin, Wenxiao Cao, Dake Guo, Wenjie Tian, Jun Wu, Hanlin Wen, Ruixuan Shang, Hongmei Liu, Zhiqi Jiang, Yuepeng Jiang, Wenxi Chen, Ruiqi Yan, Jiale Qian, Yichao Yan, Shunshun Yin, Ming Tao, Xie Chen, Lei Xie, Xinsheng Wang
- **Categories**: eess.AS, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.23541v1](http://arxiv.org/abs/2510.23541v1)
- **PDF**: [http://arxiv.org/pdf/2510.23541v1](http://arxiv.org/pdf/2510.23541v1)

尽管文本转语音合成技术近期在语音表现力与自然度方面取得显著进展，但现有系统多局限于单人语音生成，难以实现连贯的多说话人对话语音。本技术报告提出SoulX-Podcast系统，专为播客风格的多轮次多说话人对话语音生成而设计，同时在传统文本转语音任务中达到最优性能。

为满足多轮对话对自然度的更高要求，该系统集成多种副语言控制功能，支持普通话、英语及四川话、河南话、粤语等汉语方言，实现更具个性化的播客风格语音生成。实验表明，SoulX-Podcast可连续生成超过90分钟对话，保持稳定的音色特征与平滑的说话人切换。各说话人还能根据对话语境自适应调整韵律，呈现符合对话进程的自然节奏与语调变化。在多项评估指标中，该系统在独白式文本转语音与多轮对话语音合成任务中均达到当前最优性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现有文本转语音系统主要针对单说话人场景，在多说话人多轮对话中难以保持流畅性与自然度。  
- **既有问题**：  
  - 传统方法缺乏对副语言特征（如笑声、叹息）和方言多样性的支持；  
  - 多轮对话中音色稳定性与韵律适应性不足；  
  - 长时对话生成易出现连贯性断裂。  

2)  
- **核心方法**：SoulX-Podcast 基于大语言模型构建两阶段生成框架，通过文本-语音交错序列建模多轮对话。  
- **关键技术**：  
  - **多模态令牌组织**：在序列中嵌入说话人标签、方言标识及副语言标记，实现细粒度控制；  
  - **课程学习策略**：先训练单语料与对话数据基础能力，再通过方言数据微调优化多样性；  
  - **上下文正则化**：逐步丢弃历史语音令牌但保留文本上下文，增强长对话稳定性；  
  - **方言引导推理**：通过添加方言典型例句强化目标方言生成。  
- **创新点**：  
  - 支持普通话、英语及三种汉语方言的跨方言零样本语音克隆；  
  - 融合副语言事件建模，提升对话表现力；  
  - 实现超过90分钟的长时对话生成，保持音色一致与韵律自适应。  

3)  
- **任务与效果**：  
  - **单说话人合成**：在 Seed-TTS-eval 中取得最优中文 CER（1.10）与高说话人相似度（0.743）；  
  - **多轮对话合成**：在 ZipVoice-Dia 基准上中英文均达到最低 CER/WER 与最高跨说话人一致性；  
  - **副语言控制**：对5类副语言事件整体识别准确率达82%，笑声控制近乎完美；  
  - **方言生成**：支持四川话、河南话、粤语，在单语与对话任务中均保持稳定相似度。
</div>

</details>

---

## Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization
- **Authors**: Bernardo Torres, Manuel Moussallam, Gabriel Meseguer-Brocal
- **Categories**: cs.SD, cs.AI, cs.LG, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.23530v1](http://arxiv.org/abs/2510.23530v1)
- **PDF**: [http://arxiv.org/pdf/2510.23530v1](http://arxiv.org/pdf/2510.23530v1)

音频自编码器能够学习有效的压缩音频表征，但其非线性潜在空间阻碍了直观的代数运算（如混合或缩放）。本文提出一种简单的训练方法，通过数据增强在高压缩比的 Consistency 自编码器（CAE）中诱导线性特性，从而在不改变模型架构或损失函数的前提下实现齐次性（对标量增益的等变性）与可加性（解码器保持加法运算）。采用本方法训练后，CAE 的编码器与解码器均呈现线性行为，同时保持重建保真度。我们通过潜在空间算术运算在音乐源合成与分离任务中验证了所学表征的实用价值。本研究提供了一种构建结构化潜在空间的简洁技术，为更直观高效的音频处理开辟了新途径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频自编码器能学习压缩的音频表示，但非线性潜在空间阻碍了直观的代数操作（如混合或缩放）。现有方法需针对特定任务设计适配器或重构模型，缺乏通用性。  
- **既有问题**：高压缩率下，潜在空间结构复杂且纠缠，无法直接支持线性运算，限制了在潜在空间中进行高效音频处理的能力。

2)  
- **核心方法**：通过数据增强实现隐式正则化，诱导一致性自编码器（CAE）的编码器和解码器具备线性特性，无需修改模型结构或损失函数。  
  - **隐式同质性**：训练时对潜在向量施加随机增益，要求解码器从潜在向量幅度推断输出尺度，实现缩放不变性。  
  - **隐式可加性**：构造人工混合音频，用各源潜在向量之和替代混合音频的潜在向量，训练解码器保持加法运算。  
- **训练机制**：基于一致性训练目标，结合增益调度和混合批次生成，使模型在单步解码中同时满足同质性与可加性。

3)  
- **任务与效果**：  
  - **重建质量**：在MusicCaps数据集上，线性化模型（Lin-CAE）与基线模型相当，信噪比（SNR）达3.19，多尺度谱距离（MSS）为1.01。  
  - **线性验证**：同质性误差降至0.69，可加性误差仅0.60，显著优于基线。  
  - **音源分离**：在MUSDB18-HQ上通过潜在算术实现分离，尺度不变信噪比（SI-SDR）提升至1.58（鼓）、1.16（贝斯）等，MSS均低于1.23。
</div>

</details>

---

## Evaluation of Spherical Wavelet Framework in Comparsion with Ambisonics
- **Authors**: Ş. Ekmen, H. Lee
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.23403v1](http://arxiv.org/abs/2510.23403v1)
- **PDF**: [http://arxiv.org/pdf/2510.23403v1](http://arxiv.org/pdf/2510.23403v1)

【中文摘要】
针对球面小波框架（SWF）与高阶Ambisonics的对比研究：SWF近期被提出，旨在通过高度局部化的基函数融合Ambisonics与对象音频（OBA）的优势。该框架可在保持声场稀疏表征的同时扩大最佳听音区、降低定位模糊度，从而提升存储与传输效率。尽管初步向量分析与听感实验已展现潜力，但现有研究局限于特定条件且缺乏感知指标。本研究通过计算IACC（双耳互相关系数）、ITD（双耳时间差）与ILD（双耳声级差），并结合生态效度声源的听感实验，系统比较SWF与Ambisonics在不同重构布局（正多面体、t-design及Lebedev网格）及其对应Ambisonics阶数/通道数下的表现。结果表明：在整体空间感与音色保真度方面，SWF相较于Ambisonics更接近参考信号，但其性能显著依赖于球面细分方案，且无法原生表征连续方向抵达的声波。文末探讨了可能的解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：Ambisonics 是当前主流空间音频技术，通过球谐函数表示声场，但存在以下问题：  
  - 需要大量声道才能实现高质量重放，导致处理复杂且存储传输效率低。  
  - 球谐函数在球面上连续分布，引入声道间串扰，扭曲感知线索并限制甜区范围。  
  - 商业音乐和电影中更常用基于对象的音频（OBA），但其依赖布局、计算成本高且无法完整表示声场。  

- **既有方法局限**：先前对球形小波框架（SWF）的研究仅限于特定条件（如单一扬声器阵列），缺乏基于感知指标（如IACC、ITD、ILD）的系统性评估。

2)  
- **核心方法**：SWF利用局部化基函数（球形小波）替代球谐函数，结合多分辨率分析，实现稀疏声场表示。具体解决方式包括：  
  - **降低声道数**：通过紧凑支持的基函数，用更少的声道实现与高阶Ambisonics相当的重放质量（如6声道SWF性能接近16声道三阶Ambisonics）。  
  - **减少定位模糊**：局部化基函数降低声道间串扰，提升甜区范围，使声源定位更精确。  
  - **灵活处理**：支持频率相关处理，例如在不同频段采用不同分辨率，优化频谱保真度。  
  - **适配不规则布局**：通过改进的小波分解方法（如Narvaez的扩展方案），使SWF适用于非规则扬声器阵列，减少对专用解码器的依赖。  

- **技术改进**：  
  - 采用客观分析（IACC、ITD、ILD、PSD误差计算）和主观听音测试（MUSHRA），全面评估空间和音色保真度。  
  - 针对SWF在非顶点位置的插值问题，提出潜在解决方案（如立方样条插值或更平滑的小波设计），以提升一致性。

3)  
- **评估任务与效果**：  
  - **客观指标**：在50点Lebedev网格上，SWF的IACC、ITD、ILD误差均低于Ambisonics，PSD误差（1.211）表明频谱保真度更优；向量分析显示SWF的能量向量幅度更接近1，扩散度更低（3.567），说明定位更精准、声源宽度更窄。  
  - **主观听音测试**：SWF在空间和音色保真度上均被评级为“略微不同”（中值评分≈4），显著优于Ambisonics的“不同”（评分≈3.4），且在50点Lebedev布局中部分位置达到“相同”（评分=5）。  
  - **局限性**：SWF性能高度依赖球面细分，对非顶点位置需插值，可能导致图像不稳定；无法原生表示连续方向波前，需进一步优化小波设计。
</div>

</details>

---

## LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization
- **Authors**: Máté Gedeon, Péter Mihajlik
- **Categories**: eess.AS, cs.CL, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.23320v1](http://arxiv.org/abs/2510.23320v1)
- **PDF**: [http://arxiv.org/pdf/2510.23320v1](http://arxiv.org/pdf/2510.23320v1)

本文提出LibriConvo——基于说话人感知对话仿真技术构建的多说话人模拟对话数据集，旨在支持说话人日志分割与自动语音识别系统的训练评估。相较于现有主要依赖语义割裂语句与非自然停顿时长的资源，本数据集通过三重技术保障语义连贯性与对话时序真实性：采用CallHome结合外部语音活动检测确定可靠边界，通过压缩算法消除非自然长静音，按书籍组织LibriTTS语句以保持上下文一致性。在声学真实性方面，提出新型房间脉冲响应筛选机制，通过空间合理性排序实现真实性与多样性的平衡。数据集包含1,496段对话（240.1小时）、830位独立说话人，采用说话人分离划分方式以支持鲁棒性评估。基线实验表明：在日志分割任务中sortformer模型优于pyannote流程；采用序列化输出训练微调的Fast Conformer-CTC XLarge模型取得7.29%词错误率，超越零样本Whisper-large-v3。LibriConvo通过真实对话动态与受控实验条件，为多说话人语音处理研究提供了重要资源。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代语音处理系统（如端到端说话人日志和多人语音识别）依赖大量带标注的对话数据，但真实多说话人对话的采集成本高、标注困难。  
- **既有方法问题**：  
  - 早期合成数据常混合语义无关的语音片段，缺乏连贯性。  
  - 时间动态性不足，存在不自然的静默间隙或重叠模式。  
  - 忽略说话人身份一致性及声学环境真实性。  

2)  
- **语义一致性**：基于LibriTTS语料库，按书籍组织语音片段，确保同一对话中文本内容关联。  
- **时间动态优化**：  
  - 采用CallHome语料与外部VAD模型提取可靠时间边界。  
  - 通过时间压缩技术减少不自然长静默，保留短间隙。  
  - 基于核密度估计建模统一间隙/重叠分布，结合马尔可夫链模拟说话人轮换。  
- **声学真实性**：  
  - 设计RIR选择策略，根据空间合理性（如高度、距离、方位角）对说话人-麦克风配置评分，过滤不现实布局。  
  - 保留声学多样性，避免共线性排列。  
- **数据划分**：按说话人互斥原则划分训练/验证/测试集，确保评估公平性。  

3)  
- **说话人日志任务**：  
  - Sortformer模型在测试集上DER为11.1%，显著优于Pyannote（24.4%），显示其在重叠语音和说话人连续性上的优势。  
- **语音识别任务**：  
  - 采用序列化输出训练，微调后的FastConformer-CTC XLarge模型在测试集上达到7.29% WER和6.97% cpWER，优于零样本Whisper-large-v3（7.46% WER）。  
- **整体效果**：LibriConvo为多说话人语音处理提供了高真实性、可复现的基准数据集。
</div>

</details>

---

## Arabic Little STT: Arabic Children Speech Recognition Dataset
- **Authors**: Mouhand Alkadri, Dania Desouki, Khloud Al Jallad
- **Categories**: cs.CL, cs.AI, cs.HC, cs.LG, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.23319v1](http://arxiv.org/abs/2510.23319v1)
- **PDF**: [http://arxiv.org/pdf/2510.23319v1](http://arxiv.org/pdf/2510.23319v1)

人工智能系统的性能从根本上取决于高质量的训练数据。然而，阿拉伯语等低资源语言面临严重的数据稀缺问题，其中儿童专用语音语料库的缺失更构成关键瓶颈。为填补这一空白，本研究推出阿拉伯语Little STT数据集——该语料库采集自课堂环境下的黎凡特方言儿童语音，包含288名6至13岁儿童产生的355条语音样本。我们进一步基于该数据集对前沿语音识别模型Whisper进行系统评估，并将其性能与阿拉伯语成人语音基准进行比较。对八个Whisper变体的测试表明，性能最优的Large_v3模型在儿童语音上的词错误率高达0.66，与其在成人数据集上低于0.20的词错误率形成鲜明对比，该结果与英语儿童语音研究结论一致。本研究凸显了开发专用儿童语音基准及构建包容性训练数据的迫切性，同时强调此类数据必须遵循严格的伦理与隐私保护框架。我们期望这项研究能为开发面向阿拉伯语儿童的公平语音技术奠定基础，并通过公开数据集促进语音识别资源中儿童群体的代表性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：阿拉伯语作为低资源语言，自动语音识别（ASR）领域面临数据稀缺问题，尤其缺乏针对儿童语音的专用语料库。  
- **既有方法问题**：现有大规模ASR数据集（如LibriSpeech、CommonVoice）和先进模型（如Whisper、FastConformer）主要基于成人语音训练，无法有效处理儿童语音的声学与语言特性，导致识别性能显著下降。  

2)  
- **构建专用数据集**：论文创建了首个针对阿拉伯语儿童语音的公开数据集“Arabic Little STT”，聚焦黎凡特方言，包含288名6-13岁儿童的355条课堂录音，覆盖科技相关主题。  
- **数据采集与处理**：  
  - 在真实课堂环境中通过智能手机录制，保留环境噪声以增强实用性。  
  - 对音频进行人工可听性筛选和转写，并由母语者按Whisper非英语规范处理。  
  - 应用阿拉伯语文本标准化（如去除变音符号、统一字符形式），减少拼写变异。  
- **系统性评估方法**：  
  - 选取Whisper系列8个模型（参数量39M至1.5B），覆盖不同计算需求场景。  
  - 采用转录模式，依赖模型内置语言检测，输出经相同标准化处理。  
  - 使用词错误率（WER）和字符错误率（CER）量化性能，并与成人阿拉伯语数据集（如FLEURS）对比。  

3)  
- **任务与效果**：在阿拉伯语儿童语音识别任务中，Whisper所有模型均表现不佳：  
  - 最佳模型（Large-v3）WER达66%，较其在成人数据（WER≤20%）显著上升。  
  - 其他模型（如Tiny、Base）WER超过300%，凸显儿童语音识别的挑战性。  
- **结论意义**：结果证实当前ASR系统对儿童语音的泛化能力不足，强调需开发包容性训练数据以提升公平性。
</div>

</details>

---

## Low-Resource Audio Codec (LRAC): 2025 Challenge Description
- **Authors**: Kamil Wojcicki, Yusuf Ziya Isik, Laura Lechler, Mansur Yesilbursa, Ivana Balić, Wolfgang Mack, Rafał Łaganowski, Guoqing Zhang, Yossi Adi, Minje Kim, Shinji Watanabe
- **Categories**: cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.23312v1](http://arxiv.org/abs/2510.23312v1)
- **PDF**: [http://arxiv.org/pdf/2510.23312v1](http://arxiv.org/pdf/2510.23312v1)

尽管当前神经音频编解码器在超低码率下实现了优于传统方法的语音质量，但其实际应用仍受限于资源受限场景下的运行能力及对声学失真的鲁棒性。边缘部署场景要求编解码器在严格的计算约束下保持低延迟与低码率，而背景噪声与混响的存在进一步要求设计具备抗退化能力。神经编解码器在此类约束下的性能及其与语音增强技术的结合尚未得到充分探索。为推动该领域发展，我们提出2025年低资源音频编解码挑战赛，旨在开发面向资源受限应用的神经与混合编解码器。参赛者将获得标准化训练数据集、两套基线系统及完整评估框架的支持。本挑战赛有望为编解码器设计及相关下游音频任务提供重要参考。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代通信依赖语音编码，传统方法（如Opus、AMR-WB）在低计算和低延迟场景有效，但低比特率下质量显著下降。  
- **既有问题**：  
  - 神经编解码器（如SoundStream、EnCodec）在极低比特率下质量更优，但计算和内存需求高，难以部署于资源受限的边缘设备。  
  - 现有方法缺乏对现实噪声和混响的鲁棒性，且低资源场景下的实时性能基准不足。  

2)  
- **核心方法**：LRAC挑战赛通过多维度约束和任务设计推动解决方案发展：  
  - **联合资源限制**：要求编解码器在计算复杂度（如Track 1接收端≤300 MFLOP/s）、延迟（≤30 ms）和比特率（≤1 kbps或6 kbps）下协同优化，确保边缘部署可行性。  
  - **鲁棒性增强**：通过真实噪声和混响数据评估系统，Track 1需在轻度失真下保持质量，Track 2需主动集成语音增强功能（如去噪和去混响）。  
  - **集成增强与编码**：Track 2鼓励端到端神经或模块化流水线，在压缩同时处理声学失真，突破传统编解码结构限制。  
  - **评估框架**：使用众包主观测试（如MUSHRA-1S、DRT）量化质量、清晰度和鲁棒性，避免客观指标偏差。  

3)  
- **任务与效果**：  
  - **Track 1**：在轻度噪声和混响条件下，实现高透明度的语音重建，同时满足严格资源限制。  
  - **Track 2**：在挑战性声学环境中（如强噪声、混响），通过集成增强功能提升语音清晰度和自然度。  
  - **整体成果**：挑战赛吸引了多组提交（Track 1六项、Track 2九项），推动了低资源编解码器的实用化进展。
</div>

</details>

---

## Matching Reverberant Speech Through Learned Acoustic Embeddings and Feedback Delay Networks
- **Authors**: Philipp Götz, Gloria Dal Santo, Sebastian J. Schlecht, Vesa Välimäki, Emanuël A. P. Habets
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.23158v1](http://arxiv.org/abs/2510.23158v1)
- **PDF**: [http://arxiv.org/pdf/2510.23158v1](http://arxiv.org/pdf/2510.23158v1)

混响传递着环境的关键声学线索，能够增强空间感知与沉浸感。在听觉增强现实（AAR）系统中，如何实时生成感知可信的混响仍是一大挑战——尤其是在缺乏显式声学测量的场景下。本文通过将人工混响参数盲估计转化为混响信号匹配任务，利用学习得到的房间声学先验知识解决该问题。此外，我们提出一种反馈延迟网络（FDN）结构，可同时复现目标空间的频率相关衰减时间与直达声-混响比。通过与主流自动FDN调参方法进行实验对比，本方法在房间声学参数估计精度与人工混响语音的感知可信度方面均展现出优势。这些成果凸显了本方法在AAR应用中实现高效、感知一致混响渲染的潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：听觉增强现实系统需实时生成感知逼真的混响，但缺乏明确声学测量时难以实现。  
- **既有问题**：  
  - 传统深度神经网络方法计算复杂，未针对实时音频处理优化。  
  - 基于卷积的混响渲染计算成本高，缺乏动态场景灵活性。  
  - 现有FDN参数估计方法（如ARP-net）结构庞大，且固定混合矩阵和共享衰减滤波器限制了感知逼真度提升。  

2)  
- **核心方法**：  
  - **声学先验学习**：通过变分自编码器学习房间脉冲响应的紧凑表示，编码器从混响语音中提取与干声信号无关的隐空间嵌入。  
  - **可微分FDN结构**：  
    - 采用灵活的频率相关衰减控制，解除传统FDN中衰减滤波器与延迟长度的比例约束。  
    - 引入可学习的正交混合矩阵、独立通道衰减滤波器和音调校正滤波器，提升对能量衰减和反射密度的建模能力。  
  - **参数估计与优化**：  
    - 使用浅层多层感知机从声学嵌入预测FDN参数，合成混响信号。  
    - 训练结合多分辨率STFT损失（对齐合成与真实混响信号）和稀疏惩罚损失（提升混响尾部的感知平滑度）。  

3)  
- **任务与效果**：  
  - **声学参数匹配**：在T30（混响时间）和C50（清晰度指数）估计上误差更低，中频段（500Hz–2kHz）性能最优，Pearson相关系数显著优于基线。  
  - **感知质量**：Fr´echet音频距离得分（0.109）远低于基线（0.523），表明生成语音的混响感知逼真度更高。  
  - **应用价值**：适用于听觉增强现实中的实时、轻量级混响渲染。
</div>

</details>

---

## Treble10: A high-quality dataset for far-field speech recognition, dereverberation, and enhancement
- **Authors**: Sarabeth S. Mullins, Georg Götz, Eric Bezzam, Steven Zheng, Daniel Gert Nielsen
- **Categories**: eess.AS, cs.LG
- **arXiv**: [http://arxiv.org/abs/2510.23141v1](http://arxiv.org/abs/2510.23141v1)
- **PDF**: [http://arxiv.org/pdf/2510.23141v1](http://arxiv.org/pdf/2510.23141v1)

精确的远场语音数据集对自动语音识别、去混响、语音增强及声源分离等任务至关重要。然而现有数据集受限于声学真实性与可扩展性之间的权衡：实测语料虽能忠实反映物理特性，但成本高昂、覆盖范围有限，且鲜少包含配对纯净与混响数据；而多数基于仿真的数据集采用简化的几何声学模型，无法复现复杂环境中支配声波传播的关键物理现象（如衍射、散射与干涉）。本文提出Treble10——一个大规模高精度房间声学数据集，包含在10个真实装修房间中通过混合仿真范式生成的3000余条宽带房间脉冲响应。该范式基于Treble SDK实现，结合波动方程求解器与几何声学求解器，提供六类互补子集：单声道、八阶Ambisonics、六通道设备脉冲响应，以及与LibriSpeech语料配对的预卷积混响语音场景。所有信号以32kHz采样率仿真，精确建模低频波动效应与高频反射。Treble10弥合了实测与仿真之间的真实性鸿沟，为远场语音任务提供可复现的物理基础评估框架与大规模数据增强方案。数据集已通过Hugging Face Hub开源，既可作为基准评测集，也可作为新一代仿真驱动音频研究的模板。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：远场语音任务（如识别、去混响）依赖高质量声学数据，但现有数据集在真实性与规模间难以兼顾。  
- **既有问题**：  
  - 实测数据集（如BUT ReverbDB）物理精度高，但成本高昂、覆盖范围有限，且缺乏配对干净与混响语音。  
  - 仿真数据集多基于简化几何声学，忽略衍射、散射等关键波动效应，导致复杂环境声学建模失真。  

2)  
- **核心方法**：提出Treble10数据集，通过混合仿真范式结合波动方程求解器与几何声学，突破传统局限。  
- **解决方案**：  
  - **物理精度**：在10个真实家具布置房间中，采用32kHz宽带仿真，低频（≤5kHz）用波动方法（DGM）捕捉衍射与干涉，高频用几何声学处理反射，完整建模散射、吸收等现象。  
  - **规模与多样性**：生成超过3000条房间脉冲响应，包含单声道、8阶Ambisonics和6通道设备RIR，覆盖多高度接收网格与不同声源-接收器配置。  
  - **数据配套**：提供预卷积混响语音（基于LibriSpeech），支持直接用于模型训练与评估，弥补实测数据缺乏配对样本的不足。  
  - **可扩展性**：通过Treble SDK实现可控参数（如材料属性、设备布局），支持大规模可复现数据生成。  

3)  
- **任务与效果**：  
  - **远场语音识别**：提供高真实性混响语音，提升模型在复杂环境中的鲁棒性。  
  - **去混响与语音增强**：配对干净与混响数据支持端到端训练，改善信号分离质量。  
  - **多通道处理**：Ambisonics与6通道设备RIR助力波束成形与声源分离研究。  
  - **基准价值**：作为开源数据集，推动仿真驱动音频研究的可复现评估与数据增强。
</div>

</details>

---

## TwinShift: Benchmarking Audio Deepfake Detection across Synthesizer and Speaker Shifts
- **Authors**: Jiyoung Hong, Yoonseo Chung, Seungyeon Oh, Juntae Kim, Jiyoung Lee, Sookyung Kim, Hyunsoo Cho
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.23096v1](http://arxiv.org/abs/2510.23096v1)
- **PDF**: [http://arxiv.org/pdf/2510.23096v1](http://arxiv.org/pdf/2510.23096v1)

音频深度伪造技术构成的威胁日益严峻，已被广泛应用于欺诈与虚假信息传播领域。当前核心挑战在于确保检测模型对未知合成方法及多样化说话人保持鲁棒性，因为生成技术正持续快速演进。尽管现有系统在基准测试中表现优异，但其泛化能力在新场景下显著受限，影响实际应用的可靠性。为此，我们提出TWINSHIFT基准测试，该基准专为严格评估未知场景下的检测鲁棒性而设计。我们基于六种不同合成系统构建测试集，每个系统均配以互斥的说话人集合，从而实现对生成模型与说话人身份双重变化时检测器泛化能力的系统评估。大量实验表明：TWINSHIFT能有效揭示关键鲁棒性缺陷，发掘被忽视的系统局限，并为音频深度伪造检测系统的开发提供理论指导。TWINSHIFT基准测试集可通过https://github.com/intheMeantime/TWINSHIFT 获取。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频深度伪造技术快速发展，已被用于欺诈和虚假信息传播，对公共安全构成威胁。现有音频深度伪造检测系统在已知条件下表现良好，但面临泛化能力不足的问题。  
- **既有方法问题**：  
  - 检测器容易过拟合特定合成器、说话人或预处理流程的伪线索，导致面对新合成方法时性能急剧下降。  
  - 传统评估方法采用随机划分训练测试数据，泄露分布信息，高估模型鲁棒性，无法反映真实场景中的未见过条件。  

2)  
- **核心方法**：提出TWINSHIFT基准，通过严格分离训练和测试数据中的合成器与说话人，系统评估检测器在双重未见过条件下的泛化能力。  
  - **双轴设计**：  
    - 合成器轴：涵盖六种不同架构的语音合成系统（如MeloTTS、HierSpeech++等），确保测试时使用训练未见的合成器。  
    - 说话人轴：每个合成器配对互斥的说话人集合，避免说话人身份在训练和测试中重叠。  
  - **环境构建**：  
    - 创建六个独立环境，每个环境包含专属的真实语音库与伪造语音生成器配对。  
    - 采用8:2划分训练和测试数据，支持域内基线评估与跨环境迁移测试。  
  - **实验设计**：  
    - 在四种控制条件下评估：合成器与说话人均可见、仅说话人未见过、仅合成器未见过、两者均未见过。  
    - 使用四种主流检测器（如Se-Res2Net、RawNet2等）和等错误率作为指标，全面分析泛化漏洞。  

3)  
- **评估任务与效果**：  
  - **任务**：跨合成器与说话人双重偏移的音频深度伪造检测。  
  - **效果**：  
    - 在域内测试中，检测器表现优异（EER接近0）；但在跨环境测试中，性能显著下降，未见过条件下的平均EER最高达0.548。  
    - 揭示了合成器偏移是主要误差来源，而说话人偏移会进一步加剧性能退化。  
    - 发现高保真合成器（如OZSpeech）虽难以检测，但其训练模型泛化能力有限，突显了当前系统在真实场景中的脆弱性。
</div>

</details>

---

## Adapting Speech Foundation Models with Large Language Models for Unified Speech Recognition
- **Authors**: Jing-Xuan Zhang, Genshun Wan, Jin Li, Jianqing Gao
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.22961v1](http://arxiv.org/abs/2510.22961v1)
- **PDF**: [http://arxiv.org/pdf/2510.22961v1](http://arxiv.org/pdf/2510.22961v1)

统一语音识别旨在通过单一模型框架实现听觉、视觉及视听融合的语音识别任务。尽管语音基础模型在听觉任务中表现出色，但其在多模态场景下的适应性仍待探索。本文提出UASR-LLM框架，通过引入大语言模型作为文本解码器，将冻结参数的语音基础模型适配于视觉语音识别、自动语音识别与视听语音识别任务。该方法通过视觉注入模块将视觉表征融入语音基础模型的多层结构中，实现多模态输入处理与统一隐表征生成。增强后的语音基础模型通过前馈适配器与仅解码架构的大语言模型连接，利用拼接表征与指令提示指导语音转写。我们采用两阶段训练策略：先进行视觉注入预训练，再进行语音识别微调。训练过程中语音基础模型参数始终保持冻结，仅优化视觉注入模块，并后续使用LoRA参数对大语言模型进行微调。实验结果表明，该方法在清晰与噪声环境下均优于当前最先进的基准模型。消融研究验证了框架在不同语音基础模型与大语言模型间的泛化能力，证实了所提训练策略的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音基础模型在听觉任务中表现出色，但环境噪声会显著降低识别准确率，限制了实际应用。  
- **既有问题**：  
  - 现有方法多针对单一模态（如仅音频或仅视觉）设计，缺乏统一处理多模态任务的框架。  
  - 传统方法依赖知识蒸馏或独立模型，无法充分利用语音基础模型和大型语言模型的互补优势。  

2)  
- **核心方法**：提出UASR-LLM框架，通过以下设计解决上述问题：  
  - **视觉注入模块**：在语音基础模型的多层中引入视觉表示，通过跨模态注意力机制融合音频和视觉输入，生成统一隐藏表示。  
  - **两阶段训练**：  
    - **视觉注入预训练**：使用带噪声的音频-视觉输入预测干净音频表示，增强模型对多模态数据的处理能力。  
    - **语音识别微调**：连接适配器与大型语言模型，通过指令提示和随机模态丢弃策略，统一支持ASR、VSR和AVSR任务。  
  - **参数优化策略**：冻结语音基础模型参数，仅优化视觉注入模块和大型语言模型的低秩适配参数，实现高效训练并保留模型泛化能力。  

3)  
- **任务与效果**：在LRS3测试集上，UASR-LLM显著优于基线方法：  
  - **VSR**：词错误率降至20.9%，优于多数视觉语音识别模型。  
  - **ASR**：词错误率达0.84%，在干净条件下接近最优性能。  
  - **AVSR**：词错误率仅0.69%，在噪声环境中鲁棒性显著提升，尤其在低信噪比条件下优势明显。
</div>

</details>

---

## DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching
- **Authors**: Yuepeng Jiang, Huakang Chen, Ziqian Ning, Jixun Yao, Zerui Han, Di Wu, Meng Meng, Jian Luan, Zhonghua Fu, Lei Xie
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.22950v1](http://arxiv.org/abs/2510.22950v1)
- **PDF**: [http://arxiv.org/pdf/2510.22950v1](http://arxiv.org/pdf/2510.22950v1)

生成完整且高质量歌曲面临严峻挑战，需同时维持文本与音乐模态间及音乐模态内部的长期连贯性。现有非自回归框架虽能生成优质歌曲，但常存在歌词与人声的对齐问题。为满足多样化音乐偏好，需采用基于人类反馈的强化学习，然而现有方法在多偏好优化时依赖多模型融合，易导致性能显著下降。针对这些挑战，我们提出DiffRhythm 2——面向高保真可控歌曲生成的端到端框架。为解决歌词对齐问题，本框架采用基于块流匹配的半自回归架构，可在不依赖外部标签与约束的前提下实现歌词与演唱人声的精准对齐，同时保持非自回归模型的高生成质量与效率。为实现长序列计算的可行性，我们开发了音乐变分自编码器，在5Hz低帧率下仍能实现高保真音频重建。此外，为突破多偏好优化的局限，提出交叉配对偏好优化法，有效缓解模型融合导致的性能衰减，实现跨人类偏好的稳健优化。通过引入随机块表示对齐损失，进一步增强了音乐性与结构连贯性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：歌曲生成需同时处理歌词与音乐模态的长期一致性，现有非自回归方法虽能生成高质量歌曲，但常出现歌词与歌声对齐困难的问题。  
- **既有方法问题**：  
  - 自回归框架（如Yue、SongGen）生成速度慢，且音轨独立预测导致人声与伴奏不一致。  
  - 非自回归方法（如DiffRhythm）依赖外部时间戳约束对齐，限制了创造性与数据要求。  
  - 多偏好优化（如RLHF）需合并多个模型，导致性能显著下降。  

2)  
- **核心方法**：DiffRhythm 2提出基于块流匹配的半自回归端到端框架，具体通过以下设计解决问题：  
  - **块流匹配架构**：  
    - 将潜在序列划分为固定长度块，每块内部通过流匹配非自回归生成，块间通过自回归依赖保持连贯性。  
    - 无需外部标签即可实现歌词与歌声的精准对齐，同时保留非自回归模型的高效性。  
  - **高压缩音乐VAE**：  
    - 设计帧率低至5 Hz的变分自编码器，大幅压缩序列长度，使长序列（如210秒歌曲）建模可行。  
    - 结合多尺度损失函数与判别器，保障重建音频的高保真度。  
  - **随机块REPA损失**：  
    - 在块流匹配中引入基于MuQ的表示对齐损失，增强音乐结构与层次一致性，避免直接约束导致的乐感下降。  
  - **跨对偏好优化**：  
    - 将冲突或协同的偏好分组（如“乐感-歌词对齐”与“风格相似性-音频质量”），进行跨组 pairwise 优化。  
    - 减少模型合并数量，显著缓解多偏好优化中的性能衰减问题。  

3)  
- **任务与效果**：  
  - **歌曲生成任务**：生成全长（最高210秒）、含人声与伴奏的歌曲，支持文本或音频风格控制。  
  - **客观指标**：  
    - 歌词对齐（PER 0.13）与文本风格相似性（Mulan-T 0.40）显著优于开源模型。  
    - 音乐质量（SongEval）与音频质量（Aesthetics）接近商业模型水平。  
  - **主观评价**：  
    - 在乐感、人声-伴奏和谐度、整体质量上超越现有开源模型，伴奏生成尤其突出。  
    - 仅在人声质量上略逊于单独建模音轨的方法（如ACE-Step）。
</div>

</details>

---
