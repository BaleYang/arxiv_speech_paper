---
layout: post
title: "arXiv Daily – 2025-11-27"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-11-27（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-11-26 08:50 — 2025-11-27 08:50
- 抓取总数：14 篇 | 本页显示：14 篇（去重/过滤后）

## Harmonic-Percussive Disentangled Neural Audio Codec for Bandwidth Extension
- **Authors**: Benoît Giniès, Xiaoyu Bie, Olivier Fercoq, Gaël Richard
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.21580v1](https://arxiv.org/abs/2511.21580v1)
- **PDF**: [https://arxiv.org/pdf/2511.21580v1](https://arxiv.org/pdf/2511.21580v1)

带宽扩展作为音频处理领域的经典任务，旨在通过低通信号重建高频成分。传统方法长期伴随信号处理技术演进，而近期神经架构的突破显著提升了各类音频任务性能。本研究通过将带宽扩展构建为音频令牌预测问题，进一步推动该领域发展。具体而言，我们基于解耦神经音频编解码器产生的离散表示训练Transformer语言模型，其中解耦过程通过谐波-打击乐分解引导，突出对带宽扩展至关重要的频谱结构。本方法提出了一种新颖的编解码器设计，显式考虑下游令牌预测任务，实现编解码结构与Transformer建模的高效耦合。客观指标与主观评估均表明，该联合设计能高质量重建原始信号。这些成果揭示了编解码解耦、表示学习与生成建模阶段协同优化的重要性，展现了全局化、表示感知的设计思路对推动带宽扩展技术发展的潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**
- 带宽扩展任务旨在从低通信号重建高频成分，传统方法依赖手工信号处理技术，如频谱复制或缩放。
- 现有神经音频编解码器虽在压缩任务中表现优异，但其表示缺乏可解释性，且未针对下游生成任务优化。
- 基于扩散或混合信号处理的方法虽取得进展，但未充分利用编解码器结构与语言模型的协同设计潜力。

2. **论文核心方法如何解决上述问题**
- **HP-codec设计**：  
  - 采用双分支结构，分别处理低频（16kHz）和高频（48kHz）成分，通过残差连接强化频带间依赖。  
  - 在每分支内引入谐波、打击乐和残差三组并行RVQ模块，显式分离不同语义的音频结构。  
- **HP-codecX预测模型**：  
  - 基于Transformer语言模型，分别对三组RVQ模块的离散令牌进行自回归预测，从低频令牌生成高频令牌。  
  - 通过多阶段训练策略（先分阶段训练编解码器分支，再联合微调）确保表示质量与预测精度。  
- **联合优化优势**：  
  - 编解码器的解耦设计直接服务于下游令牌预测任务，提升高频成分的可预测性。  
  - 语义分解（谐波/打击乐）增强了表示的 interpretability，并改善了跨频带模式的学习效率。

3. **在哪些任务上取得了怎样的效果**
- **任务**：音频带宽扩展，将8kHz低通信号扩展至24kHz全带宽信号。  
- **效果**：  
  - 在MUSDB18等音乐数据集上，HP-codecX在客观指标（Mel损失、STFT损失）和主观MUSHRA测试中均优于Apollo与AudioSR基线。  
  - 在未见的单乐器、鼓组及合成数据上表现鲁棒，验证其泛化能力。  
  - 消融实验证实语义模块对谐波/打击乐信号的重建具有专项优势。
</div>

</details>

---

## HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal
- **Authors**: Kexin Li, Xiao Hu, Ilya Grishchenko, David Lie
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2511.21577v1](https://arxiv.org/abs/2511.21577v1)
- **PDF**: [https://arxiv.org/pdf/2511.21577v1](https://arxiv.org/pdf/2511.21577v1)

高质量AI生成音频的普及带来了安全挑战，如虚假信息传播和语音克隆欺诈。针对AI生成音频滥用的关键防御手段是添加水印，以便与真实音频区分。为应对恶意使用者可能实施的音频水印移除行为，研究有效的水印去除技术对客观评估音频水印的鲁棒性至关重要。现有水印移除方案或需预设待去除水印的完整信息而缺乏实用性，或计算成本高昂，可能导致对当前水印方案的误判。

本文提出HarmonicAttack——一种高效音频水印移除方法，仅需掌握目标水印方案的基本生成能力即可实现去除。基于此，我们训练出通用水印移除模型，可去除任意含水印音频样本中的目标水印。该方法采用双路径卷积自编码器，在时域与频域协同工作，结合GAN式训练实现水印与原始音频的分离。在AudioSeal、WavMark和Silentcipher等前沿水印方案上的测试表明，HarmonicAttack在保持近实时性能的同时，其水印去除能力优于现有方法。值得注意的是，虽然需要训练过程，但该方法能有效迁移至分布外样本且性能衰减极小。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：AI生成音频技术快速发展，引发虚假信息传播和语音克隆欺诈等安全风险。音频水印技术作为防御手段，用于区分AI生成音频与真实音频。  
- **既有方法问题**：  
  - 传统信号处理方法（如滤波、压缩）难以完全去除现代水印，且易引入可听伪影。  
  - 基于优化的方法（如HopSkipJump、Square攻击）需假设已知水印类型或检测器内部参数，计算成本高，无法满足实时场景需求。  

2)  
- **核心方法**：HarmonicAttack采用基于学习的自适应跨域水印去除框架，通过以下设计解决既有问题：  
  - **双路径自编码器**：结合时域和频域编码器，分别处理原始音频波形和STFT频谱图，捕捉水印在多尺度时空和频谱上的特征。  
  - **GAN式对抗训练**：生成器学习从水印音频中分离水印，判别器区分生成器输出与原始干净音频，确保输出音频的统计自然性。  
  - **多组件损失函数**：  
    - 重建损失（L1+L2）保障音频保真度。  
    - 心理声学损失基于Mel频带权重，聚焦水印能量集中的感知掩蔽区域。  
    - 去相关损失使生成器修改方向与水印模式相反，提升去除强度。  
    - 对抗损失通过判别器反馈优化生成器输出质量。  
  - **闭盒威胁模型**：仅需水印生成API，无需检测器内部参数，支持跨域泛化。  

3)  
- **任务与效果**：  
  - 在AudioSeal、WavMark和Silentcipher三种先进水印方案上测试，攻击成功率（ASR）达95%-100%。  
  - 跨域泛化能力强：在LibriSpeech（语音）上训练后，在FMA（音乐）数据集上ASR仍接近100%，PEAQ音频质量评分高于0.9。  
  - 计算高效：单样本处理时间约0.03-0.06秒，支持实时应用，且性能不受音频长度影响。
</div>

</details>

---

## Generating Separated Singing Vocals Using a Diffusion Model Conditioned on Music Mixtures
- **Authors**: Genís Plaja-Roglans, Yun-Ning Hung, Xavier Serra, Igor Pereira
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2511.21342v1](https://arxiv.org/abs/2511.21342v1)
- **PDF**: [https://arxiv.org/pdf/2511.21342v1](https://arxiv.org/pdf/2511.21342v1)

本文针对音乐分析及实践中至关重要的混合音轨分离任务展开研究。传统方法通常采用神经网络对混合音频的时频表示进行掩码或变换以提取目标声源，而生成式扩散模型凭借其灵活性与泛化能力，为这一复杂任务提供了全新解决方案。我们提出基于扩散模型的歌声分离方法，通过训练模型根据混合音乐生成对应独唱人声。实验表明：该方法在补充训练数据条件下，不仅超越了现有生成式系统，更在客观指标上达到与非生成式基线模型相当的竞争力。扩散采样过程的迭代特性使用户能够自主权衡质量与效率，并在必要时优化输出结果。我们进一步通过消融实验解析采样算法，揭示了可配置参数对分离效果的影响机制。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐源分离（MSS）旨在从混合音频中提取独立音源，对音乐分析与创作至关重要。  
- **既有方法问题**：  
  - 传统神经网络依赖时频掩码或变换，但音乐中源信号重叠严重，分离效果受限。  
  - 相位处理易引入伪影，且需完整孤立音源数据，实际难以获取。  
  - 现有生成模型（如扩散模型）多聚焦合成器乐，在真实人声分离中效率与性能仍有不足。  

2)  
- **核心方法**：提出基于扩散模型的生成式分离系统，通过条件生成解决传统问题。  
  - **条件扩散框架**：  
    - 训练生成器从高斯噪声中迭代重构人声，条件模块注入混合音频特征，引导生成与混合一致的目标音源。  
    - 采用DDIM采样器，平衡生成质量与步数（仅需约20步即可生成可用结果）。  
  - **网络结构优化**：  
    - 生成器为1D卷积U-Net，引入残差块与自注意力机制以保留时序结构。  
    - 条件器基于UNIVERSE模块改进，增加Transformer与旋转编码，增强长程依赖建模能力。  
  - **可控采样机制**：  
    - 引入随机细化噪声，通过高频滤波（如5kHz截止）针对性优化高頻段生成，减少伪影。  
    - 用户可调节参数（如步数T、随机强度η），实现质量与效率的灵活权衡。  
  - **辅助损失函数**：  
    - 添加潜在空间与重建损失，强化条件器对分离目标的约束，提升生成准确性。  

3)  
- **任务与效果**：  
  - **人声分离任务**：在musdb18hq等开源多轨数据集上测试。  
  - **性能表现**：  
    - 仅用公开数据时，SDR达5.38dB，超越同期生成模型（如MSDM的3.64dB）。  
    - 结合额外数据训练后，SDR提升至8.77dB，与非生成基线HT-Demucs（9.37dB）竞争力相当。  
    - 在干扰抑制（SIR指标）上接近理想掩码 oracle，生成人声更干净且与混合音频高度匹配。
</div>

</details>

---

## SONAR: Spectral-Contrastive Audio Residuals for Generalizable Deepfake Detection
- **Authors**: Ido Nitzan HIdekel, Gal lifshitz, Khen Cohen, Dan Raviv
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2511.21325v1](https://arxiv.org/abs/2511.21325v1)
- **PDF**: [https://arxiv.org/pdf/2511.21325v1](https://arxiv.org/pdf/2511.21325v1)

【中文摘要】
深度伪造音频检测器在面对分布外数据时仍存在泛化能力不足的问题，其核心症结在于频谱偏差——神经网络倾向于优先学习低频结构而非高频细节，这导致深度伪造生成器遗留高频伪影，而现有检测器却未能充分挖掘这些特征。为解决此问题，我们提出频谱对比音频残差框架，通过频率感知的对比学习机制显式解耦音频信号：采用XLSR编码器捕获主导低频内容，同时通过可学习SRM模块与值约束高通滤波器构成的并行支路提取微弱高频残差。频率交叉注意力机制融合双路径的长短程频域依赖关系，结合频率感知的Jensen-Shannon对比损失函数，在拉近真实音频内容-噪声对的同时推远伪造样本嵌入，从而加速优化过程并锐化决策边界。在ASVspoof 2021基准与真实场景数据上的实验表明，本方法以四倍于基线模型的速度收敛，并达到当前最优性能。通过将微弱高频残差提升为一级学习信号，该框架构建出数据驱动的频率感知对比体系，将潜在空间解耦为自然高频（真实音频）与失真高频（合成音频）两个互斥流形，进而强化分类边界。由于该方案完全在表征层面操作，其具备架构无关性，未来可无缝集成至任何依赖细微高频线索的模型或模态中。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**
- 深度伪造音频检测面临泛化能力不足，主要由于**频谱偏差**：神经网络优先学习低频结构，忽视高频细节。
- 现有方法（如固定高通滤波器或残差分支）仅孤立处理高频残差，忽略其与语义内容的关联。
- 缺乏对真伪音频中低频-高频联合统计分布差异的建模，导致检测器无法有效利用高频伪影。

2. **论文核心方法如何解决上述问题**  
SONAR通过频率引导的双路径框架和对比学习机制解决上述问题：
- **双路径特征提取**：
  - **内容路径**：使用XLSR编码器捕获低频语义信息。
  - **噪声路径**：通过可学习的SRM滤波器组（带零和约束）提取高频残差，强化生成器遗留的细微伪影。
- **频率交叉注意力融合**：整合双路径特征，建模长短程频率依赖关系。
- **Jensen-Shannon对比损失**：
  - 拉近真实音频的低频-高频嵌入分布（增强耦合）。
  - 推远伪造音频的分布（放大统计差异）。
- **理论支撑**：基于Pinsker不等式，JS损失直接优化贝叶斯误差上界，加速收敛并提升决策边界清晰度。
- **扩展设计**：
  - SONAR-Lite：轻量级MLP分类器验证双路径特征本身具备强判别性。
  - SONAR-Finetune：在预训练模型上仅微调噪声路径与融合模块，兼顾效率与性能。

3. **在哪些任务上取得了怎样的效果**  
- **ASVspoof 2021**：在Logical Access（LA）和Deep Fake（DF）任务中，SONAR-Full分别达到1.55%和1.57%的EER，SONAR-Finetune进一步优化至1.20%（LA）和1.45%（DF）。
- **In-the-Wild数据集**：SONAR-Full与SONAR-Finetune分别取得6.00%和5.43%的EER，显著优于基线模型。
- **收敛速度**：训练周期缩短至4–12轮（基线需100轮），且对音频编解码和重采样具有强鲁棒性。
</div>

</details>

---

## Acoustic neural networks: Identifying design principles and exploring physical feasibility
- **Authors**: Ivan Kalthoff, Marcel Rey, Raphael Wittkowski
- **Categories**: cs.SD, cond-mat.dis-nn, cs.NE, eess.AS, physics.app-ph
- **arXiv**: [https://arxiv.org/abs/2511.21313v1](https://arxiv.org/abs/2511.21313v1)
- **PDF**: [https://arxiv.org/pdf/2511.21313v1](https://arxiv.org/pdf/2511.21313v1)

基于波导的物理系统为超越传统电子技术的高效能模拟计算提供了可行路径。声学神经网络作为该领域的重要分支，能够在电子设备效率低下或受限的环境中实现低功耗计算，但其系统性设计原则尚未得到充分探索。本文提出一种声学神经网络的设计与仿真框架，通过声波传播实现计算任务。采用数字孪生方法，我们在物理约束条件下训练传统神经网络架构，包括非负信号与权重、无偏置项设计，以及适用于基于强度的非负声学信号的非线性处理。该框架将可学习的网络组件直接关联至物理可测的声学特性，为构建可实现的声学计算系统提供了系统化设计方法。研究证明，经过约束的循环与分层架构能够实现精确的语音分类任务，我们进一步提出SincHSRNN混合模型，该模型将可学习的声学带通滤波器与分层时序处理相结合。在AudioMNIST数据集上，SincHSRNN达到95%的分类准确率，同时保持与无源声学元件的兼容性。除计算性能外，模型学习参数对应着衰减系数、透射率等可测量的材料与几何特性。本研究确立了物理可实现声学神经网络的通用设计原则，为发展低功耗的波动神经网络计算指明了技术路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：传统电子计算面临晶体管微缩的物理极限，亟需低功耗替代方案。声学神经网络利用声波传播进行模拟计算，在边缘设备等场景潜力显著。  
- **既有问题**：现有声学系统多为固定功能设计（如单一滤波或微分操作），缺乏可重构的完整神经网络架构；非线性激活函数实现困难，且参数与物理属性（如衰减、传输）的映射关系不明确。  

2)  
- **核心方法**：提出“数字孪生”框架，通过物理约束构建可实现的声学神经网络模型：  
  - **约束设计**：信号与权重限制为非负值（对应声强与衰减），忽略偏置项，采用幅值依赖的非线性激活函数（如偏移ReLU）。  
  - **架构优化**：  
    - 使用循环神经网络（RNN）处理声波动态特性，避免需符号运算的门控机制（如LSTM）。  
    - 引入分层降采样RNN（HSRNN）压缩长序列，提升时序建模效率。  
    - 提出SincHSRNN，结合可学习的Sinc带通滤波器与分层递归结构，实现频率选择与时序依赖的协同处理。  
  - **物理映射**：网络参数直接对应声学材料的衰减、传输系数等可测量属性，为硬件实现提供蓝图。  

3)  
- **任务与效果**：在AudioMNIST语音分类任务中评估：  
  - 受限RNN在二分类任务中准确率达68%，但长序列稳定性不足。  
  - HSRNN将十分类准确率提升至67%，分层结构有效改善泛化能力。  
  - SincHSRNN在8kHz采样率下达到95.1%准确率，逼近传统数字模型（96.5%），且参数规模仅为同类CNN的13%，兼具高效性与物理可解释性。
</div>

</details>

---

## Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale
- **Authors**: Yicheng Zhong, Peiji Yang, Zhisheng Wang
- **Categories**: cs.SD, cs.CV
- **arXiv**: [https://arxiv.org/abs/2511.21270v1](https://arxiv.org/abs/2511.21270v1)
- **PDF**: [https://arxiv.org/pdf/2511.21270v1](https://arxiv.org/pdf/2511.21270v1)

大型语言模型（LLM）的最新进展正在重塑文本转语音（TTS）合成领域，催生了将语音表示为离散编解码器序列的自回归框架。其中，单码本TTS大语言模型以其紧凑可流式传输的架构，实现了语义与声学特征的联合建模。然而这类模型在保持高效性的同时，常出现韵律不稳定、说话人特征漂移及自然度下降等问题。为解决这些缺陷，我们提出多奖励组相对策略优化（GRPO）框架，直接优化单码本TTS大语言模型的令牌生成策略。除基础的可懂度与说话人相似度目标外，本框架引入三项规则化奖励：确保时长一致性的长度惩罚、提升解码稳定性的熵正则奖励，以及通过LLM标注的韵律对齐奖励实现显式节奏监督。该韵律奖励通过上下文学习使外部推理大语言模型预测多种合理停顿结构，为GRPO训练提供符合人类偏好的监督信号。为验证普适性，我们在GRPO优化的自回归骨干网络上接入流匹配解码器，观察到持续的性能提升，表明强化优化有效增强了自回归策略的内在质量。通过不同数据规模与模型体量的可扩展性分析进一步证明，本方法能持续提升单码本TTS大语言模型的韵律稳定性、说话人相似度及整体语音自然度。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**
- 单码本TTS大语言模型因联合建模语义与声学特征，具有结构紧凑、支持流式生成的优势。
- 但现有方法在解码策略上存在不足，导致韵律不稳定、说话人特征漂移及自然度下降。
- 传统强化学习方法对偏好噪声敏感，且依赖大量标注数据，难以扩展优化。

2. **论文核心方法如何解决上述问题**
- 提出多奖励GRPO框架，通过组间优势归一化稳定策略优化，避免对密集偏好标签的依赖。
- 融合可懂度与说话人相似度两类客观指标奖励，确保语义准确性和音色一致性。
- 引入三项规则奖励：
  - **长度惩罚**：约束生成语音时长，避免过早截断或过度延长。
  - **熵正则化**：抑制令牌生成过程中的高熵波动，提升解码稳定性。
  - **韵律对齐奖励**：基于外部推理大模型生成多候选停顿结构，通过离线标注与在线匹配引导节奏自然化。
- 进一步结合流匹配解码器，验证优化后的自回归策略具备内在增强性，与声学后处理模块形成互补。

3. **在哪些任务上取得了怎样的效果**
- 在SEED双语评测集上，显著降低中/英文字符错误率，提升说话人相似度与自然度MOS评分。
- 在困难测试集上表现鲁棒，CER与SIM同步改善，证明其对复杂场景的适应能力。
- 扩展性实验表明：该方法在不同数据规模与模型参数量下均能稳定提升韵律质量与解码一致性。
</div>

</details>

---

## The Spheres Dataset: Multitrack Orchestral Recordings for Music Source Separation and Information Retrieval
- **Authors**: Jaime Garcia-Martinez, David Diaz-Guerra, John Anderson, Ricardo Falcon-Perez, Pablo Cabañas-Molero, Tuomas Virtanen, Julio J. Carabias-Orti, Pedro Vera-Candeas
- **Categories**: eess.AS, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.21247v1](https://arxiv.org/abs/2511.21247v1)
- **PDF**: [https://arxiv.org/pdf/2511.21247v1](https://arxiv.org/pdf/2511.21247v1)

本文介绍Spheres数据集——一套专为推进古典音乐领域音源分离及相关音乐信息检索研究而设计的多轨管弦乐录音资料。该数据集收录了Colibrì乐团在Spheres录音棚演奏的总时长超过一小时的音乐内容，包含柴可夫斯基《罗密欧与朱丽叶》与莫扎特《第40号交响曲》两部经典作品，以及各乐器的半音阶练习曲和独奏片段。录音采用23支麦克风阵列（含近距离点麦克风、主麦克风及环境麦克风），既能生成具有可控串音效应的真实立体声混音，又可为音源分离模型的监督训练提供独立音轨。此外，通过估算各乐器位置的房间脉冲响应，实现了对录音空间声学特性的精准刻画。我们详细阐述了数据集结构、声学分析结果，并基于X-UMX模型开展了管弦乐声部分离与麦克风串音消除的基线评估。实验结果既揭示了复杂管弦乐场景下音源分离的潜力与挑战，也凸显了本数据集在建立基准测试、探索古典音乐分离/定位/去混响/沉浸式渲染新方法方面的价值。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**
   - 音乐源分离（MSS）在古典音乐领域进展缓慢，主要问题包括：
     - 缺乏适用于深度学习训练的多轨录音数据集，受版权限制和录制复杂性影响。
     - 古典乐团乐器数量多、音色相似（如小提琴与中提琴），分离难度大。
     - 现场录音常包含房间混响和乐器间串扰，导致信号不纯净。
   - 现有数据集（如MUSDB18）多针对流行音乐，古典音乐专用资源稀缺，且合成数据训练模型在真实录音上泛化能力有限。

2. **论文核心方法如何解决上述问题**
   - **数据集设计**：
     - 录制了超过1小时的多轨古典音乐，包括柴可夫斯基《罗密欧与朱丽叶》和莫扎特《第40号交响曲》，涵盖完整乐团配置。
     - 采用23个麦克风（主麦克风、环境麦克风及近距离点麦克风），分别录制每个乐器的独立音轨，避免串扰，同时通过叠加生成真实立体声混音。
     - 提供房间脉冲响应（RIR）数据，支持声学特性分析与去混响研究。
   - **技术亮点**：
     - 通过独立录制乐器音轨生成无串扰的参考信号，适用于监督学习。
     - 包含独奏片段和音阶练习，扩展数据多样性。
     - 支持多任务研究，如源分离、定位、去混响和沉浸式音频渲染。
   - **基线实验**：
     - 使用X-UMX模型进行乐器族分离（弦乐、木管等）和麦克风去串扰任务，证明数据集在训练分离模型中的有效性。

3. **在哪些任务上取得了怎样的效果**
   - **任务**：
     - 乐器族分离：从立体声混音中分离弦乐、木管、铜管和打击乐。
     - 麦克风去串扰：增强近距离麦克风的目标乐器信号，抑制其他乐器干扰。
   - **效果**：
     - 在数据集内部评估中，分离模型的SDR、SIR显著提升，但跨数据集（如Operation Beethoven）泛化能力仍受限。
     - 去串扰模型能有效提高目标乐器的信噪比，尤其对音色差异大的乐器效果更佳。
</div>

</details>

---

## Evaluation of an ITD-to-ILD Transformation as a Method to Restore the Spatial Benefit in Speech Intelligibility in Hearing Impaired Listeners
- **Authors**: Timm-Jonas Bäumer, Johannes W. de Vries, Stephan Töpken, Richard C. Hendriks, Peyman Goli, Steven van de Par
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.21222v1](https://arxiv.org/abs/2511.21222v1)
- **PDF**: [https://arxiv.org/pdf/2511.21222v1](https://arxiv.org/pdf/2511.21222v1)

为提升复杂日常环境中的言语清晰度，人类听觉系统部分依赖于双耳时间差（ITD）与双耳强度差（ILD）。然而，听力受损者通常对ITD的感知灵敏度受限，导致言语清晰度下降。本研究旨在探究将低频ITD转换为ILD是否能为听力受损者重建双耳听觉优势。我们针对听力受损者开展了两组实验：首组实验采用不同频率的双耳相位偏移正弦信号，评估受试者的ITD灵敏度阈值。所有受试者在高频段均表现出ITD阈值升高，低频段的ITD灵敏度则存在个体差异。第二组实验通过操控头相关传递函数（HRTF），测量了不同双耳配置下的言语接收阈值（SRT）。结果表明：尽管ITD灵敏度降低，但在保留ITD与ILD的未处理基线条件下，移除ITD会使SRT降低约1 dB；将低频ITD替换为ILD可提升侧向目标说话人的清晰度；而在保留ITD的同时添加低频ILD，则对所有方向说话人的清晰度均有显著改善。这些发现证明所提出的转换方法能有效恢复听力受损者的双耳听觉优势。本研究建议将此类转换技术应用于助听器与人工耳蜗设备，为听力受损者提供直接帮助。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：听力受损者常对双耳时间差（ITD）敏感度降低，影响复杂环境下的言语清晰度，尤其在依赖ITD的低频区域。  
- **既有问题**：传统助听设备和人工耳蜗未能有效传递ITD信息，导致双耳听觉优势减弱，言语理解能力下降。  

2)  
- **核心方法**：提出ITD-to-ILD转换技术，将低频ITD转换为双耳强度差（ILD），通过以下步骤实现：  
  - 估计HRTF中的ITD，按三分之一倍频程分频计算。  
  - 使用线性映射将ITD转换为ILD，并应用增益调整左右耳频谱。  
  - 在1000-2000 Hz频段过渡处理，保留原始高频ILD，并平滑频谱以避免失真。  
- **解决机制**：  
  - 利用HI者对ILD的保留敏感度，补偿ITD感知缺陷。  
  - 增强空间分离目标的听觉流分离和better-ear效应，提升信噪比。  
  - 实验显示，保留ITD同时添加转换ILD可最大化改善效果。  

3)  
- **任务与效果**：  
  - **言语清晰度测试**：在侧向目标说话人场景中，SRT改善达2.6 dB（最高4 dB）。  
  - **中央目标场景**：干扰声空间分离后，SRT小幅提升（约0.7 dB）。  
  - **整体表现**：转换方法在所有方向均提高言语可懂度，验证其在助听设备中的应用潜力。
</div>

</details>

---

## AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control
- **Authors**: Xinyue Guo, Xiaoran Yang, Lipan Zhang, Jianxuan Yang, Zhao Wang, Jian Luan
- **Categories**: cs.MM, cs.CV, cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.21146v1](https://arxiv.org/abs/2511.21146v1)
- **PDF**: [https://arxiv.org/pdf/2511.21146v1](https://arxiv.org/pdf/2511.21146v1)

当前音效编辑技术——通过添加、移除或替换元素来修改音频——仍受限于仅依赖低层级信号处理或粗粒度文本提示的现有方法，常导致编辑灵活性不足且音质欠佳。为此，我们提出AV-Edit：一种通过联合利用视觉、音频与文本语义实现视频音轨细粒度编辑的生成式音效编辑框架。该方法采用专门设计的对比式视听掩码自编码器（CAV-MAE-Edit）进行多模态预训练，以学习对齐的跨模态表征。这些表征随后用于训练编辑型多模态扩散变换器（MM-DiT），通过基于相关性的特征门控训练策略，该模型能够消除视觉无关声音并生成与视频内容一致的缺失音频元素。此外，我们构建了专用的视频音效编辑数据集作为评估基准。实验表明，AV-Edit能基于视觉内容生成具有精确修改的高质量音频，在音效编辑领域达到最先进性能，并在音频生成领域展现出强劲竞争力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：传统音效编辑依赖信号处理或文本提示，灵活性差且难以保持音质。  
- **既有问题**：  
  - 信号处理方法需手动调参，缺乏语义控制。  
  - 基于文本的扩散模型无法动态同步视觉内容，导致音画对齐不佳。  

2)  
- **核心方法**：提出AV-Edit框架，通过多模态联合控制实现细粒度音效编辑。  
  - **编码器设计**：采用改进的CAV-MAE-Edit，结合对比学习与掩码自编码，学习音视频对齐表征。  
    - 引入频谱图分段，提升音视频事件对齐精度。  
    - 通过音频混合策略，增强模型对视觉相关语义的提取能力。  
  - **生成模型**：基于多模态扩散变换器（MM-DiT），融合音、视、文三模态特征。  
    - 采用相关性特征门控训练策略，过滤视觉无关音频并生成缺失音效。  
    - 结合分类器无关引导技术，平衡生成质量与多样性。  

3)  
- **任务与效果**：  
  - **音效编辑**：在VGG-Edit数据集上实现添加、删除、替换操作，客观指标（FD/KL距离）与主观评分均最优。  
  - **音频生成**：在VGGSound测试中，音频质量（IS=22.48）、时序对齐（DeSync=0.441）达到SOTA水平。
</div>

</details>

---

## ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features
- **Authors**: Ye Bhone Lin, Thura Aung, Ye Kyaw Thu, Thazin Myint Oo
- **Categories**: cs.CL, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.21088v1](https://arxiv.org/abs/2511.21088v1)
- **PDF**: [https://arxiv.org/pdf/2511.21088v1](https://arxiv.org/pdf/2511.21088v1)

本文针对低资源缅甸语场景，研究基于序列到序列Transformer的自动语音识别纠错方法，重点探索了整合国际音标与对齐信息等特征策略。据我们所知，这是首个专门针对缅甸语的ASR纠错研究。通过评估五种ASR基础模型，发现所提出的纠错方法在词级和字符级准确率上均持续优于基线输出。融合音标与对齐特征的纠错模型将ASR模型的平均词错误率从51.56降至39.82（数据增强前）/43.59（数据增强后），同时将chrF++分数从0.5864提升至0.627。实验结果表明，在低资源环境下，纠错技术具有显著鲁棒性，且特征设计对提升ASR输出质量至关重要。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：缅甸语作为低资源语言，自动语音识别系统存在转录错误，影响下游任务性能。  
- **既有方法问题**：  
  - 传统方法依赖外部语言模型重评分，但效果有限。  
  - 现有研究多关注声学模型优化，缺乏针对ASR输出的后处理纠错方法。  
  - 缅甸语开源数据稀缺，且语音增强技术未充分应用于错误修正任务。  

2.  
- **核心方法**：提出基于序列到序列Transformer的ASR错误修正模型，结合语音特征与对齐信息。  
- **解决思路**：  
  - **语音特征集成**：使用国际音标转换模型提取IPA特征，通过多层感知机与词嵌入结合，增强模型对语音-文本关联的理解。  
  - **对齐约束**：采用fast-align工具生成词级对齐，作为解码器注意力分布的约束，减少幻觉错误。  
  - **数据增强**：对原始语音进行波形和频谱增强，生成合成ASR错误数据，提升模型鲁棒性。  
  - **模型架构**：Encoder-Decoder Transformer结构，支持多特征并行输入，平衡流畅性与准确性。  

3.  
- **任务与效果**：  
  - 在缅甸语通用领域ASR任务中，所提方法显著降低词错误率，如基线WER从51.56降至39.82（无增强）。  
  - 同时提升字符级F值，chrF++从0.5864提高至0.627。  
  - 在多规模Whisper及MMS模型上均取得稳定改进，验证了方法在低资源场景下的有效性。
</div>

</details>

---

## CartoonSing: Unifying Human and Nonhuman Timbres in Singing Generation
- **Authors**: Jionghao Han, Jiatong Shi, Zhuoyan Tao, Yuxun Tang, Yiwen Zhao, Gus Xia, Shinji Watanabe
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.21045v1](https://arxiv.org/abs/2511.21045v1)
- **PDF**: [https://arxiv.org/pdf/2511.21045v1](https://arxiv.org/pdf/2511.21045v1)

歌唱声音合成与歌声转换技术已在生成自然人类歌声方面取得显著进展，但现有系统受限于人类音色，难以合成人声范围外的声音，而这类需求在电子游戏、影视制作及虚拟角色等创意场景中日益增长。本文提出非人类歌声生成任务，涵盖非人类歌声合成与非人类歌声转换，旨在生成具有非人类音色特征且音乐连贯的歌声。该任务面临三大挑战：非人类歌声数据稀缺、符号对齐缺失，以及人声与非人声间的巨大音色差异。为此，我们提出统一框架CartoonSing，通过两阶段流水线整合歌唱合成与转换：首先利用标注人类歌声数据训练乐谱表征编码器，再通过音色感知声码器重构人类与非人类音频的波形。实验表明，CartoonSing不仅能成功生成非人类歌声、适配新音色，更将传统歌唱合成与转换技术拓展至富有创造力的非人类歌声生成领域。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**
- 传统歌唱合成与转换技术主要专注于生成自然的人类音色，优化与真实人声的感知差异。
- 在游戏、电影等创意应用中，常需生成非人类音色（如卡通、动物或机械声），但现有方法缺乏相关能力。
- 非人类歌唱生成面临数据稀缺、缺乏符号对齐及人声与非人声音色差异大等挑战。

2. **论文核心方法如何解决上述问题**  
CartoonSing提出统一的两阶段框架，整合歌唱合成与转换，并桥接人类与非人类歌唱生成：
- **阶段一：乐谱表示编码器**  
  - 使用标注的人类歌唱数据训练，从符号乐谱预测内容标记和基频轮廓。
  - 解决了非人类音频缺乏自然语音对齐的问题，通过人类数据提供可靠监督。
- **阶段二：音色感知声码器**  
  - 联合训练人类歌唱和非人类音频，从内容标记、基频轮廓和音色嵌入重建波形。
  - 通过多层级内容标记（基于自监督学习特征量化）和音色嵌入，捕捉跨域一致性。
  - 支持零样本合成与转换，泛化至新音色，无需非人类数据的显式标注。
- **关键设计**  
  - 分解内容、音色和基频信息，确保音乐连贯性。
  - 领域特定微调策略提升生成稳定性，结合无配对音色条件训练增强鲁棒性。

3. **在哪些任务上取得了怎样的效果**  
- **任务**：非人类歌唱合成与转换，涵盖乐器、鸟类和通用音频音色。
- **效果**：  
  - 在音色相似性上显著优于传统方法（如VISinger 2和SaMoye），客观指标（SIM-A）提升约20%。
  - 保持音高准确性和时序结构，微调后进一步改善生成质量与稳定性。
  - 同时支持人类歌唱重建，未引入性能退化，验证了跨域生成的实用性。
</div>

</details>

---

## RosettaSpeech: Zero-Shot Speech-to-Speech Translation from Monolingual Data
- **Authors**: Zhisheng Zheng, Xiaohang Sun, Tuan Dinh, Abhishek Yanamandra, Abhinav Jain, Zhu Liu, Sunil Hadap, Vimal Bhat, Manoj Aggarwal, Gerard Medioni, David Harwath
- **Categories**: eess.AS, cs.CL, cs.LG
- **arXiv**: [https://arxiv.org/abs/2511.20974v1](https://arxiv.org/abs/2511.20974v1)
- **PDF**: [https://arxiv.org/pdf/2511.20974v1](https://arxiv.org/pdf/2511.20974v1)

平行语音语料的稀缺严重制约了语音到语音翻译（S2ST）的发展，往往迫使系统依赖复杂的多阶段流程。本文提出RosettaSpeech——一种基于单语语音-文本数据并通过机器翻译监督增强的零样本S2ST创新框架。该方法在利用神经机器翻译模型内在语言学知识的同时，完全规避了对平行语音数据的依赖。我们的模型在训练阶段以文本作为中间桥梁，但在推理时可直接实现端到端的语音转换。这种简洁架构在标准基准测试中取得了领先性能：在CVSS-C测试集上，德语到英语翻译的ASR-BLEU得分达25.17，西班牙语到英语达29.86，相对现有最优系统分别提升27%和14%。此外，我们验证了单一模型可实现多对一翻译（法/西/德→英）的强劲表现，并首次系统分析了训练数据规模对模型性能的影响。通过重点利用易获取的平行文本而非稀缺的平行语音数据，本方法为构建更多语言的高质量、保留说话人特征的S2ST系统提供了可扩展路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音到语音翻译(S2ST)技术面临平行语音语料库稀缺的瓶颈，尤其对低资源语言影响显著。  
- **既有方法问题**：  
  - 传统级联系统依赖自动语音识别、机器翻译和文本到语音模块串联，导致错误传播、高延迟，且无法保留源语音的副语言信息（如音色、情感）。  
  - 端到端模型需大量平行语音数据训练，数据获取成本高、覆盖语言有限；现有无监督方法多依赖复杂多阶段流程或专用架构，难以扩展。  

2)  
- **核心方法**：RosettaSpeech提出基于单语语音-文本数据的零样本S2ST框架，通过机器翻译监督构建伪平行语料，无需任何源-目标平行语音对。  
- **解决路径**：  
  - **数据构建**：分别从源语言单语数据（语音+文本）和目标语言单语数据出发，利用现成神经机器翻译模型生成伪目标文本或伪源文本，形成三元组（源语音、源文本、伪目标文本）或（伪源文本、目标文本、目标语音），并通过COMET指标过滤低质量翻译对。  
  - **模型架构**：  
    - 使用Whisper-medium编码器提取源语音特征，CosyVoice2语音分词器将目标语音离散化为语义令牌。  
    - 以Qwen3-0.6B大语言模型为核心骨干，利用其多语言理解能力。  
    - 设计多头投影层，将LLM隐藏状态分别映射至文本令牌和语音令牌空间，支持多模态输出。  
  - **训练机制**：  
    - 联合训练语音到文本翻译（输入源语音，输出伪目标文本）和文本到语音翻译（输入伪源文本，输出目标文本与语音），通过交叉熵损失优化。  
    - 避免多阶段训练的灾难性遗忘问题，确保模型同步掌握跨模态映射能力。  
  - **推理流程**：直接输入源语音，自回归生成目标文本与语音令牌，再通过条件流匹配模型和HiFi-GAN声码器合成最终语音，支持说话人身份控制。  

3)  
- **任务与效果**：在CVSS-C基准测试中，针对法语、西班牙语、德语到英语的翻译任务：  
  - 零样本设置下，ASR-BLEU分数达27.86（法→英）、29.86（西→英）、25.17（德→英），相比此前最优系统相对提升超14%（西→英）和27%（德→英）。  
  - 单一模型支持多对一翻译（法/西/德→英），且微调少量平行数据后性能进一步提升（如德→英ASR-BLEU从25.17升至29.90）。  
  - 在跨语言说话人相似性评估中，显著优于CVSS-T基线，证明其有效保留源语音的副语言特征。
</div>

</details>

---

## Towards Audio Token Compression in Large Audio Language Models
- **Authors**: Saurabhchand Bhati, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass
- **Categories**: eess.AS, cs.AI, cs.CL
- **arXiv**: [https://arxiv.org/abs/2511.20973v1](https://arxiv.org/abs/2511.20973v1)
- **PDF**: [https://arxiv.org/pdf/2511.20973v1](https://arxiv.org/pdf/2511.20973v1)

大型音频语言模型（LALMs）在语音识别与通用音频理解等多样化任务中展现出卓越性能，但其可扩展性受限于注意力机制的二次复杂度及音频信号的高令牌率。这些挑战使得LALMs难以应用于长音频场景及边缘设备等资源受限平台。

本文探索了无监督分割、均匀平均池化等技术，旨在减少LALM音频编码器生成（但尚未被LLM解码器处理）的音频令牌数量。为缓解压缩表征可能引发的性能衰减，我们采用低秩适配器对模型进行微调。通过在自动语音识别和语音间翻译两项任务上的评估——这些任务依赖于有效揭示输入信号的底层词汇内容——我们系统研究了降采样操作的影响。实验表明：压缩后的LALMs在将输入音频令牌数削减至LLM骨干网络处理前的三分之一时，仍能实现接近帧级LALMs的性能水平。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**  
- 大型音频语言模型（LALMs）在音频理解任务中表现优异，但面临两大挑战：  
  - 注意力机制的二次复杂度限制了长音频处理能力。  
  - 音频编码器生成高频率特征（如每秒50个标记），导致输入序列过长，计算成本高。  
- 现有方法如均匀池化或语音转文本虽能减少标记数量，但存在缺陷：  
  - 池化后标记数仍显著高于文本标记。  
  - 转文本会丢失副语言信息（如说话人身份、情感等）。  

2)  
**论文核心方法如何解决上述问题**  
- **音频标记压缩技术**：通过以下方法减少输入LLM的音频标记数量：  
  - **无监督分割**：基于相邻帧的相似性检测边界，合并帧级特征为段级特征，保留语义内容的同时降低标记数。  
  - **均匀平均池化**：对特征进行滑动窗口平均池化，合并信息而非直接丢弃。  
  - **均匀采样**：定期选择特征帧，但实验表明其效果不如池化。  
- **低秩适配器（LoRA）微调**：  
  - 压缩后的特征与LLM输入空间不匹配，通过添加LoRA（仅调整注意力层的查询和键投影）重新对齐特征表示。  
  - LoRA仅引入约900万参数，大幅降低训练成本，且能恢复接近原始模型的性能。  
- **任务适应性设计**：  
  - 聚焦语音识别和语音翻译任务，确保压缩后仍能准确提取词汇内容。  
  - 通过多语言数据集（如CommonVoice、CoVoST）验证方法的泛化性。  

3)  
**在哪些任务上取得了怎样的效果**  
- **任务范围**：  
  - 自动语音识别（ASR）和语音到文本翻译（S2TT）。  
- **性能表现**：  
  - 标记数减少至1/2时，词错误率（WER）增长小于1，BLEU分数略有下降。  
  - 标记数减少至1/3时，WER增长仍低于1.5，且无监督分割和均匀池化优于均匀采样。  
  - 压缩后模型在语义保留和可读性评分（通过ChatGPT评估）上接近原始模型。
</div>

</details>

---

## SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications
- **Authors**: Jionghao Han, Jiatong Shi, Masao Someki, Yuxun Tang, Lan Liu, Yiwen Zhao, Wenhao Feng, Shinji Watanabe
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.20972v1](https://arxiv.org/abs/2511.20972v1)
- **PDF**: [https://arxiv.org/pdf/2511.20972v1](https://arxiv.org/pdf/2511.20972v1)

随着自动语音识别、大语言模型与文本转语音技术的进步，口语对话系统已得到广泛应用。然而现有系统大多局限于常规语音应答。本文提出SingingSDS——一种通过歌唱而非说话进行应答的级联式口语对话系统，可在角色扮演和互动娱乐场景中建立更具感染力、记忆点与趣味性的人机交互。该系统采用模块化ASR-LLM-SVS架构，支持角色设定、ASR/LLM后端、歌声合成模型、旋律来源及音色配置的灵活组合，可根据延迟、质量与音乐风格的差异化需求进行定制。系统提供即插即用的网页演示界面，其模块化开源代码支持功能扩展与定制化开发。演示地址：https://huggingface.co/spaces/espnet/SingingSDS 代码仓库：https://github.com/SingingSDS/SingingSDS

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现有口语对话系统（SDS）在角色扮演和互动娱乐中广泛应用，但响应形式局限于常规语音，缺乏情感表达和艺术吸引力。  
- **既有问题**：传统SDS无法通过歌唱增强交互的沉浸感；同时，歌唱合成技术（SVS）多基于预定义歌词，缺乏动态响应用户输入的能力。  

2)  
- **模块化流程**：构建ASR-LLM-SVS级联管道，用户语音经ASR转文本，LLM生成角色化歌词，SVS结合旋律合成歌唱响应。  
- **旋律控制**：支持随机生成或从数据集检索旋律，提供音节对齐策略（如一对一或保留原曲多音节点），确保歌词与节奏匹配。  
- **灵活配置**：集成5种ASR模型、7种LLM及多语言SVS模型，支持350种系统组合，兼顾延迟、质量与风格需求。  
- **无需微调**：通过提示模板约束LLM输出音节数，实现歌词与乐句结构对齐，避免模型重构。  

3)  
- **任务场景**：在角色扮演对话和互动娱乐（如虚拟演唱会、音乐游戏）中测试。  
- **效果**：  
  - 最佳配置（Whisper+Gemini）在歌唱质量（SingMOS≈4.59）和用户体验（新颖性、角色一致性≥4.0）上表现优异；  
  - Paraformer+LLaMA配置延迟最低，适合实时交互；  
  - 人类评估证实歌唱响应显著提升娱乐性，且不影响语音清晰度（PER<1%）。
</div>

</details>

---
