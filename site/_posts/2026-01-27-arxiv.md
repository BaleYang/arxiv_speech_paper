---
layout: post
title: "arXiv Daily – 2026-01-27"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-27（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-26 08:50 — 2026-01-27 08:50
- 抓取总数：19 篇 | 本页显示：19 篇（去重/过滤后）

## Learning to Discover: A Generalized Framework for Raga Identification without Forgetting
- **Authors**: Parampreet Singh, Somya Kumar, Chaitanya Shailendra Nitawe, Vipul Arora
- **Categories**: eess.AS, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.18766v1](https://arxiv.org/abs/2601.18766v1)
- **PDF**: [https://arxiv.org/pdf/2601.18766v1](https://arxiv.org/pdf/2601.18766v1)

印度艺术音乐中的拉格识别仍面临挑战，主要原因在于大量极少演奏的拉格未出现在现有训练数据集中。传统分类模型在此场景下表现不佳，因其假设已知类别构成封闭集合，导致无法识别或有效归类未知拉格。近期研究尝试对未知拉格进行分类，但遭遇灾难性遗忘问题——模型对已学习拉格的识别能力会显著下降。为解决该问题，我们提出一种统一学习框架，同时利用标注与未标注音频数据，使模型能够在保持已知拉格识别能力的同时，自主发现与未知拉格对应的连贯类别。我们在拉格识别基准数据集上验证模型性能，展示其对已知、未知及全部拉格类别的分类效果。该方法即使在发现未知拉格类别方面，也超越了此前基于新类别发现（NCD）的流程，为印度艺术音乐任务的表征学习提供了新思路。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：印度艺术音乐中的拉格识别面临挑战，因为大量罕见拉格在训练数据中缺失。传统分类模型假设封闭类别集，无法识别或有效组织未见拉格。  
- **既有方法问题**：  
  - 无监督聚类等方法忽略已知类别结构，无法利用已有监督信息。  
  - 新类发现方法仅针对未见类别训练，导致对已见拉格的“灾难性遗忘”，即模型遗忘已学知识。

2)  
- **核心框架**：采用广义类别发现框架，通过联合优化监督与非监督对比学习目标，在共享嵌入空间中同时利用标注和未标注数据。  
- **方法细节**：  
  - **特征提取**：使用CNN-LSTM模型预训练于标注数据，提取固定长度嵌入。  
  - **编码器训练**：通过自注意力编码器细化嵌入，结合两种损失：  
    - **监督对比损失**：基于标注数据的类别标签构建正负样本对，增强类间区分。  
    - **非监督对比损失**：利用音频来源标识（同一录音片段共享拉格标签）构建正负对，适用于所有数据（忽略标注）。  
  - **联合优化**：平衡参数λ加权两种损失，使模型在保持已见拉格表征的同时，发现未见拉格的连贯类别。  
  - **聚类策略**：训练后使用K-Means或余弦相似度阈值聚类，识别已知与未见类别。  
- **解决思路**：该统一框架避免将标注与未标注数据视为独立集合，从而缓解灾难性遗忘，并利用数据内在结构提升拉格发现能力。

3)  
- **任务与效果**：在PIM和Saraga数据集上评估拉格识别与发现任务：  
  - **已见拉格**：在PIM数据集上，旧类别准确率达91.16%，优于基线（79.24%），显著缓解遗忘。  
  - **未见拉格**：在PIM的新类别上准确率提升至84.68%，超过基线（79.34%），证明发现能力增强。  
  - **整体性能**：全部类别准确率达77.49%（PIM）和74.80%（Saraga），在归一化互信息、调整兰德指数等指标上均优于基线与非监督对比学习方法。
</div>

</details>

---

## Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings
- **Authors**: Aayush M. Shrestha, Aditya Bajracharya, Projan Shakya, Dinesh B. Kshatri
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.18694v1](https://arxiv.org/abs/2601.18694v1)
- **PDF**: [https://arxiv.org/pdf/2601.18694v1](https://arxiv.org/pdf/2601.18694v1)

本研究提出了一种面向尼泊尔语使用者的少样本语音克隆系统，该系统能够利用少量数据，将天城体文本合成为特定说话人语音。由于尼泊尔语属于低资源语言，其语音克隆研究尚处于探索阶段。为此，我们构建了两个独立数据集：用于训练说话人编码器的无标注音频数据，以及用于训练基于Tacotron2合成器的文本-音频配对数据。说话人编码器通过生成式端到端损失函数优化，生成能够捕捉说话人声学特征的嵌入向量，并利用均匀流形逼近与投影（UMAP）进行降维可视化验证。这些嵌入向量与Tacotron2的文本嵌入融合后生成梅尔频谱图，再通过WaveRNN声码器转换为音频。音频数据来源多样（包括自主录制），并经过严格的质量与对齐预处理。系统在多种超参数设置下，采用梅尔损失函数和门控损失函数进行训练。实验表明，该系统能有效克隆未见过的说话人声学特征，验证了尼泊尔语少样本语音克隆的可行性，为低资源场景下的个性化语音合成奠定了基础。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音克隆技术在高资源语言（如英语）中已取得显著进展，但在低资源语言（如尼泊尔语）中研究匮乏。尼泊尔语具有丰富的方言和文化多样性，但现有语音合成系统多为单说话人，缺乏多说话人克隆能力。
- **既有方法问题**：  
  - 现有尼泊尔语TTS系统（如Shruti、Aawaj）仅支持单说话人合成，无法克隆新说话人声音。  
  - 早期尼泊尔语语音克隆尝试（如使用WaveNet）依赖有限公开数据，且未进行多说话人建模。  
  - 多数先进语音克隆模型集中于高资源语言，缺乏针对低资源语言的适配。

2)  
论文提出一个基于少样本学习的多说话人尼泊尔语语音克隆系统，核心方法如下：  
- **系统架构**：采用三阶段流水线——说话人编码器、合成器（基于Tacotron2）和声码器（WaveRNN）。  
- **说话人编码器**：  
  - 使用三层LSTM网络提取256维说话人嵌入向量。  
  - 通过GE2E损失函数优化，确保嵌入能有效区分不同说话人。  
  - 利用UMAP降维可视化验证嵌入的聚类效果，显示其能区分性别和个体。  
- **合成器**：  
  - 基于Tacotron2，将Devanagari文本编码与说话人嵌入拼接，生成梅尔频谱图。  
  - 在有限配对数据（约8.67小时）上微调，注重音频质量与文本对齐。  
- **声码器**：  
  - 使用预训练WaveRNN模型，将梅尔频谱图转换为时域音频波形。  
- **数据处理**：  
  - 构建两个独立数据集：说话人编码器使用833人、235小时未转录音频；合成器使用6,046条配对文本-音频数据。  
  - 进行音频标准化、降噪、分段及文本规范化（如数字转写、句子分割）等预处理。  
- **关键创新**：  
  - 针对低资源场景，结合公开与自采数据，平衡数据量与质量。  
  - 通过说话人嵌入实现少样本克隆，仅需少量目标说话人音频即可生成个性化语音。

3)  
- **任务**：尼泊尔语多说话人少样本语音克隆。  
- **效果**：  
  - 说话人编码器EER低于0.04，嵌入余弦相似度均值达0.90，UMAP可视化显示清晰聚类。  
  - 合成语音平均MOS得分为3.924（质量）和3.87（相似度），优于先前尼泊尔语克隆研究。  
  - 系统在10位未见说话人上验证有效，支持男女声克隆，为低资源语言语音合成提供了可行方案。
</div>

</details>

---

## Audio Inpainting in Time-Frequency Domain with Phase-Aware Prior
- **Authors**: Peter Balušík, Pavel Rajmic
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18535v1](https://arxiv.org/abs/2601.18535v1)
- **PDF**: [https://arxiv.org/pdf/2601.18535v1](https://arxiv.org/pdf/2601.18535v1)

时域音频修复问题旨在估计信号中缺失的样本段。多年来，针对此类音频修复已发展出多种方法。与此不同，学界还出现了时频域修复的变体，其挑战在于利用可靠信息重建缺失的频谱图列。本文提出一种解决时频音频修复问题的方法。该方法基于近期提出的相位感知信号先验，该先验利用了瞬时频率估计。我们构建了优化问题，并采用广义Chambolle-Pock算法进行求解。通过客观与主观评估，将所提方法与其它时频修复方法（特别是深度先验神经网络和基于自回归的Janssen-TF方法）进行对比。实验表明，本文方法在客观评估和听音测试中均优于对比方法，且计算需求显著低于其他方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频修复旨在填补音频信号中的缺失部分。时域修复方法（如基于稀疏性或自回归的方法）已较为成熟，但针对时频域（如频谱图）的修复研究相对较少。时频域修复需填补缺失的频谱图列，现有方法包括基于深度先验的DPAI和基于自回归的Janssen-TF。  
- **既有问题**：传统基于ℓ₁范数最小化的稀疏性方法存在“能量损失”问题，即重构信号在缺失区域中心幅度下降；同时，这些方法未能充分考虑时频域中正弦分量间的时序关联性。

2)  
- **核心方法**：本文提出U-PHAIN-TF方法，将相位感知先验（iPCTV）引入时频域修复。该方法通过优化问题最小化iPCTV惩罚项，以同时促进频谱图的稀疏性和时间连续性。  
- **技术细节**：  
  - **相位校正**：利用瞬时频率估计对频谱图相位进行校正，使正弦分量在时频表示中更稳定。  
  - **总变分约束**：对校正后的频谱图施加时间方向的总变分惩罚，增强分量连续性。  
  - **优化求解**：将问题形式化为凸优化问题，采用广义Chambolle-Pock算法求解，并定期更新瞬时频率估计以提升精度。  
- **优势**：  
  - 直接针对时频域设计，避免了传统方法中的能量损失。  
  - 通过相位感知先验更好地建模信号结构，提升修复质量。  
  - 计算效率显著高于对比的深度学习方法。

3)  
- **任务**：在时频域音频修复任务中，填补频谱图中整列缺失的系数（缺口长度对应1至6列）。  
- **效果**：  
  - 在DPAI和IRMAS数据集上，U-PHAIN-TF在客观指标（ODG和SNR）上均优于DPAI和Janssen-TF，尤其在ODG上优势显著。  
  - 主观听力测试（MUSHRA）中，其评分中位数最高，且与参考方法差异具有统计显著性。  
  - 计算耗时仅为0.1–0.7分钟，远低于DPAI（19分钟）和Janssen-TF（5–9分钟）。
</div>

</details>

---

## Geneses: Unified Generative Speech Enhancement and Separation
- **Authors**: Kohei Asai, Wataru Nakata, Yuki Saito, Hiroshi Saruwatari
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.18456v1](https://arxiv.org/abs/2601.18456v1)
- **PDF**: [https://arxiv.org/pdf/2601.18456v1](https://arxiv.org/pdf/2601.18456v1)

现实音频录音常包含多说话人及多种退化现象，这限制了构建先进语音处理模型时可用的数据数量与质量。尽管将语音增强与语音分离串联以获取各说话人纯净语音信号的端到端方法前景广阔，但传统串联方法难以处理加性噪声之外的复杂退化问题。为此，我们提出\textbf{Geneses}——一个实现高质量一体化语音增强与分离的生成式框架。该方法基于隐空间流匹配技术，通过以含噪混合信号的自监督学习表示为条件的多模态扩散Transformer，估计各说话人的纯净语音特征。我们在LibriTTS-R双说话人混合数据集上开展实验，涵盖纯加性噪声与复杂退化两种条件。结果表明，Geneses在多项客观指标上显著优于传统基于掩码的串联方法，且对复杂退化具有强鲁棒性。音频示例已发布于演示页面。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现实音频常包含多说话人重叠及多种复杂退化（如噪声、混响、编解码失真、丢包），限制了高质量语音数据的获取。  
- **既有方法问题**：传统方法（如Hu等人提出的级联SE-SS）主要针对“仅加性噪声”场景，基于掩码的SE-SS难以处理复杂退化，在真实环境中适用性受限。

2)  
- **核心框架**：提出**Geneses**，一个统一的生成式语音增强与分离框架。它利用**隐空间流匹配**，通过多模态扩散Transformer预测每个说话人的干净语音特征。  
- **关键组件**：  
  - **特征提取器**：使用预训练自监督模型w2v-BERT 2.0从退化混合语音中提取鲁棒特征。  
  - **隐空间建模**：通过VAE将音频压缩到低维隐空间，流匹配在该空间中进行。  
  - **流预测器**：基于MM-DiT架构，融合VAE隐变量、SSL特征及时步信息，预测指向目标干净表示的向量场。  
- **训练与推理**：  
  - 训练时最小化预测向量场与目标向量场的MSE损失，无需排列不变训练。  
  - 推理时从高斯分布采样，通过ODE求解器沿预测流场迭代得到分离后的干净隐表示，再由VAE解码为波形。  
- **优势**：生成式方法能处理退化到干净信号的非唯一映射问题，对复杂退化具有强鲁棒性；统一框架避免了级联方法的误差累积。

3)  
- **任务与效果**：在LibriTTS-R双说话人混合数据集上测试，包含“仅加性噪声”和“复杂退化”两种条件。  
- **客观指标**：在复杂退化条件下，Geneses在DNSMOS、NISQA、UTMOSv2等主观质量预测指标上接近真实干净语音，显著优于传统方法；WER从传统方法的5.54大幅降至0.43。  
- **综合表现**：在语音可懂度、频谱失真、说话人相似度等参考感知指标上全面领先，尤其在复杂退化场景下传统方法失效时，Geneses仍能实现高质量分离与增强。
</div>

</details>

---

## 3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control
- **Authors**: Xuanmeng Sha, Liyun Zhang, Tomohiro Mashita, Naoya Chiba, Yuki Uranishi
- **Categories**: cs.CV, cs.AI, cs.LG, cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18451v1](https://arxiv.org/abs/2601.18451v1)
- **PDF**: [https://arxiv.org/pdf/2601.18451v1](https://arxiv.org/pdf/2601.18451v1)

现有基于部件分解或帧级回归的方法在生成融合全身运动与面部表情的协同语音手势时，常出现身体动作语义不协调及空间不稳定无意义运动的问题。为此，我们提出3DGesPolicy——一种基于动作控制的新型框架，通过机器人学中的扩散策略将整体手势生成重构为连续轨迹控制问题。该方法将帧间变化建模为统一的整体动作，有效学习帧间整体手势运动模式，确保生成空间与语义协调的运动轨迹，并符合真实运动流形。为进一歩提升表达对齐效果，我们设计了手势-音频-音素融合模块，能够深度融合并优化多模态信号，实现语音语义、身体运动与面部表情之间的结构化细粒度对齐。在BEAT2数据集上的大量定量与定性实验表明，3DGesPolicy在生成自然、富有表现力且与语音高度对齐的整体手势方面，优于现有先进方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：生成融合全身动作与面部表情的协同语音手势，对虚拟助手、社交机器人等应用至关重要。现有方法主要分为两类：
  - **基于部件分解的方法**：使用离散码本预测身体部位动作，但量化会削弱部位间协调性与细节，导致语义不连贯。
  - **基于帧级回归的方法**：使用扩散模型直接回归手势序列，虽保留细节与多样性，但对噪声敏感且依赖数据分布，易产生语义无意义、空间不稳定的浮动运动。
- **核心问题**：两类方法均常忽视三维空间中的空间运动与交互，导致整体手势在语义协调性与运动稳定性上存在不足。

2)  
论文提出 **3DGesPolicy** 框架，通过两大核心设计解决上述问题：

- **将手势生成重新定义为基于动作的轨迹控制问题**：
  - 受机器人学中扩散策略的启发，将帧间变化建模为统一的**整体动作**，而非预测绝对姿态或离散身体部件代码。
  - 通过训练一个视觉运动策略，根据当前运动状态、音频和音素级语义生成手势运动轨迹。
  - 这种**轨迹控制范式**使模型能够学习三维空间中语义感知的运动模式，确保运动在空间和语义上连贯，并遵循真实的运动流形，从而产生稳定、接地气的自然运动。

- **提出 Gesture-Audio-Phoneme (GAP) 融合模块以实现细粒度多模态对齐**：
  - **模块设计**：使用独立的编码器处理音频和音素特征，然后通过跨模态注意力机制将它们与手势表征进行对齐和融合。
  - **核心作用**：
    - 确保生成的动作直接受到**音素级**语言语义的引导。
    - 防止产生过多或不相关的无意义运动。
    - 促进身体动作与面部表情之间与语音对齐的协调，实现细粒度的音素级同步。
  - 该模块解决了语义对齐的差距，是生成自然、富有表现力结果的关键。

- **整体架构与训练**：
  - 框架包含环境、感知和决策三个模块，形成闭环策略学习系统。
  - 通过扩散过程的去噪来迭代优化动作序列。
  - 训练目标结合了扩散损失、重建损失和速度损失，共同确保动作的多样性、准确性和运动平滑性。

3)  
- **任务**：在 **BEAT2** 数据集上进行了全面的协同语音整体手势生成评估，涵盖全身动作与面部表情。
- **效果**：
  - **定量评估**：在 Fréchet Gesture Distance (FGD)、Diversity (DIV)、Mean Squared Error (MSE) 和 Lip Vertex Distance (LVD) 等多个指标上均优于现有最先进方法，尤其在 FGD 和 LVD 上优势显著，表明其在生成稳定、语义有意义运动以及精确的唇语同步方面表现突出。
  - **定性评估**：生成的全身手势更自然、多样且与语音语义强对齐，减少了不自然的滑动伪影；面部动画能准确捕捉细微的音素特征（如闭唇音），实现了优越的唇语同步。
  - **用户研究**：在自然度、语音同步性和情感表现力方面，均获得比对比方法更高的用户偏好。
</div>

</details>

---

## UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment
- **Authors**: Wei Wang, Wangyou Zhang, Chenda Li, Jiahe Wang, Samuele Cornell, Marvin Sach, Kohei Saijo, Yihui Fu, Zhaoheng Ni, Bing Han, Xun Gong, Mengxiao Bi, Tim Fingscheidt, Shinji Watanabe, Yanmin Qian
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18438v1](https://arxiv.org/abs/2601.18438v1)
- **PDF**: [https://arxiv.org/pdf/2601.18438v1](https://arxiv.org/pdf/2601.18438v1)

随着现代语音生成系统的不断发展，自动语音质量评估的重要性日益凸显，而人工听音测试仍存在成本高、耗时长且难以规模化的问题。现有大多数基于学习的评估模型主要依赖稀缺的人工标注平均意见分（MOS）数据，这限制了模型的鲁棒性和泛化能力，尤其是在跨异构数据集训练时。本研究提出UrgentMOS，一个统一的语音质量评估框架，能够联合学习多样化的客观与感知质量指标，同时在训练过程中显式容忍任意子集指标的缺失。通过利用异构监督下的互补质量维度，UrgentMOS实现了对部分标注数据的有效利用，并在大规模多源数据集训练中提升了鲁棒性。除绝对分数预测外，UrgentMOS通过直接预测比较性MOS（CMOS）显式建模成对质量偏好，使其特别适用于系统基准测试中常用的基于偏好的评估场景。在涵盖模拟失真、语音增强和语音合成的广泛语音质量数据集上的大量实验表明，UrgentMOS在绝对评估和比较性评估设置中均持续取得最先进的性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：随着语音生成系统快速发展，自动语音质量评估变得日益重要，但人工听测成本高、耗时长且难以扩展。
- **既有方法问题**：
  - 现有基于学习的评估模型主要依赖稀缺的人工标注平均意见分数（MOS）数据，限制了模型的鲁棒性和泛化能力。
  - 在跨异构数据集训练时，MOS标注在不同数据集间存在协议和听众差异，导致对齐困难。
  - 绝对评分（ACR）设置下，分数饱和和听众间变异性削弱了判别力，难以捕捉现代高质量语音系统间的细微质量差异。

2) **论文核心方法如何解决上述问题**
UrgentMOS通过一个统一的框架，结合多指标监督和偏好学习，以解决上述问题。

- **多指标联合学习与缺失容忍**：
  - 模型同时学习多种客观和感知质量指标（如PESQ、DNSMOS、UTMOS等），这些指标被组织成语义连贯的组别，覆盖噪声失真、自然度、可懂度等不同质量维度。
  - 关键创新在于训练时明确容忍任意指标子集的缺失。通过有效性掩码，损失函数仅对可用的标注进行计算，使得模型能够有效利用大规模、多来源但标注不完全的数据集进行训练，从而提升鲁棒性和泛化能力。

- **显式偏好学习**：
  - 除了绝对分数预测，模型通过自然度条件偏好模块（NCPM）直接建模成对样本的质量偏好，预测比较MOS（CMOS）。
  - NCPM利用来自自然度相关指标组的表征，通过交叉注意力机制对输入对进行交互建模，输出偏好标签或CMOS。
  - 为了缓解偏好标注数据稀缺的问题，模型从现有的ACR（绝对评分）数据集中，基于MOS差异和可调的平局阈值，构造出成对的偏好监督信号，并采用对称对构造以鼓励顺序不变性。

- **统一架构与灵活训练**：
  - 模型采用模块化设计，包含共享的特征提取器、绝对指标预测模块（AMPM）和NCPM，支持绝对和比较评估的统一训练。
  - 特征提取器融合了多种预训练音频编码器的多分辨率表征，增强了模型的表示能力。
  - 通过范围约束激活函数确保预测值符合各指标的有效数值范围，同时保持梯度连续以优化稳定。

3) **在哪些任务上取得了怎样的效果**
- **评估任务**：在广泛的语音质量数据集上进行了实验，涵盖**模拟失真**、**语音增强**和**语音合成**等多个领域。
- **取得的效果**：
  - **偏好评估**：在多个测试集（如SOMOS、TMHINT-QI、URGENT24-SQA、SpeechEval、SpeechJudge）的成对偏好预测任务中，UrgentMOS consistently outperforms strong baselines。例如，在SpeechEval上准确率超过80%，显著优于DNSMOS、UTMOS等基线模型。
  - **绝对评分评估**：在具有MOS标注的数据集（如SOMOS、BVCC、NISQA-FOR）上，预测分数与真实MOS的Pearson和Spearman相关系数达到最先进水平，展示了优异的跨领域鲁棒性。
  - 总体而言，UrgentMOS在**绝对和比较评估**两种设置下均实现了**state-of-the-art**的性能。
</div>

</details>

---

## Pisets: A Robust Speech Recognition System for Lectures and Interviews
- **Authors**: Ivan Bondarenko, Daniil Grebenkin, Oleg Sedukhin, Mikhail Klementev, Roman Derunets, Lyudmila Budneva
- **Categories**: cs.CL, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.18415v1](https://arxiv.org/abs/2601.18415v1)
- **PDF**: [https://arxiv.org/pdf/2601.18415v1](https://arxiv.org/pdf/2601.18415v1)

本研究面向科研人员与新闻工作者，提出了一种名为“Pisets”的语音转文本系统。该系统采用三模块架构，旨在提升语音识别准确率，同时减少基于Whisper模型常见的误识别与幻觉现象。架构包含三个核心环节：首先基于Wav2Vec2进行初始识别，随后通过音频谱图变换器（AST）进行误报过滤，最终利用Whisper完成语音识别。通过采用课程学习方法并整合多样化的俄语语音语料库，系统性能得到显著提升。此外，引入先进的不确定性建模技术进一步优化了转录质量。与WhisperX及原始Whisper模型相比，本系统提出的方法能够在不同声学条件下对长时音频数据实现更稳健的转录。Pisets系统的源代码已在GitHub开源：https://github.com/bond005/pisets。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：针对讲座和访谈等长音频转录，现有端到端模型（如Whisper）虽能生成语法连贯的文本，但仍存在**幻觉**（生成语义不合理内容）和**计算效率低**（自回归解码延迟高）的问题。  
- **既有方法局限**：后续改进的WhisperX通过优化解码和语音活动检测（VAD）提升效率，但其VAD基于固定阈值，在复杂声学条件下**准确性不足**，且缺乏对误检片段的过滤机制，**幻觉问题未根本解决**。

2)  
论文提出名为“Pisets”的三组件架构，通过分阶段处理提升鲁棒性并减少错误：  
- **Wav2Vec2进行初步识别与分段**：替代传统VAD，利用其在大规模音频数据上训练得到的上下文理解能力，更精确地检测语音片段边界。采用课程学习策略，从清晰标注数据逐步过渡到含口音、噪声的复杂数据，增强对俄语等语言的适应性。  
- **AST进行误报过滤**：在Wav2Vec2输出后，引入音频谱图变换器（AST）对片段进行二次分类，滤除非语音片段（如噪声误判为语音），减少输入后续阶段的错误数据。  
- **Whisper完成最终转录**：对过滤后的片段使用Whisper进行转录，并通过贝叶斯不变风险最小化（BIRM）算法及构建高质量标注语料库进行微调，提升准确性和对声学变化的鲁棒性。  
- **不确定性建模与校验**：额外引入一致性检查机制，对比Whisper与Wav2Vec2的输出差异，识别不确定词汇；同时利用Whisper的词元概率或音频拉伸后的预测分歧来量化不确定性，辅助错误定位和后处理。

3)  
- **任务**：俄语长音频转录，特别是科学讲座、访谈等场景，并在安静及高噪声（信噪比1dB）条件下测试。  
- **效果**：在7段20-40分钟俄语讲座数据集上，相比WhisperX，Pisets在词错误率（WER）上显著降低（0.1065 vs. 0.1683），语义相似度（BERT-F1）更高（0.9652 vs. 0.9479）。在强噪声环境中，其微调模型（Whisper-Podlodka）优于原始Whisper-Large。不确定性建模能通过标记约5%的不确定词汇捕获35%的转录错误，提升实用性和可修正性。
</div>

</details>

---

## Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder
- **Authors**: Zhengyang Li, Thomas Graave, Björn Möller, Zehang Wu, Matthias Franz, Tim Fingscheidt
- **Categories**: eess.AS, cs.CL, cs.CV, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18396v1](https://arxiv.org/abs/2601.18396v1)
- **PDF**: [https://arxiv.org/pdf/2601.18396v1](https://arxiv.org/pdf/2601.18396v1)

在视听自动语音识别（AV-ASR）系统中，将视觉特征融合至预训练ASR模型已被证明是提升噪声鲁棒性的有效方法。本研究基于先进的Whisper ASR模型，首先提出一种简洁高效的视觉融合方法——在编码器和解码器中同时利用视觉特征（双路使用），以学习编码器中的视听交互并在解码器中权衡多模态信息。其次，我们比较了不同规模Whisper模型中的视觉融合方法。所提出的双路使用法在噪声鲁棒性上展现出持续改进，例如在0dB信噪比的混响噪声环境中，基于Whisper small模型相对典型的中层融合基准实现了35%的相对提升（词错误率：4.41% vs. 6.83%），基于Whisper medium模型则实现57%的相对提升（词错误率：4.07% vs. 9.53%）。第三，我们通过消融实验探究了不同模块设计与融合策略的影响。经过1929小时视听数据的微调，采用Whisper medium的双路使用方法在LRS3 AV-ASR基准测试中，于不同信噪比下分别取得4.08%（MUSAN混响噪声）和4.43%（NoiseX混响噪声）的平均词错误率，从而在噪声环境下确立了新的最优性能。代码已开源：https://github.com/ifnspaml/Dual-Use-AVASR

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：视听自动语音识别（AV-ASR）系统利用唇部视觉信息提升噪声鲁棒性，在汽车、智能眼镜等真实场景中具有应用价值。  
- **既有方法问题**：  
  - **早期融合**：将视听特征在编码器早期结合，能有效建模交互，但应用于大型预训练模型（如Whisper）时，梯度难以有效回传，导致训练困难。  
  - **中间融合**：在解码器（如Whisper解码器）中融合视听特征，虽能利用先进解码器的潜力，但未在编码器中建模视听交互，导致噪声鲁棒性不足。

2)  
论文提出**双重使用视觉特征（dual-use）**方法，在Whisper编码器和解码器中均引入视觉特征，以同时建模视听交互并实现模态感知解码。具体解决方案如下：  
- **在编码器中的使用（早期融合变体）**：  
  - 从AV-HuBERT编码器提取视觉特征，经上采样和线性投影后，与Whisper编码器的音频特征相加。  
  - 通过**零初始化的可训练标量**控制视觉特征的注入，实现平滑的微调启动，使编码器能在注意力层中有效学习视听交互。  
- **在解码器中的使用（中间融合增强）**：  
  - 在Whisper解码器的每个解码块前插入**Flamingo块**，该块通过多头交叉注意力机制处理视觉特征。  
  - Flamingo块采用零初始化门控机制，使解码器能根据上下文和噪声条件动态权衡音频与视觉模态的重要性。  
- **方法优势**：  
  - 克服了纯早期融合在大型模型中的训练难题，同时弥补了纯中间融合缺乏编码器交互的缺陷。  
  - 仅需少量参数增加（主要来自Flamingo块），即可显著提升噪声鲁棒性。

3)  
- **任务**：在LRS3 AV-ASR基准测试上，评估了在干净及多种信噪比（-5dB、0dB、5dB）的babble噪声（MUSAN和NoiseX数据集）条件下的性能。  
- **效果**：  
  - 在0dB MUSAN噪声中，基于Whisper small和Whisper medium的dual-use方法相比中间融合基线，词错误率（WER）分别相对降低35%（4.41% vs. 6.83%）和57%（4.07% vs. 9.53%）。  
  - 使用1929小时视听数据微调后，在MUSAN和NoiseX噪声上的平均WER分别达到**4.08%**和**4.43%**，在LRS3噪声条件下取得了新的最先进性能。
</div>

</details>

---

## OCR-Enhanced Multimodal ASR Can Read While Listening
- **Authors**: Junli Chen, Changli Tang, Yixuan Li, Guangzhi Sun, Chao Zhang
- **Categories**: cs.SD, cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.18393v1](https://arxiv.org/abs/2601.18393v1)
- **PDF**: [https://arxiv.org/pdf/2601.18393v1](https://arxiv.org/pdf/2601.18393v1)

视觉信息（如电影字幕）常能提升自动语音识别性能。本文提出Donut-Whisper——一种采用双编码器的视听语音识别模型，通过利用视觉信息提升中英文场景下的语音识别效果。该模型通过交叉注意力模块融合线性结构与基于Q-Former的模态对齐架构优势，生成更具表现力的视听特征。同时，我们提出一种轻量级知识蒸馏方案，展示了利用视听模型指导纯音频模型实现性能提升的潜力。此外，基于包含中英文片段的电影剪辑，我们构建了一个新的多语言视听语音识别数据集。实验表明，Donut-Whisper在数据集的中英文部分均显著优于Donut与Whisper large V3基线模型：相较于Whisper基线，在英文集上实现绝对5.75%的词错误率降低，在中文集上实现绝对16.5%的字错误率降低。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：人类通过多模态（如视听结合）感知语音。视频中的字幕等视觉文本信息能辅助语音识别，尤其在音频嘈杂或包含专业词汇时。
- **既有方法问题**：
  - 主流端到端ASR系统仅依赖音频，在噪声或领域外词汇上表现不佳。
  - 现有视听ASR研究多集中于唇部运动，或通过非端到端的上下文偏置列表利用视觉文本，缺乏端到端可微分的融合方法。

2)  
论文提出 **Donut-Whisper**，一个端到端的视听语音识别模型，通过创新的融合架构解决上述问题：
- **双编码器设计**：
  - 使用预训练的 **Whisper-large-V3** 作为音频编码器，提取语音特征。
  - 使用预训练的 **Donut**（OCR模型）作为视觉编码器，从视频帧中提取字幕文本特征。
- **特征融合模块**：
  - **线性层**：将视觉特征投影到音频特征空间。
  - **滑动窗口 Q-Former**：对音频特征序列进行局部聚合，捕捉语音的时序局部性。
  - **交叉注意力层**：以Q-Former输出的聚合特征为查询，以投影后的视觉特征为键和值，实现音频特征对视觉信息的关注，完成深度融合。
- **端到端训练**：冻结预训练编码器，仅训练融合模块与Whisper解码器（使用LoRA高效更新），实现完全可微分的多模态融合。
- **轻量级知识蒸馏**：
  - 使用训练好的Donut-Whisper作为教师模型，在少量带字幕视频数据上生成软标签（温度缩放后的logits）和伪标签（argmax tokens）。
  - 学生模型（仅音频的Whisper-large V3）冻结编码器，通过结合交叉熵损失和KL散度损失的蒸馏目标更新解码器，从而利用多模态知识提升纯音频模型的性能。

3)  
- **任务与数据集**：在自建的多语言视听数据集（中文57小时，英文33小时，含电影对话字幕）上进行评估。
- **效果**：
  - **多模态识别**：Donut-Whisper在测试集上显著优于单模态基线（Whisper-large V3和Donut）。
    - 英文：词错误率（WER）**绝对降低5.75%**（从10.08%降至4.33%）。
    - 中文：字错误率（CER）**绝对降低16.5%**（从21.2%降至4.7%）。
  - **知识蒸馏**：蒸馏后的纯音频学生模型（Whisper-large V3）性能也得到提升，例如英文WER从10.08%降至9.86%。
  - **融合结构有效性**：所提出的“滑动窗口Q-Former+交叉注意力”融合方案在多种对比实验中表现最优。
</div>

</details>

---

## A Dataset for Automatic Vocal Mode Classification
- **Authors**: Reemt Hinrichs, Sonja Stephan, Alexander Lange, Jörn Ostermann
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.18339v1](https://arxiv.org/abs/2601.18339v1)
- **PDF**: [https://arxiv.org/pdf/2601.18339v1](https://arxiv.org/pdf/2601.18339v1)

完整声乐技术（CVT）是由 Cathrin Sadolin 等人近几十年发展形成的一种歌唱学派。CVT 将人声运用归纳为四种声乐模式，即中性模式、抑制模式、过载模式与边缘模式。掌握目标声乐模式对声乐学习者具有重要指导意义，因此声乐模式的自动分类对于技术辅助声乐教学至关重要。此前由于数据匮乏，声乐模式自动分类研究尚未取得显著突破。为此，我们录制了新型声乐模式数据集，包含四位歌手（其中三位为具有五年以上 CVT 经验的专业歌手）持续元音发音样本，共计 3,752 个独立样本，覆盖演唱者全音域。通过四麦克风同步录制实现自然数据增强，组合样本量超 13,000 条。数据集由三位 CVT 专家分别进行独立标注，最终发布版本包含合并标注结果及三组独立标注。此外，我们提供了基线分类实验结果：ResNet18 模型在五折交叉验证中取得最佳平衡准确率 81.3%。数据集可通过 https://zenodo.org/records/14276415 下载获取。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于“完整声乐技术”（CVT）的声乐模式（Neutral、Curbing、Overdrive、Edge）自动分类，对辅助声乐教学有重要价值。  
- **既有问题**：  
  - 先前研究因缺乏公开、专用的数据集，导致分类效果不佳（如平衡准确率仅约68.6%）。  
  - 现有通用声乐数据集（如VocalSet）无法直接用于CVT术语下的声乐分析。  
  - 数据不足严重限制了机器学习算法的应用与发展。

2)  
- **构建专用数据集**：  
  - 录制了涵盖四位歌手（三位为专业歌手）全音域的3,752个独特样本，覆盖CVT全部四种声乐模式。  
  - 使用四支麦克风（包括两支智能手机）同步录音，通过设备差异实现自然数据增强，总样本量超过13,000个。  
  - 由三位熟悉CVT的标注者进行独立标注，并提供合并后的标注结果与个体标注，以提升标注可靠性。  
- **提供基线分类方法**：  
  - 测试了多种分类器（如XGBoost、SVM、随机森林、ResNet等），并采用5折交叉验证进行评估。  
  - 在数据预处理中进行了响度与能量归一化，以消除录音设备差异带来的能量偏差。  
  - 训练/测试划分确保同一发音的不同麦克风录音不会同时出现在训练集和测试集中，避免数据泄露。  
- **解决标注不一致问题**：  
  - 通过多数投票机制合并个体标注，对无法达成一致的样本使用名义模式作为判据，最终仅排除29个“无法确定”样本。  
  - 构建“强共识”与“弱共识”标注子集，以探究标注一致性对分类性能的影响。

3)  
- **任务与效果**：  
  - 在声乐模式自动分类任务上，使用ResNet18在完整标注数据上取得了**81.3%**的平衡准确率（5折交叉验证）。  
  - 若以名义模式作为标注真值，预训练ResNet34的平衡准确率提升至**95.3%**，显著优于先前研究（提升超过25%）。  
  - 模型在手机录音子集上表现稳健，准确率下降很小，显示出对设备差异的鲁棒性，有利于未来移动端应用。
</div>

</details>

---

## Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification
- **Authors**: Zexia Fan, Yu Chen, Qiquan Zhang, Kainan Chen, Xinyuan Qian
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.18335v1](https://arxiv.org/abs/2601.18335v1)
- **PDF**: [https://arxiv.org/pdf/2601.18335v1](https://arxiv.org/pdf/2601.18335v1)

声源定位在受控环境中表现优异，但在实际部署中面临双重不平衡挑战：由到达方向长尾分布引发的任务内不平衡，以及跨任务偏移与重叠导致的任务间不平衡。这些问题常引发灾难性遗忘，严重降低定位精度。为缓解上述问题，我们提出一个包含两项关键创新的统一框架。具体而言，我们设计了一种基于GCC-PHAT的数据增强方法，利用峰值特征缓解任务内分布偏移；同时提出具有任务自适应正则化的解析动态不平衡校正器，通过解析式更新适应任务间动态变化。在SSLR基准测试中，本方法取得了89.0%准确率、5.3°平均绝对误差和1.6反向迁移的先进性能，展现出对持续演化不平衡问题的鲁棒性，且无需存储样本数据。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声源定位在受控环境下表现良好，但在实际部署中面临双重不平衡挑战：任务内不平衡（由到达方向的长尾分布引起）和任务间不平衡（由跨任务的数据偏斜和类别重叠导致）。这些不平衡会引发灾难性遗忘，严重降低定位精度。  
- **既有方法问题**：现有的类增量学习方法（如基于正则化、回放或解析的方法）在SSL的广义类增量学习设置中，难以有效处理上述动态不平衡，尤其是在隐私限制禁止存储历史数据时，性能下降显著。

2)  
论文提出一个统一框架 **SSL-GCIL**，包含两个核心组件以解决双重不平衡问题：  
- **GCC-PHAT 数据增强**：  
  - 针对任务内不平衡，利用 GCC-PHAT 特征的峰值统计信息（如峰值位置和幅度），通过算法合成尾部类别的样本。  
  - 具体操作包括对丰富类别特征的峰值进行循环移位和幅度缩放，以匹配尾部类别的统计特性，并添加高斯噪声以增加多样性。  
  - 该方法无需外部数据，在平衡类别表示的同时保持了麦克风间相关性的统计一致性。  

- **解析动态不平衡矫正器**：  
  - 针对任务间不平衡，设计了一个解析分类器，其核心是**自适应正则化模块**。  
  - 该模块通过任务特定的统计量（如基尼系数）动态调整正则化强度，以缓解跨任务的类别分布偏斜。  
  - 采用解析更新公式，基于每个类别的自相关和互相关矩阵进行递归最小二乘更新，从而适应类别重复出现的动态场景。  
  - 整个框架无需存储原始音频或样本，符合实际应用中的隐私约束，同时实现了轻量且自适应的增量学习。

3)  
- **任务**：在 SSLR 基准测试的广义类增量学习设置下进行声源定位，包含 10 个顺序任务，每个任务具有长尾分布和类别重叠。  
- **效果**：  
  - 在干净条件下，取得了 89.0% 的准确率、5.3° 的平均绝对误差和 +1.6 的后向传递值，均达到最先进水平。  
  - 在多种信噪比噪声条件下（20dB 至 -10dB），始终优于基线方法，表现出优异的噪声鲁棒性。  
  - 通过消融实验验证，两个组件均显著提升性能，其中 ADIR 有效缓解了灾难性遗忘（将后向传递从 -17.7 提升至 +1.4），GDA 进一步将准确率从 82.4% 提升至 89.0%。
</div>

</details>

---

## Residual Learning for Neural Ambisonics Encoders
- **Authors**: Thomas Deppisch, Yang Gao, Manan Mittal, Benjamin Stahl, Christoph Hold, David Alon, Zamir Ben-Hur
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18322v1](https://arxiv.org/abs/2601.18322v1)
- **PDF**: [https://arxiv.org/pdf/2601.18322v1](https://arxiv.org/pdf/2601.18322v1)

随着智能眼镜和扩展现实头显等可穿戴设备的兴起，对紧凑型头戴式麦克风阵列的高质量空间音频采集需求日益增长。Ambisonics技术通过将阵列信号映射至球谐函数系数，提供了一种与设备无关的空间音频表示方法。然而在实际应用中，精确编码仍面临挑战：传统线性编码器虽具有信号无关性和鲁棒性，但会放大低频噪声并受高频空间混叠影响；而神经网络方法虽能超越线性编码器，却常基于理想化麦克风假设，在实际场景中表现不稳定。为融合二者优势，本文提出一种残差学习框架，通过神经网络修正量优化线性编码器。基于智能眼镜实测阵列传递函数，我们将文献中的UNet编码器与新型循环注意力模型进行对比。分析表明，仅当神经网络编码器嵌入残差学习框架时，才能持续超越线性基线。在残差配置下，两种神经模型在域内数据的所有测试指标上均取得稳定且显著的提升，对域外数据亦获得适度改进。然而相干性分析显示，所有神经编码器配置在高频方向性精确编码方面仍存在局限。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：智能眼镜等可穿戴设备需要从紧凑的麦克风阵列中捕获高质量空间音频。Ambisonics通过将阵列信号映射到球谐系数，提供了一种设备无关的空间音频表示。
- **既有方法的问题**：
  - 传统线性编码器虽稳健，但会放大低频噪声，并在高频处出现空间混叠。
  - 神经网络方法虽能超越线性编码器，但通常假设理想麦克风，在真实场景中表现不一致，且缺乏与线性方法的全面性能对比分析。

2)  
论文提出了一种**残差学习框架**，将线性编码器与神经网络的修正能力相结合，以解决上述问题。具体方法如下：

- **核心架构**：
  - 线性编码器与神经网络编码器并行处理输入信号。
  - 神经网络被训练为学习**残差校正**，即对线性编码器的输出进行改进，而非完全替代它。
  - 最终输出是线性编码器与神经网络输出的和，确保两者处理延迟匹配。

- **神经网络设计**：
  - 对比了两种架构：一种是基于UNet的编码器（来自文献），另一种是新提出的**循环注意力模型**。
  - 新模型结合了频带注意力机制和循环神经网络，以学习跨麦克风阵列的频域依赖关系和时序特征。

- **训练策略**：
  - 使用复合损失函数，包括STFT域的平均绝对误差和球谐相干性损失，以促进准确的方向性模式重建。
  - 损失函数设计考虑了人类感知的频带权重，无需针对特定阵列进行调整。

- **优势**：
  - 残差框架利用了线性编码器的稳健性，同时通过神经网络修正其不足。
  - 实验表明，仅当神经网络集成在残差框架内时，才能在线性基线上实现一致且显著的性能提升。

3)  
- **任务**：在智能眼镜的五麦克风阵列上，进行Ambisonics编码（最大球谐阶数N=1）。
- **效果**：
  - **域内数据**：所有残差模型在相干性、幅度误差、SI-SDR和空间功率图误差上均一致优于线性基线。例如，相干性从0.44提升至0.52–0.54，SI-SDR从-7.4 dB提升至最高1.0 dB。
  - **域外数据**：残差模型仍能取得适度提升（如相干性从0.39提升至0.40），而非残差神经网络常表现更差。
  - **局限性**：所有方法（包括神经网络）均未能显著解决高频方向性精度不足的问题，表明空间采样稀疏的根本限制依然存在。
</div>

</details>

---

## Noise-Robust Contrastive Learning with an MFCC-Conformer For Coronary Artery Disease Detection
- **Authors**: Milan Marocchi, Matthew Fynn, Yue Rong
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18295v1](https://arxiv.org/abs/2601.18295v1)
- **PDF**: [https://arxiv.org/pdf/2601.18295v1](https://arxiv.org/pdf/2601.18295v1)

心血管疾病是全球主要致死病因，其中冠状动脉疾病占据最大比重。近年来，利用心音图信号检测冠状动脉疾病受到广泛关注，在低噪声环境和理想传感器布局的临床场景中已取得显著成效。多通道技术被证明对噪声具有更强鲁棒性，但在真实场景数据中实现稳定性能仍面临挑战。本研究提出一种基于能量的新型多通道噪声片段抑制算法，通过心音麦克风与噪声参考麦克风，在训练深度学习分类器前剔除包含大量非平稳噪声的音频片段。该分类器基于Conformer架构，从多通道信号中提取梅尔频率倒谱系数，进一步增强模型抗噪能力。在297名受试者数据上，所提方法取得了78.4%的准确率与78.2%的平衡准确率，相较于未采用噪声片段抑制的训练方式，分别提升4.1%和4.3%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：冠状动脉疾病（CAD）是心血管疾病的主要亚型，需要及时诊断。传统听诊准确率低，而金标准血管造影术具有侵入性且昂贵。基于深度学习的心音图（PCG）方法在低噪声临床环境中表现良好，但在真实世界嘈杂数据上鲁棒性不足。
- **既有方法问题**：现有PCG方法在真实医院环境（存在说话声、关门声等非平稳噪声）且传感器放置不理想时，性能显著下降。多通道技术和MFCC特征虽能提升噪声鲁棒性，但缺乏结合噪声段剔除、多通道信号处理以及先进Transformer架构（如Conformer）的完整方案在真实嘈杂数据集上的验证。

2)  
论文提出一个集成化流程，核心方法包括：
- **多通道能量感知噪声段剔除算法**：
    - 利用心音麦克风（HM）和噪声参考麦克风（NM）同步信号。
    - 将信号分帧计算能量，通过与中位数能量的阈值比较（阈值设为2.5倍），识别并剔除包含突发噪声（如摩擦声、关门声）的帧。
    - HM和NM的噪声索引合并，确保仅保留纯净段用于后续训练。
- **MFCC-Conformer分类器**：
    - 从多通道PCG信号中提取MFCC特征（频率范围25-450Hz），沿时间通道轴拼接，形成统一表示。
    - 采用Conformer编码器（包含多头自注意力、卷积模块和前馈网络），能同时捕捉局部和全局依赖。
- **监督混合对比学习**：
    - 损失函数结合监督对比损失、交叉熵分类损失和中心损失，通过拉近同类样本嵌入、推远异类样本嵌入，增强特征空间的判别性和鲁棒性。
- **训练与推理优化**：
    - 训练时使用片段级平衡采样，避免类别偏差；推理时用SVM替换MLP分类头，并通过多数投票聚合片段级预测得到主体级结果。

3)  
- **任务**：在真实医院环境采集的297名男性受试者（155名CAD，142名正常）的多通道PCG数据上，进行CAD检测。
- **效果**：
    - 在主体级别，所提方法（含噪声段剔除）达到**78.4%准确率**和**78.2%平衡准确率**，较未剔除噪声的基线分别提升4.1%和4.3%。
    - 相比先前基于Wav2Vec 2.0的方法，准确率提升1.3%，平衡准确率提升3.9%，且模型更小、更平衡（TPR与TNR差异减小）。
    - 结果表明该方法在真实嘈杂场景下具有更优的噪声鲁棒性和分类性能。
</div>

</details>

---

## Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue
- **Authors**: Yuhang Jia, Pei Liu, Haoqin Sun, Jiaming Zhou, Xuxin Cheng, Cao Liu, Ke Zeng, Xunliang Cai, Yong Qin
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18281v1](https://arxiv.org/abs/2601.18281v1)
- **PDF**: [https://arxiv.org/pdf/2601.18281v1](https://arxiv.org/pdf/2601.18281v1)

端到端口语模型在副语言感知方面具有巨大潜力，众多研究致力于提升其能力，尤其在共情对话领域。然而，当前方法主要依赖于刚性监督信号，例如监督微调中的真实响应或强化学习中的偏好分数。这种依赖在建模复杂共情时存在根本性局限，因为不存在单一的“正确”回应，且简单的数值评分无法完整捕捉情感表达的细微差别或共情行为的恰当性。为突破这些限制，我们首先提出EmpathyEval——一种基于描述性自然语言的评估模型，用于衡量口语对话中的共情质量。在此基础上，我们进一步提出ReEmpathy，这是一个通过新型“共情自反思交替推理机制”增强共情对话能力的端到端口语模型。该机制将口语响应生成与自由形式的共情反思推理交替进行。大量实验表明，ReEmpathy通过引入反思推理能力，显著提升了对话的共情敏感性，为实现更具情感智能和共情意识的人机交互提供了有前景的路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：端到端口语语言模型在副语言感知方面潜力巨大，但现有方法主要依赖**刚性监督信号**（如监督微调中的真实响应或强化学习中的偏好分数）来优化共情对话。  
- **既有问题**：共情表达复杂且主观，**不存在单一“正确”响应**，简单的数值分数无法充分捕捉情感表达的细微差别或共情行为的恰当性，导致性能提升有限。

2)  
论文提出**ReEmpathy**框架，通过**共情自反思交替推理机制**解决上述问题，具体步骤如下：  
- **构建描述性评估模型**：首先开发**EmpathyEval**，这是一个基于自然语言的自动评估模型，用于对口语对话的共情质量生成**描述性评估**（而非简单分数）。该模型通过多阶段微调（语音情感识别、情感描述生成、共情评估）训练，能综合处理语言内容和副语言线索。  
- **设计交替推理机制**：在推理时，模型以固定长度块交替生成**响应块**（包含语音及其文本转录）和**反思块**（包含内部未说出的推理文本标记）。反思内容基于对话上下文进行自由形式的共情相关推理，评估已生成响应的共情质量并指导后续响应。  
- **监督训练**：利用大规模共情口语对话数据集，通过EmpathyEval生成描述性评估，将其转换为反思文本标记，并与响应音频标记交错，形成**交替监督训练数据**，用于微调基础语音模型（如GLM-4-Voice），使模型学会交替生成模式并整合推理。  
- **关键优势**：  
  - 摆脱对单一参考或分数的依赖，通过**自由文本反思**实现细粒度、上下文感知的共情优化。  
  - 反思与响应在全局层面相互条件化，实现动态相互影响，促进副语言感知和共情推理。  
  - 实验表明，反思频率和注意力权重的合理调整能进一步提升共情表现。

3)  
- **评估任务**：在**口语对话共情质量评估**任务上，EmpathyEval在描述性评估中取得显著性能（如BLEU-1达0.48以上），在基于分数的评估中与人工评分高度相关（线性相关系数达0.71）。  
- **对话生成任务**：在**共情口语对话生成**中，ReEmpathy在多个评估指标（包括EmpathyEval的四个维度分数、GPT-4评分、人工A/B测试）上均优于基线模型（如GLM-4-Voice）及其他优化方法（如监督微调、直接偏好优化、思维链），共情分数接近真实响应，且A/B测试显示其输出在多数情况下优于对比方法。  
- **消融实验**：验证了交替推理机制的有效性，禁用反思后性能下降，而调整反思频率和注意力权重能进一步优化共情表现。
</div>

</details>

---

## Efficient Rehearsal for Continual Learning in ASR via Singular Value Tuning
- **Authors**: Steven Vander Eeckt, Hugo Van hamme
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18266v1](https://arxiv.org/abs/2601.18266v1)
- **PDF**: [https://arxiv.org/pdf/2601.18266v1](https://arxiv.org/pdf/2601.18266v1)

自动语音识别（ASR）中的持续学习在适应新任务、领域或说话人时，常面临灾难性遗忘问题。一种常见的缓解策略是在内存中存储部分历史数据进行回放学习。然而，基于回放的方法存在关键局限：存储数据通常成本高昂、在预训练模型中难以实现，或受隐私法规限制。若为缓解这些问题而缩小内存规模运行现有回放方法，通常会导致性能下降。

本文提出一种基于回放的持续学习方法，即使在极小内存条件下仍能保持高效。该方法分两阶段运行：首先对新任务进行微调；其次对线性层的变化进行奇异值分解，并以参数高效的方式，仅通过回放数据对奇异值对应的门控向量进行重训练，这些向量控制着第一阶段更新被接受的程度。我们在两个单语和两个多语言基准测试上对该方法进行了全面验证与分析。实验表明，该方法显著减少了遗忘现象，其性能优于当前最先进的ASR持续学习方法，即使在每个历史任务仅保留一条语音样本的极端限制下依然有效。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
研究背景与既有方法的问题：
- **背景**：自动语音识别系统在动态环境中需持续适应新任务，但面临灾难性遗忘问题。
- **既有方法问题**：
  - 基于排练的方法需存储历史数据，但受隐私、存储成本或预训练模型限制，难以大规模存储。
  - 现有方法在内存受限时性能显著下降，无法在极小内存下有效缓解遗忘。

2)  
论文核心方法如何解决上述问题：
- **方法概述**：提出基于奇异值调优的排练方法，通过两阶段训练在极小内存下平衡新旧任务性能。
- **具体步骤**：
  - **阶段1**：在新任务上微调模型，获得参数更新。
  - **阶段2**：对线性层权重更新进行奇异值分解，引入可学习的门控向量控制每个奇异值分量的保留程度。
    - 仅训练门控向量，固定其他参数，实现参数高效更新。
    - 使用极小内存中的排练数据联合优化门控向量，通过损失函数结合交叉熵和知识蒸馏，平衡新旧任务。
- **关键创新**：
  - 通过奇异值分解结构化更新，允许模型选择性地接受或抑制更新方向。
  - 门控向量将更新约束在[0,1]区间，避免过拟合，并初始化为接近零以保持旧任务性能。
  - 对非线性层参数进行平均初始化，进一步减少遗忘。
- **优势**：在极低内存下（如每任务仅一个话语）仍能有效减少遗忘，且训练参数量不足模型总参数的0.1%。

3)  
在哪些任务上取得了怎样的效果：
- **任务**：在四个ASR持续学习基准上测试，包括两个单语（口音和麦克风偏移）和两个多语任务。
- **效果**：
  - 在极小内存下显著优于现有方法，即使每任务仅一个话语，遗忘减少超过80%。
  - 在单语任务中，平均词错误率优于基于排练的基线方法，且遗忘更少。
  - 在多语任务中，使用稍大内存（如50个话语）可匹配或超越基线使用200个话语的性能。
  - 方法在平衡新旧任务性能方面表现最佳，显著缩小了与理想单独模型之间的差距。
</div>

</details>

---

## LLM-ForcedAligner: A Non-Autoregressive and Accurate LLM-Based Forced Aligner for Multilingual and Long-Form Speech
- **Authors**: Bingshen Mu, Xian Shi, Xiong Wang, Hexin Liu, Jin Xu, Lei Xie
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.18220v1](https://arxiv.org/abs/2601.18220v1)
- **PDF**: [https://arxiv.org/pdf/2601.18220v1](https://arxiv.org/pdf/2601.18220v1)

强制对齐（FA）用于预测语音中单词或字符的起止时间戳，但现有方法通常局限于特定语言，且易产生累积性时间偏移。语音大语言模型（SLLMs）具备多语言语音理解与长序列处理能力，为多语言、跨语言及长语音场景下的FA任务提供了潜力。然而，直接将SLLM的下一词预测范式应用于FA会导致幻觉现象与推理速度缓慢。为此，我们提出LLM-ForcedAligner，将FA重构为槽填充范式：将时间戳视为离散索引，并在文本转录中插入特殊时间戳标记作为填充槽。在语音嵌入和带槽文本的条件下，SLLM直接预测各槽位对应的时间索引。训练过程中，通过因果注意力掩码配合非偏移的输入与标签序列，使每个槽位能基于自身及前文上下文预测时间索引，且损失仅计算于槽位位置。动态槽插入机制支持在任意位置进行FA。此外，模型支持非自回归推理，从而避免幻觉并提升速度。在多语言、跨语言及长语音场景的实验表明，相较于现有方法，LLM-ForcedAligner在累积平均偏移量上实现了69%~78%的相对降低。模型检查点与推理代码将于后续发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：强制对齐（FA）旨在预测语音中词或字符的起止时间戳，是语音处理中的基础任务。  
- **既有方法的问题**：  
  - **语言依赖性强**：传统方法（如MFA）或端到端模型（如CTC、CIF、WhisperX）通常依赖语言特定的音素、词典或结构设计，导致多语言场景需维护多个独立系统，工程成本高。  
  - **长语音时序漂移**：现有方法基于局部声学相似度与单调路径搜索，在短语音上表现良好，但在长语音（如数百秒）中容易累积系统性时间偏移，影响对齐精度。  
  - **SLLM直接应用的缺陷**：若将语音大语言模型（SLLM）的下一词预测范式直接用于FA，会导致时间戳幻觉（非单调性）且推理速度慢。

2)  
论文提出 **LLM-ForcedAligner**，通过以下方法解决上述问题：  
- **槽填充范式重构**：  
  - 将时间戳视为离散时间索引，在文本转录中插入特殊标记 `[time]` 作为槽位。  
  - 模型以语音嵌入和带槽位的文本为条件，直接预测槽位对应的时间索引，实现从声学特征到词/字符级时间戳的端到端映射。  
- **训练策略优化**：  
  - 采用因果注意力掩码，保持输入与标签序列无偏移，使每个槽位能基于自身及上文预测时间索引。  
  - 损失函数仅计算在槽位位置，聚焦时间戳预测任务。  
  - 引入动态槽位插入：训练中随机决定是否为每个词/字符插入槽位，提升模型泛化能力，支持任意位置的时间戳预测。  
- **非自回归推理**：  
  - 推理时一次性预测所有槽位时间索引，避免自回归解码带来的幻觉问题，显著提升推理速度。  
- **多语言与长序列处理能力**：  
  - 利用预训练的多语言语音编码器（AuT）和LLM，无需语言特定资源，统一处理多语言及跨语言语音。  
  - 通过长上下文建模能力，有效缓解长语音中的累积时序漂移。

3)  
- **任务与效果**：  
  - **多语言语音对齐**：在10种语言（中、英、法、德等）测试集上，相比传统方法（如NFA、WhisperX），平均累积平均偏移（AAS）降低66%~73%。  
  - **长语音对齐**：在长达300秒的语音上，AAS相对降低69%~78%，显著优于现有方法（其他方法在长语音上AAS急剧上升）。  
  - **跨语言语音对齐**：混合多语言长语音场景下仍保持低AAS（如34.2ms），体现强大泛化能力。  
  - **噪声与定制化预测**：在带噪语音及用户自定义位置预测中均表现鲁棒，且推理速度仅略有增加（RTF≈0.016）。
</div>

</details>

---

## VIBEVOICE-ASR Technical Report
- **Authors**: Zhiliang Peng, Jianwei Yu, Yaoyao Chang, Zilong Wang, Li Dong, Yingbo Hao, Yujie Tu, Chenyu Yang, Wenhui Wang, Songchen Xu, Yutao Sun, Hangbo Bao, Weijiang Xu, Yi Zhu, Zehua Wang, Ting Song, Yan Xia, Zewen Chi, Shaohan Huang, Liang Wang, Chuang Ding, Shuai Wang, Xie Chen, Furu Wei
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.18184v1](https://arxiv.org/abs/2601.18184v1)
- **PDF**: [https://arxiv.org/pdf/2601.18184v1](https://arxiv.org/pdf/2601.18184v1)

本报告介绍了VibeVoice-ASR，这是一个基于VibeVoice构建的通用语音理解框架，旨在解决长音频（如会议、播客）中语境碎片化和多说话人复杂性等持续存在的挑战，这些问题在短语音识别技术近期取得进展后仍未得到充分解决。与依赖音频分块的传统流水线方法不同，VibeVoice-ASR支持对长达60分钟的音频进行单次处理，并将自动语音识别、说话人日志和时间戳标注统一为单一的端到端生成任务。此外，VibeVoice-ASR支持超过50种语言，无需显式设置语言参数，并能原生处理语句内及跨语句的语码转换。我们还引入了一种基于提示的上下文注入机制，允许用户提供定制化上下文，从而显著提升领域专有术语的识别准确性和多音字消歧能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：长音频（如会议、播客）的语音理解面临两大挑战：上下文碎片化与多说话人复杂性。现有主流方法采用级联流水线，将长音频分割为短片段（如<30秒）独立处理。
- **既有问题**：
  - **上下文碎片化**：分割处理切断了全局语义依赖，导致模型无法利用跨句上下文，难以消歧同音词或指代。
  - **流水线复杂性**：传统系统将语音识别、说话人日志、时间戳标注作为独立任务，由不同模型处理。输出对齐需复杂启发式规则，错误易传播。

2)  
VIBEVOICE-ASR 通过统一端到端生成框架解决上述问题，核心方法包括：
- **单次处理长音频**：摒弃滑动窗口分割范式，利用超低帧率（7.5 Hz）双分词器（声学与语义）压缩音频。一小时音频仅需约27,000个token，可完整放入现代LLM上下文窗口，实现全局上下文注意力。
- **统一生成任务**：将长音频转录重构为端到端生成任务，模型直接输出**富转录流**，显式交织：
  - **说话人身份**（Who）
  - **精确时间戳**（When）
  - **语音内容**（What）
  从而在单次推理中同时完成识别、日志和时间戳标注，无需外部聚类算法。
- **基于提示的上下文注入**：支持用户提供定制化上下文（如热词列表、背景描述），通过提示机制注入，显著提升领域术语识别和复杂语码转换处理能力。
- **数据与训练策略**：
  - 采用课程学习，逐步增加输入序列长度（8,192至65,536 token）。
  - 监督微调阶段精心混合高质量基准数据、音乐数据、合成数据及经GPT-5全局语义修正的长音频数据，平衡模型各项能力。

3)  
- **任务与效果**：在多个公开长音频多说话人基准测试中，VIBEVOICE-ASR 均取得最先进性能，显著优于Gemini-2.5/3-Pro等闭源多模态模型。
  - **说话人日志**：在DER指标上全面领先，显示更强的说话人建模与追踪能力。
  - **时间对齐转录**：在tcpWER指标上表现最佳，表明在说话人归属、内容与时间戳对齐方面更准确。
  - **多语言支持**：在涵盖中、英、法、德等16种语言的MLC挑战集上，在多数语言上取得最低WER，展现了出色的多语言泛化能力。
</div>

</details>

---

## OneVoice: One Model, Triple Scenarios-Towards Unified Zero-shot Voice Conversion
- **Authors**: Zhichao Wang, Tao Li, Wenshuo Ge, Zihao Cui, Shilei Zhang, Junlan Feng
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.18094v1](https://arxiv.org/abs/2601.18094v1)
- **PDF**: [https://arxiv.org/pdf/2601.18094v1](https://arxiv.org/pdf/2601.18094v1)

近年来，语音转换研究在说话人克隆与语音内容保持方面取得了显著进展，但该领域仍处于割裂状态，依赖不同专用模型分别处理语音保持、情感表达及歌唱转换三种场景。本文提出OneVoice，一个统一的零样本语音转换框架，能够通过单一模型处理上述全部三种场景。OneVoice基于无需变分自编码器的连续补丁扩散语言模型构建，确保了高保真度与高效的序列建模能力。其统一性设计的核心在于采用混合专家模型，显式建模共享的转换知识与场景特定的表现力。专家选择通过双路径路由机制协调，包括共享专家隔离以及结合全局-局部线索的场景感知领域专家分配。为实现精准的条件控制，场景特定的韵律特征通过门控机制融合至每一网络层，从而自适应地利用韵律信息。此外，为实现核心设计思想并缓解数据不平衡问题（语音数据丰富而歌唱数据稀缺），我们采用两阶段渐进式训练策略，包括基础预训练与基于LoRA的领域专家场景增强。实验表明，OneVoice在全部三种场景中均达到或超越了专用模型的性能，同时验证了对场景的灵活控制能力，并提供了仅需2步的快速解码版本。代码与模型即将公开。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音转换（VC）技术已在说话人克隆和内容保持方面取得进展，但当前研究领域呈现碎片化，针对不同场景（如内容保持、情感表达、歌唱转换）需使用专门的模型，缺乏统一框架。  
- **既有问题**：现有方法难以在一个模型中同时处理多场景的核心信息（如语言内容、副语言表达、旋律轮廓），且面临场景间干扰、数据不平衡（语音数据远多于歌唱数据）以及长序列建模效率与保真度难以兼顾的挑战。

2)  
- **核心架构**：OneVoice 基于连续语言模型，结合混合专家（MoE）结构，通过共享专家捕获跨场景通用知识（如内容保持和说话人克隆），并通过领域专家增强场景特定表达。  
- **双路径路由机制**：  
  - **共享专家隔离**：固定一个专家专门学习跨场景共性，不受路由决策影响。  
  - **场景感知专家分配**：结合全局先验（语音/歌唱模式）和局部韵律上下文线索，动态激活领域专家，避免场景间干扰。  
- **场景特定韵律条件**：针对情感语音转换使用浅层ASR特征，针对歌唱转换使用离散基频特征，并通过门控融合机制自适应地融入每一层，提升建模精度。  
- **两阶段渐进训练**：  
  - **基础预训练**：在大规模语音数据上训练共享专家，掌握核心转换能力。  
  - **场景增强训练**：引入基于LoRA的领域专家，在平衡语音与歌唱数据采样的同时，以分组学习率微调，缓解数据不平衡问题。  
- **高效序列建模**：采用无需VAE的下一块扩散范式，将序列压缩至更低帧率（如10Hz），结合局部DiT头部实现高保真生成，兼顾效率与质量。

3)  
- **任务与效果**：在三个零样本语音转换任务上，OneVoice 均达到或超越专用模型性能：  
  - **内容保持转换**：在语音质量和可懂度上优于对比模型，说话人相似度接近最佳专用模型。  
  - **情感语音转换**：在说话人相似度与韵律保持间取得更好平衡，综合指标领先。  
  - **歌唱转换**：在歌唱表达质量和说话人相似度上均表现优异。  
- **附加优势**：支持通过禁用领域专家或调整全局先验灵活切换转换模式，并提供快速解码版本（如2步采样），在A100 GPU上实现实时因子低至0.37，展现出实用潜力。
</div>

</details>

---

## From Human Speech to Ocean Signals: Transferring Speech Large Models for Underwater Acoustic Target Recognition
- **Authors**: Mengcheng Huang, Xue Zhou, Chen Xu, Dapeng Man
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.18086v1](https://arxiv.org/abs/2601.18086v1)
- **PDF**: [https://arxiv.org/pdf/2601.18086v1](https://arxiv.org/pdf/2601.18086v1)

水下声学目标识别在海洋应用中至关重要，但由于标注数据有限及海洋环境复杂性，该任务仍具挑战性。本文探讨一个核心问题：基于大规模人类语音语料训练的语言大模型能否有效迁移至水下声学领域？为此，我们提出UATR-SLM框架，该框架复用语音特征处理流程，将语言大模型适配为声学编码器，并添加轻量级分类器。在DeepShip和ShipsEar基准测试上的实验表明：UATR-SLM在域内准确率超过99%，在不同信号长度下保持强鲁棒性，跨域评估准确率最高达96.67%。这些结果凸显了语言大模型向水下声学目标识别任务的可迁移性，为利用语音基础模型处理水下声学问题建立了可行范式。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：水下声学目标识别（UATR）对海洋应用至关重要，但面临两大挑战：  
  - **数据稀缺**：大规模标注水下声学数据集成本高昂、劳动密集。  
  - **领域复杂性**：水下环境存在非平稳噪声、多径传播和时空变异性，限制了传统深度学习方法的表现。  
- **既有方法问题**：现有方法（如数据增强、域适应）虽能带来渐进式改进，但受限于预训练数据的规模和多样性，难以捕获跨环境通用的声学模式。

2)  
论文提出 **UATR-SLM** 框架，通过迁移语音大模型（SLM）解决上述问题：  
- **特征提取对齐**：将原始水下声学信号通过与语音处理相同的标准流程（如提取Log-Mel滤波器组特征）进行预处理，确保与SLM输入格式兼容，直接复用预训练模型。  
- **SLM编码器微调**：将处理后的声谱图输入SLM编码器作为通用声学编码器。与常规迁移学习不同，本研究对编码器进行**全参数微调**，以充分适应人类语音与水下声学之间的分布差异，从而更全面地学习水下信号的独特特征。  
- **轻量级分类器**：替换SLM原有的解码器，采用均值池化聚合编码器输出，后接单个线性层和Softmax函数，实现高效的目标分类。  
- **整体优势**：该框架仅需少量任务特定微调和最小架构修改，即能利用SLM从海量语音数据中学到的强大声学表示能力，有效缓解数据稀缺和领域变异性的挑战。

3)  
在以下任务中取得显著效果：  
- **域内分类**：在DeepShip和ShipsEar基准测试中，准确率均超过99%（F1分数分别达99.32%和99.00%），超越了ResNet、HUAT等基线模型，达到最先进性能。  
- **鲁棒性测试**：在变长信号（1-20秒）的零样本评估中，仅1秒音频即可达到95.87%的准确率，显著优于对时长敏感的ResNet模型（差距约8%-15%）。  
- **跨域泛化**：在零样本跨数据集（DeepShip训练，ShipsEar测试）评估中，对完整音频取得96.67%的准确率，远高于ResNet基线（最高70%），证明了模型学习领域不变声学特征的能力。
</div>

</details>

---
