---
layout: post
title: "arXiv Daily – 2026-01-28"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-28（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-27 08:50 — 2026-01-28 08:50
- 抓取总数：16 篇 | 本页显示：16 篇（去重/过滤后）

## SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation
- **Authors**: Helin Wang, Bowen Shi, Andros Tjandra, John Hoffman, Yi-Chiao Wu, Apoorv Vyas, Najim Dehak, Ann Lee, Wei-Ning Hsu
- **Categories**: eess.AS, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19702v1](https://arxiv.org/abs/2601.19702v1)
- **PDF**: [https://arxiv.org/pdf/2601.19702v1](https://arxiv.org/pdf/2601.19702v1)

音频分离的性能评估仍是一个复杂挑战，现有评估指标常与人类感知存在偏差、粒度较粗，且依赖真实信号。另一方面，主观听音测试虽是实际场景评估的金标准，但成本高昂、耗时且难以规模化。本文针对无需人工干预的自动化音频分离评估系统日益增长的需求，提出了一种新型评估指标——SAM Audio Judge（SAJ）。该指标是一种多模态细粒度无参考客观度量方法，展现出与人类感知的高度一致性。SAJ支持三种音频领域（语音、音乐和通用声音事件）及三种提示输入（文本、视觉和片段标注），涵盖四个评估维度（召回率、精确度、保真度和整体质量）。SAM Audio Judge在数据过滤、大规模数据集伪标注及音频分离模型重排序等方面亦展现出潜在应用价值。代码与预训练模型已开源：https://github.com/facebookresearch/sam-audio。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.19702v1）
</div>

</details>

---

## A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models
- **Authors**: Iwona Christop, Mateusz Czyżnikiewicz, Paweł Skórzewski, Łukasz Bondaruk, Jakub Kubiak, Marcin Lewandowski, Marek Kubis
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19673v1](https://arxiv.org/abs/2601.19673v1)
- **PDF**: [https://arxiv.org/pdf/2601.19673v1](https://arxiv.org/pdf/2601.19673v1)

当前用于测试多模态大语言模型音频能力的基准主要集中于孤立地评估说话人日志或性别识别等单一音频任务。这些基准无法验证多模态模型是否具备整合不同类别音频任务所需的推理能力。为此，我们提出音频推理任务（ART）这一新基准，旨在评估多模态模型在处理需要基于音频信号进行推理的问题时的综合能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前评估多模态大语言模型音频能力的基准（如AudioBench、AIR-Bench）主要关注孤立任务（如语音识别、声学场景分类），缺乏对跨任务音频推理能力的评估。  
- **既有问题**：现有方法无法验证模型是否具备结合不同类别音频任务进行推理的能力，即使模型在单一任务上表现优异，其综合推理性能仍未知。

2)  
- **核心方法**：论文提出了音频推理任务基准（ART），旨在系统评估多模态模型在音频信号上的推理能力。  
- **任务设计原则**：  
  - **规则1**：任务不能仅通过单一专用模块（如语音转录）解决，必须结合多种音频现象进行推理。  
  - **规则2**：任务应能被无专业训练的普通人理解，以简化错误分析。  
- **任务构建流程**：  
  - 通过领域专家调研，筛选出9类需跨技能推理的任务（如音频算术、跨录音说话人识别）。  
  - 使用模板化方法生成任务实例，通过语音合成（基于Voicebox）将问题、语音和声音片段整合为连贯音频输入。  
  - 确保数据平衡：共9000个样本，涵盖9类任务，正负答案各半，总时长超30小时。  
- **评估方式**：  
  - 提供“是/否”和“描述性”两种回答模式，支持自动评估与LLM-as-a-judge评估，减少偏差风险。  
- **解决思路**：通过设计需多技能组合的复杂任务，迫使模型进行跨模态推理，而非依赖单一能力，从而弥补现有基准的不足。

3)  
- **评估任务**：在ART基准的9类任务上测试了多种模型（如Whisper+LLM、Audio Flamingo 3、Qwen2-Audio）。  
- **效果**：  
  - 所有模型表现均未达满意水平，最佳绝对准确率仅约56%（Whisper+Qwen），远低于人类基线（92.9%）。  
  - 跨录音说话人识别任务最难（平均准确率38.6%），选择性文本推理相对最佳。  
  - 实验表明，当前模型在需跨任务音频推理的场景中仍面临显著挑战。
</div>

</details>

---

## GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining
- **Authors**: Shentong Mo, Zehua Chen, Jun Zhu
- **Categories**: cs.CV, cs.AI, cs.LG, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19606v1](https://arxiv.org/abs/2601.19606v1)
- **PDF**: [https://arxiv.org/pdf/2601.19606v1](https://arxiv.org/pdf/2601.19606v1)

近年来，视频-音频理解与生成领域的研究日益依赖于联合视频-音频嵌入，这已成为跨模态检索与生成等任务的基础。尽管现有方法（如CAVP）通过对比学习有效建模了模态间的语义与时间对应关系，但其性能仍有提升空间。关键局限在于对视频与音频信号密集、多尺度特性的建模不足——对应关系往往跨越从细粒度到粗粒度的时空结构，而现有框架未能充分利用这一特性。为此，我们提出GMS-CAVP，一种结合多尺度视频-音频对齐与基于多尺度时空扩散的预训练目标的新型框架，以增强视频-音频对应关系建模。首先，GMS-CAVP引入多尺度对比学习策略，捕捉不同粒度下的语义与时间关系。其次，我们超越传统对比学习，融入基于扩散的生成目标，实现视频与音频间的模态转换与合成。这种统一的判别-生成框架促进了更深层次的跨模态理解，并为高保真生成开辟了新途径。在VGGSound、AudioSet和Panda70M数据集上的大量实验表明，GMS-CAVP在生成与检索任务上均优于现有方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：视频-音频（V-A）联合嵌入是跨模态检索与生成任务的基础。现有方法（如CAVP）通过对比学习对齐模态，但性能仍有不足。  
- **既有问题**：  
  - 现有框架未能充分建模视频和音频信号密集、多尺度的特性，忽略了从细粒度到粗粒度的时空结构对应关系。  
  - 当前预训练目标仅依赖对比学习，缺乏模态转换能力，导致在生成任务（如视频到音频合成）中表现次优。  

2)  
论文提出GMS-CAVP框架，通过结合多尺度对比学习与生成式预训练目标，解决上述问题：  
- **多尺度视频-音频对齐（MSA）**：  
  - 构建分层多尺度特征表示，捕获不同粒度（如局部与全局）的时空依赖关系。  
  - 引入多尺度对比学习目标，在每一尺度上计算视频与音频特征的相似度，通过加权损失增强细粒度对齐。  
  - 采用自适应时间对齐机制，通过注意力权重突出关键时间片段，提升同步性。  
- **生成式多尺度视频-音频对齐（MSD）**：  
  - 引入基于扩散模型的生成目标，通过多尺度条件机制逐步去噪生成音频。  
  - 利用分层视频特征作为条件，在扩散过程中细化音频表示，弥补模态间的结构差异。  
- **统一框架优势**：  
  - 判别式与生成式目标结合，既强化跨模态对齐，又提升生成质量与真实性。  
  - 多尺度设计使模型能同时处理复杂运动模式与精细的视听关系，增强泛化能力。  

3)  
GMS-CAVP在以下任务中取得显著效果：  
- **视频到音频生成**：在VGGSound数据集上，关键指标全面领先（如KLD降至1.63、FAD降至0.75、对齐准确率达95.87%），生成音频的语义一致性与时间同步性优于已有方法。  
- **跨模态检索**：在视频到音频及音频到视频检索任务中，召回率（R@1, R@5, R@10）大幅提升，例如视频到音频检索R@1达28.9%（对比基线CAVP的9.5%）。  
- **多数据集验证**：在VGGSound、AudioSet和Panda70M上均达到最优性能，证明其泛化能力。
</div>

</details>

---

## Audio Deepfake Detection at the First Greeting: "Hi!"
- **Authors**: Haohan Shi, Xiyu Shi, Safak Dogan, Tianjin Huang, Yunxiao Zhang
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19573v1](https://arxiv.org/abs/2601.19573v1)
- **PDF**: [https://arxiv.org/pdf/2601.19573v1](https://arxiv.org/pdf/2601.19573v1)

本文聚焦于真实通信环境下的音频深度伪造检测，重点针对超短时输入（0.5-2.0秒），旨在实现对话开场时（如诈骗者说“你好”）的合成语音检测能力。我们提出了一种轻量级新型模型——短时多粒度自适应时频注意力网络（S-MGAA），该模型基于多粒度自适应时频注意力架构扩展而来，专门针对经过通信处理与干扰的短时退化音频信号，增强其判别性表征学习能力。S-MGAA整合了两个定制化模块：像素-通道增强模块（PCEM）用于放大细粒度时频显著性特征，以及频率补偿增强模块（FCEM）通过多尺度频率建模与自适应频时交互机制，补充有限时域信息。大量实验表明，S-MGAA在九种前沿基线方法中均取得稳定优势，同时对信号退化表现出强鲁棒性，并在效率与精度间实现优异平衡——包括低实时因子、具有竞争力的计算量、紧凑的参数规模以及较低的训练成本，凸显了其在通信系统与边缘设备中实时部署的潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：深度伪造音频技术快速发展，其滥用带来了严重的社会风险。现有的音频深度伪造检测方法主要针对干净、高保真的长音频（3-4秒）设计。
- **既有问题**：
  - **现实场景退化**：真实通信场景中的编解码压缩和丢包会严重降低音频质量，现有方法在此类退化下的鲁棒性不足。
  - **超短音频输入**：现有方法在处理超短音频（0.5-2.0秒）时，通常采用截断或填充，导致训练与测试时长不匹配，检测精度下降。
  - **轻量化需求**：多数方法未充分考虑在资源受限设备（如智能手机）上实时部署所需的轻量化设计。

2)  
论文提出了名为 **S-MGAA** 的轻量级框架，专门用于解决在通信退化条件下对超短音频的检测问题。其核心方法通过两个定制模块增强对短时、退化输入的判别性表征学习：

- **像素-通道增强模块**：
  - **目标**：解决因音频超短和通信退化导致的伪造时频特征稀疏、显著性低的问题。
  - **机制**：联合建模像素级显著性、通道级重要性和时频耦合关系。
  - **组件**：
    - **像素级检测器**：通过深度可分离卷积等操作，突出细粒度的像素级伪造线索。
    - **通道级放大器**：通过全局平均池化和压缩-扩展卷积，强调跨通道收集线索的重要性。
    - **时频耦合组件**：使用分解卷积联合建模频率和时间维度的依赖关系。

- **频率补偿增强模块**：
  - **目标**：补偿超短音频因时间信息有限而难以捕捉判别性时序模式的问题。
  - **机制**：利用多尺度频率分析和自适应频率-时间交互来补充特征。
  - **组件**：
    - **多尺度频率分析**：通过三个具有不同感受野的并行分支，捕获不同尺度的频率模式。
    - **自适应池化操作**：结合最大池化和平均池化，捕获全局频率模式。
    - **自适应频率-时间交互**：通过沿频率维度的深度卷积，捕捉时间维度上最重要的频率特征。

- **整体架构与优势**：
  - S-MGAA 将 PCEM、原有的 MGAA 注意力块和 FCEM 顺序集成，形成一个两阶段处理流程，逐步提炼和放大伪造相关线索。
  - 该设计是轻量级的，在计算量、参数量、实时性因子和训练时间上取得了良好的平衡，使其适合在通信系统和边缘设备上实时部署。

3)  
- **任务**：在包含六种真实通信退化条件（C0-C5，从干净到严重丢包）的 **ADD-C 测试集** 上，对 **超短音频**（0.5秒、1秒、1.5秒、2秒）进行深度伪造检测。
- **效果**：
  - **性能领先**：S-MGAA（使用MFCC特征时最佳）在全部四种时长和六种退化条件下，**一致超越了九种先进的基线模型**。
  - **显著提升**：在最具挑战性的0.5秒音频上，S-MGAA-MFCC 将平均等错误率相比最佳基线降低了 **23.89%**。相比其前身MGAA，EER相对降低幅度最高达 **70.67%**。
  - **高效轻量**：同时保持了极低的计算开销、参数量和稳定的实时推理速度，实现了精度与效率的优越权衡。
</div>

</details>

---

## SLM-SS: Speech Language Model for Generative Speech Separation
- **Authors**: Tianhua Li, Chenda Li, Wei Wang, Xin Zhou, Xihui Chen, Jianqing Gao, Yanmin Qian
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19533v1](https://arxiv.org/abs/2601.19533v1)
- **PDF**: [https://arxiv.org/pdf/2601.19533v1](https://arxiv.org/pdf/2601.19533v1)

基于神经网络的语音分离方法已取得显著进展，在信号级指标上表现出持续提升的性能。然而，这些方法在分离信号中往往难以保持语音清晰度，可能对语音识别等下游任务产生负面影响。本文提出SLM-SS，一种将语音语言模型应用于语音分离的新方法，旨在提升分离信号的清晰度与连贯性。我们将语音分离建模为离散多码本序列生成任务，采用编码器-解码器模型将量化后的混合语音映射为目标标记序列。除自回归建模策略外，本文还引入非自回归模型以提升残差标记的解码效率。在LibriMix数据集上的实验结果表明，相较于现有方法，本方法能显著更好地保持语音清晰度，并在多种下游任务中展现出更优的语言一致性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音分离旨在从混合语音中分离出独立说话人，对自动语音识别等下游任务至关重要。  
- **既有方法问题**：  
  - 判别式方法（如基于SI-SDR训练）虽能重建波形，但常损害语音可懂度，引入失真，影响下游任务性能。  
  - 生成式方法能输出更连贯的语音，但通常解码速度慢，且可能产生“幻觉”语音。

2)  
论文提出SLM-SS框架，将语音分离重构为离散多码本序列生成任务，核心方法如下：  
- **离散化与序列构建**：  
  - 使用Encodec将连续语音编码为32阶离散码本序列（每阶码本大小1024）。  
  - 采用串行输出训练策略，将多说话人码本序列拼接为单一序列，并用特殊符号（如`<SOS>`、`<SC>`、`<EOS>`）标记说话人边界。  
- **混合自回归与非自回归生成**：  
  - **自回归模型**：基于编码器-解码器架构，编码器使用预训练WavLM提取混合语音特征，解码器通过交叉注意力自回归预测零阶码本序列。  
  - **非自回归模型**：采用相同架构但移除单向掩码，利用独立词嵌入层整合所有低阶码本信息，高效预测高阶码本序列。  
- **解码与重建**：预测的多阶码本序列按说话人边界切分，输入Encodec解码器重建为分离后的纯净语音。

3)  
- **任务与效果**：在LibriMix数据集上评估，与BSRNN、Sepformer等基线相比：  
  - **语音可懂度**：主观听力测试得分更高（MOS: 4.19），词错误率显著降低（WER: 7.24 vs. 基线约29）。  
  - **下游任务一致性**：在语音识别相关指标（如LPS、SBS）上接近真实语音上限，显示更好的语言一致性。  
  - **效率**：非自回归设计提升了高阶码本的解码效率，且对温度参数不敏感，更易于实际应用。
</div>

</details>

---

## Permutation-Invariant Physics-Informed Neural Network for Region-to-Region Sound Field Reconstruction
- **Authors**: Xingyu Chen, Sipei Zhao, Fei Ma, Eva Cheng, Ian S. Burnett
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19491v1](https://arxiv.org/abs/2601.19491v1)
- **PDF**: [https://arxiv.org/pdf/2601.19491v1](https://arxiv.org/pdf/2601.19491v1)

现有声场重建方法大多针对点对区域重建，即对固定位置声源与接收区域间的声学传递函数进行插值。由于实际声学传递函数会随声源和接收区域位置连续变化，此类方法的应用范围受限。本文提出一种置换不变物理信息神经网络，用于区域对区域声场重建，旨在实现对连续变化声源与测量区域间声学传递函数的插值。该方法采用深度集合架构，将接收点与声源位置作为无序集合处理，以保持声学互易性。同时，通过引入亥姆霍兹方程作为物理约束指导网络训练，确保预测结果符合物理一致性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声场重建是许多音频应用的基础。传统方法（如基函数分解）和新兴方法（如物理信息神经网络）主要针对**点对区域**场景，即声源位置固定，仅重建接收区域内的声场。
- **既有问题**：实际声学传递函数会随声源和接收区域的**位置连续变化**而改变。现有方法未能有效处理声源和测量区域位置同时变化带来的挑战，限制了其适用性。

2)  
论文提出了一种**置换不变物理信息神经网络**，以解决区域到区域的声场重建问题。其核心方法通过以下两个关键设计实现：

- **置换不变网络架构**：
    - 采用**深度集合**架构，将接收点和声源点位置视为一个**无序集合**进行处理。
    - 通过特征提取子网络φ将每个坐标映射到隐空间，然后对特征进行**求和聚合**，再输入主网络ρ进行预测。
    - 求和操作确保了输入顺序的置换不变性，从而**内在地满足声学互易性原理**，无需在损失函数中显式约束。

- **物理约束引导训练**：
    - 将控制声波传播的**亥姆霍兹方程**作为物理损失项引入总损失函数。
    - 总损失由**数据损失**（确保预测与测量值一致）和**物理损失**（确保预测符合物理规律）加权构成。
    - 通过自动微分计算物理损失所需的二阶导数，引导网络输出符合物理规律，增强了在未测量区域的泛化能力。

该方法将物理先验知识与数据驱动模型相结合，利用有限测量数据，实现了对连续变化的声源和接收区域之间声学传递函数的准确插值。

3)  
- **任务**：在**区域到区域声场重建**任务上，使用真实世界（UTS）数据集进行评估，并与基于核的方法进行比较。
- **效果**：
    - 在消声室和半消声室环境中，所提方法在**大部分频率**（特别是1000 Hz以上）的重建精度优于核岭回归方法。
    - 在较高频段（1100-2000 Hz），所提方法能显著降低归一化均方误差（平均改善超过5 dB），并更好地捕捉声场的空间变化结构。
    - 消融实验验证了**置换不变架构**和**物理损失**两个组件对提升性能均有关键贡献。
</div>

</details>

---

## Dual-Strategy-Enhanced ConBiMamba for Neural Speaker Diarization
- **Authors**: Zhen Liao, Gaole Dai, Mengqiao Chen, Wenqing Cheng, Wei Xu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19472v1](https://arxiv.org/abs/2601.19472v1)
- **PDF**: [https://arxiv.org/pdf/2601.19472v1](https://arxiv.org/pdf/2601.19472v1)

Conformer与Mamba在语音建模中表现出色，但在说话人日志任务中存在局限。Mamba计算高效但难以捕捉局部细节与非线性模式；Conformer的自注意力机制对长语音序列内存开销大，且长程依赖建模可能不稳定。这些局限对说话人日志至关重要，因其需同时精确建模局部变化并保持长时段的说话人一致性。为解决这些问题，我们首次将ConBiMamba应用于说话人日志任务，基于Pyannote框架提出双策略增强的ConBiMamba神经说话人日志系统。ConBiMamba融合了Conformer与Mamba的优势：利用Conformer的卷积与前馈结构增强局部特征提取，并通过ExtBiMamba替换其自注意力模块，在高效处理长音频序列的同时降低内存消耗。针对说话人切换点附近错误率较高的问题，我们提出边界增强转移损失以提升切换点检测精度，并设计分层特征聚合机制以增强多层表征的利用率。该系统在六个日志数据集上进行评估，在四个数据集上取得了最先进的性能。本研究源代码已公开：https://github.com/lz-hust/DSE-CBM。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：说话人日志旨在确定音频中“谁在何时说话”。传统聚类方法存在子系统独立优化、无法处理重叠语音等问题。端到端神经方法（如EEND）虽支持重叠语音，但计算复杂度随说话人数和音频长度快速增长。  
- **既有方法问题**：  
  - **BiLSTM**：依赖顺序递归计算，难以捕获长程依赖。  
  - **Conformer**：其自注意力机制在长序列上计算和内存开销高，且长程依赖建模不稳定。  
  - **Mamba**：虽能高效建模长程依赖，但侧重全局信息，局部细节建模能力弱，影响说话人转换点检测精度。  

2)  
论文提出**双策略增强的ConBiMamba系统**，核心方法如下：  
- **主干网络设计**：  
  - 采用**ConBiMamba**作为局部EEND模块，融合Conformer与Mamba优势。  
  - 保留Conformer的卷积和前馈结构以增强**局部特征提取**；将其自注意力替换为**ExtBiMamba**，利用双向状态空间模型高效捕获长程依赖，降低内存开销。  
- **双策略增强**：  
  - **边界增强转换损失**：  
    - 问题：传统方法仅依赖帧级监督，需同时学习说话人识别与边界定位，导致转换点预测不稳定。  
    - 解决：引入**说话人转换点检测**作为辅助任务，显式建模相邻帧间的状态变化。  
    - 实现：设计专用损失函数（基于Focal Loss），动态调整正负样本权重，增强模型对边界区域的敏感性。  
  - **分层特征聚合**：  
    - 问题：现有方法多仅使用编码器最后一层输出，忽略了多层特征的互补性。  
    - 解决：提出**基于掩码的分层特征聚合策略**，自适应融合深层特征。  
    - 实现：通过可学习权重与静态掩码结合，选择性地聚合最后几层（如最后三层）输出，抑制浅层噪声，增强表征能力。  
- **训练与推理**：  
  - 遵循Pyannote流程，使用预训练WavLM提取特征，经ConBiMamba编码后输出多说话人状态。  
  - 采用两阶段训练（复合数据集预训练 + 各数据集微调），推理时结合ECAPA-TDNN提取说话人嵌入并进行聚类。  

3)  
- **任务与效果**：在六个公开数据集（AISHELL-4、RAMC、VoxConverse、MSDWild、AMI、AliMeeting）上进行说话人日志任务评估。  
- **性能表现**：  
  - 在**四个数据集**（AISHELL-4、RAMC、VoxConverse、MSDWild）上取得**最先进性能**，超越此前SOTA方法。  
  - 尤其在**边界检测**方面表现突出（如MSDWild上误报率5.02%、漏报率7.53%）。  
  - 消融实验验证了边界增强损失与分层特征聚合策略的有效性，均能显著降低DER。
</div>

</details>

---

## Residual Tokens Enhance Masked Autoencoders for Speech Modeling
- **Authors**: Samir Sadok, Stéphane Lathuilière, Xavier Alameda-Pineda
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19399v1](https://arxiv.org/abs/2601.19399v1)
- **PDF**: [https://arxiv.org/pdf/2601.19399v1](https://arxiv.org/pdf/2601.19399v1)

近期语音建模主要依赖音高、内容及说话人身份等显式属性，但这些属性本身无法完整捕捉自然语音的全部丰富特征。本文提出RT-MAE——一种新颖的掩码自编码器框架，通过引入无监督可训练残差标记来增强基于显式属性的监督建模。这些残差标记专门用于编码未被显式标注因素（如音色变化、噪声、情感等）解释的信息。实验表明，RT-MAE显著提升了重建质量，在保持内容与说话人相似度的同时增强了语音表现力。我们进一步验证了该框架在语音增强任务中的适用性，能够在推理阶段有效去除噪声，同时保持语音的可控性与自然度。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前语音建模主要依赖音高、内容和说话人身份等显式属性，但这些属性无法完全捕捉自然语音的丰富性（如音色变化、情感、微韵律等）。  
- **既有方法问题**：  
  - 现有方法（如AnCoGen）仅使用显式属性，未建模的残余信息被隐式吸收为数据集特定偏差，限制了泛化能力和可控性。  
  - 基于解耦的方法（如VAE、GST）虽能捕获部分残余信息，但依赖复杂的目标函数和任务特定损失，导致计算成本高、通用性差。

2)  
论文提出RT-MAE，通过引入**可训练的残余令牌**来显式编码显式属性未捕获的信息，并采用**正则化策略**控制信息流。具体方法如下：  
- **架构设计**：基于掩码自编码器（MAE）框架，在AnCoGen基础上增加连续型残余令牌。  
  - 使用**交叉注意力机制**（受Perceiver启发），通过固定数量的可学习查询向量从梅尔频谱中提取残余信息，压缩为紧凑的令牌表示。  
  - 残余令牌与离散化的属性令牌、梅尔频谱令牌拼接，由Transformer编码器-解码器处理，支持语音分析（从频谱预测属性）和生成（从属性与残余令牌重建频谱）。  
- **正则化策略**：  
  - 训练时对残余令牌应用**基于丢弃的正则化**：以阈值τ随机丢弃整个残余令牌向量，强制模型仅依赖显式属性进行重建，防止过度依赖残余令牌。  
  - 该机制确保模型平衡利用属性与残余信息，保持生成语音的可控性和可解释性。  
- **扩展应用**：  
  - 可引入**专用噪声残余令牌**，在训练时通过最小化互信息确保与语音残余令牌解耦；推理时禁用该令牌即可实现语音去噪。

3)  
RT-MAE在以下任务中取得显著效果：  
- **语音分析与合成**：在LibriSpeech和EmoV-DB数据集上，相比AnCoGen，STOI（可懂度）、N-MOS（自然度）、SpeechBertScore（语义准确性）和说话人相似性（COS）均提升，尤其在包含情感的EmoV-DB上情绪分类准确率从96.79%提升至98.65%。  
- **语音去噪**：在LibriMix数据集上，通过禁用噪声残余令牌实现去噪，N-MOS和信号质量（SIG）分数优于基线，同时保持高说话人相似性（COS从0.73提升至0.86）。  
- **音高操控**：在PTDB数据集上，保持与AnCoGen相同的音高控制精度，同时自然度和语义一致性进一步提高。
</div>

</details>

---

## Phase-Retrieval-Based Physics-Informed Neural Networks For Acoustic Magnitude Field Reconstruction
- **Authors**: Karl Schrader, Shoichi Koyama, Tomohiko Nakamura, Mirco Pezzoli
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19297v1](https://arxiv.org/abs/2601.19297v1)
- **PDF**: [https://arxiv.org/pdf/2601.19297v1](https://arxiv.org/pdf/2601.19297v1)

本文提出了一种基于稀疏空间幅度测量来重建声场幅度分布的方法，适用于相位测量不可靠或难以获取的场景。物理信息神经网络（PINN）通过将偏微分方程约束融入神经网络，已在声场估计中展现出潜力，但其损失函数依赖于相位信息，无法直接应用于仅含幅度测量的情况。为此，我们提出一种基于相位恢复的PINN方法，通过分别用两个网络表示幅度与相位分布，从而基于重建的复振幅计算偏微分方程损失。实验结果表明，该方法在仅使用幅度测量的条件下能有效实现声场幅度重建。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声场估计旨在从稀疏传感器数据重建声场分布。传统方法（如基展开、核回归）和新兴的物理信息神经网络（PINN）通常依赖于复值振幅（含相位）测量，以结合控制偏微分方程（如亥姆霍兹方程）作为物理约束。  
- **既有问题**：在实际场景中（如异步麦克风阵列、乐器指向性测量），相位测量往往不可靠或无法获取，仅能获得声压幅度数据。由于控制方程的计算需要相位信息，现有PINN无法直接应用于纯幅度场估计，导致方法通常只能依赖纯数据驱动技术，缺乏物理约束，估计精度受限。

2)  
- **核心思路**：提出**基于相位检索的PINN（PRB-PINN）**，通过联合估计幅度和相位分布，间接引入物理约束。即使只有幅度测量，也能利用亥姆霍兹方程正则化重建过程。  
- **网络架构**：  
  - 使用两个独立的多层感知机（MLP）分别建模幅度场和相位场，输入为空间坐标经随机傅里叶特征编码后的高维特征。  
  - 将两个网络的输出组合为复值声压场，从而可计算控制方程残差。  
- **损失函数设计**：  
  - **数据损失**：采用对数尺度幅度误差（等效于单频对数谱距离），强制重建幅度在传感器位置与测量值匹配。  
  - **物理损失**：基于重建的复值场计算亥姆霍兹方程残差的均方误差，作为物理正则项。  
  - 加权总损失为两者加权和，通过梯度下降优化。  
- **解决机制**：  
  - 通过相位检索隐式恢复相位信息，使重建的复值场满足物理规律。  
  - 物理损失约束了函数空间，即使相位不唯一，仍能提升幅度场重建的准确性和泛化能力，避免纯数据驱动的过拟合问题。

3)  
- **任务**：在模拟房间环境中，从稀疏空间点测量重建频率域声压幅度场。  
- **效果**：  
  - 在200Hz、400Hz、600Hz多个频率及不同测量点数量（5-50个）下，PRB-PINN均优于最近邻插值和无物理损失的神经场方法。  
  - 幅度重建误差随测量点增加而降低，且在高频（空间变化更快）场景下仍保持优势。  
  - 可视化显示，尽管恢复的相位与真实相位不完全一致，但物理约束使幅度分布的空间变化频率更接近真实场，显著提升重建质量。
</div>

</details>

---

## SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper
- **Authors**: Alexander Polok, Dominik Klement, Samuele Cornell, Matthew Wiesner, Jan Černocký, Sanjeev Khudanpur, Lukáš Burget
- **Categories**: eess.AS, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.19194v1](https://arxiv.org/abs/2601.19194v1)
- **PDF**: [https://arxiv.org/pdf/2601.19194v1](https://arxiv.org/pdf/2601.19194v1)

在多说话人环境中，说话人归属的自动语音识别（ASR）仍是一项重大挑战。尽管部分方法在特定领域微调后表现优异，但鲜有系统能在跨领域数据集上实现良好泛化。我们先前提出的工作——基于说话人日志化条件约束的 Whisper 模型（DiCoW），利用说话人日志化输出作为条件信息，仅需少量微调即可在多语言与多领域任务中展现出强大性能。本文针对 DiCoW 的一个关键局限进行改进：其静默-目标-非目标-重叠（STNO）掩码存在歧义，当两个或多个说话人完全重叠时，即使其转写内容不同，模型接收的条件信息仍可能近乎相同。为此，我们提出 SE-DiCoW（自注册的说话人日志化条件约束 Whisper 模型），该模型利用日志化输出在对话中定位目标说话人最活跃的片段作为注册段，并通过各编码器层的交叉注意力机制将其作为固定条件信息。我们进一步通过改进数据分割策略、模型初始化方法及数据增强技术优化 DiCoW。综合这些改进，SE-DiCoW 在 EMMA MT-ASR 基准测试中，相比原始 DiCoW 将宏观平均 tcpWER 相对降低了 52.4%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：多说话人环境下的说话人归属自动语音识别（ASR）是核心挑战。现有方法存在局限：
  - 模块化方法（结合语音分离、说话人日志和ASR）复杂、泛化性差且易产生级联错误。
  - 端到端方法（如说话人令牌条件化）简化了流程，但性能通常不及模块化方法。
  - 目标说话人ASR方法依赖于说话人嵌入或注册音频，在训练数据有限或说话人变异性高时难以泛化。
- **既有方法问题**：本文的前期工作DiCoW利用说话人日志输出的STNO（静音-目标-非目标-重叠）掩码作为条件信息，避免了显式的说话人身份建模，在多语言和多领域表现良好。但其关键**局限性**在于：当语音完全重叠时，不同说话人的STNO掩码可能近乎相同，导致模型无法区分说话人，产生转录歧义。

2)  
论文提出了SE-DiCoW，通过**自注册机制**和多项技术改进来解决上述STNO掩码歧义问题，并提升整体性能。核心方法包括：

- **自注册机制**：
  - **问题定位**：在完全重叠的语音区域，STNO掩码无法提供区分不同说话人的有效信息。
  - **解决方案**：模型利用说话人日志输出，自动在整个对话中定位**目标说话人最活跃的片段**（通过最大化目标说话人概率和来选择）。该片段被用作**固定的注册条件**。
  - **集成方式**：将注册片段的音频特征通过**跨注意力机制**集成到Whisper编码器的每一层。这使得模型即使在STNO掩码模糊时，也能利用注册片段中的说话人声学特征来维持对目标说话人的追踪，从而解决歧义。

- **对原始DiCoW框架的额外改进**：
  - **模型初始化与结构**：
    - 在卷积下采样后、位置编码相加前，新增了一个**帧级说话人日志依赖变换（FDDT）层**，以更早地利用STNO信息进行调制。
    - 调整了FDDT中非目标和静音变换矩阵的初始化缩放因子，以减轻过度抑制。
  - **训练数据分割校正**：修正了训练片段的分割方式，使其更符合Whisper原始的数据格式，不再强制30秒窗口结束时必须有一个明确的“段结束”时间戳，而是使用序列结束（EOS）标记。
  - **数据增强**：
    - 对STNO掩码添加高斯噪声并重新归一化，以增强模型对说话人日志错误的鲁棒性。
    - 引入了段级别的STNO最可能类别翻转。
    - 联合应用SpecAugment和MUSAN噪声。

- **整体效果**：这些改进共同作用，使SE-DiCoW能够更有效地处理重叠语音，并提升在多种场景下的鲁棒性和准确性。

3)  
SE-DiCoW在多个多说话人ASR任务上取得了显著效果提升：
- **整体性能**：在EMMA MT-ASR基准测试中，相比原始DiCoW，**宏观平均tcpWER相对降低了52.4%**。
- **高重叠场景**：在完全重叠的合成数据集Libri3Mix-clean上，错误率相对降低超过75%。
- **真实对话数据**：在AMI（单远场麦克风）和NOTSOFAR-1等真实数据集上也取得了稳定的绝对性能提升。
- **与前沿系统对比**：在使用真实说话人日志（DiariZen）时，SE-DiCoW在AMI SDM和Libri2Mix上达到了最先进的性能，并在其他数据集上与经过领域调优的系统表现相当，同时保持了良好的跨领域泛化能力。
</div>

</details>

---

## LuSeeL: Language-queried Binaural Universal Sound Event Extraction and Localization
- **Authors**: Zexu Pan, Shengkui Zhao, Yukun Ma, Haoxu Wang, Yiheng Jiang, Biao Tian, Bin Ma
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19153v1](https://arxiv.org/abs/2601.19153v1)
- **PDF**: [https://arxiv.org/pdf/2601.19153v1](https://arxiv.org/pdf/2601.19153v1)

现有通用声音提取算法大多专注于从单通道音频混合信号中分离目标声音事件。然而，真实世界是三维的，模拟人类听觉的双耳音频能够捕捉更丰富的空间信息，包括声源位置。这种空间上下文对于理解和建模复杂听觉场景至关重要，因为它天然地为声音检测与提取提供了信息。本研究提出一种语言驱动的通用声音提取网络，通过有效利用双耳信号中的空间线索，从双耳混合音频中分离出文本描述的目标声音事件。此外，我们联合利用提取网络中的空间特征预测目标声音的到达方向。这种双任务方法通过利用互补的位置信息提升提取性能，同时实现精确的到达方向估计。在真实场景AudioCaps数据集上的实验结果表明，我们提出的LuSeeL模型显著优于单通道及单任务基线方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频场景分析对增强现实、机器人等领域至关重要。现有通用声音事件提取方法多依赖单声道音频，无法有效利用空间信息，在复杂重叠声源场景中性能受限。  
- **既有问题**：  
  - 多数方法仅使用单声道音频，缺乏空间线索，难以区分不同方向的声音源。  
  - 现有方法多依赖外部提示（如文本、视觉），在复杂声学条件下处理重叠声音的能力不足。  
  - 声音事件提取与定位任务通常独立研究，未能利用两者间的互补性提升整体性能。

2)  
论文提出 **LuSeeL** 模型，通过多模态联合学习框架解决上述问题：  
- **双耳音频输入**：采用模拟人耳听力的双声道音频作为输入，天然包含丰富的空间信息，为声音分离提供额外的方位线索。  
- **语言驱动的条件提取**：  
  - 使用预训练的T5文本编码器将自由形式的文本描述（如关键词、句子）编码为语义向量。  
  - 通过FiLM层将文本嵌入与音频特征（时域和频域）进行融合，实现基于语言描述的针对性声音提取。  
- **双任务联合架构**：  
  - **提取网络**：采用双域混合Transformer（HTDemucs）作为主干，同时处理时域和频域信号，并通过自注意力与跨注意力机制融合多模态信息。  
  - **定位网络**：利用提取网络中的频谱特征，并结合计算双耳互相关（GCC-PHAT）特征，共同预测目标声源的到达方向（DoA）。  
- **端到端联合优化**：  
  - 使用混合损失函数（时频域信号重建损失 + DoA估计的均方误差损失）进行训练。  
  - 通过梯度反向传播，使提取任务和定位任务共享并优化同一套空间感知的音频-文本表示，相互促进。

3)  
在 **AudioCaps** 数据集生成的2源和3源混合音频上进行评估：  
- **声音提取效果**：在2源混合上，SI-SNRi达到20.3 dB，显著优于单声道基线（7.7 dB）和仅做提取的双耳基线（17.6 dB）。在更复杂的3源混合上也保持领先。  
- **声音定位效果**：在2源混合上，方位估计准确率（±5°容差）达89.9%，平均绝对误差（MAE）为7.0°，优于仅做定位的基线（准确率41.1%，MAE 51.6°）。  
- **关键结论**：双耳输入与联合训练共同提升了性能；模型学习到的空间表示甚至可部分替代手工设计的GCC-PHAT特征。
</div>

</details>

---

## Beyond Lips: Integrating Gesture and Lip Cues for Robust Audio-visual Speaker Extraction
- **Authors**: Zexu Pan, Xinyuan Qian, Shengkui Zhao, Kun Zhou, Bin Ma
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19130v1](https://arxiv.org/abs/2601.19130v1)
- **PDF**: [https://arxiv.org/pdf/2601.19130v1](https://arxiv.org/pdf/2601.19130v1)

当前多数视听说话人提取方法依赖同步唇部记录，从多人混合语音中分离目标说话人的语音。然而，在自然人际交流中，伴随语音的手势在时间上也与语音对齐，常用于强调特定词语或音节。这些手势提供了互补的视觉线索，在面部或唇部区域被遮挡或距离较远时尤其有价值。本研究突破以唇部为中心的传统思路，提出SeLG模型，该模型融合唇部与上半身手势信息以实现鲁棒的说话人提取。SeLG采用基于交叉注意力的融合机制，使每种视觉模态能够查询并选择性关注混合语音中的相关语音特征。为提升手势表征与语音动态的对齐效果，SeLG还引入对比性InfoNCE损失函数，促使手势嵌入与对应唇部嵌入更紧密对齐（后者与语音相关性更强）。在包含TED演讲的YGD数据集上的实验结果表明：所提出的对比学习策略显著提升了基于手势的说话人提取性能；通过注意力机制与InfoNCE损失有效融合唇部与手势线索的SeLG模型，在完整模态及部分模态（即模态缺失）条件下均优于基线方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：鸡尾酒会问题旨在从多人语音混合中提取目标说话人。现有音频-视觉方法主要依赖唇部运动作为视觉线索。
- **既有方法的问题**：
  - **唇部线索的局限性**：在面部遮挡、远距离或低分辨率场景下，唇部区域可能无法可靠检测或跟踪。
  - **单模态的脆弱性**：仅依赖唇部或仅依赖手势的方法在对应线索缺失时性能会显著下降。
  - **融合方式简单**：先前多模态工作通常采用简单的特征拼接融合，未能充分挖掘不同视觉线索与语音的相关性。

2)  
论文提出 **SeLG** 模型，通过整合唇部与上半身伴随手势线索，并引入两项核心创新来解决上述问题：

- **基于交叉注意力的多模态融合机制**：
  - 模型为唇部嵌入和手势嵌入分别设计了交叉注意力层。
  - 每个视觉模态作为“查询”，去混合语音编码中“检索”并选择与之相关的语音特征，而非简单的特征拼接。
  - 这种机制允许模型动态地、有选择地关注不同视觉线索所对应的语音内容，实现了更精细的跨模态信息整合。

- **基于对比学习的表征增强**：
  - 为了弥补手势与语音关联性弱于唇部与语音关联性的不足，论文引入了 **InfoNCE 对比损失**。
  - 该损失函数的核心是拉近同一时间步的手势嵌入与唇部嵌入（作为正样本），同时推远与其他时间步唇部嵌入（作为负样本）的距离。
  - 此举利用唇部运动与语音强相关的特性作为“锚点”，间接提升了手势表征与语音动态的对齐质量，增强了手势线索的判别力。

- **整体架构与训练**：
  - 模型包含独立的唇部编码器、手势编码器和语音编码器。
  - 分离器通过重复的交叉注意力层和双路径BLSTM来估计目标语音的掩码。
  - 总损失函数为语音重建的尺度不变信噪比损失与上述对比损失之和，共同优化模型。

3)  
- **任务**：在模拟的 YouTube Gesture Dataset (YGD) 上，进行了 **2人** 和 **3人** 混合语音下的说话人提取任务评估，并专门测试了**视觉线索完整**与**部分缺失**（如只有唇部或只有手势）的场景。
- **效果**：
  - **性能提升**：在2人混合任务上，完整测试集的SI-SNRi达到13.2 dB；在3人混合任务上达到11.7 dB，均显著优于仅用唇部或仅用手势的基线模型。
  - **鲁棒性验证**：在视觉线索部分缺失的测试子集上，SeLG性能下降幅度远小于单模态基线，证明了其利用互补信息应对不完整观测的鲁棒性。
  - **消融实验证实**：交叉注意力融合策略优于拼接融合；引入的对比损失有效提升了手势表征质量，尤其在模态缺失时增益更明显。
</div>

</details>

---

## A Hybrid Discriminative and Generative System for Universal Speech Enhancement
- **Authors**: Yinghao Liu, Chengwei Liu, Xiaotao Liang, Haoyin Yan, Shaofei Xue, Zheng Xue
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19113v1](https://arxiv.org/abs/2601.19113v1)
- **PDF**: [https://arxiv.org/pdf/2601.19113v1](https://arxiv.org/pdf/2601.19113v1)

通用语音增强旨在处理具有多种语音失真和录制条件的输入。本研究提出一种新型混合架构，将判别式建模的信号保真度与生成式建模的重构能力相协同。该系统采用具有采样率无关策略的判别式TF-GridNet模型，以通用方式处理可变采样率。同时，结合频谱映射建模的自回归模型能够生成细节丰富的语音，并有效抑制生成伪影。最后，在信号级损失和综合语音质量评估（SQA）损失的联合优化下，融合网络学习两个输出的自适应权重。所提出的系统在ICASSP 2026 URGENT挑战赛（赛道1）中评估，位列第三名。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：通用语音增强旨在处理各种失真和录音条件下的语音信号。现有方法主要分为判别式和生成式两类。
- **既有问题**：
  - 判别式模型（如TF-GridNet）在信号保真度和噪声抑制方面表现优异，但对严重受损语音成分的重建能力不足。
  - 生成式模型能重建高质量语音，但常因学习到的生成先验与真实干净语音分布不完全对齐，导致产生幻觉和伪影。

2)  
论文提出一种混合架构，协同整合判别式建模的信号保真度与生成式建模的重建能力，具体通过三个核心组件解决上述问题：
- **判别式分支**：采用TF-GridNet作为主干，处理复数语谱图。通过**采样频率无关策略**，固定STFT窗长和跳数，根据输入采样率调整频点数，从而普适地处理不同采样率的输入，保持频谱完整性。
- **语义条件细化分支**：利用自回归生成模型处理复杂失真。
  - 使用预训练WavLM提取退化语音的鲁棒语义和声学表示，作为条件前缀。
  - 引导仅解码器语言模型自回归预测干净语音的离散令牌（通过X-Codec提取，仅使用第一层RVQ令牌以捕获主要语义和感知信息）。
  - 为规避离散令牌的信息瓶颈，采用DPRNN模型直接预测干净语音频谱：将来自语言模型最后一层的语义丰富表示与从退化频谱导出的声学丰富特征融合，通过交叉注意力整合全局语义引导与局部声学细节，最终估计复数掩码。该分支以16 kHz运行，聚焦低频语音信息。
- **融合网络**：使用轻量级USES网络自适应整合两个分支的输出。通过估计时频融合掩码，按元素加权组合判别式输出和生成式输出（经重采样以匹配采样率），生成最终频谱。
- **损失函数设计**：
  - 判别式分支使用多分辨率STFT损失。
  - 生成式分支结合令牌预测的负对数似然损失和掩码估计的回归损失（加权组合复数、幅度谱误差及感知损失）。
  - 融合网络使用多分辨率STFT损失、L1损失及综合语音质量评估损失（基于多个感知指标，如MOS、DNSMOS等）进行优化，确保输出在信号保真度和感知质量间取得平衡。

3)  
- **任务**：在ICASSP 2026 URGENT挑战赛（Track 1，通用语音增强）的非盲测试集上进行评估。
- **效果**：
  - 在信号保真度指标（PESQ、ESTOI）上保持竞争力，同时显著提升感知质量（DNSMOS、NISQA）。
  - 在说话人相似度（SpkSim）和下游语音识别准确率（CAcc）上取得最佳性能。
  - 最终在挑战赛中排名第三，验证了混合架构在整合判别式精度与生成式丰富性方面的有效性。
</div>

</details>

---

## Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation
- **Authors**: Nanhan Shen, Zhilei Liu
- **Categories**: cs.AI, cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19112v1](https://arxiv.org/abs/2601.19112v1)
- **PDF**: [https://arxiv.org/pdf/2601.19112v1](https://arxiv.org/pdf/2601.19112v1)

情感说话人脸合成在多媒体与信号处理领域至关重要，但现有三维方法面临两大关键挑战：音视频情感对齐能力不足，表现为音频情感提取困难以及对情感微表情的控制不充分；以及采用忽视不确定性与特征质量差异的“一刀切”多视角融合策略，导致渲染质量下降。本文提出UA-3DTalk——一种基于情感先验蒸馏的不确定性感知三维情感说话人脸合成方法，其包含三个核心模块：先验提取模块将音频解耦为用于对齐的内容同步特征和用于个性化的对象特定互补特征；情感蒸馏模块引入多模态注意力加权融合机制及多分辨率码本的四维高斯编码，实现细粒度音频情感提取与情感微表情的精准控制；不确定性形变模块通过不确定性块估计视角特定的偶然性（输入噪声）与认知性（模型参数）不确定性，实现自适应多视角融合，并结合多头解码器优化高斯基元以克服均匀权重融合的局限性。在常规与情感数据集上的大量实验表明，UA-3DTalk在情感对齐指标E-FID上较DEGSTalk、EDTalk等前沿方法提升5.2%，唇形同步指标SyncC提升3.1%，渲染质量指标LPIPS提升0.015。项目页面：https://mrask999.github.io/UA-3DTalk

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：3D情感说话人脸合成在多媒体领域至关重要，但现有方法面临两大挑战。  
- **既有问题**：  
  - **音视频情感对齐差**：音频情感提取困难，且对情感微表情的控制不足。  
  - **多视图融合策略僵化**：采用“一刀切”的融合方式，忽视不同视图的特征质量差异和不确定性，导致渲染质量下降。

2)  
论文提出 **UA-3DTalk** 框架，通过三个核心模块解决上述问题：  
- **先验提取模块**：  
  - 将音频解耦为内容同步特征（用于音视频对齐）和个性化互补特征（用于身份保持），共同确保同步精度和身份一致性。  
- **情感蒸馏模块**：  
  - 扩展音频窗口并采用多模态注意力加权融合机制，结合频谱图、MFCC和音频波形特征，增强情感信息提取。  
  - 通过多分辨率码本和4D高斯编码对情感特征进行离散化处理，实现对情感微表情的精细控制。  
- **基于不确定性的形变模块**：  
  - 为每个视图特征部署不确定性块，分别估计**偶然不确定性**（来自输入噪声）和**认知不确定性**（来自模型参数）。  
  - 基于不确定性动态调整视图权重（不确定性越高，权重越低），实现自适应多视图融合。  
  - 采用多头高斯形变解码器，分别优化高斯基元的位置、旋转和缩放参数，提升渲染保真度。

3)  
- **任务与效果**：在常规和情感数据集上进行了综合评估。  
- **性能提升**：  
  - **情感对齐**：E-FID指标相比EDTalk提升5.2%。  
  - **唇部同步**：Sync-C指标相比DEGSTalk提升3.1%。  
  - **渲染质量**：LPIPS指标降低0.015，图像质量更优。  
- **优势**：在纯音频驱动场景下，无需依赖参考视频或3DMM真值参数，即实现更准确的唇部同步、更自然的眼部运动及更精确的情感表情预测。
</div>

</details>

---

## Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings
- **Authors**: Arhan Vohra, Taketo Akama
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19109v1](https://arxiv.org/abs/2601.19109v1)
- **PDF**: [https://arxiv.org/pdf/2601.19109v1](https://arxiv.org/pdf/2601.19109v1)

基于感知相似度的表征能够使音乐检索系统判断哪些歌曲在听感上最为相似。当前最先进的方法通过自监督度量学习进行任务特定训练，显示出与人类判断的良好一致性，但由于可用数据集有限，其可解释性和泛化能力存在不足。本文研究表明，预训练的文本-音频嵌入模型（CLAP和MuQ-MuLan）无需额外微调即可在相似性任务上达到相当的感知对齐效果。为超越这一基线，我们提出一种创新方法：利用听感测试中的ABX偏好数据，通过音源分离和线性优化技术对预训练嵌入进行感知对齐。该模型提供可解释且可控制的乐器维度权重，使音乐制作人能够基于混合参考曲目检索分轨级别的循环乐段和采样素材。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐检索系统需基于人类感知的相似性。现有方法依赖特定任务的自监督度量学习，虽与人类判断有较好对齐，但存在局限。
- **既有问题**：
  - **可解释性与泛化性差**：模型多为黑箱，难以解释其决策依据；且因依赖特定数据集训练，泛化能力有限。
  - **数据与资源限制**：需要大规模、高质量的感知标注数据进行训练，流程资源密集。
  - **混合音频处理不足**：传统方法难以从完整曲目中分离并比较特定乐器音色，限制了制作场景下的实用性。

2)  
论文提出了一种**基于预训练嵌入的可解释、感知对齐的音乐相似性方法**，核心包含两个部分：

- **零射评估预训练嵌入**：
  - 直接评估了CLAP和MuQ-MuLan等通用文本-音频预训练模型在感知相似性任务上的表现，发现它们无需微调即可达到与专用模型相当的性能。这证明了大规模多模态预训练能有效捕获感知相关信息。

- **提出乐器感知的加权相似性模型**：
  - **核心思想**：将完整混音的感知相似性建模为其各乐器分轨相似性的加权和。
  - **方法步骤**：
    1. **源分离**：使用音乐源分离模型（如Demucs）或利用数据集提供的真实分轨，将混合音频分解为独立的乐器音轨（如贝斯、鼓、吉他等）。
    2. **嵌入提取**：使用冻结的预训练音频编码器（CLAP或MuQ-MuLan）为每个分轨和完整混音分别提取嵌入向量。
    3. **相似性计算与加权**：对于给定的参考音频X和候选音频A、B，计算每个乐器类别k上的余弦相似度差值，形成一个特征向量。然后，通过线性回归（OLS或岭回归）学习一组可解释的权重，这些权重代表了各乐器对人类感知相似性的贡献度。
  - **如何解决前述问题**：
    - **提升可解释性与可控性**：模型输出明确的乐器权重，使用户（如音乐制作人）能够理解相似性判断的依据，并可能通过调整权重来控制检索偏好（例如，更关注鼓的相似性）。
    - **避免资源密集型训练**：方法基于冻结的预训练嵌入，仅需在相对较小的ABX偏好数据上进行轻量级线性优化，无需端到端微调。
    - **处理混合音频**：通过源分离显式地解构混合音频，使模型能够评估乐器级别的相似性，从而支持“根据参考曲目检索相似鼓点”等实用工作流。

3)  
- **任务**：在Inst-Sim-ABX数据集上进行音乐感知相似性判断任务，包括XAB（所有片段来自不同曲目）和XYC（参考与一个候选来自同一曲目）两种配置。
- **效果**：
  - **零射预训练模型**：MuQ-MuLan在多个乐器类别和完整混音上表现出色，尤其在XYC任务中，对鼓和完整混音的判断准确率超过96%，与专用模型（如Cascade-PAFT）竞争力相当。
  - **加权相似性模型**：基于MuQ-MuLan的6音轨加权模型取得了最佳性能，在XAB任务上达到了**90.4%**的感知一致率，超越了所有基线模型（包括专用模型和纯余弦相似度基线）。该模型还提供了可解释的乐器权重（如鼓、吉他权重较高）。
</div>

</details>

---

## Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback
- **Authors**: Siddhant Arora, Jinchuan Tian, Jiatong Shi, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19063v1](https://arxiv.org/abs/2601.19063v1)
- **PDF**: [https://arxiv.org/pdf/2601.19063v1](https://arxiv.org/pdf/2601.19063v1)

针对语音输入/输出的对话系统，基于人类或人工智能反馈的强化学习研究仍显不足，现有工作大多局限于在话语层面应用单一语义奖励。此类方法未能充分考虑对话质量的多维性与多模态特性，包括语义连贯性、音频自然度、说话人一致性、情感对齐及话轮转换行为。此外，它们与采用增量式生成的双工语音对话系统存在根本性不匹配——这类系统需基于部分话语片段进行实时决策。为此，我们首次提出面向语音对话系统的多奖励人工智能反馈强化学习框架，融合了语义质量、音频质量与情感一致性三类奖励。为使话语级偏好与双工模型的增量分块解码机制对齐，我们采用话轮级偏好采样方法，并在单一直接偏好优化目标中聚合分块对数概率。本研究首次系统探讨了通过偏好学习提升语音对话系统质量的方案，涵盖多轮思维链模型与分块双工模型，并发布了多奖励直接偏好优化数据集以支持可复现研究。实验表明，单奖励人工智能反馈强化学习可针对性提升其目标指标，而联合多奖励训练能在语义质量与音频自然度上实现一致增益。这些结果凸显了面向实用对话系统时，进行整体性多奖励对齐的重要性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：口语对话系统正从轮次交互转向实时全双工交互，但现有方法在优化对话质量方面存在不足。  
- **既有问题**：  
  - **单维度奖励**：先前工作主要依赖单一语义奖励，忽略了对话质量的多维度特性（如语义连贯性、音频自然度、情感一致性等）。  
  - **反馈不匹配**：传统方法基于完整话语的反馈，与全双工系统增量式、分块生成的解码模式不兼容。  
  - **音频奖励缺失**：现有研究缺乏对音频质量（如自然度、清晰度）的系统性优化。

2)  
- **核心方法**：提出首个面向口语对话系统的**多奖励RLAIF框架**，通过统一偏好学习联合优化多个对话质量维度。  
- **具体解决方案**：  
  - **多奖励设计**：构建四个独立的偏好数据集，分别针对：  
    - **语义质量**：使用LLM评估生成文本的连贯性、相关性与事实性。  
    - **音频质量**：基于UTMOS语音质量评估器优化自然度。  
    - **清晰度**：通过Whisper识别词错误率提升可懂度。  
    - **情感一致性**：利用Emo2Vec编码器确保生成语音与人类参考在情感上对齐。  
  - **流式偏好学习**：为兼容全双工系统的分块解码，提出**话语级偏好应用于分块生成**的机制：  
    - 将完整话语拆分为固定大小的块，在DPO目标中聚合每个块的对数概率，使话语级偏好能指导增量生成决策。  
  - **训练框架**：采用Direct Preference Optimization，通过联合采样多奖励偏好数据，在单一目标中共同优化语义、音频与情感维度。

3)  
- **任务与效果**：  
  - **语义质量提升**：在Multi-turn CoT E2E模型上，单一语义奖励训练使LLM评分显著提高（6.18→6.33），低质量回复减少28.5%。  
  - **音频质量改进**：单一音频奖励训练将UTMOS分数从2.16提升至3.06；单一清晰度奖励将WER从6.1%降至3.3%。  
  - **多奖励联合优化**：联合训练（语义+音频+清晰度）在保持语义质量的同时，进一步提升音频自然度（UTMOS 2.85）与可懂度（WER 1.0%）。  
  - **全双工模型适配**：在SCoT-Response全双工模型上，RLAIF显著提升ROUGE-L（19.8→23.1）与LLM评分（5.95→6.00），验证了框架的泛化能力。
</div>

</details>

---
