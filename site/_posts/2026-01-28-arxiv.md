---
layout: post
title: "arXiv Daily – 2026-01-28"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-28（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-27 08:50 — 2026-01-28 08:50
- 抓取总数：21 篇 | 本页显示：20 篇（去重/过滤后）

## Rethinking Discrete Speech Representation Tokens for Accent Generation
- **Authors**: Jinzuomu Zhong, Yi Wang, Korin Richmond, Peter Bell
- **Categories**: eess.AS, cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19786v1](https://arxiv.org/abs/2601.19786v1)
- **PDF**: [https://arxiv.org/pdf/2601.19786v1](https://arxiv.org/pdf/2601.19786v1)

离散语音表征标记已成为语音生成的基础组件。尽管先前研究已深入探讨了DSRT中的音素和说话人信息，但其口音信息的编码机制仍鲜有研究。本文首次系统性地探究了DSRT中的口音信息。我们提出了一个统一的评估框架，通过新颖的"口音ABX任务"衡量口音信息的可访问性，并通过跨口音语音转换重合成评估其可恢复性。基于该框架，我们对多种语音编码器衍生的DSRT进行了分析。实验结果表明：当使用自动语音识别监督微调编码器时，口音信息会显著减少；而通过简单缩减码本规模无法有效分离口音信息与音素、说话人信息。基于这些发现，我们提出了新型的纯内容标记与内容-口音标记，在可控口音生成任务中显著优于现有设计方案。本研究强调了口音感知评估的重要性，并为面向口音可控语音生成的DSRT设计提供了实践指导。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：离散语音表示标记（DSRTs）已成为语音生成的基础组件。现有研究主要关注DSRTs中的音素和说话人信息，而口音信息如何编码在其中则基本未被探索。  
- **既有方法的问题**：  
  - 当前DSRTs的设计、评估和应用普遍忽视口音信息。  
  - 现有评估框架主要针对音素和说话人信息，缺乏对口音的系统性评估。  
  - 一些方法（如简单调整码本大小或使用ASR监督）声称能实现可控口音生成，但缺乏系统性验证，且可能削弱口音信息。

2)  
- **提出统一评估框架**：  
  - **可访问性评估**：提出新颖的“口音ABX”任务，衡量DSRTs对口音信息的区分能力。  
  - **可恢复性评估**：通过跨口音语音转换（VC）重合成，衡量从DSRTs中恢复口音、说话人和音素信息的能力。  
- **系统分析DSRTs**：  
  - 使用该框架分析了多种语音编码器（如HuBERT、HuBERT-ft、Whisper）衍生的DSRTs。  
  - 关键发现：  
    - 口音信息在HuBERT的中早期层（如第6、9层）最显著，与音素或说话人信息的分布不同。  
    - 使用ASR监督微调编码器会大幅减少口音信息。  
    - 单纯减少码本大小无法有效解耦口音、音素和说话人信息。  
- **提出新DSRTs设计方案**：  
  - 基于上述发现，提出了新的“仅内容”和“内容-口音”DSRTs。  
  - “仅内容”DSRTs：使用HuBERT-ft第18层（经ASR监督，口音信息少）搭配较小码本（如256），旨在最小化口音信息，用于适应目标说话人口音的VC。  
  - “内容-口音”DSRTs：使用HuBERT第9层（口音信息丰富）搭配大码本（如8192），旨在保留源说话人口音，用于口音保持的VC。  
- **方法优势**：  
  - 通过层选择和码本大小调整，更精细地控制口音信息的编码。  
  - 评估框架首次系统地将口音纳入DSRTs评估，为设计可控口音生成的DSRTs提供实用指导。

3)  
- **任务**：在跨口音语音转换（VC）任务上评估了提出的DSRTs。  
- **效果**：  
  - 提出的“内容-口音”和“仅内容”DSRTs在多项客观指标上均优于现有方法（如Vevo的“内容-风格”标记）。  
  - 具体而言，在口音相似性、说话人相似性、音素后验图距离和词错误率方面均取得更好性能。  
  - 实现了更可控的口音生成：能更好地在VC中保留源说话人口音或适应目标说话人口音。
</div>

</details>

---

## Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means
- **Authors**: Kentaro Onda, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19781v1](https://arxiv.org/abs/2601.19781v1)
- **PDF**: [https://arxiv.org/pdf/2601.19781v1](https://arxiv.org/pdf/2601.19781v1)

近年来，利用离散化标记表示语音的研究日益受到关注，这些标记既可作为语音语言模型的伪文本，也可作为下游任务的高效中间表示。现有标记通常分为声学标记与音素标记两类：前者保留用于重建的详细声学信息，后者则主要捕捉语言内容。然而在人类语音交流中，无关的声学细节（如说话人信息）会被抽象化，而语言信息与韵律信息均被用于语音理解与生成。基于此，现有两类标记均非对韵律敏感的语音语言模型等任务的理想表示。本研究提出音系标记器，该方法通过可微分K均值算法，以语音识别与语音重合成的多任务目标对音素标记进行微调。在多类任务上的实验验证表明，所提标记在有效去除说话人身份信息的同时，完整保留了音系层面的语言与韵律信息。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音离散化表示（如声学token和语音学token）是语音语言模型（speechLM）和下游任务的关键。声学token保留过多细节（如说话人信息），语音学token则过度偏向语言内容，两者均未充分兼顾人类语音理解所需的**韵律信息**。
- **既有问题**：现有方法（如混合token）多基于多码本的残差向量量化，导致**压缩效率低**，且下游模型需复杂架构处理多码本流。同时，这些方法未能理想地平衡语言内容、韵律与说话人信息的分离。

2)  
- **核心方法**：提出**音系学分词器**，通过**可微分k均值**对预训练SSL模型（WavLM-large）提取的语音学token进行**多目标微调**，损失函数结合自动语音识别（ASR）和语音重建任务，并加权平衡（α=0.1）。
- **解决思路**：
  - **平衡韵律与说话人信息**：ASR损失促使token捕捉语言内容并抑制韵律和说话人信息；重建损失驱动token保留包括韵律在内的声学细节。两者加权调和，使token具有介于声学和语音学token之间的特性。
  - **提升压缩效率**：基于可微分k均值优化，仅使用**单码本**（词汇量2000），相比多码本方法显著提高数据压缩效率（比特率548.3）。
  - **降低数据需求**：微调仅需44小时额外数据（VCTK），远少于基线方法所需的大规模数据（如960小时LibriSpeech）。
- **训练细节**：在微调时，向声码器提供预训练说话人编码器的嵌入作为条件输入，以帮助分离说话人信息；整个模块（除说话人编码器外）联合优化。

3)  
- **任务与效果**：
  - **判别任务**：在情感识别（ER）上取得最佳性能（准确率51.7%），显著优于基线；自动语音识别（ASR）词错误率接近纯语音学token（4.6/8.5 vs 4.3/7.1）；说话人识别（SID）准确率低（29.5%），表明有效抑制了说话人信息。
  - **生成任务**：在语音重建和语音转换（VC）上，综合表现优于或接近基线，尤其在VC中能保持源语音韵律并转换音色。
  - **语音语言模型**：在语音延续任务中，生成困惑度最低（5.60）且语音自然度最高（UTMOS 3.86），表明在speechLM应用中兼具语言内容保持与高质量合成能力。
</div>

</details>

---

## Advanced Modeling of Interlanguage Speech Intelligibility Benefit with L1-L2 Multi-Task Learning Using Differentiable K-Means for Accent-Robust Discrete Token-Based ASR
- **Authors**: Kentaro Onda, Satoru Fukayama, Daisuke Saito, Nobuaki Minematsu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19767v1](https://arxiv.org/abs/2601.19767v1)
- **PDF**: [https://arxiv.org/pdf/2601.19767v1](https://arxiv.org/pdf/2601.19767v1)

在全球化背景下，构建对外国口音语音具有鲁棒性的自动语音识别系统是一项重要挑战。先前研究尝试通过复现“中介语语音可懂度增益”现象来提升基于音素单元的ASR在口音语音上的性能——该现象指带有外国口音的语音对与说话者母语相同的听者比对本族语听者更易理解。已有技术方案通过使用说话者的母语在自监督学习特征空间中学习k-means聚类中心来获取音素单元，从而实现对ISIB的建模。本研究提出了一种更先进的ISIB建模方法：通过采用可微分k-means算法，并针对母语与目标语ASR任务联合优化整个模块，该方法在使用纯本族语语音时，以及在额外加入少量口音语音时，均超越了基线系统。特别在后一种场景下，本方法在识别准确率上实现了约20%的相对提升。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：全球化背景下，带口音的非母语语音识别是ASR的重要挑战。现有方法通常依赖大量带口音语音数据，但这类数据稀缺，尤其在非英语语言中。
- **既有方法问题**：先前研究通过模拟“跨语言语音可懂度优势”现象，仅利用说话者母语训练离散语音单元，以提升口音鲁棒性。但该方法仅基于L1模拟听者的语音感知，过于简化，未考虑听者同时学习L2的双语背景影响。

2)  
- **核心方法**：提出一种结合**可微分K均值**与**L1-L2多任务学习**的先进ISIB建模方法，以联合优化整个系统。
- **解决思路**：
  - **可微分K均值**：将离散语音单元的学习与下游ASR训练端到端联合优化，使单元能适应特定目标。
  - **L1-L2多任务学习**：在训练中同时优化L1（说话者母语）和L2（目标语言）的ASR任务，共享SSL特征提取器与聚类中心。通过加权损失函数平衡两者贡献，使学习到的语音单元能同时捕捉L1和L2的语音特性。
  - **更真实的ISIB模拟**：该方法模拟了非母语听者（兼为L2学习者）受L1和L2共同影响的语音感知，而非仅依赖L1，从而更准确地再现ISIB。
- **优势**：仅需丰富的母语语音数据，即可提升对口音语音的识别鲁棒性；系统可灵活作为ASR模型或语音单元生成器使用。

3)  
- **任务与效果**：在**日语口音英语**识别任务上进行了实验，涵盖两种场景：
  - **仅用母语数据**：模型直接用于识别带口音语音，在口音较重的子集上取得最佳效果。
  - **加入少量口音数据适配**：使用模型作为语音单元生成器，仅用2-5小时带口音数据训练独立ASR。相比基线，词错误率相对降低约20%，提升显著。
</div>

</details>

---

## Physics-Aware Novel-View Acoustic Synthesis with Vision-Language Priors and 3D Acoustic Environment Modeling
- **Authors**: Congyi Fan, Jian Guan, Youtian Lin, Dongli Xu, Tong Ye, Qiaoxi Zhu, Pengming Feng, Wenwu Wang
- **Categories**: cs.SD, cs.MM
- **arXiv**: [https://arxiv.org/abs/2601.19712v1](https://arxiv.org/abs/2601.19712v1)
- **PDF**: [https://arxiv.org/pdf/2601.19712v1](https://arxiv.org/pdf/2601.19712v1)

空间音频对于沉浸式体验至关重要，然而由于反射、衍射和材料吸收等复杂物理现象，新视角声学合成（NVAS）仍面临挑战。现有基于单视角或全景输入的方法虽提升了空间保真度，但未能有效捕捉全局几何结构及物体布局、材料属性等语义信息。为此，我们提出Phys-NVAS——首个融合空间几何建模与视觉-语言语义先验的物理感知NVAS框架。该方法通过多视角图像与深度图重建全局三维声学环境，以估计房间尺寸与形状，从而增强声传播的空间感知能力。同时，利用视觉-语言模型提取物体、布局与材料的物理感知先验，捕捉超越几何结构的吸收与反射特性。通过声学特征融合适配器，将这些信息统一为物理感知表征，用于双耳音频生成。在RWAVS数据集上的实验表明，Phys-NVAS能够生成具有更高真实感与物理一致性的双耳音频。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：空间音频对沉浸式体验至关重要，但新视角声学合成面临复杂物理现象（如反射、衍射、材料吸收）的挑战。  
- **既有方法问题**：  
  - 现有方法（如AV-NeRF）依赖单视图或全景输入，虽提升空间保真度，但**无法捕捉全局几何结构**（如房间形状与大小）。  
  - **忽略语义线索**（如物体布局、材料属性），导致无法区分不同材料（如地毯与瓷砖）的声学特性，合成音频缺乏物理一致性。  

2)  
论文提出Phys-NVAS框架，通过**多模态融合**解决上述问题：  
- **3D声学环境建模**：  
  - 使用多视图图像与深度图，通过3D高斯泼溅重建全局场景几何，增强对**声波传播路径**（如直达声、早期反射）的空间感知。  
- **物理感知的视觉-语言先验**：  
  - 利用视觉-语言模型（如Chat-UniVi）生成描述物体、布局、材料的文本，编码**材料相关吸收/反射特性**（如“木质咖啡桌”的阻尼效应）。  
- **特征融合与生成**：  
  - 设计声学特征融合适配器，将几何特征（RGB、深度）与语义特征统一为**物理感知表征**，输入双耳音频生成器（基于AV-NeRF），合成更真实且物理一致的音频。  

3)  
- **任务**：在RWAVS数据集的新视角声学合成任务中评估，涵盖室内外多种场景（办公室、住宅、公寓、户外）。  
- **效果**：  
  - 在客观指标（MAG、ENV）上全面优于基线（包括AV-NeRF），**整体MAG/ENV降低至1.431/0.142**。  
  - 波形可视化显示，合成音频在能量包络、时序动态及通道间差异上更接近真实录音，**提升空间真实感与物理一致性**。
</div>

</details>

---

## Hyperbolic Additive Margin Softmax with Hierarchical Information for Speaker Verification
- **Authors**: Zhihua Fang, Liang He
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19709v1](https://arxiv.org/abs/2601.19709v1)
- **PDF**: [https://arxiv.org/pdf/2601.19709v1](https://arxiv.org/pdf/2601.19709v1)

基于欧几里得空间的说话人嵌入学习已取得显著进展，但在建模说话人特征的层次信息方面仍显不足。双曲空间凭借其负曲率几何特性，能够在有限体积内高效表征层次信息，更适用于说话人嵌入的特征分布。本文提出基于双曲空间的双曲Softmax（H-Softmax）与双曲加性间隔Softmax（HAM-Softmax）。H-Softmax通过将嵌入向量与说话人中心投影至双曲空间并计算双曲距离，将层次信息融入说话人嵌入表示；HAM-Softmax在此基础上引入间隔约束以进一步增强类间可分性。实验结果表明，相较于标准Softmax与AM-Softmax，H-Softmax与HAM-Softmax分别实现平均相对等错误率下降27.84%与14.23%，证明所提方法在有效提升说话人验证性能的同时，保持了层次结构建模能力。代码将在https://github.com/PunkMale/HAM-Softmax 发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：说话人验证旨在确认语音是否属于目标说话人。主流方法在欧几里得空间中学习说话人嵌入，但现实中的说话人特征（如基频、共振峰结构、韵律）具有树状层次结构，欧几里得空间表达能力有限，难以充分建模此类复杂分布，限制了嵌入的区分性和泛化能力。  
- **既有方法问题**：现有基于边际的Softmax损失（如AM-Softmax）虽能提升类间可分性，但仍基于欧几里得空间，无法有效捕捉说话人特征的层次信息，导致模型对复杂数据分布建模不足。

2)  
论文提出两种基于双曲空间的损失函数，以解决欧几里得空间建模层次信息不足的问题：  
- **核心方法**：  
  - **双曲Softmax（H-Softmax）**：将说话人嵌入和类别中心投影到双曲空间（采用庞加莱球模型），使用双曲距离替代欧几里得空间中的点积相似度。双曲空间因其负曲率几何特性，能在有限体积内容纳指数级数据点，更自然地建模层次结构，从而将层次信息融入嵌入学习。  
  - **双曲加性边际Softmax（HAM-Softmax）**：在H-Softmax基础上引入加性边际约束。具体而言，对正样本的双曲距离增加一个边际值，强制模型在缩小类内距离的同时，显式扩大类间间隔，进一步提升嵌入的区分性。  
- **解决思路**：  
  - 通过双曲距离度量直接捕获嵌入的层次关系，弥补欧几里得空间表达能力的不足。  
  - 结合边际机制，在建模层次结构的同时增强类间分离性，使学习到的嵌入更具判别力。  
- **技术细节**：使用投影函数防止嵌入超出庞加莱球边界，并通过调整曲率参数控制空间约束强度，以平衡层次建模与数值稳定性。

3)  
- **任务**：在VoxCeleb1、VoxCeleb2和CNCeleb数据集上进行说话人验证实验。  
- **效果**：  
  - H-Softmax相比标准Softmax，平均相对EER降低27.84%，在跨域复杂数据（如CNCeleb）上甚至优于所有边际损失方法，验证了双曲空间建模层次信息的有效性。  
  - HAM-Softmax相比AM-Softmax，平均相对EER降低14.23%，在多数测试集上取得最优或次优性能（如VoxCeleb2上EER低至1.048%），表明结合边际约束能进一步提升判别力。  
  - 联合训练双曲与欧几里得损失（E.-H. AM-Softmax）在部分测试集上达到最佳性能，体现了混合空间建模的鲁棒性优势。
</div>

</details>

---

## SAM Audio Judge: A Unified Multimodal Framework for Perceptual Evaluation of Audio Separation
- **Authors**: Helin Wang, Bowen Shi, Andros Tjandra, John Hoffman, Yi-Chiao Wu, Apoorv Vyas, Najim Dehak, Ann Lee, Wei-Ning Hsu
- **Categories**: eess.AS, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19702v1](https://arxiv.org/abs/2601.19702v1)
- **PDF**: [https://arxiv.org/pdf/2601.19702v1](https://arxiv.org/pdf/2601.19702v1)

音频分离的性能评估仍是一个复杂挑战，现有评估指标常与人类感知存在偏差、粒度粗糙且依赖真实信号。另一方面，主观听音测试虽是实际场景评估的金标准，但成本高昂、耗时且难以规模化。本文针对无需人工干预的自动化音频分离评估系统日益增长的需求，提出一种多模态细粒度无参考客观评估指标——SAM Audio Judge（SAJ），该指标与人类感知高度吻合。SAJ支持三种音频领域（语音、音乐与通用声音事件）和三种提示输入（文本、视觉与片段标注），涵盖四个评估维度（召回率、精确度、保真度与整体质量）。该框架在数据过滤、大规模数据集伪标注及音频分离模型重排序等方面展现出应用潜力。代码与预训练模型已开源：https://github.com/facebookresearch/sam-audio。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频分离的性能评估长期依赖失真度量（如SDR、SI-SDR），这些方法需要干净参考信号，且与人类听觉感知相关性弱。  
- **既有问题**：  
  - 失真度量无法反映感知质量，相同SDR的音频可能听起来差异巨大。  
  - 主观听测虽可靠，但成本高、耗时长、难以扩展。  
  - 现有自动评估工具缺乏对多提示输入（如文本、视觉、时间跨度）的支持。

2)  
- **核心方法**：提出SAM Audio Judge（SAJ），一个多模态、细粒度、无参考的客观评估框架。  
- **解决方案**：  
  - **多维度评估**：定义四个感知维度——召回率（目标声音完整性）、精确率（非目标声音去除程度）、保真度（目标声音与原混合音相似性）和整体质量，并通过大规模人工标注数据训练模型。  
  - **多模态支持**：模型支持文本、视觉和时间跨度提示的任意组合，使用预训练的音频、文本和视觉编码器提取特征，并通过Transformer融合多模态信息进行联合预测。  
  - **训练优化**：  
    - 采用回归损失（MAE+MSE）保持评分连续性，优于分类损失。  
    - 引入辅助任务（预测分离音频是否遵循提示）提升跨模态对齐能力。  
    - 使用多阶段数据收集策略平衡评分分布，提升模型鲁棒性。  
  - **知识迁移**：从文本提示SAJ扩展到多模态SAJ，实现跨提示类型的统一评估。

3)  
- **评估任务与效果**：  
  - 在语音、音乐和通用声音的分离任务上，SAJ与人类评分在皮尔逊相关系数（PCC）和斯皮尔曼相关系数（SRCC）上均显著优于基线（如CLAP、SDR估计器、Gemini-2.5-pro）。  
  - 应用层面：SAJ在重排序、数据过滤和伪标注中有效提升分离质量，例如重排序使整体质量评分从3.2提升至4.2，且能高效筛选高质量训练数据。
</div>

</details>

---

## A Benchmark for Audio Reasoning Capabilities of Multimodal Large Language Models
- **Authors**: Iwona Christop, Mateusz Czyżnikiewicz, Paweł Skórzewski, Łukasz Bondaruk, Jakub Kubiak, Marcin Lewandowski, Marek Kubis
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19673v1](https://arxiv.org/abs/2601.19673v1)
- **PDF**: [https://arxiv.org/pdf/2601.19673v1](https://arxiv.org/pdf/2601.19673v1)

当前用于测试多模态大语言模型音频能力的基准主要集中于孤立地评估说话人日志或性别识别等单一任务。这些基准无法验证多模态模型是否具备整合不同类别音频任务所需的推理能力。针对这一问题，我们提出音频推理任务（ART）——一个用于评估多模态模型在音频信号处理中进行问题推理能力的新基准。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前评估多模态大语言模型音频能力的基准（如AudioBench、AIR-Bench）主要关注孤立任务（如语音识别、声学场景分类），缺乏对跨任务音频推理能力的综合评估。  
- **既有问题**：这些基准无法验证模型是否具备结合不同类别音频任务（如同时处理语音、环境音和音乐）进行推理的能力，即使模型在单一任务上表现优异。

2)  
- **核心方法**：论文提出了音频推理任务基准，旨在系统评估模型跨音频任务的推理能力。  
- **任务设计原则**：  
  - **规则1**：任务不能仅通过单一专用模块（如语音转录）解决，必须结合多种音频现象。  
  - **规则2**：任务应无需专业训练即可被人理解，确保可访问性。  
- **任务构建流程**：  
  - 通过领域专家调研，筛选出需跨技能推理的候选任务。  
  - 排除主观性任务（如情感分类）和无法转为二值问答的任务。  
  - 最终确定9类任务，包括音频算术、跨录音说话人识别、文本与声音推理等。  
- **数据集生成**：  
  - 使用模板化方法生成任务实例，通过语音合成整合问题、语音片段和环境音。  
  - 确保数据平衡（9,000个样本，正负标签各半），涵盖超过30小时音频。  
- **评估方式**：  
  - 提供“是/否”和“描述性”两种回答模式，采用自动评估与LLM-as-a-judge结合的方法，减少评估偏差。

3)  
- **评估任务**：在9类音频推理任务上测试了多个模型，包括级联系统（Whisper+LLM）和多模态模型（如Audio Flamingo 3、Qwen2-Audio）。  
- **主要效果**：  
  - 所有模型表现均未超过60%准确率，最高为级联系统（56.21%），显示当前模型在音频推理上存在显著挑战。  
  - 人类基线准确率达92.9%，突显模型与人类能力的差距。  
  - 跨录音说话人识别任务最难（平均准确率38.63%），选择性文本推理任务相对最佳。
</div>

</details>

---

## GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining
- **Authors**: Shentong Mo, Zehua Chen, Jun Zhu
- **Categories**: cs.CV, cs.AI, cs.LG, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19606v1](https://arxiv.org/abs/2601.19606v1)
- **PDF**: [https://arxiv.org/pdf/2601.19606v1](https://arxiv.org/pdf/2601.19606v1)

近年来，视频-音频理解与生成领域的研究日益依赖于联合跨模态嵌入，这已成为跨模态检索与生成等任务的基础。尽管现有方法（如CAVP）通过对比学习有效建模了模态间的语义与时间对应关系，但其性能仍有提升空间。一个关键局限在于对视频与音频信号密集、多尺度特性的建模不足——模态间的对应关系往往跨越从细粒度到粗粒度的时空结构，而现有框架未能充分利用这一特性。为此，我们提出GMS-CAVP，一种融合多尺度视频-音频对齐与基于多尺度时空扩散的预训练目标的新框架，以增强视频-音频对应关系建模。首先，GMS-CAVP引入多尺度对比学习策略，捕捉不同粒度下的语义与时间关联。其次，我们超越传统对比学习，引入基于扩散的生成式目标，实现视频与音频间的模态转换与合成。这种判别式与生成式统一的学习框架促进了更深层次的跨模态理解，并为高保真生成奠定了基础。在VGGSound、AudioSet和Panda70M数据集上的大量实验表明，GMS-CAVP在生成与检索任务上均优于现有方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：视频-音频（V-A）跨模态理解与生成任务（如检索、生成）依赖于联合嵌入。现有方法（如CAVP）主要使用对比学习进行模态对齐，但存在不足。  
- **既有问题**：  
  - **生成能力受限**：仅使用判别式（对比）预训练，缺乏对模态间转换的建模，导致生成任务（如视频到音频合成）效果不佳。  
  - **多尺度信息利用不足**：视频和音频信号具有密集的多尺度时空结构，但现有方法多采用单尺度全局对齐，未能充分捕捉细粒度到粗粒度的跨模态对应关系。  

2)  
论文提出 **GMS-CAVP** 框架，通过结合多尺度对比学习与生成式预训练目标，解决上述问题：  

- **多尺度视频-音频对齐（MSA）**：  
  - 构建视频和音频的**分层多尺度特征表示**（通过时间金字塔池化和多分辨率卷积），捕捉不同粒度的时空依赖。  
  - 在**每个尺度上应用对比学习**（基于InfoNCE损失），强制跨模态特征在局部与全局层次上对齐。  
  - 引入**自适应时间对齐机制**，通过注意力权重强调关键时间片段，提升对齐精度。  

- **生成式多尺度视频-音频对齐（MSD）**：  
  - 引入**基于扩散模型的生成目标**，以多尺度视频特征为条件，逐步去噪生成音频。  
  - 扩散过程**显式建模跨模态映射**，弥补纯对比学习在生成能力上的不足。  
  - 通过**分层多尺度条件机制**，利用视频在不同分辨率下的动态信息，提升生成音频的时序同步性与真实性。  

- **统一框架优势**：  
  - **判别与生成协同**：对比学习确保跨模态对齐，扩散模型增强模态转换与合成能力。  
  - **多尺度建模**：同时利用细粒度（局部动作/声音）与粗粒度（全局场景）信息，提升复杂场景下的泛化能力。  

3)  
在多个大规模数据集（VGGSound、AudioSet、Panda70M）上评估，GMS-CAVP在以下任务取得显著效果：  

- **视频到音频生成**：  
  - 在VGGSound上，关键指标全面超越基线：KLD（1.63↓）、FAD（0.75↓）、对齐准确率（95.87↑），表明生成音频更逼真、与视频更同步。  
- **跨模态检索**：  
  - 视频到音频检索的R@1/R@5/R@10分别达到28.90/43.70/57.90，音频到视频检索达到30.50/45.30/58.20，显著优于CAVP等先前方法。  
- **消融实验验证**：多尺度对齐（MSA）与多尺度扩散（MSD）均对性能提升有重要贡献，且数据规模扩大可进一步改善效果。
</div>

</details>

---

## Audio Deepfake Detection at the First Greeting: "Hi!"
- **Authors**: Haohan Shi, Xiyu Shi, Safak Dogan, Tianjin Huang, Yunxiao Zhang
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19573v1](https://arxiv.org/abs/2601.19573v1)
- **PDF**: [https://arxiv.org/pdf/2601.19573v1](https://arxiv.org/pdf/2601.19573v1)

本文聚焦于真实通信环境下的音频深度伪造检测，重点针对超短时输入（0.5-2.0秒），旨在实现对话开场时（如诈骗者说“你好”）的合成语音检测能力。我们提出了一种轻量级新型模型——短时多粒度自适应时频注意力（S-MGAA），该模型基于多粒度自适应时频注意力架构扩展而来，专门针对经过通信处理与干扰的短时退化音频，增强其判别性表征学习能力。S-MGAA整合了两个定制化模块：像素-通道增强模块（PCEM）用于放大细粒度时频显著性特征，以及频率补偿增强模块（FCEM）通过多尺度频率建模与自适应频时交互，补充有限的时间线索。大量实验表明，S-MGAA在九种前沿基线方法中均表现优异，同时对音频退化具有强鲁棒性，并在效率与精度间取得良好平衡——包括低实时因子、具有竞争力的计算量、紧凑的参数规模以及较低的训练成本，凸显了其在通信系统与边缘设备中实时部署的潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：深度伪造音频技术快速发展，其滥用带来了严重的社会风险。现有的音频深度伪造检测方法在现实通信场景下面临挑战。
- **既有方法的问题**：
    - **鲁棒性不足**：现有方法通常在高质量音频上表现良好，但在经过编解码压缩、丢包等现实通信降质后，检测性能显著下降。
    - **输入长度固定**：大多数方法针对3-4秒的固定长度音频设计，对超短音频（0.5-2.0秒）进行截断或填充会导致训练与测试不匹配，降低准确性。
    - **轻量化设计缺失**：现有针对短音频的检测方法，往往忽略了对抗现实通信降质的鲁棒性要求，也未充分考虑在智能手机等资源受限设备上部署所需的轻量化设计。

2)  
论文提出了名为**S-MGAA**的轻量级框架，专门用于解决超短、降质音频输入的检测难题。其核心方法是对原有MGAA框架的扩展，通过两个定制模块增强判别性表征学习：

- **像素-通道增强模块**：
    - **目标**：解决因音频过短和通信降质导致的伪造线索稀疏、显著性低的问题。
    - **机制**：联合建模像素级显著性、通道级重要性和时频耦合关系。
    - **组成**：包含像素级检测器（突出细粒度伪造线索）、通道级放大器（强调跨通道线索聚合）和时频耦合组件（联合建模频域与时域依赖）。

- **频率补偿增强模块**：
    - **目标**：补偿超短音频缺乏的充分时域特征。
    - **机制**：利用频域特征进行补偿，包含两个核心部分：
        - **多尺度频率分析**：通过三个具有不同感受野的并行分支，捕获丰富的多尺度频率模式。
        - **自适应频率-时间交互**：通过沿频率维度的深度卷积，有效捕获跨时间维度最重要的频率特征。

- **整体架构与优势**：
    - S-MGAA将PCEM、原始MGAA块和FCEM顺序集成，形成两阶段处理流程，对输入特征进行渐进式增强和提炼。
    - 该方法专为超短输入设计，能有效放大和定位与伪造相关的判别性线索。
    - 框架保持了轻量化特性，通过精心设计的模块实现了效率与精度的良好平衡，适合在资源受限的环境中实时部署。

3)  
- **任务**：在现实通信降质条件下的**超短音频深度伪造检测**任务上进行了评估，输入时长为0.5秒、1秒、1.5秒和2秒。
- **效果**：
    - **性能领先**：在ADD-C测试集上，S-MGAA（使用LFCC、CQCC、MFCC特征）在全部四种时长和六种降质条件下，其等错误率均**持续优于**所比较的九种先进基线模型。
    - **显著提升**：尤其在最具挑战性的0.5秒输入上，S-MGAA-MFCC相比最佳基线RawGAT-ST，平均EER降低了23.89%。相比其前身MGAA，EER相对降低幅度在28.39%到70.67%之间。
    - **高效轻量**：同时保持了极低的计算开销、参数量、实时处理因子和训练时间，展现了优异的效率-精度权衡，证明了其在通信系统和边缘设备中实时部署的强潜力。
</div>

</details>

---

## SLM-SS: Speech Language Model for Generative Speech Separation
- **Authors**: Tianhua Li, Chenda Li, Wei Wang, Xin Zhou, Xihui Chen, Jianqing Gao, Yanmin Qian
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19533v1](https://arxiv.org/abs/2601.19533v1)
- **PDF**: [https://arxiv.org/pdf/2601.19533v1](https://arxiv.org/pdf/2601.19533v1)

基于神经网络的语音分离方法在信号级指标上取得了显著进展，但分离信号在语音清晰度方面仍存在不足，可能影响语音识别等下游任务性能。本文提出SLM-SS，一种将语音语言模型应用于语音分离的新方法，旨在提升分离信号的清晰度与连贯性。我们将语音分离建模为离散多码本序列生成任务，采用编码器-解码器模型将量化后的混合语音映射为目标标记序列。除自回归建模策略外，本文还引入非自回归模型以提升残差标记的解码效率。在LibriMix数据集上的实验表明，相较于现有方法，本方法能显著更好地保持语音清晰度，从而在各种下游任务中实现更优的语言一致性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音分离是语音处理的关键任务，旨在从重叠信号中分离出单个语音源。现有方法主要基于判别式神经网络，以信号级指标（如SI-SDR）为目标进行训练。
- **既有问题**：这些方法在波形重建上有效，但常无法保持分离后语音的清晰度，引入失真，从而损害下游任务（如语音识别）的性能。生成式方法能产生更连贯的输出，但受限于解码速度慢和可能产生虚假语音的风险。

2)  
- **核心框架**：提出SLM-SS，将语音分离建模为离散多码本序列生成任务。使用Encodec将连续语音量化为离散码本序列，并通过序列化输出训练（SOT）策略拼接多说话人序列。
- **混合解码策略**：
  - **自回归模型**：采用编码器-解码器（AED）结构，编码器基于预训练的WavLM提取混合语音特征，解码器通过交叉注意力自回归预测零阶码本序列。
  - **非自回归模型**：采用相同架构但移除单向掩码，利用独立的词嵌入层，在给定低阶码本序列的条件下，高效预测高阶码本序列，提升解码效率。
- **优势**：该生成式框架通过离散化建模和混合解码，在保持语音清晰度和连贯性的同时，提高了生成质量与效率。

3)  
- **任务与效果**：在LibriMix数据集上进行实验，与BSRNN、Sepformer等基线方法对比。
  - **语音清晰度与感知质量**：在主观听力测试（MOS）中得分更高，表明分离语音的感知质量更好。
  - **语言一致性**：在词错误率（WER）、音素相似度（LPS）和SpeechBERTScore（SBS）等指标上接近使用Encodec重建的上界，显著优于基线，证明其能更好地保持语音的语义连贯性。
  - **下游任务适用性**：分离出的语音在自动语音识别等下游任务中表现出更优的性能。
</div>

</details>

---

## Permutation-Invariant Physics-Informed Neural Network for Region-to-Region Sound Field Reconstruction
- **Authors**: Xingyu Chen, Sipei Zhao, Fei Ma, Eva Cheng, Ian S. Burnett
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19491v1](https://arxiv.org/abs/2601.19491v1)
- **PDF**: [https://arxiv.org/pdf/2601.19491v1](https://arxiv.org/pdf/2601.19491v1)

现有声场重建方法大多针对点对区域重建，即对固定位置声源与接收区域间的声学传递函数进行插值。由于实际声学传递函数会随声源和接收区域位置连续变化，这类方法的适用性受限。本文提出一种置换不变物理信息神经网络，用于区域对区域声场重建，旨在实现对连续变化声源与测量区域间声学传递函数的插值。该方法采用深度集合架构，将接收点与声源位置作为无序集合处理，保持声学互易性。同时引入亥姆霍兹方程作为物理约束指导网络训练，确保预测结果符合物理一致性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声场重建是许多音频应用的基础。传统方法（如球谐函数分解）和新兴的物理信息神经网络方法，大多局限于**点对区域**场景，即声源位置固定，仅重建接收区域内的声场。
- **既有问题**：实际声学传递函数会随声源和接收区域的**位置连续变化**而改变。现有方法未能有效处理声源和测量区域位置同时变化带来的挑战，限制了其在实际场景中的适用性。

2)  
论文提出了一种**置换不变物理信息神经网络**，以解决区域到区域的声场重建问题。其核心方法通过以下两个关键设计实现：

- **置换不变的深度集合架构**：
    - 将接收点和声源点的坐标视为一个**无序集合**输入网络。
    - 采用特征提取子网络ϕ和主网络ρ，通过求和聚合特征：`ˆP(r, s, f) = ρ(ϕ(r) + ϕ(s))`。
    - 求和操作具有置换不变性，**天然满足声学互易性原理**（`P(r,s,f)=P(s,r,f)`），无需在损失函数中显式约束。

- **物理约束的嵌入**：
    - 将控制声波传播的**亥姆霍兹方程**作为物理损失项引入总损失函数。
    - 总损失是数据损失（拟合测量数据）与物理损失（满足控制方程）的加权和：`L_total = L_data + λL_PDE`。
    - 通过自动微分计算二阶导数，确保网络预测在物理上一致，增强了在未测量区域的泛化能力。

- **方法优势**：
    - 直接处理**声源和接收点位置均可变**的区域到区域映射。
    - 结合数据驱动与物理规律，利用有限测量数据实现更准确、物理一致的插值。

3)  
- **任务**：在**区域到区域声场重建**任务上，使用真实世界UTS数据集进行评估，并与基于核的岭回归方法进行对比。
- **效果**：
    - 在消声室和半消声室环境中，所提方法在**1000 Hz至2000 Hz的高频段**显著优于KRR方法，平均归一化均方误差改善超过5 dB。
    - 可视化结果表明，该方法能更好地捕捉声场的空间变化和波状结构，而KRR方法在较高频段重建失败，结果过度平滑。
    - 消融实验证实了置换不变架构和物理损失项对提升性能均有关键贡献。
</div>

</details>

---

## Dual-Strategy-Enhanced ConBiMamba for Neural Speaker Diarization
- **Authors**: Zhen Liao, Gaole Dai, Mengqiao Chen, Wenqing Cheng, Wei Xu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19472v1](https://arxiv.org/abs/2601.19472v1)
- **PDF**: [https://arxiv.org/pdf/2601.19472v1](https://arxiv.org/pdf/2601.19472v1)

Conformer与Mamba在语音建模中表现出色，但在说话人日志任务中存在局限。Mamba计算高效但难以捕捉局部细节与非线性模式；Conformer的自注意力机制对长语音序列内存开销大，且长程依赖建模可能不稳定。这些局限对日志任务尤为关键，因其需同时实现局部变化的精细建模与长时跨度的说话人一致性保持。为应对这些挑战，我们首次将ConBiMamba应用于说话人日志任务，基于Pyannote框架提出双策略增强的ConBiMamba神经说话人日志系统。ConBiMamba融合Conformer与Mamba优势：利用Conformer的卷积与前馈结构增强局部特征提取，并通过ExtBiMamba替换其自注意力模块，在高效处理长音频序列的同时降低内存消耗。针对说话人切换点附近日志错误率较高的问题，我们提出边界增强转移损失以提升切换点检测精度，并设计分层特征聚合机制以增强多层表征的利用率。在六个日志数据集上的实验表明，本系统在四个数据集上达到最优性能。研究源代码已公开于https://github.com/lz-hust/DSE-CBM。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：说话人日志旨在确定音频中“谁在何时说话”。传统聚类方法存在子系统独立优化、无法处理重叠语音等问题。端到端神经方法（如EEND）支持重叠语音，但计算复杂度随说话人数和音频长度快速增长。  
- **既有方法问题**：  
  - **BiLSTM**：难以有效捕获长程依赖。  
  - **Conformer**：其自注意力机制在长序列上计算和内存开销高，长程依赖建模不稳定。  
  - **Mamba**：虽能高效建模长程依赖，但侧重全局信息，局部细节建模能力不足，影响说话人转换点检测精度。  

2)  
论文提出**双策略增强的ConBiMamba**系统，核心方法如下：  
- **主干网络设计**：  
  - 采用**ConBiMamba**作为局部EEND模块，融合Conformer与Mamba优势。  
  - 保留Conformer的卷积和前馈结构以增强**局部特征提取**；将其自注意力替换为**ExtBiMamba**，利用双向状态空间模型高效捕获**长程依赖**，同时降低内存开销。  
  - 进一步将卷积模块重构为多分支结构（卷积核尺寸{15,31,63}），增强对时序数据的**多尺度感知**。  
- **双策略增强**：  
  - **边界增强转换损失**：  
    - 问题：现有方法仅依赖帧级监督，需同时学习说话人识别和边界定位，导致转换点附近预测不稳定、错误率高。  
    - 解决：引入**说话人转换点检测**作为辅助任务，显式建模相邻帧间的说话人状态变化。  
    - 实现：设计独立分支，通过多层感知机和Sigmoid输出每帧的转换信号；采用**Focal Loss**形式的边界增强转换损失，动态调整正负样本权重，提升模型对边界区域的敏感性。  
  - **分层特征聚合**：  
    - 问题：大多数方法仅使用编码器最后一层输出，忽略了中间层与深层特征的互补性。  
    - 解决：提出**基于掩码的分层特征聚合**策略，自适应聚合不同ConBiMamba层的输出特征。  
    - 实现：为每层引入可学习标量权重，并通过预设静态掩码选择性地聚合最后几层（如最后三层）的特征，抑制浅层噪声，融合深层判别性信息与中层互补信息，增强模型表示能力。  

3)  
- **任务与效果**：在六个公开数据集（AISHELL-4、RAMC、VoxConverse、MSDWild、AMI、AliMeeting）上进行评估。  
- **主要成果**：  
  - 在**四个数据集**（AISHELL-4、RAMC、VoxConverse、MSDWild）上取得了**当前最优性能**，超越了包括PyannoteAI、Mamba-diarization和Diarizen在内的对比系统。  
  - 尤其在**边界检测**方面表现突出：例如在MSDWild上，误报率和漏报率分别为5.02%和7.53%，优于对比方法；在RAMC上分别为2.82%和1.56%。  
  - 消融实验证实了**边界增强转换损失**和**分层特征聚合**策略均能有效降低DER，提升系统整体性能与稳定性。
</div>

</details>

---

## Residual Tokens Enhance Masked Autoencoders for Speech Modeling
- **Authors**: Samir Sadok, Stéphane Lathuilière, Xavier Alameda-Pineda
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.19399v1](https://arxiv.org/abs/2601.19399v1)
- **PDF**: [https://arxiv.org/pdf/2601.19399v1](https://arxiv.org/pdf/2601.19399v1)

近期语音建模主要依赖音高、内容及说话人身份等显式属性，但这些属性本身无法完整捕捉自然语音的全部丰富特征。本文提出RT-MAE——一种新颖的掩码自编码器框架，通过引入无监督可训练残差标记来增强基于显式属性的监督建模。这些残差标记专门用于编码未被显式标注因素（如音色变化、噪声、情感等）解释的信息。实验表明，RT-MAE显著提升了重建质量，在保持内容与说话人相似度的同时增强了语音的表现力。我们进一步验证了该框架在语音增强任务中的适用性，能够在推理阶段有效去除噪声，同时保持语音的可控性与自然度。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前语音建模主要依赖音高、内容和说话人身份等显式属性，但这些属性无法完全捕捉自然语音的丰富性（如音色变化、情感、微韵律等）。  
- **既有方法问题**：现有方法（如AnCoGen）仅使用显式属性，将未被解释的残余信息视为数据集特定的偏差，导致模型泛化能力受限、可控性不足，且无法灵活建模残余信息。

2)  
- **核心方法**：提出RT-MAE，在掩码自编码器（MAE）框架中引入可训练的连续**残余令牌**，以编码显式属性未捕获的信息。  
- **关键设计**：  
  - **残余令牌提取**：通过受Perceiver启发的交叉注意力机制，使用固定数量的可学习查询向量从梅尔频谱中压缩提取残余信息，形成紧凑的连续令牌。  
  - **正则化策略**：在训练中引入基于丢弃的机制，以一定概率屏蔽残余令牌，强制模型依赖显式属性进行重建，避免过度依赖残余令牌，从而平衡重建质量与属性可控性。  
  - **整合方式**：将残余令牌与离散化的属性令牌、梅尔频谱令牌拼接，通过Transformer编码器-解码器进行掩码建模，共同重建语音。  
- **解决思路**：残余令牌显式建模残余信息，弥补显式属性的不足；正则化确保属性仍起主导作用，保持可控性；整体框架支持语音分析、生成与操纵任务。

3)  
- **语音合成与重建**：在LibriSpeech和EmoV-DB数据集上，RT-MAE相比基线（AnCoGen）在可懂度（STOI）、自然度（N-MOS）、语音BERT分数和说话人相似度（COS）上均有提升，尤其在包含情感的语音上效果显著。  
- **语音增强**：通过专用噪声残余令牌建模噪声，在推理时禁用该令牌以实现去噪。在LibriMix数据集上，RT-MAE在语音质量（SIG）和自然度（N-MOS）上优于或媲美专用去噪模型，同时保持较高的说话人相似度。  
- **属性操控**：在音高操纵任务中，RT-MAE在保持精确控制的同时，进一步提升了合成语音的自然度和语义一致性。
</div>

</details>

---

## Phase-Retrieval-Based Physics-Informed Neural Networks For Acoustic Magnitude Field Reconstruction
- **Authors**: Karl Schrader, Shoichi Koyama, Tomohiko Nakamura, Mirco Pezzoli
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19297v1](https://arxiv.org/abs/2601.19297v1)
- **PDF**: [https://arxiv.org/pdf/2601.19297v1](https://arxiv.org/pdf/2601.19297v1)

本文提出了一种基于稀疏空间幅度测量来估计声场幅度分布的方法，适用于相位测量不可靠或难以获取的场景。物理信息神经网络（PINN）通过将偏微分方程（PDE）约束融入神经网络，已在声场估计中展现出潜力，但其损失函数依赖于相位信息，难以直接应用于无相位测量的情况。为此，我们提出一种基于相位恢复的PINN方法，用于幅度场估计。该方法通过独立的网络分别建模幅度与相位分布，从而基于重建的复振幅计算PDE损失。实验结果表明，所提出的基于相位恢复的PINN方法具有显著有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：声场估计旨在从稀疏传感器数据重建声场分布。传统方法（如基展开、核回归）和新兴的物理信息神经网络（PINN）通常依赖于复值声压（包含幅度和相位）的测量。
- **既有问题**：在许多实际场景（如异步麦克风阵列、乐器指向性测量）中，相位信息不可靠或无法获取，仅能获得幅度测量。现有PINN方法无法直接应用于纯幅度场估计，因为其控制方程（亥姆霍兹方程）的损失函数计算需要相位信息，导致纯幅度估计通常只能依赖纯数据驱动技术，缺乏物理约束。

2)  
- **核心思路**：提出**基于相位检索的PINN（PRB-PINN）**，通过联合估计幅度和相位分布，使得重建的复值声压既能匹配测量幅度，又能满足物理方程。
- **网络架构**：
  - 使用两个独立的多层感知机（MLP）分别预测幅度和相位。
  - 输入空间坐标通过随机傅里叶特征（RFF）映射至高维，以捕捉高频空间变化。
- **损失函数设计**：
  - **数据损失**：采用对数尺度幅度误差（等效于单频对数谱距离），强制网络在传感器位置匹配测量幅度。
  - **物理损失**：基于重建的复值声压计算亥姆霍兹方程的残差（PDE损失），通过自动微分实现。
  - 总损失为加权和（λ_data·L_data + λ_PDE·L_PDE），平衡数据匹配与物理约束。
- **解决机制**：即使相位未知，通过隐式相位检索重建复值场，从而能够计算PDE损失，将物理先验引入幅度估计，起到正则化作用，避免纯数据驱动的过拟合。

3)  
- **任务**：在模拟房间环境中，从稀疏空间点（5、10、20、50个测量点）的幅度测量，重建固定频率（200Hz、400Hz、600Hz）的声压幅度场。
- **效果**：
  - 在全部频率和测量点数量下，PRB-PINN的测试数据损失均低于基线（最近邻插值）和无物理损失的神经场（NF），重建质量最优。
  - 可视化显示，尽管估计相位与真实相位不完全一致，但重建的幅度分布空间变化频率正确，尤其在低频（200Hz）和测量点较多时接近真实幅度场。
  - 实验验证了PDE损失作为正则化的有效性，但需平衡损失权重以避免过度约束。
</div>

</details>

---

## SE-DiCoW: Self-Enrolled Diarization-Conditioned Whisper
- **Authors**: Alexander Polok, Dominik Klement, Samuele Cornell, Matthew Wiesner, Jan Černocký, Sanjeev Khudanpur, Lukáš Burget
- **Categories**: eess.AS, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.19194v1](https://arxiv.org/abs/2601.19194v1)
- **PDF**: [https://arxiv.org/pdf/2601.19194v1](https://arxiv.org/pdf/2601.19194v1)

在多说话人环境中，说话人归属的自动语音识别（ASR）仍是一项重大挑战。尽管部分方法在特定领域微调后表现优异，但鲜有系统能在跨领域数据集上普遍适用。我们先前提出的工作——基于说话人日志化条件约束的 Whisper 模型（DiCoW），利用说话人日志化输出作为条件信息，仅需少量微调即可在多语言、多领域任务中展现出强大性能。本文针对 DiCoW 的一个关键局限进行改进：静音-目标-非目标-重叠（STNO）掩码存在歧义，即当两个或多个说话人完全重叠时，即使其转写内容不同，其条件信息也可能近乎相同。我们提出 SE-DiCoW（自注册说话人日志化条件约束 Whisper 模型），该模型利用日志化输出在对话中定位目标说话人最活跃的任意片段作为注册段，并通过各编码器层的交叉注意力将其作为固定条件信息。此外，我们通过改进数据分割、模型初始化及数据增强进一步优化 DiCoW。综合这些改进，SE-DiCoW 在 EMMA MT-ASR 基准测试中，相比原始 DiCoW 将宏观平均 tcpWER 相对降低了 52.4%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：多说话人场景下的说话人归属自动语音识别（ASR）是重要挑战。现有方法存在局限：
  - 模块化方法（结合声学分离、说话人日志和ASR）复杂、泛化性差且易产生级联错误。
  - 端到端方法（如说话人令牌条件化）简化了流程，但性能通常不如模块化方法。
  - 目标说话人ASR方法依赖于说话人嵌入或注册音频，在训练数据有限或说话人变化大时难以泛化。
- **既有方法问题**：本文的前期工作DiCoW利用说话人日志输出的STNO（静音-目标-非目标-重叠）掩码作为条件信息，避免了显式的说话人身份建模，但在**语音完全重叠的区域**，不同说话人的STNO掩码可能几乎相同，导致模型无法区分说话人并产生准确转录。

2)  
论文提出了SE-DiCoW，通过**自注册机制**和多项技术改进来解决上述STNO掩码的歧义问题，具体方法如下：

- **核心创新：自注册机制**
  - **问题**：在完全重叠的语音区域，不同目标说话人的STNO条件几乎相同，模型难以区分。
  - **解决方案**：模型利用说话人日志输出，自动在整个录音中为每个目标说话人**定位一个“注册片段”**。该片段是目标说话人活动概率最高的一个固定长度区间。
  - **如何整合**：将这个自注册片段（及其对应的STNO掩码）通过**跨注意力机制**整合到Whisper编码器的每一层中，为模型提供额外的、明确的说话人参考信息，从而在STNO掩码模糊时也能维持对特定说话人的追踪。

- **其他关键改进**
  - **模型初始化与结构**：
    - 在卷积下采样后、位置编码相加前，新增了一个**帧级说话人日志依赖变换（FDDT）层**，以更早地利用STNO信息。
    - 调整了FDDT中非目标和静音变换矩阵的初始化缩放因子，以减轻过度抑制。
  - **数据增强**：
    - 对STNO掩码添加高斯噪声，并应用**最可能类活动翻转**，以增强模型对说话人日志错误的鲁棒性。
    - 联合应用SpecAugment和MUSAN噪声。
  - **训练数据分割校正**：
    - 修正了训练片段的分割方式，使其更符合Whisper原始的数据格式，避免强制添加片段结束时间戳。

- **整体作用**：这些改进共同作用，使SE-DiCoW能够有效利用清晰的说话人参考片段（自注册）来化解重叠语音中的歧义，同时通过增强的数据处理和模型优化提升了整体鲁棒性和性能。

3)  
SE-DiCoW在多个多说话人ASR任务上取得了显著效果提升：
- **整体性能**：在EMMA MT-ASR基准测试中，相比原始DiCoW，**宏平均tcpWER相对降低了52.4%**。
- **高重叠场景**：在完全重叠的合成数据集Libri3Mix-clean上，错误率相对降低超过75%。
- **真实对话数据**：在AMI（单远场麦克风）和NOTSOFAR-1等真实数据集上也观察到一致的绝对性能提升。
- **与前沿系统对比**：在使用真实说话人日志（DiariZen系统）时，SE-DiCoW在AMI SDM和Libri2Mix上达到了**最先进的性能**，并在其他数据集上与经过领域调优的系统表现相当。
</div>

</details>

---

## LuSeeL: Language-queried Binaural Universal Sound Event Extraction and Localization
- **Authors**: Zexu Pan, Shengkui Zhao, Yukun Ma, Haoxu Wang, Yiheng Jiang, Biao Tian, Bin Ma
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19153v1](https://arxiv.org/abs/2601.19153v1)
- **PDF**: [https://arxiv.org/pdf/2601.19153v1](https://arxiv.org/pdf/2601.19153v1)

现有通用声音提取算法大多专注于从单通道音频混合信号中分离出目标声音事件。然而，真实世界是三维的，模拟人类听觉的双耳音频能够捕捉更丰富的空间信息，包括声源位置。这种空间上下文对于理解和建模复杂听觉场景至关重要，因为它本身就能为声音检测与提取提供信息。本研究提出一种语言驱动的通用声音提取网络，通过有效利用双耳信号中的空间线索，从双耳混合音频中分离出文本描述的声音事件。此外，我们联合利用提取网络中的空间特征预测目标声音的到达方向。这种双任务方法通过利用互补的位置信息来提升提取性能，同时实现精确的到达方向估计。在真实场景的AudioCaps数据集上的实验结果表明，我们提出的LuSeeL模型显著优于单通道及单任务基线方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频场景分析对增强现实、机器人等领域至关重要。现有通用声音事件提取方法主要处理单声道音频，难以有效分离重叠声源，且严重依赖外部提示（如文本、视觉），在复杂声学环境中性能受限。  
- **既有问题**：单声道音频无法捕捉空间信息，而空间线索对理解声源关系至关重要。同时，声音定位与提取任务通常被独立研究，未能利用其内在互补性提升整体性能。

2)  
论文提出 **LuSeeL** 模型，通过多模态联合学习框架解决上述问题：  
- **双耳音频输入**：采用模拟人耳听感的双声道音频，天然包含声源方位信息，为分离重叠声音提供空间线索。  
- **语言引导的通用提取**：使用 T5 文本编码器将自由格式文本描述（如关键词、句子）编码为条件提示，通过 FiLM 层与音频特征融合，实现灵活、用户友好的目标声音查询与提取。  
- **双任务联合学习**：  
  - **声音提取**：采用双域（时域+频域）混合 Transformer 架构（HTDemucs）作为主干，同时处理时域波形与频谱表示，提升分离精度。  
  - **声源定位**：在提取网络中，利用频谱流输出的特征，结合双耳信号的 GCC-PHAT 空间特征，共同预测目标声源的到达方向（DoA）。  
- **互补性利用**：通过共享表征的端到端训练，定位任务为提取提供空间约束，提取任务为定位提供判别性频谱特征，两者相互增强。  
- **损失函数**：结合时频域混合信号损失（SI-SNR + 多分辨率频谱损失）与 DoA 的均方误差损失，加权共同优化，促进跨模态空间表征学习。

3)  
在 **AudioCaps** 数据集生成的动态双耳混合音频上进行评估：  
- **任务**：语言查询的通用声音事件提取与定位。  
- **效果**：  
  - 在 **2 声源混合**中，提取性能（SI-SNRi）达到 20.3 dB，较单通道基线提升 12.6 dB；定位在 ±5° 容差内准确率达 89.9%，显著优于单任务定位基线。  
  - 在 **3 声源混合**中，提取性能（SI-SNRi）为 12.9 dB，定位准确率 77.8%，证明模型在更复杂场景中仍有效。  
  - 联合训练模型在分离角度变化时表现稳健，且端到端学习减少了对手工 GCC-PHAT 特征的依赖。
</div>

</details>

---

## Beyond Lips: Integrating Gesture and Lip Cues for Robust Audio-visual Speaker Extraction
- **Authors**: Zexu Pan, Xinyuan Qian, Shengkui Zhao, Kun Zhou, Bin Ma
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19130v1](https://arxiv.org/abs/2601.19130v1)
- **PDF**: [https://arxiv.org/pdf/2601.19130v1](https://arxiv.org/pdf/2601.19130v1)

当前多数视听说话人分离方法依赖同步的唇部录制，从多人混合语音中分离目标说话人的语音。然而，在自然人际交流中，伴随语音的手势在时间上也与语音对齐，常用于强调特定词语或音节。这些手势提供了互补的视觉线索，在面部或唇部区域被遮挡或距离较远时尤其有价值。本研究突破以唇部为中心的传统思路，提出SeLG模型，该模型融合唇部与上半身手势信息，实现鲁棒的说话人分离。SeLG采用基于交叉注意力的融合机制，使每种视觉模态能够查询并选择性关注混合语音中的相关语音特征。为提升手势表征与语音动态的对齐效果，SeLG还引入对比性InfoNCE损失函数，促使手势嵌入与语音相关性更强的唇部嵌入更紧密对齐。在包含TED演讲的YGD数据集上的实验结果表明：所提出的对比学习策略显著提升了基于手势的说话人分离性能；通过注意力机制与InfoNCE损失有效融合唇部与手势线索的SeLG模型，在完整模态及部分模态（即模态缺失）条件下均优于基线方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：鸡尾酒会问题旨在从多人语音混合中分离目标说话人。现有音频-视觉说话人提取方法主要依赖唇部运动作为视觉线索。
- **既有方法的问题**：
  - **唇部线索的局限性**：在面部遮挡、远距离或低分辨率场景下，唇部区域可能无法可靠检测或跟踪。
  - **单模态的脆弱性**：仅依赖唇部或仅依赖手势的方法在对应线索缺失时性能会显著下降。
  - **融合方式简单**：先前工作多采用简单的特征拼接进行多模态融合，未能充分挖掘模态间的互补关系。

2)  
论文提出 **SeLG** 模型，通过整合唇部与手势线索，并引入新颖的融合与表征学习机制来解决上述问题。

- **核心架构**：模型包含独立的唇部编码器、手势编码器、语音编码器、语音解码器以及一个**分离器**。分离器是核心，它利用唇部和手势嵌入来估计一个掩码，以从混合语音中提取目标语音。

- **跨注意力融合机制**：
  - **问题**：传统的拼接融合无法让不同模态有选择地关注语音特征中的相关部分。
  - **解决方案**：在分离器中，为唇部和手势分别设计**Transformer跨注意力层**。每个视觉模态的嵌入作为查询（Query），去查询（Key）和选择（Value）混合语音编码中的相关特征。
  - **效果**：这种机制允许模型动态地、有选择地根据每个视觉线索的重要性来整合信息，特别是在某一模态缺失或质量较差时，能更有效地利用可用信息。

- **对比学习增强手势表征**：
  - **问题**：手势与语音的相关性比唇部与语音的相关性更弱、更间接，导致基于手势的提取性能存在差距。
  - **解决方案**：引入基于**InfoNCE的对比损失**。该损失函数鼓励手势嵌入向对应的、与语音强相关的唇部嵌入对齐。
  - **实现**：将同一时间步的唇部嵌入作为正样本，其他时间步的唇部嵌入作为负样本，通过对比学习拉近手势与对应唇部表征的距离。
  - **效果**：显著提升了手势表征的质量及其与语音动态的对齐程度，使得在唇部线索缺失时，手势线索能更可靠地指导语音提取。

- **整体训练**：模型总损失函数为语音重建的尺度不变信噪比损失与上述对比损失之和，共同优化。

3)  
在**YGD数据集**（源自TED演讲）上进行了评估，任务是从2人（YGD-2mix）和3人（YGD-3mix）语音混合中提取目标说话人。

- **性能表现**：
  - **全面超越基线**：SeLG在完整模态和部分模态缺失的条件下，均显著优于仅使用唇部、仅使用手势以及简单拼接融合的基线模型。
  - **具体指标**：在YGD-2mix全测试集上，SI-SNRi达到13.2 dB；在YGD-3mix上达到11.7 dB，均为最优。
  - **关键优势**：
    - **注意力融合有效**：相比拼接融合，带来稳定性能提升。
    - **对比损失有效**：显著提升了纯手势模型的性能，并在多模态模型中增强了缺失模态情况下的鲁棒性。
    - **分布改善**：结果直方图显示，SeLG将更多样本推向高信噪比区域，减少了严重失败案例。
</div>

</details>

---

## A Hybrid Discriminative and Generative System for Universal Speech Enhancement
- **Authors**: Yinghao Liu, Chengwei Liu, Xiaotao Liang, Haoyin Yan, Shaofei Xue, Zheng Xue
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.19113v1](https://arxiv.org/abs/2601.19113v1)
- **PDF**: [https://arxiv.org/pdf/2601.19113v1](https://arxiv.org/pdf/2601.19113v1)

通用语音增强旨在处理具有多种语音失真和录制条件的输入。本研究提出一种新型混合架构，将判别式建模的信号保真度与生成式建模的重构能力相协同。该系统采用具有采样频率无关策略的判别式TF-GridNet模型，以通用方式处理可变采样率。同时，结合频谱映射建模的自回归模型能够生成细节丰富的语音，并有效抑制生成伪影。最后，在信号级损失和综合语音质量评估（SQA）损失的优化下，融合网络学习两个输出的自适应权重。所提系统在ICASSP 2026 URGENT挑战赛（赛道1）中评估，位列第三名。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：通用语音增强旨在处理各种失真和录音条件下的语音。现有方法主要分为判别式和生成式两类。
- **既有问题**：
  - 判别式模型（如TF-GridNet）在信号保真度和噪声抑制方面表现优异，但难以重建严重受损的语音成分。
  - 生成式模型能重建高质量语音，但常因学习到的生成先验与真实干净语音分布不完全对齐，导致产生幻觉和伪影。

2)  
论文提出一种混合架构，协同整合判别式建模的信号保真度与生成式建模的重建能力，具体通过三个核心组件解决上述问题：

- **判别式分支**：
  - 采用TF-GridNet作为主干，处理复数频谱图以保持高信号保真度。
  - 引入**采样频率无关策略**：固定STFT窗长和跳数时间，根据输入采样率调整频点数，从而普适地处理不同采样率的输入，保持频谱完整性。

- **语义条件细化分支（生成式）**：
  - 利用自回归模型结合频谱映射来生成细节丰富的语音，同时有效抑制生成伪影。
  - 具体流程：使用预训练的WavLM提取退化语音的鲁棒语义和声学表示，作为条件前缀，引导仅解码器语言模型自回归预测干净语音的离散令牌（通过X-Codec提取）。
  - 为规避离散令牌的信息瓶颈，采用DPRNN模型直接预测干净语音频谱：将来自语言模型层的语义丰富表示与从退化频谱导出的声学丰富特征融合，通过交叉注意力整合全局语义引导与局部声学细节，最终估计复数掩码。

- **融合网络**：
  - 使用轻量级USES网络自适应地融合两个分支的输出。
  - 该网络学习一个时频融合掩码，以加权方式结合判别式分支的高保真输出与生成式分支的细节丰富输出。
  - 优化时不仅使用信号级损失（如多分辨率STFT损失、L1损失），还引入**综合语音质量评估损失**，该损失基于多个感知指标（如MOS、DNSMOS、ScoreQ、UTMOS、NISQA）进行监督，确保最终输出在保真度和感知质量间取得平衡。

3)  
- **任务**：在ICASSP 2026 URGENT挑战赛的Track 1（通用语音增强）上进行评估。
- **效果**：
  - 在非盲测试集上，混合方法在**感知质量**（DNSMOS和NISQA）和**说话人相似度**（SpkSim）上优于基线及单独的分支。
  - 同时保持了有竞争力的**信号保真度**（PESQ和ESTOI）。
  - 在**下游语音识别任务**（CAcc）上也取得了最佳性能。
  - 最终在挑战赛中排名第三，证明了该混合架构的有效性。
</div>

</details>

---

## Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation
- **Authors**: Nanhan Shen, Zhilei Liu
- **Categories**: cs.AI, cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19112v1](https://arxiv.org/abs/2601.19112v1)
- **PDF**: [https://arxiv.org/pdf/2601.19112v1](https://arxiv.org/pdf/2601.19112v1)

情感说话人脸合成在多媒体与信号处理领域至关重要，然而现有三维方法面临两大关键挑战：音视频情感对齐能力不足，表现为音频情感提取困难以及对情感微表情的控制不充分；以及采用忽视不确定性与特征质量差异的“一刀切”式多视角融合策略，导致渲染质量下降。本文提出UA-3DTalk——一种基于情感先验蒸馏的不确定性感知三维情感说话人脸合成方法，其包含三个核心模块：先验提取模块将音频解耦为用于对齐的内容同步特征和用于个性化的人物专属互补特征；情感蒸馏模块引入多模态注意力加权融合机制及多分辨率码本的四维高斯编码，实现细粒度音频情感提取与情感微表情的精准控制；基于不确定性的形变模块通过不确定性块估计视角特定的偶然性（输入噪声）与认知性（模型参数）不确定性，实现自适应多视角融合，并结合多头解码器对高斯基元进行优化，以克服均匀权重融合的局限性。在常规与情感数据集上的大量实验表明，UA-3DTalk在情感对齐指标E-FID上较DEGSTalk、EDTalk等前沿方法提升5.2%，在唇形同步指标SyncC上提升3.1%，在渲染质量指标LPIPS上提升0.015。项目页面：https://mrask999.github.io/UA-3DTalk

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：3D情感说话人脸合成在多媒体领域至关重要，但现有方法面临两大核心挑战。  
- **既有问题**：  
  - **音视频情感对齐差**：音频情感提取困难，且对情感微表情的控制不足。  
  - **多视图融合策略僵化**：采用“一刀切”的融合方式，忽视不同视图的特征质量差异和不确定性，导致渲染质量下降。

2)  
论文提出 **UA-3DTalk** 框架，通过三个核心模块解决上述问题：  
- **先验提取模块**：  
  - 将音频解耦为内容同步特征（用于音视频对齐）和个性化互补特征（用于身份保持），共同确保同步精度与身份一致性。  
- **情感蒸馏模块**：  
  - 引入多模态注意力加权融合机制，结合频谱图、MFCC和音频波形特征，增强音频情感提取能力。  
  - 采用多分辨率码本和4D高斯编码，实现对情感微表情的细粒度控制。  
- **基于不确定性的形变模块**：  
  - 为每个视图特征部署不确定性块，分别估计偶然性不确定性（来自输入噪声）和认知不确定性（来自模型参数）。  
  - 基于不确定性动态调整视图融合权重（不确定性越高，权重越低），实现自适应多视图融合。  
  - 使用多头高斯形变解码器优化高斯基元，提升渲染保真度。  

3)  
- **任务与效果**：在常规和情感数据集上进行了广泛实验，UA-3DTalk在多项指标上超越SOTA方法：  
  - **情感对齐**：E-FID指标提升5.2%。  
  - **唇部同步**：Sync-C指标提升3.1%。  
  - **渲染质量**：LPIPS指标降低0.015，显示更优的图像细节保真度。  
- **关键优势**：在纯音频驱动场景下无需依赖参考视频或3DMM真值参数，实现了更准确的情感表达与更自然的动态面部渲染。
</div>

</details>

---

## Interpretable and Perceptually-Aligned Music Similarity with Pretrained Embeddings
- **Authors**: Arhan Vohra, Taketo Akama
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.19109v1](https://arxiv.org/abs/2601.19109v1)
- **PDF**: [https://arxiv.org/pdf/2601.19109v1](https://arxiv.org/pdf/2601.19109v1)

基于感知相似度的表征使得音乐检索系统能够判断哪些歌曲在听感上最为接近。当前最先进的方法通过自监督度量学习进行任务特定训练，显示出与人类判断的良好一致性，但由于可用数据集有限，其可解释性和泛化能力存在不足。本文研究表明，预训练的文本-音频嵌入模型（CLAP和MuQ-MuLan）在相似性任务上无需额外微调即可达到相当的感知对齐效果。为超越这一基线，我们提出一种创新方法：通过源分离技术和基于听感测试ABX偏好数据的线性优化，使预训练嵌入模型实现感知对齐。该模型提供可解释且可控制的乐器维度权重，使音乐制作人能够根据混合参考曲目检索分轨级别的循环乐段和采样素材。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐检索系统依赖感知相似性来匹配听众的听觉感受。传统方法（如基于标签或手工特征）难以准确捕捉“听起来相似”这一主观感知。
- **既有方法问题**：当前最先进方法通过自监督度量学习进行任务特定训练，虽与人类判断有较好对齐，但存在明显局限：
  - 需要大量特定任务数据和资源密集型训练。
  - 模型可解释性差，难以理解其决策依据。
  - 泛化能力有限，通常在训练和测试数据分布相近时表现良好。

2)  
论文提出了一种新颖的、可解释的感知对齐方法，利用预训练嵌入和线性优化来解决上述问题。其核心方法包含两个关键部分：

- **利用预训练文本-音频嵌入作为强基线**：
  - 直接评估了CLAP和MuQ-MuLan等通用预训练嵌入模型在感知相似性任务上的零样本性能。
  - 发现这些未经微调的模型在标准余弦相似度度量下，其感知对齐能力已与需要专门训练的先进方法（如Cascade-PAFT, D-CSN）相当。这证明了大规模多模态预训练能有效捕获与人类感知相关的语义信息。

- **提出乐器感知的加权相似性模型**：
  - **核心思想**：将混合音频的整体感知相似性建模为其各乐器组成部分相似性的加权和。这模拟了人类在听音乐时可能对不同乐器成分赋予不同注意力的认知过程。
  - **方法步骤**：
    1.  **源分离**：使用音乐源分离模型（如Demucs）或利用数据集提供的真实多轨音频，将混合歌曲分解为独立的乐器音轨（如鼓、贝斯、吉他、钢琴、残余音等）。
    2.  **嵌入提取**：使用冻结的预训练编码器（CLAP或MuQ-MuLan）分别提取每个乐器音轨以及原始混合音频的嵌入向量。
    3.  **相似性计算**：对于给定的ABX三元组（参考X，候选A和B），计算每个乐器类别k上，候选A与B相对于参考X的余弦相似度差值，形成一个特征向量。
    4.  **线性优化**：在人类偏好数据（Inst-Sim-ABX数据集）上，通过线性回归（OLS或Ridge）学习每个乐器类别的权重。这些权重代表了该乐器对人类整体相似性判断的感知重要性。
  - **优势**：
    - **可解释性与可控性**：模型输出明确的乐器权重，使相似性决策过程透明化。音乐制作人可以根据需求（如更关注鼓或贝斯音色）调整权重或进行检索。
    - **无需微调**：该方法在冻结的预训练嵌入上操作，仅通过线性层学习权重，避免了昂贵的端到端再训练。
    - **超越基线**：实验表明，基于MuQ-MuLan的加权模型在特定配置下（6音轨分离）取得了超越其自身基线（纯余弦相似度）和现有先进方法的感知对齐准确率。

3)  
- **任务**：在乐器感知音乐相似性判断任务上进行评估，使用了Inst-Sim-ABX数据集。该数据集包含针对鼓、贝斯、吉他、钢琴等乐器以及混合音频的ABX三元组人类偏好数据。
- **效果**：
  - **预训练嵌入的零样本能力**：CLAP和MuQ-MuLan仅使用余弦相似度，在混合音频相似性判断上就达到了与专门训练模型（如D-CSN, Cascade-PAFT）相当的水平，证明了其强大的迁移能力。
  - **加权模型的提升**：提出的乐器加权相似性模型，特别是基于MuQ-MuLan并使用6音轨源分离的版本，取得了最佳性能（在XYC配置下准确率达90.4%），超越了所有对比基线。
  - **可解释输出**：模型成功学习了可解释的乐器权重（如表4所示），例如在MuQ-MuLan 6-stem模型中，鼓、吉他和残余音的权重较高，量化了不同乐器成分对感知相似性的贡献。
</div>

</details>

---
