---
layout: post
title: "arXiv Daily – 2025-10-31"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-31（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-30 08:50 — 2025-10-31 08:50
- 抓取总数：2 篇 | 本页显示：2 篇（去重/过滤后）

## UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens
- **Authors**: Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Yinghao Liu, Zheng Xue, Gang Song, Boyang Zhou
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.26372v1](http://arxiv.org/abs/2510.26372v1)
- **PDF**: [http://arxiv.org/pdf/2510.26372v1](http://arxiv.org/pdf/2510.26372v1)

生成式建模近年来在文本、图像与音频领域取得显著突破，展现出统一表征学习的强大潜力。然而，音频生成模型仍在音频质量与跨任务泛化能力上面临挑战，这种碎片化现状导致开发冗余、性能不稳定及可扩展性受限。为此，我们提出\textbf{UniTok-Audio}——一个可扩展的统一音频生成框架。其核心创新包括：1）通过自回归方式将连续条件特征映射至目标音频的离散编解码令牌；2）引入特殊任务标识符，在单一框架中统一多任务学习范式；3）开发包含声学与语义分支的双流音频编解码器，实现高保真波形重建。实验表明，UniTok-Audio在语音修复、目标说话人提取、语音分离、语音转换及语言查询音源分离五项时间对齐任务中，均达到与当前最优单任务或多任务系统相当的性能。为促进后续研究，我们将开源代码库，演示页面详见：https://alibaba.github.io/unified-audio。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频生成模型在质量与跨任务泛化能力上面临挑战，现有方法多为任务专用架构，导致开发冗余、性能不一致及扩展性受限。  
- **既有问题**：  
  - 任务专用模型难以迁移至新任务，缺乏统一框架。  
  - 离散编解码器在重建质量上存在量化损失，而连续生成方法需复杂设计以融合多模态条件。  
  - 现有统一模型（如UniAudio、Metis）在语音分离等多轨输出任务中表现不足。

2)  
- **统一框架设计**：  
  - 采用仅解码器的自回归语言模型，将任务特定条件（如语音、文本）作为前缀序列输入，统一预测目标音频的离散令牌。  
  - 引入特殊任务标识符（如TSR、TTSE）区分不同任务模式，共享模型权重，无需任务适配。  
- **H-Codec编解码器**：  
  - 提出双流架构，分别量化声学特征（波形编码）与语义特征（自监督模型输出），减少信息损失。  
  - 通过残差向量量化与低帧率（25 Hz）设计，平衡重建质量与计算效率。  
- **多任务支持**：  
  - 使用延迟模式并行预测多层令牌，提升生成准确性；通过迭代推理（如rTSE模式）处理语音分离等多轨任务。  
  - 连续条件嵌入保留丰富语义信息，增强模型对多样化输入的适应能力。

3)  
- **任务与效果**：  
  - **语音修复**：在DNS挑战赛上，性能媲美专用模型（DNSMOS OVRL达3.42），包丢失隐藏任务中PLCMOS领先（4.58）。  
  - **目标说话人提取**：在Libri2Mix上，语音质量（OVRL 3.32）与相似度（SIM 0.95）均具竞争力。  
  - **语音分离**：在Libri2Mix和WSJ0-2mix上，通过迭代推理实现多轨输出，信号质量（OVRL 3.25）超越基线。  
  - **语音转换**：在VCTK数据集上，说话人相似度（SIM 0.50）与可懂度（WER 4.23）优于多数生成式模型。  
  - **语言查询音频源分离**：在DCASE任务中，CLAPScore（26.21）显著提升，证明文本条件生成的有效性。
</div>

</details>

---

## Modeling strategies for speech enhancement in the latent space of a neural audio codec
- **Authors**: Sofiene Kammoun, Xavier Alameda-Pineda, Simon Leglaive
- **Categories**: cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.26299v1](http://arxiv.org/abs/2510.26299v1)
- **PDF**: [http://arxiv.org/pdf/2510.26299v1](http://arxiv.org/pdf/2510.26299v1)

神经音频编解码器（NAC）通过连续向量序列或离散标记序列的形式，提供紧凑的语音潜在表征。本研究系统比较了这两种语音表征作为监督式语音增强训练目标时的性能差异。我们基于Conformer架构构建了自回归与非自回归两类语音增强模型，同时设置了直接微调NAC编码器的基线方案。实验结果表明三个核心发现：连续潜在表征的预测性能始终优于离散标记预测；自回归模型虽能提升音质，但会牺牲清晰度与计算效率，使得非自回归模型更具实用价值；编码器微调方案在增强指标上表现最优，但会削弱编解码器的重建能力。代码与音频样本已公开。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强旨在从含噪录音中恢复清晰语音信号。传统方法在时频域操作，深度学习方法扩展到波形域，但表示选择影响模型性能与效率。  
- **既有问题**：神经音频编解码器提供连续向量或离散令牌两种潜在表示，但现有方法未系统比较其作为训练目标的优劣；自回归模型依赖序列生成，可能降低效率与可懂度；编解码器编码器微调策略研究不足。  

2)  
- **核心方法**：基于Conformer架构，设计四种增强模型，系统对比离散/连续表示与自回归/非自回归建模：  
  - **离散自回归模型**：按时间与量化深度逐步预测令牌，依赖双向与因果Conformer模块。  
  - **离散非自回归模型**：通过双向Conformer直接预测所有令牌，消除自回归依赖。  
  - **连续自回归模型**：对潜在向量进行序列生成，假设各维度独立，使用高斯分布建模。  
  - **连续非自回归模型**：直接预测完整连续向量序列，避免逐步生成。  
- **编码器微调基线**：直接微调NAC编码器，使其从含噪输入生成清晰潜在表示，无需额外序列模型。  
- **训练与推理**：监督学习最大化似然函数，连续表示对应MSE损失，离散表示对应交叉熵损失；推理时通过argmax获取最可能输出，确保与编解码器兼容。  

3)  
- **任务与效果**：在Libri1Mix含噪语音数据集上评估，关键结论包括：  
  - **连续表示显著优于离散表示**：在语音质量（SIG提升0.40）、自然度（UTMOS提升0.80）等指标上表现更优。  
  - **非自回归模型更具实用性**：自回归模型质量略高但可懂度差、计算成本大（如C-AR需472 GFLOPs）；非自回归模型（如C-NAR仅需6 GFLOPs）在效率与性能间取得平衡。  
  - **编码器微调效果突出**：C-NAR-FT模型在质量、可懂度与速度上达到最佳折衷，但会降低编解码器重建性能。
</div>

</details>

---
