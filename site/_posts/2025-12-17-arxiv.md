---
layout: post
title: "arXiv Daily – 2025-12-17"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-17（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-16 08:50 — 2025-12-17 08:50
- 抓取总数：13 篇 | 本页显示：13 篇（去重/过滤后）

## Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization
- **Authors**: Yen-Ju Lu, Kunxiao Gao, Mingrui Liang, Helin Wang, Thomas Thebaud, Laureano Moro-Velazquez, Najim Dehak, Jesus Villalba
- **Categories**: cs.CL, cs.AI, cs.LG, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.14687v1](https://arxiv.org/abs/2512.14687v1)
- **PDF**: [https://arxiv.org/pdf/2512.14687v1](https://arxiv.org/pdf/2512.14687v1)

当前音频语言模型已能处理长对话，但情感感知与口语对话摘要的研究因缺乏关联语音、摘要及副语言特征的数据而受限。本文提出Spoken DialogSum——首个将原始对话音频与事实摘要、情感增强摘要、说话人年龄/性别/情感等语句级标签对齐的语料库。该数据集通过两阶段构建：首先，利用大语言模型对DialogSum文本进行改写，添加类似Switchboard语料中的填充词与反馈词，并为每句话标注情感、音高和语速特征；随后，通过富有表现力的文本转语音系统，依据带标签的脚本合成语音，并与副语言标签对齐。Spoken DialogSum包含13,460段情感多样的对话，每段均配有事实摘要与情感聚焦摘要。数据集已公开于https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/。基线实验表明，相较于级联式ASR-LLM系统，端到端音频语言模型可将情感摘要的ROUGE-L指标相对提升28%，验证了端到端语音建模的价值。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：音频语言模型（Audio-LLMs）已能处理长对话，但情感感知或口语对话摘要的研究因缺乏同时关联语音、摘要和副语言信息（如情感、语调）的数据集而受限。
- **既有方法的问题**：
  - 现有文本对话摘要数据集（如DialogSum）依赖脚本化转录，缺乏真实口语中的填充词、反馈语等自然交互特征。
  - 口语语料库（如SwitchBoard）虽包含真实语音信号，但缺少人工标注的摘要。
  - 现有基准多针对单一任务（如ASR、情感识别），未能有效融合语义内容与声学信息的交互。

2) **论文核心方法如何解决上述问题**
本文提出了**Spoken DialogSum**，首个大规模对齐原始对话音频、事实摘要、情感摘要及话语级副语言标签的数据集。其构建方法分为两个阶段，旨在解决数据缺失和模态割裂问题：

- **第一阶段：富文本对话生成（基于LLM）**
  - **风格转换**：使用LLaMA3.3 70B，以SwitchBoard的真实对话为风格参考，重写DialogSum的脚本化对话，插入自然的填充词、犹豫等不流利现象。
  - **反馈语插入**：在对话中上下文合适的位置插入简短的反馈语（如“嗯”、“对”），模拟真实互动中的倾听者参与。
  - **情感标注**：使用GPT-4o-mini为每个对话生成一个情感丰富的摘要，并为每个话语标注八种基本情感之一、音高和语速等级。
  - **效果**：经评估，生成对话在口语自然度和对话流畅性上均优于原始DialogSum和SwitchBoard。

- **第二阶段：语音合成与对齐（基于TTS）**
  - **说话人库构建**：从GigaSpeech中筛选并标注了超过2万条高质量语音提示，包含说话人的年龄、性别、音高、表达力和语速信息。
  - **条件式TTS合成**：采用Zonos-hybrid TTS模型，以上一阶段生成的情感向量、音高、语速标签以及从说话人库中随机选择的语音为条件，合成富有表现力的多说话人对话音频。
  - **时序驱动的语句放置**：基于真实对话语料库CANDOR的统计规律，精确安排反馈语和打断语句的插入时机，模拟真实对话中的重叠和节奏，进一步提升自然度。

- **核心创新与解决思路**
  - **数据创新**：首次创建了同时包含**原始音频**、**事实摘要**、**情感摘要**及**话语级副语言标签**的大规模语料库（13,460个对话，约160小时）。
  - **方法创新**：通过LLM与TTS的协同流水线，将脚本文本转化为高度自然、情感丰富的合成语音对话，弥补了真实口语数据与高质量摘要配对缺失的空白。
  - **评估维度**：数据集支持从纯语义（事实摘要）、纯副语言（属性分类）到跨模态（情感摘要）的连续任务评估，为端到端语音语言模型提供了全面的测试平台。

3) **在哪些任务上取得了怎样的效果**
论文在Spoken DialogSum上评估了三个互补任务：
- **事实摘要（纯语义）**：评估模型基于文本或音频捕捉对话核心内容的能力。
- **属性分类（纯副语言）**：评估模型仅从音频中识别说话人年龄、性别和情感的能力。
- **情感摘要（语义×副语言）**：评估模型融合内容与声学情感，生成情感丰富摘要的能力。

**关键效果**：实验表明，**端到端的Audio-LLM（如WavLLM）在情感摘要任务上的ROUGE-L分数，比级联系统（Whisper ASR + LLM）相对提升了28%-29%**。这证实了联合建模语义与声学信息对于理解并概括对话情感具有显著价值。
</div>

</details>

---

## Adapting Speech Language Model to Singing Voice Synthesis
- **Authors**: Yiwen Zhao, Jiatong Shi, Jinchuan Tian, Yuxun Tang, Jiarui Hai, Jionghao Han, Shinji Watanabe
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.14657v1](https://arxiv.org/abs/2512.14657v1)
- **PDF**: [https://arxiv.org/pdf/2512.14657v1](https://arxiv.org/pdf/2512.14657v1)

语音语言模型（SLM）作为一种统一范式，近期已广泛应用于语音相关任务，如文本到语音合成、语音增强及自动语音识别。然而，大规模预训练SLM的泛化能力仍有待深入探索。本研究基于仅135小时的合成歌唱数据集ACE-Opencpop，将包含17亿参数的TTS预训练SLM适配于歌声合成任务。方法以ESPNet-SpeechLM为基础，包含以下步骤：（1）乐谱条件与歌唱波形的标记化处理；（2）多流语言模型的标记预测；（3）基于条件流匹配的梅尔频谱生成；（4）梅尔频谱到波形的声码器转换。实验结果表明，适配后的SLM在歌声合成任务中展现出良好的泛化能力，其性能可与当前主流的基于离散标记的歌声合成模型相媲美。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音语言模型（SLM）作为统一框架，已成功应用于多种语音任务。然而，其在大规模预训练后的泛化能力尚未充分探索。  
- **既有问题**：  
  - 歌唱合成（SVS）任务输入复杂（包含音素、时长、音高），且公开数据集稀缺，标注成本高。  
  - 现有SLM预训练主要依赖数据丰富的任务（如TTS、ASR），直接迁移至SVS面临数据不足和领域差异的挑战。  

2)  
- **核心方法**：基于预训练的TTS-SLM（ESPNet-SpeechLM），通过以下步骤适配SVS任务：  
  - **多模态令牌化**：将乐谱条件（音素、音高、时长）和歌唱波形分别量化为离散令牌，统一输入格式。  
  - **多流语言建模**：以乐谱和说话人提示为条件，预测包含SSL语义令牌和编解码器声学令牌的多流目标序列，通过微调SLM实现条件生成。  
  - **基于条件流匹配的细化**：为解决直接解码令牌的噪声和边界不连续问题，引入条件流匹配模型：  
    - 以SLM预测的编解码器特征和音高为条件，将高斯噪声映射到目标梅尔频谱图。  
    - 训练与编解码器STFT参数一致的声码器，将梅尔频谱转换为高质量波形。  
  - **关键设计**：  
    - 通过流匹配细化，缓解了预训练编解码器（针对语音）对歌唱重建的局限性。  
    - 在流匹配中强化音高条件，提升了旋律保真度。  

3)  
- **任务与效果**：在ACE-Opencpop（135小时合成歌唱数据集）上进行评测。  
  - 在歌唱合成任务上，本方法在多项指标上达到与主流离散SVS模型（如TokSing）相当的性能：  
    - 在歌唱质量（SingMOS）上优于或接近最佳模型。  
    - 音高准确性（F0_RMSE/CORR）略低于专用模型，但通过流匹配细化后得到改善。  
  - 结果表明，SLM在低资源下游任务中具有良好的泛化能力。
</div>

</details>

---

## Robust Training of Singing Voice Synthesis Using Prior and Posterior Uncertainty
- **Authors**: Yiwen Zhao, Jiatong Shi, Yuxun Tang, William Chen, Shinji Watanabe
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.14653v1](https://arxiv.org/abs/2512.14653v1)
- **PDF**: [https://arxiv.org/pdf/2512.14653v1](https://arxiv.org/pdf/2512.14653v1)

近年来，歌声合成技术取得了显著进展。然而，与语音和通用音频数据相比，公开可用的歌唱数据集仍然有限。实践中，这种数据稀缺往往导致模型在长尾场景下性能下降，例如音高分布不均衡或罕见歌唱风格。为缓解这些问题，我们提出基于不确定性的优化方法，以改进端到端歌声合成模型的训练过程。首先，我们在对抗训练中引入可微分数据增强，以样本级操作增加先验不确定性；其次，我们加入帧级不确定性预测模块来估计后验不确定性，使模型能将更多学习能力分配给低置信度片段。在中文Opencpop和日语Ofuton-P数据集上的实验结果表明，该方法在多个维度上提升了合成性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：歌唱语音合成（SVS）面临公开数据集稀缺的挑战，导致模型在长尾场景（如高音区、罕见唱法）中性能下降。  
- **既有方法问题**：现有数据增强方法通常保守，旨在保持原始数据分布，虽能维持生成质量，但限制了模型对多样化或挑战性歌唱模式的泛化能力。

2)  
论文提出两种基于不确定性的优化方法，以提升端到端SVS模型的训练鲁棒性：  
- **先验不确定性增强**：在对抗训练中引入**可微分数据增强**。该方法在判别器的特征层面（如频谱特征）施加随机掩码或添加噪声，以样本级方式增加输入扰动，从而提升先验不确定性。这迫使生成器学习更鲁棒的特征表示，避免对训练数据的过拟合，同时保持目标分布不变。  
- **后验不确定性建模**：设计一个**帧级不确定性预测模块**。该模块以先验潜变量为输入，预测模型在每个时间步的置信度（不确定性），并通过与真实音频的L2距离进行监督。这使得模型能够识别自身预测中的低置信度片段，并在训练中分配更多学习容量，从而针对性提升困难样本的生成质量。  
- **整合与训练**：两种方法可灵活集成到现有SVS框架（如VISinger2）中。可微分增强通过对抗损失优化，而不确定性预测则作为辅助损失，共同引导模型更高效地学习长尾数据分布。

3)  
- **任务与数据集**：在中文Opencpop和日语Ofuton-P歌唱数据集上进行实验，涵盖单歌手、多歌曲场景。  
- **效果**：  
  - **客观指标**：相比基线，联合使用两种方法在音高误差（Log F0 RMSE）、频谱失真（MCD）、清浊音决策（VUV）等指标上均取得显著提升。  
  - **主观评价**：MOS评分在歌词可懂度、旋律自然度和整体自然度上均有提高，尤其在长尾案例中表现更优。  
  - **泛化性**：在数据量更小、语言不同的Ofuton-P上同样有效，证明了方法的跨语言适应性。
</div>

</details>

---

## Segmental Attention Decoding With Long Form Acoustic Encodings
- **Authors**: Pawel Swietojanski, Xinwei Li, Mingbin Xu, Takaaki Hori, Dogan Can, Xiaodan Zhuang
- **Categories**: eess.AS, cs.CL
- **arXiv**: [https://arxiv.org/abs/2512.14652v1](https://arxiv.org/abs/2512.14652v1)
- **PDF**: [https://arxiv.org/pdf/2512.14652v1](https://arxiv.org/pdf/2512.14652v1)

本文针对基于注意力机制的编码器-解码器（AED）模型与长时声学编码之间的根本性不兼容问题展开研究。在分段语音上训练的AED模型通过利用片段边界之外的有限声学上下文来学习编码绝对帧位置，但在解码长时语音片段时，由于这些线索消失而无法泛化。由于交叉注意力中键与值的排列不变性，模型丧失了排序声学编码的能力。我们提出四项改进措施：（1）在每个解码片段中向交叉注意力注入显式绝对位置编码；（2）采用扩展声学上下文的长时训练以消除隐式绝对位置编码；（3）通过片段拼接覆盖训练所需的各种分段情况；（4）使用语义分割使AED解码片段与训练片段对齐。实验表明，这些改进能够弥合连续声学编码与分段声学编码之间的准确率差距，从而实现注意力解码器的自回归使用。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于注意力的编码器-解码器（AED）模型在长音频识别（LF ASR）中面临挑战。传统方法依赖外部分割系统或窗口技术来模拟边界条件，限制了自回归解码的潜力。
- **既有问题**：AED模型在分段音频上训练时，会隐式学习利用片段边界的有限声学上下文作为位置锚点。但在处理长音频时，边界线索消失，导致模型因交叉注意力的置换不变性而无法对声学编码排序，出现重复转录和无法生成句子结束符等问题。

2)  
论文提出了四项核心修改来解决上述问题：

- **交叉注意力中的显式位置编码**：  
  在解码每个片段时，向声学编码添加绝对位置编码。这确保了注意力权重能反映时间位置，同时为每个解码片段重置位置索引，避免了全局位置的无界增长，从而解决了置换不变性问题。

- **长音频训练**：  
  在训练中扩展声学上下文，为随机选择的长音频样本添加必要的左右声学上下文，以生成有效的长音频片段编码。这消除了边界线索，迫使模型依赖显式位置编码而非隐式边界效应来理解顺序。

- **片段拼接**：  
  将连续片段及其声学特征拼接起来进行训练。这增加了训练片段的时长多样性，并让注意力解码器暴露于更多的长音频编码，增强了其处理不同长度片段的能力。

- **语义分割**：  
  使用CTC头部来建模语义分割标记，以指示语义连贯句子的边界，替代仅基于音频的语音活动检测（VAD）。当CTC输出分割标记时，即触发第二遍注意力解码，确保解码片段与训练时的语义边界对齐。

这些修改共同作用，使模型在训练时不再依赖片段边界线索，并能有效利用显式位置信息，从而实现了对长音频编码的自回归解码。

3)  
- **任务与效果**：在TED-LIUM3和Earnings-21等长音频测试集上，所提方法显著缩小了分段编码与长音频编码之间的性能差距。例如，在TED-LIUM3上，联合CTC-注意力解码的WER从基线（长音频）的295%降至4.3%-4.5%，与分段编码性能相当。
- **整体表现**：模型在LibriSpeech、CommonVoice等分段音频任务上也保持或提升了竞争力，且参数量相近时，其表现与Whisper模型相当或更优。特别是，结合语义分割的混合CTC-注意力系统在低延迟下实现了最佳性能。
</div>

</details>

---

## MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation
- **Authors**: Yash Vishe, Eric Xue, Xunyi Jiang, Zachary Novack, Junda Wu, Julian McAuley, Xin Xu
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2512.14629v1](https://arxiv.org/abs/2512.14629v1)
- **PDF**: [https://arxiv.org/pdf/2512.14629v1](https://arxiv.org/pdf/2512.14629v1)

音乐编辑在现代音乐制作中扮演着关键角色，广泛应用于影视、广播和游戏开发领域。随着音乐生成模型的快速发展，音色转换、乐器替换和风格转换等多样化编辑任务已成为可能。然而，现有研究大多忽视了对其在编辑过程中应保持不变的音乐要素保持能力的评估——这一特性我们定义为音乐上下文保持（MCP）。尽管部分研究考虑了MCP，但采用的评估协议和指标缺乏一致性，导致现有比较结果不可靠且不公平。为填补这一空白，我们提出了首个MCP评估基准MuseCPBench，涵盖四大类音乐要素，支持对五种代表性音乐编辑基线方法进行全面比较。通过对音乐要素、编辑方法和生成模型的系统性分析，我们揭示了当前音乐编辑方法中普遍存在的保持能力缺陷，并提供了具有洞察力的解释。本研究旨在为开发具备更强MCP能力的高效可靠音乐编辑策略提供实践指导。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐编辑在音乐制作中至关重要，但现有研究在评估编辑系统时，普遍忽视了对“音乐上下文保持”能力的系统衡量。MCP指编辑过程中应保持不变的音乐属性（如和声、节奏、结构）的保持能力。
- **既有问题**：
  - 现有工作对MCP的评估零散且不一致，各方法采用不同的评估协议和指标（例如，有的只评估和声与节奏，有的关注结构或音轨保真度），导致无法进行可靠、公平的跨方法比较。
  - 缺乏统一的基准来系统、全面地衡量音乐编辑系统的MCP能力，限制了我们对当前技术优劣的理解。

2)  
论文提出了首个统一的MCP评估基准 **MuseCPBench**，通过以下核心设计系统性地解决了上述问题：

- **统一音乐维度与指标**：
  - 将应被保持的音乐属性归纳为四个核心维度：**和声**、**节奏与节拍**、**结构形式**、**旋律内容与动机**。
  - 为每个维度整合并定义了广泛使用的量化指标（如五度圈距离、节拍F值、调整兰德指数、动机重叠召回率等），提供了标准化的测量方法。

- **全面的方法比较**：
  - 基准涵盖了五类具有代表性的音乐生成与编辑方法，包括基于Transformer的（如MusicGen, MusiConGen）和基于扩散模型的（如MusicMagus, ZETA, RefinPaint）技术。
  - 确保了实验设置与各基线方法原有协议一致，保证了结果的可比性与真实性。

- **系统分析与解释**：
  - 通过大量实验，对各个方法在不同音乐维度上的MCP表现进行了详细的对比分析。
  - 研究揭示了当前方法的普遍缺陷：即使表现良好的编辑方法也无法保证全面的MCP；擅长保持某一维度的方法可能在另一维度上表现不佳（例如，MusicMagus和声保持好但节奏差，ZETA节奏好但旋律动机保持弱）。
  - 基准的建立为未来开发具有更强MCP能力的、更可靠有效的音乐编辑策略提供了实用的评估框架和设计启示。

3)  
- **评估任务**：在统一的MuseCPBench基准上，对五类代表性音乐编辑方法（MusicGen, MusiConGen, MusicMagus, ZETA, RefinPaint）进行了全面的MCP能力评估。
- **取得的效果**：
  - **揭示了性能差异**：研究发现，基于扩散模型的方法（如RefinPaint, ZETA）在整体MCP上通常优于基于Transformer的方法。RefinPaint在大多数维度（尤其是旋律、节奏、结构）上表现最佳。
  - **识别了具体短板**：明确了各方法在不同音乐维度上的强弱项，例如MusicGen在节奏保持上薄弱，MusicMagus为保持和声牺牲了节奏一致性。
  - **验证了基准有效性**：通过包含迭代修正型的RefinPaint并观察到其极高的MCP分数，证实了所设计指标能有效捕捉到有利于上下文保持的技术优势。
</div>

</details>

---

## Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis
- **Authors**: Lukáš Samuel Marták, Patricia Hu, Gerhard Widmer
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.14602v1](https://arxiv.org/abs/2512.14602v1)
- **PDF**: [https://arxiv.org/pdf/2512.14602v1](https://arxiv.org/pdf/2512.14602v1)

自动音乐转录（AMT）——将音乐音频转换为音符表示的任务——在深度学习系统的推动下取得了快速进展。由于富含标注的音乐数据集有限，AMT的进展主要集中在古典钢琴音乐，甚至仅依赖于少数特定数据集。这些系统能否有效推广到其他音乐场景仍是一个开放问题。作为对近期声音分布偏移（如录音条件）研究的补充，本文从音乐维度——特别是流派、动态范围和复音层次的变化——展开探究。为此，我们构建了MDS语料库，包含三个独立子集：（1）流派集、（2）随机集和（3）MAE测试集，以模拟不同维度的分布偏移。我们使用传统信息检索指标和音乐感知性能指标，评估了多种先进AMT系统在MDS语料库上的表现。大量实验结果表明，系统在特定分布偏移下均出现不同程度性能下降：声音因素导致音符级F1值下降20个百分点，流派因素导致下降14个百分点。总体而言，动态范围估计比起始点预测更容易受音乐变化影响。音乐感知评估指标（尤其是捕捉和声结构的指标）有助于识别潜在影响因素。此外，在随机生成的非音乐序列上的实验表明，系统在极端音乐分布偏移下存在明显局限性。这些发现共同为深度AMT系统中持续存在的语料库偏差问题提供了新的证据。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动音乐转录（AMT）领域因深度学习而快速发展，但受限于高质量标注数据的稀缺，现有模型主要基于古典钢琴音乐（如MAESTRO数据集）进行训练。  
- **既有问题**：  
  - **语料库偏差**：模型倾向于记忆训练数据中常见的音符组合，难以泛化到其他音乐语境。  
  - **分布偏移脆弱性**：模型对训练数据之外的声音（如不同录音条件）和音乐特性（如流派、动态范围、复调程度）的鲁棒性不足，性能显著下降。  
  - **评估局限**：传统信息检索指标无法全面反映转录结果在音乐表现力（如动态、节奏、和声）上的质量。

2)  
- **构建MDS评估数据集**：  
  - 包含三个子集，均使用雅马哈Disklavier钢琴录制，以固定声音分布：  
    - **Genre集**：涵盖10种音乐流派（如古典、爵士、摇滚），用于评估音乐分布偏移。  
    - **Random集**：生成完全随机的音符序列，模拟极端音乐分布偏移。  
    - **MAEtest集**：从MAESTRO测试集选取，用于校准和分离声音偏移的影响。  
- **系统化评估框架**：  
  - 评估五种先进AMT模型（如OaF、Kong、T5、Toyama、Edwards），使用其官方代码和参数。  
  - **多维度指标**：  
    - **传统信息检索指标**：如音符级F1分数（考虑起始、偏移、力度）。  
    - **音乐感知指标**：评估表达性时序、连奏、和声张力、动态范围等音乐表现力维度。  
- **分离偏移影响**：  
  - 通过对比MAEtest的原录音与Disklavier录音，量化**声音分布偏移**的影响。  
  - 通过对比MAEtest（古典）与Genre集（多流派），量化**音乐分布偏移**的影响。  
  - 通过Random集揭示模型在**极端非音乐序列**下的局限性。  
- **深入分析**：  
  - 按流派、复调程度、动态范围细分结果，识别具体脆弱环节（如偏移检测对声音敏感，力度估计对音乐内容敏感）。  
  - 分析预测结果的统计分布（如力度值直方图），揭示模型偏差（如倾向于预测训练数据常见的力度范围）。

3)  
- **评估任务与效果**：  
  - **声音分布偏移**：在MAEtest集上，从原录音切换到Disklavier录音导致**音符级F1分数平均下降约20个百分点**，偏移检测受影响最大。  
  - **音乐分布偏移**：从古典音乐（MAEtest）切换到多流派音乐（Genre集）导致**音符级F1分数进一步下降约14个百分点**，力度估计下降尤为明显（约22个百分点）。  
  - **极端分布偏移**：在完全随机的音符序列（Random集）上，性能**急剧下降达51个百分点**，表明模型严重依赖训练数据中的音乐结构。  
  - **模型对比**：不同模型对偏移的敏感度不同，例如Toyama在偏移检测上较稳健，而T5在时序指标上表现较差。音乐感知指标进一步揭示了模型在保留和声、动态等音乐特性方面的不足。
</div>

</details>

---

## Linguists should learn to love speech-based deep learning models
- **Authors**: Marianne de Heer Kloots, Paul Boersma, Willem Zuidema
- **Categories**: cs.CL, cs.SD, eess.AS, q-bio.NC
- **arXiv**: [https://arxiv.org/abs/2512.14506v1](https://arxiv.org/abs/2512.14506v1)
- **PDF**: [https://arxiv.org/pdf/2512.14506v1](https://arxiv.org/pdf/2512.14506v1)

Futrell与Mahowald提出的框架，有效连接了技术导向的深度学习系统与解释导向的语言学理论。然而，目标文章聚焦于生成式文本大语言模型，这从根本上限制了其与语言学的有效互动，因为人类语言中诸多重要问题无法通过书面文本完全体现。我们认为，基于音频的深度学习模型能够且应当发挥关键作用。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：评论文章认为，Futrell & Mahowald 提出的框架虽试图连接深度学习与语言学理论，但其焦点局限于基于文本的生成式大语言模型。
- **既有问题**：文本模型存在根本性局限，无法捕捉人类口语的核心方面。
    - 文本是离散化的，丢失了语音的连续信号及韵律、语调等关键语音学特征。
    - 文本与口语的歧义模式不同，导致文本模型解决的是与人类语言使用者不同的问题。
    - 语言的本质结构源于语音（或手语），而非文字。仅通过添加声学标记或语音识别/合成组件来增强文本模型，无法解决文本瓶颈在语言建模中造成的深层问题。

2)  
论文主张并论证，应转向并重视**基于语音的深度学习模型**，以更自然、更全面地建模人类语言。
- **核心方法**：采用**自监督语音基础模型**。这些模型直接在未标注的语音录音上进行训练，学习对音频信号进行表征。
- **如何解决文本模型的问题**：
    - **直接处理语音信号**：模型以原始音频为输入，避免了文本离散化造成的信息丢失，能够潜在学习包括韵律、语调在内的完整语音结构。
    - **探究语言结构的习得**：通过**表征探针**和**行为测试**等方法，研究这些语音模型内部状态是否以及如何编码高层次语言模式。
        - 研究表明，模型能学习音素、词语、形态音位模式、超音段特征等。
        - 模型可作为“心理语言学参与者”，模拟人类语音感知实验，研究感知相似性、语音范畴化等。
    - **引入更符合认知的归纳偏置**：
        - **领域通用感知的偏置**：在音乐、环境声等多类音频上预训练的模型，在处理语音时表现出更类人的行为（如检测序列模式、体现母语效应），提示人类语言相关感知学习可能源于对更广泛声音的表征优化。
        - **双向处理的偏置**：提出在模型架构中引入**对称连接权重**（双向处理），这对应于语言理解与产出共享同一知识的语言学假设。初步模拟表明，这种偏置能预测音系库存的演化，并可能促使语用、语义、句法等领域规律的涌现。

3)  
- **任务与效果**：在**语音建模与语言结构探测**相关任务上，基于语音的模型取得了积极进展。
    - **结构编码**：成功探测到模型内部对音素、词语、形态音位、超音段模式等语言单元的编码。
    - **类人行为模拟**：在区分真词与伪词、判断韵律自然性、感知相似性、语音范畴化等行为测试中，模型表现能够模拟人类实验参与者的结果。
    - **理论启示**：证明仅从语音信号中，模型即可学习相关语言结构模式，无需预先设定符号范畴。同时，探索不同归纳偏置为开发更类人、数据效率更高的系统，以及理解人类语言习得机制提供了新方向。
</div>

</details>

---

## GLM-TTS Technical Report
- **Authors**: Jiayan Cui, Zhihan Yang, Naihan Li, Jiankun Tian, Xingyu Ma, Yi Zhang, Guangyu Chen, Runxuan Yang, Yuqing Cheng, Yizhi Zhou, Guochen Yu, Xiaotao Gu, Jie Tang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.14291v1](https://arxiv.org/abs/2512.14291v1)
- **PDF**: [https://arxiv.org/pdf/2512.14291v1](https://arxiv.org/pdf/2512.14291v1)

本研究提出了GLM-TTS，一个面向生产环境、兼顾高效性、可控性与高保真语音生成的文本转语音系统。该系统采用两阶段架构，包含文本到语音标记的自回归模型和语音标记到波形的扩散模型。仅使用10万小时训练数据，GLM-TTS便在多个开源基准测试中达到了最先进的性能水平。为满足生产需求，系统通过引入基频约束的优化语音标记器，以及基于GRPO的多奖励强化学习框架（联合优化发音准确性、说话人相似度与富有表现力的韵律），显著提升了语音质量。同时，系统支持基于参数高效的LoRA语音定制技术实现高效可控的部署，并通过混合音素-文本输入方案提供精确的发音控制。代码已开源：https://github.com/zai-org/GLM-TTS。实时语音合成演示可通过Z.ai（audio.z.ai）及智谱清言应用/网页端（chatglm.cn）获取。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代TTS系统在零样本语音克隆、多语言合成等方面取得进展，但仍面临生产级部署的挑战。  
- **既有问题**：  
  - 高质量语音克隆通常需要大规模数据和长参考录音，低资源场景应用受限。  
  - 情感表达受限，多数模型难以捕捉细微文本相关情感或依赖复杂标签。  
  - 多音字、罕见词和方言的发音精度不足，尤其在中文等语言中。  
  - 强化学习在TTS中应用不足，因奖励设计和训练稳定性问题。  
  - 个性化语音定制常需全模型微调，成本高且难以扩展。

2)  
- **核心方法**：GLM-TTS采用两阶段架构（文本到令牌自回归模型 + 令牌到波形扩散模型），并通过多项优化解决上述问题。  
  - **语音分词器优化**：基于Whisper-VQ改进，引入基频约束、词汇扩展至32k、提升令牌生成率至25Hz，并采用非因果架构，增强发音准确性和自然度。  
  - **多奖励强化学习**：基于GRPO框架，融合CER（发音准确率）、SIM（音色相似度）、Emotion（情感表达）和Laughter（副语言真实性）四项奖励，通过动态采样和自适应梯度裁剪解决奖励黑客和训练不稳定问题，提升情感表达和发音精度。  
  - **低成本语音定制**：优化LoRA微调范式，仅调整15%参数，使用1小时单说话人音频即可达到全模型性能，降低80%训练成本。  
  - **精确发音控制**：提出“混合音素+文本”输入方案，结合动态可扩展词典，针对多音字和罕见词实现精准发音控制，不牺牲韵律自然度。  
  - **波形重建增强**：设计Vocos2D声码器，用2D卷积和DiT风格残差连接改进频带建模，并混合高质量歌唱数据训练，提升音质和适应性。

3)  
- **任务与效果**：  
  - **语音克隆**：在Seed-TTS-eval中文测试集上，CER达1.03%，SIM达76.1；经强化学习后，CER提升至0.89%，SIM至76.4，接近闭源SOTA模型。  
  - **情感表达**：在CV3-eval-emotion基准上，在快乐、悲伤、愤怒等情感表达上优于商业模型。  
  - **发音控制**：在内部多音字/罕见词数据集上，启用Phoneme-in机制后，音素错误率从13.23%降至5.14%。  
  - **声码器性能**：Vocos2D在NISQA、UTMOS等客观指标和主观MOS上均优于原Vocos，提升合成语音质量。
</div>

</details>

---

## Investigating the impact of stereo processing -- a study for extending the Open Dataset of Audio Quality (ODAQ)
- **Authors**: Sascha Dick, Christoph Thompson, Chih-Wei Wu, Pablo Delgado, Phillip A. Williams, Matteo Torcoli
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.14259v1](https://arxiv.org/abs/2512.14259v1)
- **PDF**: [https://arxiv.org/pdf/2512.14259v1](https://arxiv.org/pdf/2512.14259v1)

本文针对立体声处理的影响，开展了一项扩展音频质量开放数据集（ODAQ）的初步研究。研究将ODAQ中的单声道失真与左右（LR）及中侧（MS）立体声处理相结合，应用于包括独奏乐器、典型宽立体声混音和硬声像混音在内的多种刺激材料。通过在不同呈现情境下进行听音测试——包括是否直接对比MS与LR条件——收集了超越单声道失真的主观数据，并对听音测试方法进行了检验。ODAQ数据集扩展了新的材料及来自16位专家听音员的主观评分。听音测试结果表明，刺激材料的空间特性及呈现情境均产生显著影响。值得注意的是，LR与MS之间的若干显著差异仅在直接对比时出现。研究结果显示，当空间特性一致时，听音员主要评估音色损伤；而当音色质量相近时，他们才聚焦于立体声声像。额外单声道锚点的评分在不同立体声特性下总体一致，MUSHRA量表平均分为65，进一步证实听音员更重视音色印象而非空间印象。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频质量客观评估与算法优化依赖主观真实数据，但公开数据集稀缺，尤其在立体声/多声道信号处理影响方面。现有数据集（如ODAQ）主要关注单声道音色失真。
- **既有问题**：  
  - 缺乏公开的立体声处理影响数据集，阻碍了相关客观质量度量的复现与验证。  
  - 先前研究将单声道失真独立应用于左右（LR）声道时，会无意中影响立体声像。  
  - 测试中的呈现上下文（如条件是否直接对比）可能显著影响主观评分，但未被系统研究。

2)  
- **核心方法**：本研究通过扩展ODAQ数据集，系统探究了立体声处理、空间特性及呈现上下文对感知质量的影响。具体方法包括：  
  - **信号生成**：选取两种单声道失真（量化噪声QN、频谱孔SH），通过LR处理（独立处理左右声道）和MS处理（处理中-侧信号后再转换回LR）将其扩展到立体声信号，生成五个质量等级。  
  - **测试材料**：涵盖三种空间特性的音频：独奏乐器（中心声像）、宽立体声混音、硬声像混音，以探究不同空间特性下的影响。  
  - **实验设计**：  
    - 采用MUSHRA方法进行主观听力测试。  
    - 设置“分离试验”（如SHLR、SHMS），仅能间接对比LR与MS。  
    - 设置“混合试验”（如SHmix、QNmix），在同一试验中直接对比相同质量等级的LR与MS条件，以研究呈现上下文的影响。  
    - 引入单声道锚点，以评估空间特性对整体质量的贡献。  
  - **被试与环境**：16名专家听众在受控听音环境中完成测试，确保结果可靠性。

3)  
- **取得效果**：  
  - **数据集扩展**：成功为ODAQ新增了包含立体声处理影响的测试材料及主观评分。  
  - **关键发现**：  
    - 听众主要依据音色损伤评分；仅当音色质量相近时，才会关注立体声像差异。  
    - LR与MS的评分差异在直接对比时更显著，证实了呈现上下文的重要性。  
    - 单声道锚点评分稳定（平均65分），适合作为未来空间音频测试的锚点。  
  - **任务验证**：在评估不同空间特性音频（如硬声像、宽混音）的立体声处理影响时，方法有效揭示了处理方式与信号特性的交互作用。
</div>

</details>

---

## Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting
- **Authors**: Ramesh Gundluru, Shubham Gupta, Sri Rama Murty K
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.14115v1](https://arxiv.org/abs/2512.14115v1)
- **PDF**: [https://arxiv.org/pdf/2512.14115v1](https://arxiv.org/pdf/2512.14115v1)

声学词嵌入（AWE）提升了语音检索任务（如口语词检测和关键词检出）的效率。然而，现有方法存在诸多局限，包括单模态监督、音频-音频与音频-文本对齐的分离优化，以及需要针对特定任务设计模型。为克服这些不足，我们提出了一种联合多模态对比学习框架，将声学监督与跨模态监督统一于共享嵌入空间。该方法同时优化：（1）受CLAP损失启发的音频-文本对比学习，以对齐音频与文本表示；（2）通过深度词区分损失实现的音频-音频对比学习，以增强类内紧凑性与类间分离性。所提方法在词区分任务上优于现有AWE基线，并能灵活支持口语词检测与关键词检出任务。据我们所知，这是首个实现此类全面统一的方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：大规模音频档案（如播客）缺乏转录文本，需要直接检索语音内容。口语词检测（STD）和关键词唤醒（KWS）是关键任务，传统方法依赖自动语音识别（ASR），但ASR错误或资源限制会限制性能。  
- **既有方法问题**：  
  - 动态时间规整（DTW）方法计算复杂度高，对噪声和端点敏感。  
  - 现有声学词嵌入（AWE）方法多为单模态监督（仅音频-音频或音频-文本对），且音频-音频与音频-文本目标分离优化，导致模型无法灵活支持跨模态任务（如同时处理STD和KWS）。  

2)  
- **核心方法**：提出联合多模态对比学习框架，在共享嵌入空间中统一声学和跨模态监督。  
- **具体解决方式**：  
  - **音频-文本对比学习**：采用CLAP风格损失，对齐音频和文本表示，使匹配的音频-文本对在嵌入空间中靠近。  
  - **音频-音频对比学习**：通过深度词判别（DWD）损失，增强同类紧凑性和类间分离性，提升声学词嵌入的判别力。  
  - **联合优化**：将上述两个目标通过加权损失函数（总损失 = α₁ × CLAP损失 + α₂ × DWD损失）同时优化，平衡跨模态对齐和音频判别性。  
- **优势**：  
  - 单模型同时支持STD（查询示例匹配）和KWS（文本查询匹配），提升跨模态灵活性。  
  - 增强对说话人变化和背景噪声的鲁棒性，聚焦于跨模态的词身份识别。  

3)  
- **任务与效果**：  
  - **词判别任务**：在LibriSpeech数据集上，声学视图下，模型在已知词（IV）和未知词（OOV）上分别达到85.05%和94.06%的平均精度（AP），优于单模态基线。  
  - **口语词检测（STD）**：在测试集上，模型在多数窗口尺寸下取得最低等错误率（EER），如在0.3秒窗口下IV查询EER为15.71%，显示对噪声和说话人变化的鲁棒性。  
  - **关键词唤醒（KWS）**：模型在跨模态匹配中表现稳定，但DWD损失对纯音频判别提升更显著，在KWS中略有性能波动。
</div>

</details>

---

## Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study
- **Authors**: Koji Inoue, Mikey Elmers, Yahui Fu, Zi Haur Pang, Taiga Mori, Divesh Lala, Keiko Ochi, Tatsuya Kawahara
- **Categories**: cs.CL, cs.HC, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.14085v1](https://arxiv.org/abs/2512.14085v1)
- **PDF**: [https://arxiv.org/pdf/2512.14085v1](https://arxiv.org/pdf/2512.14085v1)

本文提出了一种面向日语、英语和汉语的多语言连续反馈预测模型，并借此研究了跨语言的反馈时机行为。该模型基于Transformer架构，以帧级别运行，并利用约300小时的双人对话数据，通过多任务联合训练进行优化。实验表明，该多语言模型在所有三种语言上均达到或超越了单语言基线性能，证明其能够同时学习语言通用的线索和语言特定的时机模式。然而，仅使用两种语言训练的零样本迁移效果有限，这凸显了跨语言间存在的实质性差异。扰动分析进一步揭示了不同语言在线索使用上的区别：日语更依赖短时语言信息，而英语和汉语则对静默时长和韵律变化更为敏感；多语言训练促进了共享且可适应的表征学习，并减少了汉语对音高特征的过度依赖。上下文长度研究表明，日语对较短上下文具有相对鲁棒性，而汉语则显著受益于更长上下文。最后，我们将训练好的模型集成到实时处理软件中，实现了仅使用CPU的推理。这些研究结果不仅提供了一个统一的模型，也为不同语言间反馈时机的差异提供了实证依据，从而为设计更自然、更具文化适应性的口语对话系统提供了参考。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：构建自然流畅的语音对话系统需要准确预测倾听者的反馈信号（如“嗯”），其时机因语言文化而异。现有研究多集中于单一语言（如日语或英语），缺乏跨语言比较与统一建模。
- **既有问题**：  
  - 多数方法为单语言模型，难以捕捉不同语言中反馈时机（如日语常在说话中插入，中文多在话后）的差异。  
  - 连续帧级预测模型面临标签稀疏、数据不平衡的挑战，且跨语言零样本迁移效果有限。

2)  
- **核心方法**：提出一个基于Transformer的多语言连续反馈预测模型，在日语、英语和中文约300小时的双人对话数据上联合训练。  
- **模型架构**：  
  - 输入为说话者和倾听者的分离音频波形，通过对比预测编码（CPC）提取特征。  
  - 使用自注意力与交叉注意力Transformer融合双说话者交互信息。  
  - 通过多任务学习联合优化四个辅助任务，以缓解数据稀疏问题：  
    - 语音活动检测（VAD）  
    - 语音活动投影（VAP，用于预测未来2秒的说话状态）  
    - 反馈检测（BD，识别当前反馈）  
    - 反馈预测（BP，主任务，预测0.5秒后的反馈时机）  
- **解决思路**：  
  - **多语言联合训练**：使模型能同时学习语言通用特征（如韵律、沉默）和语言特定模式（如日语依赖短时语言信息，中文对沉默时长更敏感）。  
  - **多任务设计**：辅助任务（尤其是VAP）帮助模型更好地建模话轮转换动态，提升跨语言泛化能力。  
  - **无需显式语言标识**：模型仅从音频数据中自适应地识别语言差异。

3)  
- **任务与效果**：在日语、英语和中文的反馈时机预测任务上评估：  
  - **多语言模型**在三种语言上均达到或超过单语言基线（F1分数：日语33.69%，英语23.96%，中文22.65%），表明其有效融合了通用与语言特定特征。  
  - **零样本迁移**：仅用两种语言训练的模型在第三种语言上表现较差，突显了跨语言差异的显著性。  
  - **实际应用**：模型已集成至实时处理软件，在仅CPU环境下实现10Hz实时推理，可用于构建更自然、跨文化感知的对话系统。
</div>

</details>

---

## Scalable Frameworks for Real-World Audio-Visual Speech Recognition
- **Authors**: Sungnyun Kim
- **Categories**: eess.AS, cs.CL, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.14083v1](https://arxiv.org/abs/2512.14083v1)
- **PDF**: [https://arxiv.org/pdf/2512.14083v1](https://arxiv.org/pdf/2512.14083v1)

面向真实场景的音频-视觉语音识别（AVSR）系统在实际部署中面临根本性挑战，即在不可预测的声学噪声和视觉干扰环境下性能显著下降。本论文提出，必须采用系统化、层次化的方法应对这些挑战，实现在表征、架构和系统层面的鲁棒可扩展性。在表征层面，我们研究如何构建统一模型，学习对多样真实场景干扰具有内在鲁棒性的音视频特征，从而无需专用模块即可泛化至新环境。针对架构可扩展性，我们探索如何在确保多模态输入自适应可靠使用的同时高效扩展模型容量，开发了一种基于输入特征智能分配计算资源的框架。最后，在系统层面，我们提出通过与大规模基础模型的模块化集成来扩展系统功能，利用其强大的认知与生成能力以最大化最终识别准确率。通过在这三个层面系统性地提供解决方案，本论文旨在构建具有高可靠性的新一代鲁棒可扩展AVSR系统，以适用于真实场景应用。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **核心挑战**：现实环境中的音频-视觉语音识别系统面临不可预测的声学噪声和视觉干扰，导致性能显著下降。
- **既有方法局限**：
  - 多数研究仅关注音频噪声，对视觉干扰（如口部遮挡、模糊）的处理不足。
  - 现有方法常依赖特定架构或外部模块（如可靠性评分、生成式修复），缺乏通用性。
  - 自监督学习方法（如AV-HuBERT）主要基于干净数据，未针对多模态联合损坏进行鲁棒性优化。

2) **论文核心方法如何解决上述问题**
论文提出一个分层、系统性的框架，在三个层面解决鲁棒性和可扩展性问题：

- **表示层面**：提出 **CAV2vec** 自监督学习框架。
  - **核心思想**：通过“损坏预测”任务学习对多模态联合损坏具有内在鲁棒性的表示。
  - **关键创新**：采用**单模态多任务学习策略**，即用损坏的视频预测干净的音频目标，用损坏的音频预测干净的视频目标。
  - **作用**：促进跨模态知识蒸馏和对齐，减少损坏输入导致的表示空间分散，从而提升融合表示的鲁棒性和泛化能力，无需针对特定噪声设计模块。

- **架构层面**：提出 **MoHAVE** 模型。
  - **核心思想**：基于混合专家模型，通过稀疏门控和层次化路由，实现模型容量的高效扩展。
  - **关键创新**：设计**层次化门控结构**，根据输入特征（如噪声水平、模态可靠性）动态地将令牌路由到最相关的专家或专家组。
  - **作用**：在显著增加模型总参数量的同时，保持每个令牌的计算成本基本恒定，实现了计算资源的自适应分配，提升了模型在多变输入下的效率和性能。

- **系统层面**：提出 **DualHyp** 框架。
  - **核心思想**：通过模块化集成大型语言模型，进行生成式错误修正。
  - **关键创新**：生成**双流假设**——分别来自独立的音频和视觉识别模型，然后将这两个假设输入LLM。
  - **作用**：LLM作为组合推理器，在语言空间整合双模态信息，纠正识别错误。这种方法可以灵活利用外部ASR、VSR和LLM组件，无需重新训练整个AVSR模型，实现了系统功能的可扩展性。

3) **在哪些任务上取得了怎样的效果**
- **主要任务**：在**鲁棒音频-视觉语音识别**任务上进行评估，使用LRS2、LRS3和MuAViC等基准数据集。
- **评估环境**：模拟现实损坏，包括**未见过的音频噪声**（如DEMAND数据集）和**视觉损坏**（如手部遮挡、像素化）。
- **效果**：
  - **CAV2vec**：在损坏的LRS3上，相比AV-HuBERT基线，WER相对降低超过20%。
  - **MoHAVE**：在噪声LRS3上达到SOTA性能，同时激活参数更少，计算效率更高。
  - **DualHyp**：在损坏的LRS2上，相比仅使用ASR假设，WER相对降低超过30%。结合CAV2vec和MoHAVE后，性能进一步提升。
- **综合结论**：论文提出的三层方法在多种损坏条件下显著提升了AVSR的准确性和鲁棒性，并展示了良好的可扩展性。
</div>

</details>

---

## Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition
- **Authors**: Qilin Li, C. L. Philip Chen, TongZhang
- **Categories**: cs.SD, cs.AI, cs.MM
- **arXiv**: [https://arxiv.org/abs/2512.13998v1](https://arxiv.org/abs/2512.13998v1)
- **PDF**: [https://arxiv.org/pdf/2512.13998v1](https://arxiv.org/pdf/2512.13998v1)

音乐情感识别研究面临高质量标注数据集稀缺及跨曲目特征漂移问题。本研究提出两项核心贡献以应对这些挑战。首先，我们构建了Memo2496大规模数据集，包含2496条器乐音轨，由30位认证音乐专家标注了连续效价-唤醒度标签。通过极端情感样本校准，并设定效价-唤醒空间欧氏距离≤0.25的一致性阈值，确保了标注质量。其次，我们提出了双视角自适应音乐情感识别框架DAMER，该框架集成三个协同模块：双流注意力融合模块通过交叉注意力机制实现梅尔谱图与耳蜗图在令牌级的双向交互；渐进置信度标注模块采用课程式温度调度策略，基于Jensen-Shannon散度进行一致性量化以生成可靠伪标签；风格锚定记忆学习模块通过维护对比记忆队列缓解跨曲目特征漂移。在Memo2496、1000songs和PMEmo数据集上的实验表明，DAMER在唤醒度维度准确率分别提升3.43%、2.25%和0.17%，达到最优性能。消融实验与可视化分析验证了各模块的有效性。本研究的标注数据集与源代码均已公开。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **数据质量与规模问题**：现有音乐情感识别（MER）数据集普遍依赖众包标注，存在标注噪声和一致性低的问题，且规模有限、风格单一（如集中于流行钢琴曲），难以支持深度模型的泛化需求。  
- **方法局限性**：传统方法依赖手工特征或单模态声学特征，难以捕捉音乐情感的语义丰富性；半监督学习中的伪标签生成容易传播噪声，且现有方法对跨曲目风格漂移问题处理不足，导致模型在异构数据上性能下降。

2)  
论文提出了双视图自适应音乐情感识别框架（DAMER），通过三个协同模块系统解决上述问题：  
- **双流注意力融合模块**：以梅尔频谱图和耳蜗图为双输入，通过双向交叉注意力机制实现令牌级别的跨模态交互，充分融合互补的声学特征，提升特征表达能力。  
- **渐进置信度标注模块**：采用课程学习策略，动态调整温度参数以平滑概率分布，早期避免错误确认，后期生成高置信度伪标签；同时利用Jensen-Shannon散度量化和提升双视图预测的一致性，加权选择可靠伪标签，缓解标注稀缺与噪声问题。  
- **风格锚定记忆学习模块**：维护一个滑动记忆队列存储历史特征，使用监督对比学习损失，拉近同类情感样本、推开异类样本，从而学习风格不变的情感表征，有效抑制跨曲目风格漂移带来的特征分布偏移。  
- **整体优化**：上述模块与分类损失共同构成多目标训练，实现特征融合、伪标签质量提升和跨风格泛化的协同优化。

3)  
在三个公开数据集上进行了系统评估：  
- **Memo2496**：在唤醒维度上准确率达到82.95%，较之前最佳方法提升3.43%；在效价维度上也取得竞争性结果。  
- **1000songs**：唤醒维度准确率81.28%，提升2.25%；效价维度亦有稳定改进。  
- **PMEmo**：在效价维度上准确率提升至77.61%，综合指标达到先进水平。  
消融实验与可视化分析证实了各模块的有效性，表明DAMER在跨数据集、跨标注方式下均具有优越的泛化能力。
</div>

</details>

---
