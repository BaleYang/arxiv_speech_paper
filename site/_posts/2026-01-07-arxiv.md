---
layout: post
title: "arXiv Daily – 2026-01-07"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-07（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-06 08:50 — 2026-01-07 08:50
- 抓取总数：14 篇 | 本页显示：14 篇（去重/过滤后）

## The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization
- **Authors**: Ruixing Zhang, Zihan Liu, Leilei Sun, Tongyu Zhu, Weifeng Lv
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.03227v1](https://arxiv.org/abs/2601.03227v1)
- **PDF**: [https://arxiv.org/pdf/2601.03227v1](https://arxiv.org/pdf/2601.03227v1)

地理定位旨在推断给定信号的地理来源。在计算机视觉领域，地理定位已成为组合推理能力的重要基准，并与公共安全密切相关。相比之下，音频地理定位的研究进展因缺乏高质量的音频-地理位置配对数据而受限。为填补这一空白，我们提出了AGL1K——首个面向音频语言模型的音频地理定位基准数据集，涵盖72个国家和地区。为从众包平台中筛选出具有可靠定位价值的样本，我们提出了“音频可定位性”指标，用于量化每条录音的信息丰富度，最终精选出1,444条音频片段。对16个音频语言模型的评估表明，此类模型已初步具备音频地理定位能力。研究发现，闭源模型性能显著优于开源模型，且语言线索常作为预测的主要推理依据。我们进一步分析了音频语言模型的推理路径、区域偏见、错误成因，以及可定位性指标的可解释性。总体而言，AGL1K为音频地理定位建立了基准测试框架，有望推动音频语言模型发展出更强大的地理空间推理能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频地理定位旨在通过音频推断地理位置，对公共安全（如虚假信息核查）有重要价值。然而，该领域长期缺乏系统性基准，主要受限于两个问题：
  - **数据缺失**：缺乏公开、高质量、带地理位置标注的音频数据集。
  - **评估困难**：缺乏量化指标来筛选具有地理信息量的音频样本，即使从众包平台获取数据，也难以识别真正可定位的录音。

2)  
论文通过构建首个音频地理定位基准 **AGL1K** 并提出 **音频可定位性** 度量来解决上述问题，具体方法如下：

- **基准构建与数据筛选**：
  - **数据获取**：从众包平台 Aporee 获取大量带地理标签的音频。
  - **初步过滤**：使用声学滤波器（如信号能量、频谱平坦度）去除低质量录音。
  - **核心创新：音频可定位性度量**：
    - **定义**：量化音频的地理信息量，公式为 \( l_k = \sum_{i \in P} a_i t_{k,i} - \sum_{i \in N} \bar{a}_i t_{k,i} \)。其中，\( P \) 和 \( N \) 分别代表正面（有地理信息）和负面（无地理信息）的声音类别，\( t_{k,i} \) 是类别在音频中的时间占比，\( a_i \) 和 \( \bar{a}_i \) 是类别贡献强度。
    - **贡献度估计**：利用强音频语言模型（如 Gemini 2.5）的推理轨迹（Chain-of-Thought），结合多个语言模型判断每个声音类别对定位的贡献程度，通过线性拟合学习 \( a_i \) 和 \( \bar{a}_i \)。
    - **筛选**：设定阈值 \( l_k > \theta \)，仅保留可定位性高的音频，最终人工精选出 1,444 个高质量片段。

- **方法优势**：
  - **解决数据问题**：首次提供了覆盖72个国家、六大陆、多样声学场景（自然声、动物声、音乐、人声等）的基准数据集。
  - **解决评估问题**：提出的可定位性度量能自动、量化地筛选信息丰富的样本，其有效性通过示例（如语音、海浪声贡献度高；引擎声、雨声贡献度低）得到验证，与人类直觉一致。

3)  
- **评估任务**：在 **AGL1K** 基准上评估了16个音频语言模型的音频地理定位能力。
- **主要效果**：
  - **模型表现**：当前模型已展现出一定的音频地理定位能力，但任务仍具挑战性。闭源模型（如 Gemini 3 Pro）显著优于开源模型。
  - **关键发现**：
    - **语言线索主导**：包含语音的音频定位误差远低于非语音音频。
    - **存在区域偏差**：模型对某些大洲（如北美）存在过预测，而对大洋洲、南美洲的预测性能较弱。
    - **错误分析**：主要错误类型包括语言歧义、过度依赖单一线索（如鸟鸣）、标签误识别等，为未来模型改进指明了方向。
</div>

</details>

---

## Segment-Aware Conditioning for Training-Free Intra-Utterance Emotion and Duration Control in Text-to-Speech
- **Authors**: Qifan Liang, Yuansen Liu, Ruixin Wei, Nan Lu, Junchuan Zhao, Ye Wang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.03170v1](https://arxiv.org/abs/2601.03170v1)
- **PDF**: [https://arxiv.org/pdf/2601.03170v1](https://arxiv.org/pdf/2601.03170v1)

尽管可控文本转语音技术已取得显著进展，但现有方法大多仍局限于语句间层面的控制。由于依赖非公开数据集或复杂的多阶段训练流程，实现细粒度的语句内情感表达仍具挑战。本文提出一种面向预训练零样本TTS系统的免训练可控框架，以实现语句内情感与时长表达。具体而言，我们提出分段感知的情感调节策略，通过结合因果掩码与单调流对齐滤波技术，实现情感调节的隔离与掩码过渡调度，从而在保持全局语义连贯性的同时，实现平滑的语句内情感转换。基于此，我们进一步提出分段感知的时长引导策略，将局部时长嵌入引导与全局终止符对数调制相结合，在实现局部时长调整的同时确保全局终止一致性。为消除分段层面人工提示工程的需求，我们构建了包含3万个样本的多情感与时长标注文本数据集，以支持基于大语言模型的自动提示构建。大量实验表明，本免训练方法不仅在多情感与时长控制中实现了当前最优的语句内一致性，同时保持了底层TTS模型的基线级语音质量。音频样本请访问：https://aclanonymous111.github.io/TED-TTS-DemoPage/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：可控文本转语音（TTS）在零样本合成方面取得进展，但现有方法大多局限于**话语间**控制，即整句话只能赋予单一情感或节奏。  
- **既有方法的问题**：  
  - 实现**话语内**细粒度情感与时长控制的方法通常依赖**非公开数据集**或**复杂的多阶段训练**，限制了跨模型迁移和实际部署。  
  - 现有方法难以在**不重新训练模型**的情况下，实现稳定的片段级情感过渡与时长调节。  

2)  
论文提出一种**无需训练**的框架，在预训练的零样本TTS模型上，通过推理时重构条件信息的访问与更新机制，实现话语内情感与时长控制。  

- **片段感知情感条件策略**：  
  - 设计**二维因果注意力掩码**，将条件嵌入的可见性限制在当前片段内，防止跨片段信息泄漏，同时保持全局语义连贯性。  
  - 提出**单调流对齐算法**，在线跟踪生成语义标记与源文本标记的对齐关系，基于贝叶斯式信念更新实现稳定、单调的对齐轨迹，从而精准触发片段间条件切换，实现平滑的情感过渡。  

- **片段感知时长引导策略**：  
  - **局部时长嵌入引导**：利用时长嵌入表与语义位置嵌入的对齐关系，根据用户指定的片段时长目标初始化嵌入。在解码过程中，通过实时计算文本进度与语义进度的差异，动态调整时长嵌入，以纠正生成速度的偏差。  
  - **全局EOS引导**：通过自适应地偏置结束标记的logit，抑制非最终片段的提前终止，并在最终片段根据剩余语义预算平滑地鼓励结束生成，确保全局序列终止的一致性。  

- **自动提示构建**：  
  - 构建了一个包含3万样本的多情感与时长标注文本数据集（MED-TTS），并基于此对Qwen3-8B进行微调，实现了基于大语言模型的自动片段划分、情感描述生成与时长估计，消除了手动提示工程的需求。  

3)  
- **任务与效果**：  
  - 在**话语内多情感控制**任务上，该方法在英语和中文上均实现了**最先进的片段间过渡平滑度**，并保持了与基线模型相当的语音质量与说话人相似性。  
  - 在**话语内多时长控制**任务上，该方法在多种时长缩放因子下，取得了**最低的语义标记数量误差**，实现了准确、稳健的局部节奏调整。  
  - 综合主观与客观评估表明，该无需训练的方法在保持基线语音质量的同时，在情感与时长控制方面均达到了最优或极具竞争力的性能。
</div>

</details>

---

## Discovering and Causally Validating Emotion-Sensitive Neurons in Large Audio-Language Models
- **Authors**: Xiutian Zhao, Björn Schuller, Berrak Sisman
- **Categories**: cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.03115v1](https://arxiv.org/abs/2601.03115v1)
- **PDF**: [https://arxiv.org/pdf/2601.03115v1](https://arxiv.org/pdf/2601.03115v1)

情感是语音交流的核心维度，然而，我们仍缺乏对现代大型音频-语言模型内部如何编码情感机制的清晰解释。本研究首次在大型音频-语言模型中开展了神经元层面的可解释性研究，聚焦于情感敏感神经元，并在Qwen2.5-Omni、Kimi-Audio和Audio Flamingo 3这三个广泛使用的开源模型中提供了此类神经元存在的因果证据。我们在多个情感识别基准上，比较了基于频率、熵、幅度和对比度的神经元选择方法。通过推理时干预实验，我们揭示了一种一致的情感特异性表征：针对特定情感选出的神经元被消融后，会显著降低对该情感的识别能力，而其他类别的情感识别基本不受影响；反之，基于增益的放大操作则会将预测结果导向目标情感。这些效应在仅使用少量识别数据时即可出现，且随干预强度增强而系统性地放大。我们还进一步观察到，情感敏感神经元在模型各层中呈现非均匀的聚类分布，并具备部分跨数据集的迁移能力。综上所述，我们的研究为大型音频-语言模型中的情感决策提供了因果性、神经元层面的解释，并表明针对特定神经元的干预可作为实现可控情感行为的有效调控手段。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：大型音频-语言模型在语音交互中需具备情感理解能力，但其内部情感表征机制尚不明确。现有研究缺乏对模型中情感编码单元的因果性验证。
- **既有方法问题**：先前神经元级可解释性研究多集中于视觉或单模态语言模型，对多模态（尤其是音频-语言）模型中情感敏感神经元的存在性、选择性及因果作用缺乏系统探究。现有方法（如基于频率或熵的神经元选择器）可能无法有效分离出具有情感特异性因果效应的单元。

2)  
- **核心方法**：本研究提出一个“激活记录-识别-干预”的工作流程，旨在因果性验证大型音频-语言模型中情感敏感神经元的存在与功能。
    - **神经元识别**：在模型正确进行语音情感识别时，记录解码器MLP中SwiGLU门控单元的激活。基于这些激活，采用并比较了四种神经元选择器来识别情感敏感神经元：基于激活概率、激活概率熵、平均激活差异以及对比激活选择。
    - **因果干预**：通过推理时干预来验证所识别神经元的因果作用。
        - **失活**：将选定神经元的激活置零。若神经元对特定情感必要，则失活应导致该情感识别性能显著下降，而对其他情感影响较小。
        - **定向引导**：放大选定神经元的激活。若神经元足以影响决策，则放大应使模型预测偏向该目标情感。
        - **不可知注入**：设计了三种无需指定源情感的注入策略，以探索神经元在无标签情况下的可控性及情感通路间的潜在交互。
- **如何解决问题**：该方法首次在多个开源音频-语言模型中进行神经元级的因果分析。通过系统比较不同选择器，明确了基于幅度和对比度的选择器能更有效地识别出具有强情感特异性因果效应的神经元。干预实验提供了因果证据，表明这些神经元是情感决策的必要且充分因素，而非仅仅是相关特征。

3)  
- **验证任务**：在三个标准的语音情感识别基准数据集上进行了评估：IEMOCAP、MELD和MSP-Podcast。
- **取得效果**：
    - **因果验证**：失活干预产生了清晰的“自-交叉”效应结构，即失活针对某情感识别的神经元会显著降低该情感的识别准确率，而对其他情感平均影响很小。
    - **可控性**：定向引导能可靠地将模型预测偏向目标情感，平均可获得约2-3个百分点的准确率提升。
    - **方法有效性**：基于幅度和对比度的选择器显著优于基于频率或熵的方法。
    - **其他发现**：情感敏感神经元在模型层间呈非均匀分布，并展现出部分跨数据集的迁移能力。
</div>

</details>

---

## Towards Fine-Grained and Multi-Granular Contrastive Language-Speech Pre-training
- **Authors**: Yifan Yang, Bing Han, Hui Wang, Wei Wang, Ziyang Ma, Long Zhou, Zengrui Jin, Guanrou Yang, Tianrui Wang, Xu Tan, Xie Chen
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.03065v1](https://arxiv.org/abs/2601.03065v1)
- **PDF**: [https://arxiv.org/pdf/2601.03065v1](https://arxiv.org/pdf/2601.03065v1)

针对语言-语音表示预训练中细粒度说话风格建模的难题，现有语音-文本模型通常依赖粗粒度标注或任务特定监督，且缺乏可扩展的细粒度风格标注数据。本文提出FCaps数据集，该大规模数据集包含细粒度自由文本风格描述，涵盖4.7万小时语音及1900万条通过新型端到端流程标注的细粒度文本描述。该流程直接将详细描述与音频对齐，避免了现有级联流程中基于大语言模型重写导致的误差传播。基于大语言模型的评估表明，我们的标注在准确性、覆盖度和自然度上均优于现有级联标注方法。基于FCaps，我们提出CLSP对比式语言-语音预训练模型，该模型融合全局与细粒度监督，实现了多粒度统一表示学习。大量实验证明，CLSP能够学习细粒度及多粒度的语音-文本表示，在全局/细粒度语音-文本检索、零样本副语言分类及语音风格相似度评分等任务中均表现稳定，且与人类判断高度一致。所有资源将公开发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音风格建模需捕捉超越词汇内容的副语言信息（如音色、情感），但现有方法面临挑战。  
- **既有问题**：  
  - 数据标注依赖**级联流程**（先离散标签再LLM重写），导致错误传播与语义失准。  
  - 现有语音-文本对比预训练模型使用**粗粒度描述**或任务特定监督，难以建模细粒度、多粒度的风格动态。  
  - 缺乏大规模、高质量的细粒度风格标注数据，制约模型表达能力。  

2)  
论文通过构建新数据集与提出预训练模型CLSP，系统解决上述问题：  
- **FCaps数据集创新**：  
  - 规模达**47k小时语音与19M细粒度文本描述**，覆盖丰富风格属性。  
  - 设计**端到端标注流程**：直接基于音频生成描述，避免级联流程的错误传播；结合多模态描述生成与智能体验证，提升标注正确性、覆盖度与自然度。  
- **CLSP模型设计**：  
  - 采用双编码器架构（SPEAR-XLarge语音编码器 + RoBERTa文本编码器），映射到共享嵌入空间。  
  - **两阶段课程学习**：  
    - 第一阶段：使用细粒度描述进行标准对比学习，建立语音-文本对齐。  
    - 第二阶段：引入**多粒度对比监督**，每个语音样本配对全局描述与细粒度描述（或两个不同细粒度描述），通过多正例InfoNCE损失促进跨粒度泛化与细粒度判别。  
  - 动态任务调度器平衡跨粒度学习与细粒度一致性训练，优化表示统一性。  

3)  
CLSP在多项任务中取得显著效果：  
- **语音-文本检索**：在全局与细粒度检索任务上大幅超越基线（如细粒度检索R@1达68.1%），证明其跨粒度对齐能力。  
- **零样本副语言分类**：在情感、性别、年龄识别任务上达到最优准确率（如RAVDESS情感分类UA 46.0%），显示其泛化性。  
- **语音风格相似度评分**：与人类主观评分高度相关（Pearson系数达0.893），验证其作为自动化评估指标的可靠性。
</div>

</details>

---

## Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning
- **Authors**: Yuankun Xie, Xiaoxuan Guo, Jiayi Zhou, Tao Wang, Jian Liu, Ruibo Fu, Xiaopeng Wang, Haonan Cheng, Long Ye
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.02983v1](https://arxiv.org/abs/2601.02983v1)
- **PDF**: [https://arxiv.org/pdf/2601.02983v1](https://arxiv.org/pdf/2601.02983v1)

音频大语言模型（ALLM）的快速发展使得高质量合成音频广泛可及，这增加了语音、环境声、歌声及音乐等恶意音频深度伪造的风险。因此，现实场景中的音频深度伪造检测（ADD）需要能够泛化至异构音频类型并提供可解释判定的通用型检测器。鉴于ALLM强大的多任务泛化能力，我们首先探究了其在监督微调（SFT）与强化微调（RFT）下对通用型ADD的性能表现。然而，仅使用二元真伪标签的SFT易使模型退化为黑盒分类器，丧失可解释性；而传统稀疏监督下的RFT则易出现奖励破解问题，可能产生虚假且无依据的决策依据。为解决这些问题，我们提出一种自动标注与优化流程，构建具有频率-时间结构的思维链（CoT）决策依据，生成约34万条冷启动示范数据。基于CoT数据，我们进一步提出频率-时间分组相对策略优化（FT-GRPO）——一种两阶段训练范式：先通过SFT实现ALLM的冷启动，再在基于规则的频率-时间约束下应用GRPO。实验表明，FT-GRPO在通用型ADD任务中达到最先进性能，同时生成可解释且基于频率-时间结构的决策依据。相关数据与代码已公开。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频大语言模型（ALLM）的进步使得高质量合成音频（如语音、环境音、歌声、音乐）易于生成，增加了恶意音频深度伪造的风险。现实世界的音频深度伪造检测（ADD）需要能泛化到异构音频类型并提供可解释决策的通用检测器。  
- **既有方法问题**：现有研究多局限于单一音频类型，缺乏跨类型泛化能力。基于ALLM的方法中，仅使用二元标签的监督微调（SFT）会将模型简化为“黑盒”分类器，牺牲可解释性；而稀疏监督下的强化微调（RFT）易出现奖励破解，产生无根据的虚假推理。

2)  
论文提出 **FT-GRPO** 方法，通过两阶段训练范式解决上述问题：  
- **数据构建**：设计自动标注与精修流程，为四类公开数据集生成约34万条频率-时间（FT）结构化思维链（CoT）推理示例，作为高质量冷启动数据。  
- **第一阶段（SFT冷启动）**：在带有FT推理标注的数据上进行监督微调，使ALLM学习从频域和时域进行结构化推理的模式。  
- **第二阶段（GRPO优化）**：基于GRPO进行强化学习，使用复合奖励函数优化模型：  
  - **准确性奖励**：鼓励正确分类。  
  - **格式奖励**：确保输出符合预设结构（如包含`<think>`和`<answer>`标签）。  
  - **FT推理路径奖励**：通过规则检查，激励模型在推理中同时包含频域和时域的证据分析。  
- **关键设计**：在GRPO阶段混合使用带有有效推理的“思考”样本和标注不一致的“非思考”样本，提升检测鲁棒性；SFT阶段仅使用高质量推理样本，避免噪声干扰。

3)  
- **任务与效果**：在语音（ASVspoof2019LA）、环境音（ESDD）、歌声（CtrSVDD）和音乐（FakeMusicCaps）四类音频深度伪造检测任务上进行了评估。  
- **性能表现**：  
  - 仅用语音数据训练的3B参数模型在ASVspoof2019LA上达到99.75%准确率（SOTA）。  
  - 跨类型平均准确率较SFT基线提升显著（如语音训练时平均提升5.00%）。  
  - 多类型联合训练下，平均准确率达90.10%，优于此前小模型和ALLM基线方法。  
- **附加优势**：在实现高准确率的同时，模型能生成基于频域和时域证据的可解释推理过程。
</div>

</details>

---

## MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free
- **Authors**: Yishu Lei, Shuwei He, Jing Hu, Dan Zhang, Xianlong Luo, Danxiang Zhu, Shikun Feng, Rui Liu, Jingzhou He, Yu Sun, Hua Wu, Haifeng Wang
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.02967v1](https://arxiv.org/abs/2601.02967v1)
- **PDF**: [https://arxiv.org/pdf/2601.02967v1](https://arxiv.org/pdf/2601.02967v1)

将大型语言模型（LLM）的输入模态扩展至音频领域，是实现全面多模态感知的关键。然而，音频信息本质上是**异构的**，其内部交织着语音、音乐及环境背景等多种属性。现有研究通常采用密集且参数共享的适配器来建模这些多样化的模式，但这在优化过程中会引发**梯度冲突**——不同属性所需的参数更新方向相互矛盾。为克服这一局限，本文提出**MoE-Adapter**，一种稀疏的专家混合（MoE）架构，旨在解耦音频信息。具体而言，该架构采用动态门控机制，将音频令牌路由至捕捉互补特征子空间的专用专家，同时保留共享专家以建模全局上下文，从而缓解梯度冲突，实现细粒度特征学习。综合实验表明，MoE-Adapter在音频语义与副语言任务上均取得优异性能，在计算成本相近的情况下持续优于密集线性基线模型。此外，我们将公开相关代码与模型，以促进后续研究。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：将大语言模型（LLM）扩展至音频领域是实现全面多模态感知的关键。然而，音频信息本质上是异质的，混杂了语音、音乐、环境声音等多种属性。  
- **既有方法的问题**：现有研究主要采用**密集、参数共享的适配器**（如线性投影层或MLP）来建模这些多样化的音频模式。这种设计隐含地假设单一投影能统一处理所有音频类型，但忽略了音频数据内在的分布异质性。  
- **核心问题**：这种“一体式”适配器在优化时会导致**梯度冲突**，因为不同音频属性（如语义语音与副语言线索）所需的参数更新方向相互矛盾，产生破坏性干扰，阻碍模型同时实现对齐与解耦。

2)  
论文提出了 **MoE-Adapter**，一种稀疏的混合专家架构，旨在通过动态路由和专家专业化来解决上述问题。其核心方法包括：  
- **稀疏专家库**：用一组轻量级前馈网络（专家）替代传统的密集适配器。每个专家学习捕获互补的特征子空间。  
- **动态门控路由**：引入可学习的路由器，根据输入音频令牌的声学属性，动态地将其路由到最相关的少数专家（如Top-k选择）。这种机制实现了：  
  - **解耦优化**：将不同音频类型（如语音、音乐、声音）的梯度更新隔离到不同的专家，显著减少梯度冲突。  
  - **保留共享专家**：部分专家用于提取跨模态的全局上下文特征，促进正向知识迁移。  
- **负载均衡损失**：训练时引入辅助损失函数，防止路由器崩溃到少数专家，确保专家利用的多样性和平衡性。  
- **高效推理**：尽管总参数量与基线相当，但每次前向传播仅激活部分专家（论文中约为75%），保持了与密集适配器相近的计算成本。  
该方法通过将异质音频数据映射到由不同专家处理的解耦子空间，使模型能够显式地适应音频数据的异构结构，而非抑制它。

3)  
MoE-Adapter在多个音频理解与推理任务上取得了显著效果提升：  
- **音频知识推理**：在MMSU和OpenBookQA基准上，音频准确率分别提升3.16%和3.75%，表明模型能更有效地捕获以知识为中心的语义信息。  
- **副语言理解**：在涵盖语音、声音、音乐的MMAU基准上，准确率提升1.71%，证明模型能鲁棒地学习多样的声学线索。  
- **模态间隙缩小**：在MMSU上，音频与文本输入的性能差距（模态间隙）缩小了3.16，表明其投影表示与LLM文本嵌入空间的对齐更优。  
总体而言，在固定模型容量下，MoE-Adapter以可比的计算成本，在语义和副语言任务上均一致优于密集基线，实现了更优的性能-效率权衡。
</div>

</details>

---

## The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models
- **Authors**: Yuhuan You, Lai Wei, Xihong Wu, Tianshu Qu
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.02954v1](https://arxiv.org/abs/2601.02954v1)
- **PDF**: [https://arxiv.org/pdf/2601.02954v1](https://arxiv.org/pdf/2601.02954v1)

现有的大型音频-语言模型将世界感知为“单声道”——即忽略通用声学场景分析所需关键空间维度（“方位”）的单一音频流。为弥补这一不足，我们首先提出了一个用于听觉场景分析（ASA）的层次化框架。在此框架指导下，我们开发了一个系统，使Qwen2-Audio等模型能够理解并推理复杂的声学世界。该框架通过三项核心贡献实现这一目标：首先，我们构建了大规模合成双耳音频数据集以提供丰富的空间线索。其次，我们设计了混合特征投影器，通过并行语义编码器与空间编码器提取解耦表征，并采用密集融合机制整合这两个独立信息流，确保模型获得声学场景的整体视图。最后，我们采用渐进式训练策略，从监督微调（SFT）逐步推进至基于群体相对策略优化（GRPO）的强化学习，显式提升模型的推理能力。在综合基准测试中，该模型展现出较强的空间理解能力。通过实现空间感知，本研究为利用大模型强大的推理能力进行整体声学场景分析提供了清晰路径，推动模型从“单声道”语义识别向空间智能演进。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现有大型音频-语言模型（LALMs）主要处理单声道音频，仅关注语义内容（“是什么”），而忽略了声音的空间维度（“在哪里”）。这限制了模型在需要联合理解音频内容与空间布局的实际应用（如机器人、增强现实）中的能力。
- **既有方法的问题**：
  - 主流LALMs将音频视为单声道时间序列，缺乏对方向、距离等空间属性的显式建模。
  - 现有大规模音频数据集（如FSD50K）富含语义标签，但缺乏双耳或3D空间元数据，导致模型无法进行空间推理。
  - 专门的空间音频方法（如SELD）虽能预测声源方向，但与LALMs的开放词汇和对话式风格不匹配，难以处理开放式空间查询。

2)  
论文提出 **“The World is Not Mono (TWNM)”** 框架，通过以下核心方法解决上述问题：

- **构建大规模合成双耳音频数据集**：
  - 利用物理真实的双耳房间脉冲响应（BRIRs）和头相关传输函数（HRTFs）进行程序化模拟，生成包含完整“位置-语义-环境”标注的大规模音频-语言问答对，为模型提供丰富的空间线索。

- **设计混合特征投影器**：
  - **解耦表示**：使用并行编码器——语义编码器（基于Whisper）提取内容特征，空间编码器（专为双耳信号设计）提取位置特征，确保语义和空间信息独立处理。
  - **专家路径**：通过多个并行前馈网络（专家路径）显式建模不同空间属性（如方向、距离、混响、声源数量），强化对关键物理线索的保留。
  - **密集融合机制**：将所有专家路径的输出拼接后，通过融合MLP投影为统一的音频嵌入，确保语言模型同时接收完整的语义和空间信息，促进跨模态依赖学习。

- **采用渐进式训练课程**：
  - **阶段1（编码器预训练）**：仅训练空间编码器，学习鲁棒的空间声学表示。
  - **阶段2（投影器对齐）**：冻结编码器和LLM，仅训练混合特征投影器，初步对齐音频表示与LLM嵌入空间。
  - **阶段3（SFT 1.0）**：冻结编码器，联合训练投影器和LLM的LoRA权重，缩小模态差距。
  - **阶段4（SFT 2.0）**：引入思维链样本，继续联合训练，促进推理策略与输出格式的共同演化。
  - **阶段5（GRPO偏好优化）**：仅使用GRPO更新LoRA权重，通过基于规则的奖励函数（评估格式合规性和答案正确性）优化模型在多项选择空间问答任务中的输出准确性。

该方法通过**数据合成、解耦表示与密集融合、渐进式对齐**，系统地将空间感知能力注入预训练LALM，使其从“单声道”语义识别迈向空间智能。

3)  
- **任务与效果**：在作者构建的全面空间推理基准测试（包含1000个多项选择题）上评估模型，该测试对应听觉场景分析（ASA）的三个层级：
  - **L1感知（静态识别）**：如声源计数、语义识别、绝对定位，最终准确率达61.05%。
  - **L2集成（关系整合）**：如属性绑定（将特定声音事件与位置关联），最终准确率达57.75%。
  - **L3推理（认知推理）**：如反事实推理、场景上下文总结，最终准确率达79.60%，其中场景总结任务准确率高达97.22%。
- **整体表现**：最终模型（GRPO阶段）整体准确率为68.69%，显著优于基线模型BAT（32.50%），验证了所提框架在赋予LALMs分层空间智能方面的有效性。
</div>

</details>

---

## XLSR-MamBo: Scaling the Hybrid Mamba-Attention Backbone for Audio Deepfake Detection
- **Authors**: Kwok-Ho Ng, Tingting Song, Yongdong WU, Zhihua Xia
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.02944v1](https://arxiv.org/abs/2601.02944v1)
- **PDF**: [https://arxiv.org/pdf/2601.02944v1](https://arxiv.org/pdf/2601.02944v1)

先进的语音合成技术已能生成高度逼真的语音，这带来了安全风险，推动了音频深度伪造检测（ADD）的研究。虽然状态空间模型（SSM）具有线性复杂度，但纯因果SSM架构往往难以满足基于内容的检索需求，而该需求对于捕捉全局频域伪影至关重要。为此，我们通过提出XLSR-MamBo这一模块化框架，探索了混合架构的扩展特性。该框架将XLSR前端与协同的Mamba-Attention骨干网络相结合。我们系统评估了四种拓扑设计，采用先进的SSM变体：Mamba、Mamba2、Hydra和Gated DeltaNet。实验结果表明，在ASVspoof 2021 LA、DF和In-the-Wild基准测试中，MamBo-3-Hydra-N3配置相比其他先进系统取得了具有竞争力的性能。这一性能得益于Hydra固有的双向建模能力，其能够比先前工作中采用的启发式双分支策略更高效地捕捉整体时间依赖性。此外，在DFADD数据集上的评估表明，该模型对未见过的基于扩散和流匹配的合成方法具有鲁棒的泛化能力。关键的是，我们的分析表明，增加骨干网络深度能有效缓解浅层模型中观察到的性能波动与不稳定性。这些结果证明了混合框架在捕捉伪造语音信号伪影方面的能力，为ADD提供了一种有效方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：生成式AI的进步使得语音合成高度逼真，带来了安全风险，推动了音频深度伪造检测（ADD）的研究。  
- **既有方法问题**：  
  - 传统方法依赖手工特征，泛化能力不足。  
  - 主流端到端模型（如基于Transformer的XLSR-Conformer）虽有效，但二次计算复杂度高，处理长序列效率低。  
  - 纯因果状态空间模型（SSM，如Mamba）具有线性复杂度，但单向建模难以捕获全局频域伪造痕迹，需依赖启发式双向策略（如双分支融合）来近似非因果建模，存在结构冗余和效率限制。

2)  
论文提出**XLSR-MamBo**框架，通过**混合SSM-注意力架构**解决上述问题：  
- **前端**：采用预训练的XLSR模型提取高层语音表示，提供强大的跨语言特征基础。  
- **后端**：设计四种模块化拓扑结构（MamBo-1至MamBo-4），系统探索SSM变体（Mamba、Mamba2、Hydra、Gated DeltaNet）与注意力机制的协同组合：  
  - **MamBo-1**：用SSM块替换注意力，评估纯SSM效果。  
  - **MamBo-2**：在SSM块后紧接注意力，实现块内紧凑混合。  
  - **MamBo-3**：交替堆叠SSM层和Transformer层，分离时序压缩与全局精炼。  
  - **MamBo-4**：交替SSM层与Mamer层（SSM+注意力），增强交互密度。  
- **关键创新**：  
  - 引入**Hydra变体**，其基于拟可分矩阵的序列混合器支持**原生双向处理**，以线性复杂度捕获全局非因果依赖，无需启发式双向策略，减少了结构冗余。  
  - 引入堆叠超参数**N**，系统评估SSM模块深度缩放的影响，发现增加深度可缓解浅层模型的性能方差和不稳定性，提升泛化鲁棒性。  
- **优势**：结合SSM的高效时序压缩与注意力的精确全局检索，能同时捕捉局部高频伪影和全局频谱不一致性，更全面覆盖伪造痕迹。

3)  
在多个音频深度伪造检测任务上取得竞争性效果：  
- **ASVspoof 2021 LA**：最佳配置（MamBo-3-Hydra-N3）达到0.81% EER，优于XLSR-Mamba（0.93%）等基线。  
- **ASVspoof 2021 DF**：同一配置取得1.70% EER，表现稳健。  
- **In-the-Wild（ITW）**：取得4.97% EER，相对Fake-Mamba提升15.04%。  
- **DFADD数据集**：对未见过的扩散和流匹配合成方法展现出强泛化能力，尤其在F1/F2子集上，通过深度缩放显著降低EER（如Mamba2在F1上达3.02%）。  
- **总体**：模型在参数效率相近下，性能媲美或超越现有SOTA系统，验证了混合架构的有效性。
</div>

</details>

---

## Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis
- **Authors**: Mengze Hong, Di Jiang, Zeying Xie, Weiwei Zhao, Guan Wang, Chen Jason Zhang
- **Categories**: cs.SD, cs.CR
- **arXiv**: [https://arxiv.org/abs/2601.02914v1](https://arxiv.org/abs/2601.02914v1)
- **PDF**: [https://arxiv.org/pdf/2601.02914v1](https://arxiv.org/pdf/2601.02914v1)

随着音频深度伪造技术从研究工具演变为广泛可用的商业产品，高安全需求行业中的生物特征认证系统正面临严峻的安全威胁。本文基于大规模语音合成数据集，对当前先进的说话人认证系统进行了系统性实证评估，揭示出两大安全漏洞：1）基于极少量样本训练的现代语音克隆模型可轻易绕过商用说话人验证系统；2）反欺骗检测器难以泛化至不同类型的音频合成方法，导致其在特定数据集上的表现与实际场景中的鲁棒性存在显著差距。这些发现要求我们重新审视现有安全措施，并强调需要通过架构创新、自适应防御机制以及向多因素认证体系转型来应对挑战。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于声纹的生物认证在金融、远程身份验证等高安全领域广泛应用，市场快速增长。然而，音频深度伪造技术已从实验室研究演变为现实安全威胁，导致重大社会与经济损失。
- **既有方法问题**：
  - **声纹验证系统**：易被现代语音克隆模型以极少量目标语音样本（如几分钟）轻易绕过。
  - **反伪造检测器**：在训练时未见的合成方法上泛化能力差，导致实际部署中的鲁棒性显著低于域内性能。

2)  
- **核心方法**：本文通过构建大规模语音合成基准数据集，系统评估了当前最先进的声纹认证与反伪造检测系统在深度伪造攻击下的脆弱性。
- **具体解决路径**：
  - **构建评估框架**：
    - 使用50名普通话说话者的真实语音，训练三种开源语音合成系统（GPT-SoVITS、Bert-VITS2、RVC）生成合成语音。
    - 采用ECAPA-TDNN作为声纹验证模型，并组合XLS-R（自监督语音表示）与AASIST（基于图注意力的伪造检测器）作为深度伪造检测模型。
  - **揭示漏洞**：
    - 验证声纹系统：测试显示，合成语音的绕过率高达43.1%–82.7%，平均余弦相似度接近真实语音范围，表明仅凭声纹认证防御不足。
    - 评估检测器泛化：在域内测试中，XLS-R+AASIST的EER低至0.83%，但在未见合成模型（如扩散模型）的域外测试中，EER骤升至24.84%，暴露其过度依赖训练所见模式、无法捕捉通用合成特征的根本缺陷。
  - **深入分析**：
    - 通过跨语言（中→英）和含噪声环境测试，进一步量化了模型在真实复杂场景中的性能下降，并指出数据增强（如RawBoost）可部分缓解噪声敏感性问题。
- **贡献**：首次系统揭示了现有认证体系的两大安全漏洞，并强调需从架构创新、自适应防御及多因素认证等方面根本性提升安全性。

3)  
- **评估任务与效果**：
  - **声纹验证任务**：在对抗语音克隆攻击时，ECAPA-TDNN模型对三种合成语音的绕过率显著（43.1%–82.7%），显示其防护极易被突破。
  - **深度伪造检测任务**：
    - **域内检测**：XLS-R+AASIST在相同合成模型测试集上达到0.83% EER，表现优异。
    - **域外泛化**：在未见合成模型测试中，EER恶化至24.84%，性能下降约30倍，揭示严重泛化缺陷。
    - **跨语言检测**：在英文ASVspoof 2021数据集上，EER升至3.48%–4.59%，显示一定跨语言迁移能力但仍有差距。
    - **噪声鲁棒性**：在10dB信噪比噪声下，EER从0.83%升至16.24%；采用RawBoost数据增强后，噪声环境下EER改善至2.55%。
</div>

</details>

---

## SPO-CLAPScore: Enhancing CLAP-based alignment prediction system with Standardize Preference Optimization, for the first XACLE Challenge
- **Authors**: Taisei Takano, Ryoya Yoshida
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.02900v1](https://arxiv.org/abs/2601.02900v1)
- **PDF**: [https://arxiv.org/pdf/2601.02900v1](https://arxiv.org/pdf/2601.02900v1)

首届XACLE挑战赛（跨模态音频对齐挑战）致力于开发与人类音频-文本语义对齐感知高度相关的自动评估指标。本文介绍了我们提交至该挑战赛的"Takano_UTokyo_03"系统。该方法基于CLAPScore架构，并创新性地融合了标准化偏好优化（SPO）训练策略。SPO通过对每位听评者提供的原始对齐分数进行标准化处理，使模型能够学习相对偏好关系，有效降低个体评分偏差的影响。同时，我们采用听评者筛选机制排除评分一致性较低的参与者。实验评估表明，SPO与听评者筛选策略均能显著提升系统评分与人类判断的相关性。本系统最终以0.6142的斯皮尔曼等级相关系数（SRCC）位列挑战赛第六名，与顶尖系统的性能差距微小，展现出强劲竞争力。相关代码已开源：https://github.com/ttakano398/SPO-CLAPScore。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**  
- **背景**：文本到音频（TTA）生成模型需要评估音频与文本的语义对齐程度。目前，主观人工评估是黄金标准，但成本高昂。  
- **既有方法问题**：  
  - 现有客观评估指标（如CLAPScore）与人类主观评估的相关性较低。  
  - 缺乏统一的平台来系统比较不同评估方法。  
  - 人工标注数据存在个体评分偏差和不一致性问题，影响模型学习效果。

2)  
**论文核心方法如何解决上述问题**  
本文提出 **SPO-CLAPScore** 系统，通过以下方法提升对齐预测的准确性与鲁棒性：  

- **基础架构**：  
  - 基于CLAPScore框架，计算音频与文本CLAP嵌入的余弦相似度作为对齐分数。  
  - 采用M2D-CLAP 2025作为音频编码器，BERT作为文本编码器，并冻结文本编码器以微调音频编码器。  

- **数据预处理**：  
  - **听者筛选**：通过算法剔除评分不一致的听者。具体地，若某个听者的评分与其他听者评分差异过大（超出阈值区间），且其“异常评分”比例超过阈值，则排除该听者数据，以减少噪声影响。  
  - **标准化偏好优化（SPO）**：  
    - 针对每个听者，将其原始评分转换为标准偏好分数：  
      \[ x_{spo} = \frac{x - \mu_{\text{listener}}}{\sigma_{\text{listener}}} \]  
    - 这一转换消除个体评分偏差（如极端评分或保守评分），使模型专注于学习相对偏好模式。  

- **损失函数**：  
  - 结合回归损失（MSE）和对比损失，使用SPO转换后的标准偏好分数作为训练目标，并对预测分数进行全局标准化，以优化模型。  

- **模型集成**：  
  - 集成在不同条件下训练的模型（如是否使用听者筛选、是否使用对比损失、是否进行预热训练），以增强预测的鲁棒性和泛化能力。  

通过上述方法，SPO-CLAPScore有效缓解了听者个体偏差和数据噪声问题，使模型更贴近人类主观判断。

3)  
**在哪些任务上取得了怎样的效果**  
- **任务**：XACLE挑战赛中的音频-文本语义对齐分数预测任务。  
- **效果**：  
  - 在测试集上，SPO-CLAPScore的斯皮尔曼等级相关系数（SRCC）达到0.6142，较基线模型提升超过0.27。  
  - 在线性相关系数（LCC）、肯德尔等级相关系数（KTAU）和均方误差（MSE）上均优于基线。  
  - 在挑战赛中排名第6，与前列系统性能差距微小（与第5名SRCC仅差0.0001），且显著领先后续系统。  
  - 消融实验验证了SPO和听者筛选的有效性，能显著提升各项评估指标。
</div>

</details>

---

## UniSRCodec: Unified and Low-Bitrate Single Codebook Codec with Sub-Band Reconstruction
- **Authors**: Zhisheng Zhang, Xiang Li, Yixuan Zhou, Jing Peng, Shengbo Cai, Guoyang Zeng, Zhiyong Wu
- **Categories**: cs.SD, cs.AI, cs.MM
- **arXiv**: [https://arxiv.org/abs/2601.02776v1](https://arxiv.org/abs/2601.02776v1)
- **PDF**: [https://arxiv.org/pdf/2601.02776v1](https://arxiv.org/pdf/2601.02776v1)

神经音频编解码器（NACs）通过紧凑的压缩与重建来降低传输开销，同时旨在弥合连续信号与离散信号之间的鸿沟。现有NACs可分为多码本与单码本两类：多码本编解码器面临结构复杂、难以适配下游任务等挑战；而单码本编解码器虽结构简单，却存在保真度低、统一音频建模效果不佳、无法支持高频音频建模等问题。本文提出UniSRCodec——一种能够支持高采样率、低带宽、高保真且具备统一建模能力的单码本编解码器。我们分析了基于波形的压缩方法的局限性，引入了基于梅尔频谱的时频压缩方法，并配合声码器恢复原始音频的相位信息。此外，我们提出子带重建技术，以实现在低频与高频段均保持高质量的压缩效果。主客观实验结果表明，UniSRCodec在仅需40令牌率的条件下，即可在跨领域单码本编解码器中达到最优性能，其重建质量可与部分多码本方法相媲美。演示页面详见：https://wxzyd123.github.io/unisrcodec。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经音频编解码器（NACs）旨在压缩音频为离散令牌以降低传输开销，并连接连续与离散信号。现有方法主要分为多码本和单码本两类。
- **既有问题**：
  - 多码本编解码器结构复杂，难以适配下游任务（如音频语言模型）。
  - 单码本编解码器虽结构简单，但存在保真度低、对通用音频建模能力不足、无法有效支持高频音频建模（通常限于低采样率如24kHz）等问题。

2)  
论文提出 **UniSRCodec**，一个支持高采样率、低码率、高保真且统一的单码本编解码器，通过以下核心方法解决上述问题：
- **时频联合压缩**：
  - 采用梅尔频谱图作为压缩表示，而非原始波形。梅尔频谱图丢弃相位信息，但通过预训练神经声码器（BigVGAN-v2）在重建时恢复相位，从而更高效地利用带宽。
  - 梅尔频谱图为二维结构，压缩时同时在时间和频率维度下采样，实现二次压缩效率（例如压缩n倍相当于n²的整体压缩比），支持极低令牌率（如40 TPS）。
- **子带重建技术**：
  - 将梅尔频谱图沿频率轴分为低频和高频两半，分别计算L1重建损失，并通过加权（低频权重更高）强调低频细粒度信息的学习，提升全频带建模质量。
- **单码本量化与架构设计**：
  - 使用SimVQ作为单码本量化器，保持结构简单以利于下游任务集成。
  - 编码器-解码器基于全卷积2D架构（借鉴Open-MagViT2），通过残差块进行多级下采样和上采样，实现高压缩比。
  - 采用帧级扁平化策略（将每时间帧内的频率信息拼接），保持训练与推理的一致性，优于频带级扁平化。
- **对抗训练与损失函数**：
  - 引入频谱判别器（基于DAC架构）提供对抗损失和特征匹配损失，防止重建频谱过度平滑，增强细节。
  - 整体损失函数结合子带重建损失、判别器损失、对抗损失、特征匹配损失和码本承诺损失，优化重建质量。

3)  
- **任务与效果**：
  - **跨领域音频重建**：在语音、音乐和通用音频（如AudioSet）测试集上，UniSRCodec在单码本编解码器中达到SOTA性能。
  - **客观指标**：在40 TPS/0.52 kbps极低码率下，于音乐和通用音频域的梅尔距离、STFT距离（高低频）指标上优于同类单码本方法（如UniCodec、WavTokenizer）；较高码率变体（176 TPS）甚至超越部分多码本方法（如SNAC）。
  - **主观评估**：MUSHRA听力测试显示，UniSRCodec在音频和音乐领域得分显著高于UniCodec，在语音领域表现相当。
  - **下游理解任务**：在音频分类（UrbanSound-8k、ESC-50）、音乐流派分类（GTZAN）和情感识别（CREMA-D）任务中，其离散表示性能优于WavTokenizer，接近连续表示模型WavLM。
</div>

</details>

---

## Vclip: Face-based Speaker Generation by Face-voice Association Learning
- **Authors**: Yao Shi, Yunfei Xu, Hongbin Suo, Yulong Wan, Haifeng Liu
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.02753v1](https://arxiv.org/abs/2601.02753v1)
- **PDF**: [https://arxiv.org/pdf/2601.02753v1](https://arxiv.org/pdf/2601.02753v1)

本文探讨基于面部特征的语音合成任务，这是一种个性化语音合成方法，要求合成语音在感知上与参考人脸图像相匹配。由于缺乏符合文本到语音（TTS）质量要求的视听语料库，现有方法常面临合成质量低下或知识迁移方案导致的领域失配问题。本文提出一种名为Vclip的新方法，该方法利用CLIP编码器在噪声视听数据上学习的面部语义知识，高效建立人脸与语音的关联关系，在Voxceleb测试集上实现了89.63%的跨模态验证AUC分数。所提方法进一步采用基于检索的策略，结合基于高斯混合模型（GMM）的说话人生成模块，为下游TTS系统生成与参考图像匹配的目标说话人。实验结果表明，结合检索步骤的Vclip系统能够有效弥合人脸与语音特征之间的鸿沟，实现基于面部的语音合成。通过提取下游TTS系统的反馈信息，可合成与参考人脸高度匹配的语音。演示效果详见sos1sos2sixteen.github.io/vclip。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于人脸的语言合成任务旨在根据参考人脸图像生成与之感知匹配的语音。该任务面临的核心挑战是缺乏高质量的视听训练数据。
- **既有方法的问题**：
  - 现有高质量TTS数据集与大规模但嘈杂的视听数据集（如LRS3）之间存在**领域不匹配**。
  - 先前方法多采用**特征映射**方案，将人脸特征直接映射为说话人嵌入，但这种方式存在两个领域间隙，且上游映射模块与下游TTS模块之间缺乏协调，可能导致合成质量下降。
  - 仅从单张人脸图像确定性地预测真实说话人嵌入本质上是困难的，因为人脸信息不足以唯一确定声音。

2)  
论文提出名为Vclip的两阶段方法，以条件说话人生成的形式解决上述问题。

- **第一阶段：Vclip跨模态关联学习**
  - **方法设计**：受CLIP启发，构建一个对比学习框架。使用**预训练的CLIP图像编码器**作为人脸编码器，以利用其丰富的语义知识；使用**预训练的说话人验证网络**作为语音编码器。两者后接投影网络，通过CLIP对比损失在大量嘈杂的视听数据上学习人脸与语音的关联。
  - **解决思路**：此阶段旨在高效地从易得的在线视频数据中学习人脸与语音之间的关联知识，避免了直接依赖高质量配对数据的限制。

- **第二阶段：基于检索的说话人生成**
  - **核心策略**：不直接使用Vclip提取的人脸特征作为说话人嵌入，而是采用“生成-检索”策略来规避模态间隙。
  - **具体步骤**：
    1.  使用高斯混合模型（GMM）对已知高质量TTS说话人嵌入的分布进行建模。
    2.  从该分布中采样生成大量候选说话人嵌入。
    3.  利用训练好的Vclip作为零样本评分函数，根据参考人脸图像对候选嵌入进行评分。
    4.  选择得分最高的Top-K个候选作为最终生成的说话人，输入下游零样本TTS系统进行合成。
  - **关键创新**：
    - **TTS签名感知评分**：进一步提出一种改进的评分函数，它通过一个前馈网络**提炼下游TTS系统的“签名”信息**（即输入说话人嵌入与合成语音提取的说话人特征之间的映射关系）。该评分函数在检索时考虑了特定TTS系统固有的克隆误差，从而生成与参考人脸匹配度更高的语音。

3)  
- **跨模态验证任务**：在Voxceleb测试集上，Vclip模型取得了**89.63%的AUC分数**，超越了Self-Lifting等先进基线，证明了其高效学习人脸-语音关联的能力。
- **基于人脸的说话人生成任务**：
  - **自动评估**：与直接使用映射特征的基线相比，本文的生成-检索策略（尤其是结合TTS签名感知评分后）在**人脸-语音匹配度（f2v）** 指标上达到或超过了真实配对数据的参考水平，且生成的说话人嵌入在分布上与真实说话人更接近。
  - **主观评估**：合成语音的**自然度（MOS）** 与标准零样本语音克隆相当，显著优于直接在低质量数据上训练的模型。在**人脸-语音匹配度（m-MOS）** 上，本方法的结果与使用真实语音的克隆基线相当，且显著高于随机配对，表明生成的语音在感知上与参考人脸有意义的匹配。
</div>

</details>

---

## Omni2Sound: Towards Unified Video-Text-to-Audio Generation
- **Authors**: Yusheng Dai, Zehua Chen, Yuxuan Jiang, Baolong Gao, Qiuhong Ke, Jun Zhu, Jianfei Cai
- **Categories**: cs.SD, cs.CV, cs.MM
- **arXiv**: [https://arxiv.org/abs/2601.02731v1](https://arxiv.org/abs/2601.02731v1)
- **PDF**: [https://arxiv.org/pdf/2601.02731v1](https://arxiv.org/pdf/2601.02731v1)

本文提出了一种集成视频到音频（V2A）、文本到音频（T2A）以及视频-文本联合到音频（VT2A）生成的统一模型训练方法，该方法在应用上具有显著灵活性，但面临两个尚未解决的基础性挑战：（1）缺乏音频-视频-文本（A-V-T）严格对齐的高质量音频描述数据，导致多模态条件间存在严重的语义冲突；（2）跨任务与任务内竞争，表现为V2A与T2A性能间的负面权衡以及VT2A任务中的模态偏差。首先，针对数据稀缺问题，我们提出了SoundAtlas——一个大规模数据集（47万对样本），其质量显著超越现有基准甚至人类专家水平。通过一种新型智能流程，该数据集集成了视觉到语言压缩以减轻多模态大模型的视觉偏差，采用初级-高级智能体交接机制将成本降低至五分之一，并实施严格的后验过滤以保证数据保真度。因此，SoundAtlas能够提供语义丰富、时序细节精确且具有严格V-A-T对齐的描述。其次，我们提出了Omni2Sound——一个支持灵活输入模态的统一VT2A扩散模型。为解决固有的跨任务与任务内竞争问题，我们设计了一种三阶段多任务渐进训练策略，将跨任务竞争转化为联合优化，并缓解VT2A任务中的模态偏差，同时保持音频-视觉对齐与画外音频生成的忠实度。最后，我们构建了VGGSound-Omni作为统一评估的综合基准，包含具有挑战性的画外音频评估轨道。基于标准DiT主干网络，Omni2Sound在单一模型中实现了所有三项任务的统一最优性能，并在异构输入条件的基准测试中展现出强大的泛化能力。项目页面详见：https://swapforward.github.io/Omni2Sound。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：视频-文本-音频（VT2A）统一生成模型具有应用灵活性，但面临两大挑战。  
- **既有问题**：  
  - **数据稀缺**：缺乏高质量、视觉-音频-文本（V-A-T）严格对齐的音频描述数据，导致多模态条件间语义冲突。  
  - **任务竞争**：包括跨任务竞争（V2A与T2A性能此消彼长）和任务内竞争（VT2A生成中的模态偏差，如偏文本导致音画不同步，偏视频导致画外音频不忠实）。

2)  
论文通过构建高质量数据集和设计渐进式训练策略解决上述问题。  

- **解决数据稀缺**：  
  - 提出**SoundAtlas数据集**（47万对），通过智能体标注流程生成V-A-T严格对齐的音频描述。  
  - 流程包括：视觉到语言压缩（减少视觉偏差）、初级-高级智能体交接（降低成本）、事后过滤（保证保真度），其质量甚至超越人类标注。  

- **解决任务竞争**：  
  - 提出**Omni2Sound统一扩散模型**，采用三阶段渐进式多任务训练：  
    1. **大规模T2A预训练**：建立稳健的生成先验。  
    2. **多任务交错训练**：引入高质量VT2A数据作为桥梁，将V2A与T2A的跨任务竞争转化为联合优化，缓解资源争夺。  
    3. **解耦的鲁棒性训练**：通过**文本丢弃**增强对视频的依赖以改善音画同步，通过**画外合成**增强对文本的依赖以提高画外音频的忠实度，从而平衡模态偏差。

3)  
- **任务与效果**：Omni2Sound在**T2A、V2A、VT2A**三个任务上均取得了最先进的统一性能。  
- **具体表现**：在VGGSound-Omni等基准测试中，其在音频质量、音画同步和语义对齐等客观指标上全面领先；在画外音频生成等挑战性场景中亦表现出色，验证了其高保真生成与强泛化能力。
</div>

</details>

---

## Multi-channel multi-speaker transformer for speech recognition
- **Authors**: Guo Yifan, Tian Yao, Suo Hongbin, Wan Yulong
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.02688v1](https://arxiv.org/abs/2601.02688v1)
- **PDF**: [https://arxiv.org/pdf/2601.02688v1](https://arxiv.org/pdf/2601.02688v1)

随着远程会议和车载语音助手的发展，远场多说话人语音识别已成为研究热点。近期提出的多通道Transformer（MCT）展现了Transformer建模远场声学环境的能力，但由于说话人间的相互干扰，MCT难以从混合音频中为每个说话人编码高维声学特征。基于此，本文提出用于远场多说话人语音识别的多通道多说话人Transformer（M2Former）。在SMS-WSJ基准测试上的实验表明，M2Former在相对词错误率降低指标上，分别优于神经波束成形器、MCT、基于变换-平均-拼接的双路径RNN以及基于多通道深度聚类的端到端系统9.2%、14.3%、24.9%和52.2%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：随着远程会议和车载语音助手的普及，远场多说话人语音识别成为研究热点。当前主流方法基于置换不变训练，通常采用“分离前端 + 识别后端”的串行架构。
- **既有方法的问题**：
  - 分离前端与识别后端的功能设计不一致，限制了系统整体性能。
  - 多通道变换器虽能建模远场声学环境，但在多说话人场景下，说话人间的相互干扰使其难以从混合音频中为每个说话人生成高质量的高维声学嵌入。

2)  
论文提出的**多通道多说话人变换器**通过以下核心方法解决上述问题：
- **CNN解耦与下采样层**：
  - 使用二维CNN对多通道输入特征进行解耦，增加通道数以获得更具判别性的表示。其不同滤波器通道可聚焦于来自不同方向的信号，使得每个输出通道有较高概率仅包含一个说话人（或噪声）的信息。
  - 同时在频域和时域进行下采样，以提高计算效率。
- **多通道多说话人注意力模块**：
  - **通道内注意力**：在每个通道内使用自注意力，学习通道内的上下文连续性信息，方式与基于变换器的单通道ASR编码器类似。
  - **改进的跨通道注意力**：提出M2A跨通道注意力机制。其核心在于**仅利用高相似性通道间的跨通道信息**，而非像MCT那样使用所有通道。通过计算通道间相似度矩阵，并基于相似度进行通道组合，从而缓解不同声源信号间的相互干扰。
- **聚类与过滤层**：
  - 利用最近M2A块生成的通道间相似度矩阵进行谱聚类，为每个通道分配标签（对应不同声源）。
  - 计算每个通道的帧间相似度差异值，保留IFSD值最高的n个聚类（n为说话人数），丢弃其余聚类（对应噪声）。该方法无需训练额外的线性层，易于扩展到说话人数量更多或未知的场景。
- **整体编码流程**：输入特征先经CNN解耦，再通过M2A块编码上下文关系（可在聚类前、后使用），最后经CF层分离出各说话人对应的特征并过滤噪声，为每个说话人生成单通道声学嵌入，供解码器识别。

3)  
- **任务**：在远场多说话人语音识别任务上进行评估，使用SMS-WSJ基准数据集（含2、3、4个同时说话人场景）。
- **效果**：
  - 在SMS-2数据集上，M2Former的词错误率相对降低优于多个基线系统：相比神经波束成形、MCT、DPRNN-TAC和多通道深度聚类系统，分别提升9.2%、14.3%、24.9%和52.2%。
  - 消融实验验证了各模块的有效性，其中CNN解耦下采样层贡献最大。
  - 在说话人数量未知或更多（SMS-3, SMS-4）的场景下，模型仍能有效工作，展现了良好的泛化能力。
</div>

</details>

---
