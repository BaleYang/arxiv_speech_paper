---
layout: post
title: "arXiv Daily – 2025-10-24"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-24（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-23 08:50 — 2025-10-24 08:50
- 抓取总数：11 篇 | 本页显示：11 篇（去重/过滤后）

## Controllable Embedding Transformation for Mood-Guided Music Retrieval
- **Authors**: Julia Wilkins, Jaehun Kim, Matthew E. P. Davies, Juan Pablo Bello, Matthew C. McCallum
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.20759v1](http://arxiv.org/abs/2510.20759v1)
- **PDF**: [http://arxiv.org/pdf/2510.20759v1](http://arxiv.org/pdf/2510.20759v1)

音乐表征是现代推荐系统的核心支撑，广泛应用于播放列表生成、相似性检索与个性化推荐场景。然而现有嵌入表示大多难以实现单一音乐属性的精准调控，例如在保持流派与配器不变的前提下仅调整曲目的情绪特征。本研究针对基于嵌入变换的可控音乐检索问题，提出一种情绪引导的音乐嵌入转换框架。该框架通过学习种子音频嵌入到目标嵌入的映射关系，在保留其他音乐属性的同时实现情绪标签引导的定向转换。鉴于无法直接修改原始音频的情绪属性，我们引入代理目标采样机制，在保持种子相似性的前提下平衡检索结果的多样性。基于该采样策略训练轻量级翻译模型时，我们提出新型联合优化目标，同步保障属性转换与信息保留能力。在两个数据集上的实验表明，本方法在实现高效情绪转换的同时，其流派与配器保留能力显著优于无训练基线，证实了可控嵌入转换在个性化音乐检索领域的应用潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代音乐推荐系统依赖嵌入表示，但现有方法难以调整单一属性（如情绪），同时保持其他特征（如流派、乐器）不变。  
- **既有问题**：  
  - 生成式风格迁移方法计算成本高，难以应用于大规模检索场景。  
  - 基于解耦的表示学习方法无法直接操控输入嵌入的特定属性。  
  - 现有嵌入空间缺乏对抽象属性（如情绪）的细粒度控制机制。  

2)  
- **核心框架**：提出基于情绪引导的嵌入变换模型，通过轻量级多层感知机（MLP）将种子嵌入映射至目标情绪对应的嵌入，保留其他属性。  
- **关键设计**：  
  - **邻居采样机制**：从嵌入空间中为每种目标情绪选取与种子相似但情绪不同的代理目标，解决无法直接修改音频信号的问题。  
  - **联合损失函数**：  
    - **余弦相似度损失**：使变换后嵌入接近目标嵌入。  
    - **三元组损失**：拉近变换嵌入与目标嵌入的距离，推远其与种子嵌入的距离。  
    - **余弦BCE损失**：根据种子与目标情绪是否匹配，动态调整相似度约束（匹配时强制恒等映射，不匹配时允许自由度）。  
- **训练优化**：通过加权组合损失函数，平衡情绪转换与属性保留，并在大规模数据集上验证其有效性。  

3)  
- **任务与效果**：  
  - **情绪转换任务**：在大型专有数据集和MTG-Jamendo上，情绪转换精度（Mood P@1）分别达0.96和0.83，显著优于随机基线（0.25）。  
  - **属性保留任务**：流派保留（Genre P@1）分别达0.32和0.29，乐器一致性（Inst. J@1）在MTG-Jamendo上达0.45，均远超无需训练的基线方法。  
  - **泛化能力**：零样本迁移至新数据集仍保持较强性能，证明方法的鲁棒性。
</div>

</details>

---

## R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice Conversion
- **Authors**: Junjie Zheng, Gongyu Chen, Chaofan Ding, Zihao Chen
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.20677v1](http://arxiv.org/abs/2510.20677v1)
- **PDF**: [http://arxiv.org/pdf/2510.20677v1](http://arxiv.org/pdf/2510.20677v1)

在实际歌唱声音转换应用中，环境噪声与对表现力输出的需求构成双重挑战。传统方法因通常依赖纯净数据进行训练与推理，未能充分考虑真实部署场景，这种不匹配严重制约了实际应用。为解决这些问题，我们提出R2-SVC——一个兼具鲁棒性与表现力的SVC框架。首先，通过随机基频扰动和音乐分离伪影模拟（如混响、回声）实现基于仿真的鲁棒性增强，显著提升噪声环境下的性能。其次，利用领域特异性歌唱数据丰富说话人表征：除纯净人声外，融合DNSMOS滤波的分离人声与公开歌唱语料，使模型在保持音色特质的同时捕捉演唱风格细节。第三，集成神经源滤波模型以显式表征谐波与噪声分量，增强转换歌声的自然度与可控性。实验表明，R2-SVC在纯净与噪声场景下的多项SVC基准测试中均达到最先进水平。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：真实场景的歌声转换面临环境噪声和表达性输出的挑战。  
- **既有方法问题**：  
  - 传统方法依赖干净数据训练和推理，未考虑实际部署中的噪声和音乐分离伪影。  
  - 对抗学习和数据增强泛化能力不足，ASR-based方法对基频抖动、混响等处理效果差，导致自然度下降。  

2)  
- **模拟增强鲁棒性**：  
  - 随机基频扰动（如抖动、滑音、跳跃）模拟真实F0提取误差，降低模型对F0的依赖。  
  - 湿声模拟（和声、混响、回声）模拟音乐分离伪影，训练模型从含噪输入生成干净音频。  
- **歌声增强特征提取**：  
  - 扩展CAM++框架，融合干净歌声、DNSMOS滤波分离音频及公共歌唱数据。  
  - 捕获跨域音色一致性及歌手特定风格（如颤音、咬字），提升表达性和噪声适应性。  
- **神经源滤波集成**：  
  - 通过谐波加噪声的NSF模型显式分解激励源与频谱滤波，增强自然度和可控性。  
  - 结合内容特征优化谐波生成，改善复杂场景下的合成质量。  

3)  
- **任务与效果**：  
  - 在**SVC-Normal**和**SVC-Hard**测试集上均达到最优或可比性能。  
  - 客观指标：SPK-SIM提升至75.17%（Normal）和76.26%（Hard），CER与基线竞争，CE/CU/PQ多项领先。  
  - 主观评价：自然度（NMOS）和音色相似度（SMOS）显著优于FreeSVC，细节重建能力更强。  
  - 在含噪、混响等真实工业场景中，鲁棒性和表达性平衡最佳。
</div>

</details>

---

## Resounding Acoustic Fields with Reciprocity
- **Authors**: Zitong Lan, Yiduo Hao, Mingmin Zhao
- **Categories**: cs.SD, cs.AI, eess.AS, eess.SP
- **arXiv**: [http://arxiv.org/abs/2510.20602v1](http://arxiv.org/abs/2510.20602v1)
- **PDF**: [http://arxiv.org/pdf/2510.20602v1](http://arxiv.org/pdf/2510.20602v1)

为实现虚拟环境中的沉浸式听觉体验，需构建支持动态声源定位的灵活声学建模。本文提出“声场重振”任务，旨在通过稀疏分布的发射器测量点，推算任意发射位置的房间脉冲响应，其原理与视觉领域的重光照问题相呼应。基于声学互易性原理，我们提出受物理启发的Versa方法以优化声场学习：通过交换发射器与接收器位姿，生成具有密集虚拟发射位姿的物理有效样本。针对发射/接收器增益模式导致的互易性应用难题，我们设计了自监督学习方案进行化解。实验表明，Versa在仿真与真实数据集的多项指标上均显著提升声场学习性能。感知实验证实该方法可大幅增强沉浸式空间音频体验。项目网站已公开代码、数据集及演示视频：https://waves.seas.upenn.edu/projects/versa。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：AR/VR技术发展推动多感官合成需求，但现有声场建模多限于静态声源，难以支持动态声源位置变化。  
- **既有方法问题**：  
  - 基于神经辐射场的方法需密集部署声源（数百至数千），难以泛化到稀疏声源场景。  
  - 基于可微分射线追踪的方法依赖简化几何假设，在复杂结构中误差显著。  

2)  
- **核心方法**：提出Versa框架，基于声波传播的互易性原理，通过交换发射器与接收器位姿生成物理有效的虚拟训练样本。  
- **解决思路**：  
  - **Versa-ELE**：通过位姿交换将密集接收器位置转化为虚拟声源，缓解声源稀疏性问题，适用于同向增益模式设备。  
  - **Versa-SSL**：针对非对称增益模式（如全向扬声器与定向麦克风），通过自监督学习解耦方向性影响，强制模型在交换位姿时输出一致脉冲响应。  
- **优势**：  
  - 无需额外硬件，仅利用现有数据增强模型泛化能力。  
  - 通过物理约束提升声场估计的鲁棒性与准确性。  

3)  
- **任务与效果**：  
  - 在模拟与真实数据集（如MeshRIR、RAF）上评估，Versa-ELE使现有模型在C50和STFT指标平均提升34%和31%。  
  - Versa-SSL在非对称增益场景下进一步优化AVR模型，C50和STFT分别提升24%和48%。  
  - 感知实验表明，93.3%用户认为Versa提升空间音频真实感与方向一致性。
</div>

</details>

---

## Time-series Random Process Complexity Ranking Using a Bound on Conditional Differential Entropy
- **Authors**: Jacob Ayers, Richard Hahnloser, Julia Ulrich, Lothar Sebastian Krapp, Remo Nitschke, Sabine Stoll, Balthasar Bickel, Reinhard Furrer
- **Categories**: eess.SP, cs.IT, eess.AS, math.IT, stat.ME, stat.ML
- **arXiv**: [http://arxiv.org/abs/2510.20551v1](http://arxiv.org/abs/2510.20551v1)
- **PDF**: [http://arxiv.org/pdf/2510.20551v1](http://arxiv.org/pdf/2510.20551v1)

条件微分熵通过量化给定历史上下文条件下未来观测值的不确定性，为时间序列复杂度的相对排序提供了直观度量。然而，对于未知分布的高维过程，直接计算该熵值往往难以实现。本文基于Fang等人建立的信息论预测误差边界理论，证明条件微分熵**$h(X_k \mid X_{k-1},...,X_{k-m})$**始终被任意下一步预测模型的预测误差协方差矩阵行列式函数所上界。我们通过结合Hadamard不等式与协方差矩阵的半正定性，进一步强化了这一理论上界。

为验证该边界在时间序列复杂度排序中的适用性，我们开展了两组合成实验：（1）在加性高斯噪声的受控线性自回归过程中，将最小二乘预测误差的熵代理量与各类加性噪声的真实熵值进行对比；（2）对熵值未知的仿生合成音频数据进行复杂度排序，利用神经网络预测误差还原已知的复杂度等级。

该框架基于信息论理论基础，通过下一步预测模型的预测误差，为时间序列复杂度排序提供了计算可行的实现路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：时间序列复杂性排序在经济学、神经科学等领域至关重要，现有方法包括分形分析、非线性动力学和熵度量。  
- **既有问题**：条件微分熵虽能直观量化时间序列复杂性，但直接计算常不可行，尤其对于高维或未知分布过程，缺乏闭式概率密度函数，需依赖近似方法。

2)  
- **核心方法**：基于Fang等提出的预测误差协方差矩阵行列式上界条件微分熵的理论，进一步利用Hadamard不等式和协方差矩阵半正定性扩展上界。  
- **解决步骤**：  
  - 通过任意预测模型计算下一步预测误差的协方差矩阵。  
  - 定义预测误差条件熵代理（PECEP），结合高斯分布最大熵特性，提供条件熵的紧致上界估计。  
  - 在合成数据中验证：使用线性自回归模型比较PECEP与真实熵；在未知熵生物音频数据中，用神经网络预测误差恢复复杂性排序。  
- **优势**：避免直接计算概率密度，利用预测误差协方差提供可处理且理论坚实的复杂性评估框架。

3)  
- **任务与效果**：  
  - 线性自回归过程：PECEP在不同噪声水平下收敛至真实熵下界，验证方法有效性。  
  - 合成生物音频复杂性排序：神经网络预测误差成功恢复10个物种的已知复杂性顺序（物种0最简单到物种9最复杂），PECEP中位数单调递增，证明其在未知分布下的实用性和排序能力。
</div>

</details>

---

## Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment
- **Authors**: Zhiyu Lin, Jingwen Yang, Jiale Zhao, Meng Liu, Sunzhu Li, Benyou Wang
- **Categories**: cs.SD, cs.CL, cs.LG
- **arXiv**: [http://arxiv.org/abs/2510.20513v1](http://arxiv.org/abs/2510.20513v1)
- **PDF**: [http://arxiv.org/pdf/2510.20513v1](http://arxiv.org/pdf/2510.20513v1)

当前语音到语音转换模型虽能生成清晰语音，但在自然表现力方面仍有欠缺，这主要源于缺乏可靠的评估指标。现有方法如主观平均意见得分、低层级声学特征及情感识别等技术存在成本高昂、维度局限或评估不全面等问题。为此，我们提出DeEAR框架，通过融合语音学与心理学原理，将人类对语音表现力的主观偏好转化为客观评分体系。该框架从情感维度、韵律特征及自然度三个层面进行评估，仅需不足500个标注样本即可实现与人类感知的高度对齐（斯皮尔曼等级相关系数达0.86）。除提供可靠评分外，DeEAR还能实现公平的模型对标与精准数据筛选：不仅可识别不同语音转换模型间的表现力差距，还从数据集中精选1.4万条高表现力语音样本构建ExpressiveSpeech数据集，使语音转换模型的表现力评分（百分制）从2.0提升至23.4。演示系统与代码已开源：https://github.com/FreedomIntelligence/ExpressiveSpeech

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前语音合成模型虽能生成清晰语音，但在对话场景中缺乏自然表现力，主要因缺少可靠的客观评估指标。  
- **既有方法问题**：  
  - 主观平均意见分（MOS）成本高、难以扩展；  
  - 低层级声学特征（如音高、能量）无法捕捉感知细节；  
  - 情感识别仅覆盖表现力的单一维度，缺乏全面性。  

2)  
- **核心方法**：提出DeEAR框架，将人类对语音表现力的偏好转化为客观分数，基于语音学与心理学理论分解为三个维度：  
  - **情感强度**：通过微调预训练模型（如wav2vec2）量化语音的唤醒度；  
  - **韵律丰富性**：利用大语言模型（Gemini 2.5 Pro）作为代理标注器，生成与人类评分高度相关的韵律分数；  
  - **自然度**：结合启发式规则与深度学习，通过声学质量与基类别的匹配程度评估语音是否自然。  
- **对齐策略**：  
  - 通过多任务学习训练统一模型（DeEAR-Base），输出三个维度分数；  
  - 使用XGBoost融合层学习人类偏好数据，将维度分数映射为整体表现力分数，确保与人类感知一致（SRCC=0.86）。  
- **效率优化**：  
  - 采用知识蒸馏将复杂教师模型压缩为轻量学生模型；  
  - 模块化设计支持分维度解析与高效部署。  

3)  
- **自动化评测**：在7个主流语音合成模型上实现系统级排名，与人类评估排名高度一致（SRCC=0.96），显著区分模型表现（最高与最低分差达60.1）。  
- **数据筛选**：从开源数据集中筛选出1.4万条高表现力语音，构建ExpressiveSpeech数据集，用于微调模型后：  
  - 整体表现力分数从2.0提升至23.4（百分制）；  
  - 情感与自然度维度提升显著（情感分数从5.7升至15.9，自然度从33.7升至62.0）。
</div>

</details>

---

## Speaking Clearly: A Simplified Whisper-Based Codec for Low-Bitrate Speech Coding
- **Authors**: Xin Zhang, Lin Li, Xiangni Lu, Jianquan Liu, Kong Aik Lee
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.20504v1](http://arxiv.org/abs/2510.20504v1)
- **PDF**: [http://arxiv.org/pdf/2510.20504v1](http://arxiv.org/pdf/2510.20504v1)

语音编解码器在连续语音信号与大语言模型之间构建桥梁，但始终面临声学保真度与语义保持之间的固有矛盾。为缓解这一矛盾，主流方法通常通过复杂的语义监督来增强声学编解码器。本研究探索了相反路径：采用语义优先策略，以具备语义能力的模型为基础进行改造，实现高保真声学重建。通过实证分析发现，对Whisper（一种文本对齐的自动语音识别模型）进行针对性结构简化，可有效释放其声学建模潜力。基于此发现，我们提出SimWhisper-Codec新型编解码器，该方案利用冻结的简化Whisper编码器，在无需外部监督的条件下实现语义与声学保真的平衡。实验结果表明，在相近码率下，本方法在语义保持度和声学质量上均优于Mimi Codec、SpeechTokenizer等语义监督编解码器，验证了语义优先策略的有效性。代码已开源：https://github.com/ZhangXinWhut/SimWhisper-Codec。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音编解码器在连接连续语音信号与大语言模型时，面临语义保留与声学保真度之间的固有冲突。现有方法通常通过复杂语义监督增强声学编解码器，但增加了模型复杂性和训练难度。  
- **既有问题**：传统方法依赖外部语义模型（如HuBERT、ASR任务）进行多阶段训练，导致计算成本高，且在低比特率下难以平衡语义与声学性能。  

2)  
- **核心方法**：提出SimWhisper-Codec，采用“语义优先”思路，基于Whisper ASR模型进行架构简化，无需外部语义监督。  
- **简化策略**：  
  - 移除Whisper编码器前端的卷积层非线性激活（GELU），保留线性变换以增强频谱细节保留。  
  - 去除Transformer中的绝对位置编码，避免对重复语音结构的干扰，提升注意力机制的灵活性。  
- **架构设计**：  
  - 使用冻结的简化Whisper编码器作为特征提取器，结合FSQ量化模块和对称解码器。  
  - 通过下采样与上采样模块压缩特征维度，并采用GAN训练目标（多尺度重建损失、对抗损失和特征匹配损失）优化重建质量。  
- **优势**：单阶段训练即可同时建模语义与声学信息，显著降低复杂度。  

3)  
- **任务与效果**：在LibriSpeech测试集上，与同类低比特率编解码器（如Mimi Codec、SpeechTokenizer）相比：  
  - **语义保留**：词错误率（WER）降至3.10，优于所有基线模型。  
  - **声学质量**：说话人相似度（SIM）达0.83，可懂度（STOI）为0.91，PESQ-NB提升至2.98，在低比特率（1.1 kbps）下实现竞争性性能。
</div>

</details>

---

## UniSE: A Unified Framework for Decoder-only Autoregressive LM-based Speech Enhancement
- **Authors**: Haoyin Yan, Chengwei Liu, Shaofei Xue, Xiaotao Liang, Zheng Xue
- **Categories**: cs.SD, cs.AI
- **arXiv**: [http://arxiv.org/abs/2510.20441v1](http://arxiv.org/abs/2510.20441v1)
- **PDF**: [http://arxiv.org/pdf/2510.20441v1](http://arxiv.org/pdf/2510.20441v1)

神经音频编解码器的发展极大促进了语言模型在语音处理与理解领域的应用。然而，自回归语言模型在统一语音增强各子任务方面的有效性尚未得到验证。本研究提出UniSE——基于解码器架构的自回归语言模型统一框架，可同时处理语音修复、目标说话人提取与语音分离三类增强任务。该框架以输入语音特征为条件，通过自回归建模生成目标语音的离散表征，从而实现多任务学习范式的兼容性。在多个基准测试上的实验表明，UniSE相较判别式与生成式基线模型均能取得具有竞争力的性能，证明了语言模型在统一语音增强任务方面的潜力。演示页面详见：https://github.com/hyyan2k/UniSE。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强涵盖语音恢复、目标说话人提取和语音分离等子任务，传统深度神经网络方法虽有效，但缺乏统一框架。  
- **既有方法问题**：现有基于语言模型的方法多局限于单一任务或失真类型，且多采用掩码生成或直接映射范式，自回归建模在多任务统一中的潜力未充分验证。  

2)  
- **核心方法**：UniSE采用仅解码器自回归语言模型，通过任务令牌统一多任务框架，支持语音恢复、目标说话人提取及反向提取模式。  
- **特征提取**：使用冻结的WavLM提取语音特征，结合可训练适配器映射为条件输入。  
- **离散建模**：利用BiCodec将语音转换为全局和语义离散令牌，通过自回归生成目标令牌。  
- **任务统一**：不同模式通过输入序列格式区分，如语音恢复模式仅用退化语音特征，目标提取模式加入参考语音特征，实现灵活任务切换与组合。  

3)  
- **语音恢复任务**：在DNS和URGENT挑战测试集上取得最优或竞争性性能，尤其在多失真场景下泛化能力强。  
- **目标说话人提取**：在Libri2Mix数据集上表现与先进方法相当，支持多任务不降低单任务性能。  
- **语音分离任务**：在Libri2Mix和WSJ0-2mix数据集上超越判别式和生成式基线，验证多模式推理策略的有效性。
</div>

</details>

---

## From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era
- **Authors**: Wonil Kim, Hyeongseok Wi, Seungsoon Park, Taejun Kim, Sangeun Keum, Keunhyoung Kim, Taewan Kim, Jongmin Jung, Taehyoung Kim, Gaetan Guerrero, Mael Le Goff, Julie Po, Dongjoo Moon, Juhan Nam, Jongpil Lee
- **Categories**: cs.IR, cs.HC, cs.MA, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.20276v1](http://arxiv.org/abs/2510.20276v1)
- **PDF**: [http://arxiv.org/pdf/2510.20276v1](http://arxiv.org/pdf/2510.20276v1)

生成式人工智能正在重塑音乐创作领域，但其快速发展也暴露出在作品归属、权利管理和经济模式方面的结构性缺陷。与从现场表演到录音、下载及流媒体的传统媒介变迁不同，人工智能技术彻底改变了音乐的全生命周期，消弭了创作、分发与盈利之间的界限。然而，现有流媒体体系依赖不透明且高度集中的版税分配机制，难以应对AI驱动生产带来的规模性与复杂性挑战。本文提出一种基于内容的音乐AI智能体架构，通过区块级检索与智能体协同机制，将作品归属直接嵌入创作流程。该架构支持基于会话的迭代式交互，将音乐解构为存储于区块数据库的细粒度组件，每次调用都会触发归属层事件，实现可溯源的透明结算。这一框架将AI从生成工具升级为“公平AI媒体平台”的基础设施，通过细粒度归属认定、公平补偿机制与参与式协作，推动音乐产业从静态曲库模式转向后流媒体时代的新型范式——使音乐成为持续演进、协同创造的生态系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：生成式AI正重塑音乐创作，但现有系统（如文本到音乐平台）多为单次输出模式，缺乏迭代创作和贡献者署名机制。  
- **既有问题**：  
  - 流媒体经济模型不透明，版税分配集中于少数艺术家；  
  - AI规模化生产加剧了版权归属和收益分配的不公平性；  
  - 传统框架无法支持实时、细粒度的署名与结算需求。  

2)  
- **核心架构**：提出基于内容的音乐AI代理系统，整合三大组件：  
  - **BlockDB**：存储艺术家提交的音乐分解块（如音轨片段），附带元数据与创作者信息；  
  - **音乐AI代理**：通过检索增强生成（RAG）工作流，根据用户指令从BlockDB检索并组合音乐块；  
  - **署名层**：实时记录每个音乐块的使用，关联原创作者并触发版税结算。  
- **解决机制**：  
  - **细粒度归属**：将音乐拆解为可追溯的“块”，使每次使用均对应明确署名；  
  - **实时结算**：通过会话式交互流程，动态分配收益，避免传统流媒体的模糊分配；  
  - **参与式生态**：支持用户通过混音、协作等行为参与创作，同时保障原作者权益。  

3)  
- **任务与效果**：  
  - **音乐创作**：实现会话式分层生成，用户可通过交互迭代构建曲目（见图3界面演示）；  
  - **版权管理**：在组件级别建立可审计的归属记录，替代传统流媒体的聚合计费模式；  
  - **经济模型**：支持基于实际贡献的微结算，推动粉丝从听众转变为共创参与者；  
  - **平台范式**：初步验证“后流媒体”生态，使音乐成为动态、可适配的协作媒介。
</div>

</details>

---

## Neural Directional Filtering with Configurable Directivity Pattern at Inference
- **Authors**: Weilong Huang, Srikanth Raj Chetupalli, Emanuël A. P. Habets
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.20253v1](http://arxiv.org/abs/2510.20253v1)
- **PDF**: [http://arxiv.org/pdf/2510.20253v1](http://arxiv.org/pdf/2510.20253v1)

本文提出一种支持用户自定义指向性模式的神经定向滤波方法（UNDF），可在推理阶段根据用户设定的指向模式实现空间滤波。为实现该功能，我们设计了集成特征线性调制（FiLM）的深度神经网络架构，使用户定义的指向模式可作为条件输入参与计算。理论分析表明：基于FiLM的架构使UNDF在干扰环境下具备泛化能力，可适应训练中未出现过的高指向性、尺度变化及不同转向角度的用户定义模式。通过渐进式优化训练策略，系统在提升模式拟合精度的同时，还能实现对不规则指向形状的逼近。实验对比表明，UNDF在各项性能指标上均优于传统方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **传统固定波束成形**需大量麦克风和大孔径阵列才能保证性能，且模式改变需重新计算滤波器。  
- **参数化滤波方法**依赖准确的到达方向估计，在多源和非语音环境中表现不佳。  
- **现有深度学习方法**多聚焦于语音分离，未显式控制方向性模式，或模式在推理时无法调整。  

2)  
- **核心架构**：提出FiLM-JNF模型，将用户定义的方向性模式作为条件输入，通过特征线性调制层实现动态融合。  
  - 模式向量经线性层生成仿射参数，对网络特征进行逐元素调制，增强泛化能力。  
- **训练策略优化**：  
  - 使用随机差分麦克风阵列模式和矩形模式的线性组合，覆盖更广的模式空间。  
  - 引入多尺度与不规则模式训练，提升对复杂形状的逼近精度。  
- **优势**：  
  - 支持推理时自定义模式，无需重新训练。  
  - 泛化至未见的高阶模式、缩放变化及转向角度。  

3)  
- **任务**：空间滤波与声源提取。  
- **效果**：  
  - 在方向性模式逼近上优于传统参数化方法，信噪比提升最高达11 dB。  
  - 成功处理时变模式，在混响环境中有效分离语音与音乐信号。  
  - 训练仅使用语音数据，但可泛化至音乐等非训练信号类型。
</div>

</details>

---

## Vox-Evaluator: Enhancing Stability and Fidelity for Zero-shot TTS with A Multi-Level Evaluator
- **Authors**: Hualei Wang, Na Li, Chuke Wang, Shu Wu, Zhifeng Li, Dong Yu
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.20210v1](http://arxiv.org/abs/2510.20210v1)
- **PDF**: [http://arxiv.org/pdf/2510.20210v1](http://arxiv.org/pdf/2510.20210v1)

尽管基于语言模型、扩散模型与掩码生成技术的零样本文本转语音（TTS）研究在语音自然度方面取得显著进展，稳定性与保真度仍是核心挑战，具体表现为发音错误、可闻噪声及音质劣化。为此，我们提出Vox-Evaluator——一种多层级评估器，通过定位错误语音片段的时间边界并提供生成语音的整体质量评估，指导TTS系统进行错误修正与偏好对齐。具体而言，为提升零样本TTS模型的鲁棒性，我们基于评估器自动识别声学错误，对错误片段进行掩码处理，并依托正确片段进行语音重构。此外，Vox-Evaluator提供的细粒度信息可引导TTS模型完成偏好对齐，从而减少合成语音中的不良案例。针对现有训练数据集的缺失，我们构建了包含细粒度发音错误与音质问题标注的合成文本-语音数据集。实验结果表明，通过语音修正机制与偏好优化，所提方法能有效提升TTS系统的稳定性与保真度。演示内容详见附录。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：零样本文本转语音（TTS）技术基于语言模型、扩散模型和掩码生成方法，在语音自然度方面取得显著进展，但稳定性和保真度仍面临挑战。  
- **既有问题**：  
  - 自回归模型易出现错误累积和不稳定性；非自回归模型因隐式对齐和采样随机性导致发音错误、可听噪声及异常停顿。  
  - 现有方法依赖复杂外部工具（如ASR、MFA）定位错误，且无法有效处理低质量音频段。  
  - 偏好对齐方法需高成本人工标注或预训练评估工具，难以获取细粒度反馈。  

2)  
- **核心方法**：提出多级评估器Vox-Evaluator，通过联合检测错误段位置、语义内容及整体质量评分，指导语音修正与偏好对齐。  
  - **错误检测与修正**：  
    - 利用动态时间规整（DTW）比对生成文本与目标文本，定位语义不一致段。  
    - 基于评估器输出的时间范围掩码错误段，通过编辑式TTS模型仅重生成问题部分，降低计算成本。  
    - 迭代修正至多两次，提升稳定性。  
  - **细粒度偏好对齐**：  
    - 将评估器作为奖励模型，为语音样本提供语义正确性与质量评分。  
    - 在直接偏好优化（DPO）中聚焦错误段计算损失，避免全局信息冗余与过优化。  
  - **数据支撑**：构建FGES数据集，包含22k标注错误段及质量的合成语音，解决训练数据缺失问题。  

3)  
- **任务与效果**：  
  - **错误检测**：在FGES测试集上，错误段定位IOU达0.782，优于基线19.7%；转录词错误率（WER）降至2.64%。  
  - **语音修正**：在Seed-TTS和LibriSpeech数据集上，F5-TTS与VoiceCraft修正后WER分别降低21%与32，自然度（CMOS）显著提升。  
  - **偏好对齐**：F5-TTS经DPO优化后，WER从1.73%降至1.55%，说话人相似度与主观评分持续改善。
</div>

</details>

---

## SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance
- **Authors**: Haowei Lou, Chengkai Huang, Hye-young Paik, Yongquan Hu, Aaron Quigley, Wen Hu, Lina Yao
- **Categories**: eess.SY, cs.SD, cs.SY
- **arXiv**: [http://arxiv.org/abs/2510.20113v1](http://arxiv.org/abs/2510.20113v1)
- **PDF**: [http://arxiv.org/pdf/2510.20113v1](http://arxiv.org/pdf/2510.20113v1)

语音是人类交流的核心方式，但全球有数百万人因运动性构音障碍、口吃、失语症等言语障碍面临社交孤立与参与度下降的困境。尽管自动语音识别与文本转语音技术近年来取得显著进展，面向言语障碍群体的可访问网络与移动基础设施仍显不足，阻碍了这些技术在日常沟通中的实际应用。为此，我们提出SpeechAgent——一套端到端移动语音辅助系统，通过融合大语言模型的智能推理与先进语音处理模块，为不同类型的言语障碍者提供自适应支持。为确保实际可用性，我们构建了结构化部署流水线，实现在移动及边缘设备上的实时语音处理，在保持高准确度与语音质量的同时达到无感知延迟。基于真实世界言语障碍数据集的性能评估与边缘设备延迟测试表明，SpeechAgent兼具高效性与用户友好性，验证了其在个性化日常辅助沟通场景中的可行性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：全球数百万人受构音障碍、口吃、失语症等言语障碍困扰，导致社交孤立与沟通困难。现有自动语音识别（ASR）与文本转语音（TTS）技术虽进步，但缺乏适用于日常场景的移动端辅助工具。
- **既有问题**：
  - 传统辅助工具（如AAC设备、语法校正系统）依赖预定义规则，难以处理真实场景中碎片化、模糊的语音输入。
  - 临床康复技术（如语言治疗软件）侧重长期训练，无法实时修复对话。
  - 现有ASR模型仅关注诊断性转录，未提供实时语义重构与语音清晰化支持。

2) **论文核心方法如何解决上述问题**  
SpeechAgent构建端到端移动基础设施，通过以下模块协同解决既有问题：
- **语音障碍识别（SIR）**：  
  - 基于梅尔频谱图输入，使用Transformer编码器提取声学特征，通过多分类模型识别障碍类型（如构音障碍、口吃、失语症）。
  - 相比通用音频-文本对齐模型（如CLAP），专用SIR模型准确率超90%，能捕捉细微声学异常。
- **语音识别与语义重构**：  
  - 采用鲁棒ASR模型转录障碍语音为文本，再通过大语言模型（LLM）进行意图推理。
  - LLM结合障碍类型上下文，重构语义连贯的文本（如消除口吃重复、补全失语句子），而非仅语法校正。
- **实时语音生成与部署**：  
  - 使用ParaStyleTTS将重构文本转换为自然语音，支持个性化语音风格。
  - 采用客户端-服务器架构：移动端采集语音，云端处理并返回低延迟结果（平均响应时间0.91秒），满足实时交互需求。
- **系统整合优势**：  
  - 突破传统流程的单一功能，实现“感知-推理-生成”闭环。
  - 通过多维度评估（语义相似度、人机交互延迟）验证其兼顾准确性与实用性。

3) **在哪些任务上取得了怎样的效果**  
SpeechAgent在以下任务中表现优异：
- **障碍识别任务**：在包含多种障碍的2000条语音数据上，Transformer模型准确率达91.4%，AUC超0.99，显著优于基线模型。
- **语音清晰化任务**：  
  - 文本重构后，BERT分数提升至0.95（构音障碍）与0.97（口吃），语义一致性显著改善。
  - 人机评估显示，重构语音的清晰度评分提升40%，且91.2%的样本被分类为“健康语音”。
- **实时交互任务**：端到端延迟仅0.91秒（RTF=0.08），支持移动端日常无缝使用。
</div>

</details>

---
