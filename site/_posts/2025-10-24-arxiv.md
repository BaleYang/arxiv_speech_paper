---
layout: post
title: "arXiv Daily – 2025-10-24"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-24（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-23 08:50 — 2025-10-24 08:50
- 抓取总数：11 篇 | 本页显示：11 篇（去重/过滤后）

## Controllable Embedding Transformation for Mood-Guided Music Retrieval
- **Authors**: Julia Wilkins, Jaehun Kim, Matthew E. P. Davies, Juan Pablo Bello, Matthew C. McCallum
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.20759v1](http://arxiv.org/abs/2510.20759v1)
- **PDF**: [http://arxiv.org/pdf/2510.20759v1](http://arxiv.org/pdf/2510.20759v1)

音乐表征是现代推荐系统的核心支撑，驱动着歌单生成、相似性检索与个性化发现等功能。然而现有嵌入表示大多难以实现对单一音乐属性（如仅改变曲目情绪而保持其流派或配器不变）的精细化调控。本研究针对基于嵌入变换的可控音乐检索问题，提出一种情绪引导的音乐嵌入变换框架。该框架通过学习从种子音频嵌入到目标嵌入的映射关系，在情绪标签引导下实现目标属性转换，同时保持其他音乐特征不变。由于无法直接修改种子音频的情绪属性，我们引入了一种采样机制，通过检索代理目标来平衡内容多样性与种子相似度。基于此采样策略训练轻量级翻译模型时，提出了兼顾变换能力与信息保持的新型联合优化目标。在两个数据集上的实验表明，本方法在实现高效情绪转换的同时，其流派与配器保持能力显著优于无训练基线，证实了可控嵌入变换在个性化音乐检索领域的应用潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代音乐推荐系统依赖嵌入表示，但现有方法难以在调整单一属性（如情绪）的同时保持其他音乐特征（如流派、乐器）不变。  
- **既有问题**：  
  - 生成式风格迁移方法计算成本高，难以应用于大规模检索场景。  
  - 基于解耦的表示学习方法无法直接操控输入嵌入的特定属性。  
  - 现有嵌入空间缺乏对抽象属性（如情绪）的细粒度控制机制。  

2)  
- **核心方法**：提出基于情绪引导的嵌入变换框架，通过轻量级映射模型将种子嵌入转换为目标情绪对应的嵌入，同时保留其他属性。  
- **关键设计**：  
  - **采样机制**：从嵌入空间中检索与种子相似但情绪不同的代理目标，平衡多样性与相似性。  
  - **联合损失函数**：  
    - 余弦相似度损失：确保变换后嵌入与目标嵌入对齐。  
    - 三元组损失：推动变换嵌入远离种子嵌入，接近目标嵌入。  
    - 余弦BCE损失：根据情绪匹配情况动态调整相似度约束。  
  - **模型结构**：使用多层感知机分别处理种子嵌入和情绪标签差异，通过拼接与投影生成目标嵌入。  

3)  
- **任务与效果**：  
  - **情绪转换任务**：在大型专有数据集和MTG-Jamendo上，情绪转换精度（Mood P@1）分别达0.96和0.83，显著优于随机基线（0.25）。  
  - **属性保留任务**：流派保留（Genre P@1）分别达0.32和0.29，乐器一致性（Inst. J@1）在MTG-Jamendo上达0.45，均远超训练无关基线。  
  - **优势**：在保持高情绪转换精度的同时，有效保留种子音乐的流派与乐器特征。
</div>

</details>

---

## R2-SVC: Towards Real-World Robust and Expressive Zero-shot Singing Voice Conversion
- **Authors**: Junjie Zheng, Gongyu Chen, Chaofan Ding, Zihao Chen
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.20677v1](http://arxiv.org/abs/2510.20677v1)
- **PDF**: [http://arxiv.org/pdf/2510.20677v1](http://arxiv.org/pdf/2510.20677v1)

在实际歌唱声音转换应用中，环境噪声与表现力输出需求构成双重挑战。传统方法因通常依赖纯净数据进行训练与推理，未能充分考虑真实部署场景，导致其在实际应用中受限——音乐分离过程中产生的各类噪声与伪影往往难以避免。为解决这些问题，我们提出R2-SVC这一兼具鲁棒性与表现力的SVC框架。首先，通过随机基频扰动和音乐分离伪影模拟（如混响、回声）实现基于仿真的鲁棒性增强，显著提升噪声环境下的性能表现。其次，利用领域特异性歌唱数据丰富说话人表征：除纯净人声外，融合DNSMOS滤波的分离人声与公开歌唱语料，使模型在保持说话人音色特质的同时精准捕捉歌唱风格细节。第三，集成神经源滤波模型以显式表征谐波与噪声分量，增强转换歌声的自然度与可控性。实验表明，R2-SVC在纯净与噪声场景下的多项SVC基准测试中均达到最优性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：真实场景的歌声转换面临环境噪声和表达性输出的挑战。  
- **既有方法问题**：  
  - 传统方法依赖干净数据训练和推理，未考虑实际部署中的噪声和音乐分离伪影。  
  - 对抗学习和数据增强泛化能力不足；基于ASR的内容建模对基频抖动、混响和回声敏感，导致自然度下降。  

2)  
- **核心方法**：提出R2-SVC框架，集成三个模块解决鲁棒性和表达性问题。  
  - **模拟增强鲁棒性**：  
    - 随机基频扰动（如抖动、滑音、跳跃），降低模型对F0的依赖，提升对噪声的容忍度。  
    - 湿声模拟（和声、回声、混响），通过数据增强训练模型从含伪影输入生成干声。  
  - **歌声增强的音色与风格提取**：  
    - 扩展CAM++框架，融合干净歌声、DNSMOS滤波分离人声及公共歌唱数据。  
    - 捕获跨域音色一致性及歌手特定风格（如颤音、咬字），提升噪声下的表达性。  
  - **神经源滤波集成**：  
    - 显式分解谐波和噪声成分，通过源-滤波器架构增强生成自然度和可控性。  

3)  
- **任务与效果**：  
  - 在正常和困难测试集上评估零样本歌声转换任务。  
  - 客观指标：SPK-SIM（75.17%）、CER（13.41%）等优于或匹配基线；在困难集上鲁棒性显著，CE/CU/PQ指标领先。  
  - 主观评估：自然度（NMOS）和音色相似度（SMOS）提升，细节重建能力增强，尤其在噪声场景下保持最佳平衡。
</div>

</details>

---

## Resounding Acoustic Fields with Reciprocity
- **Authors**: Zitong Lan, Yiduo Hao, Mingmin Zhao
- **Categories**: cs.SD, cs.AI, eess.AS, eess.SP
- **arXiv**: [http://arxiv.org/abs/2510.20602v1](http://arxiv.org/abs/2510.20602v1)
- **PDF**: [http://arxiv.org/pdf/2510.20602v1](http://arxiv.org/pdf/2510.20602v1)

为实现虚拟环境中的沉浸式听觉体验，需构建支持动态声源定位的灵活声学建模。本文提出"声场重振"任务——通过稀疏分布的发射器位置测量数据，推演任意发射位点的房间脉冲响应，其原理与视觉领域的重光照问题相呼应。基于声学互易性原理，我们提出受物理规律启发的Versa方法以优化声场学习：通过交换发射端与接收端位姿，生成具有密集虚拟发射位点的物理有效样本。针对发射/接收增益模式导致的互易性应用难题，我们设计了自监督学习解决方案。实验表明，Versa在仿真与真实数据集的多项指标上均显著提升声场学习性能。感知实验证实该方法可大幅增强沉浸式空间音频体验。项目网站已公开代码、数据集及演示视频：https://waves.seas.upenn.edu/projects/versa。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：AR/VR技术发展推动多感官合成需求，但现有声场建模主要针对静态声源，难以支持动态声源位置变化。  
- **既有方法问题**：  
  - 基于神经辐射场的方法需密集部署声源（数百至数千），难以泛化到稀疏声源场景。  
  - 基于可微分射线追踪的方法依赖简化几何模型，在复杂结构中误差显著。  

2)  
- **核心方法**：提出Versa框架，基于声波传播的互易性原理，通过两种策略提升稀疏声源下的声场建模能力：  
  - **Versa-ELE（发射器-听者交换）**：  
    - 通过交换发射器与听者位姿生成物理有效的虚拟训练样本，将密集听者位置转化为虚拟声源。  
    - 作为数据增强手段，提升模型对传播路径的理解，适用于发射器与听者增益模式相同的场景。  
  - **Versa-SSL（自监督学习）**：  
    - 针对非对称增益模式（如全向扬声器与定向麦克风），通过解耦增益模式与传播效应，强制模型在交换位姿时输出一致的脉冲响应。  
    - 采用两阶段训练：先拟合真实脉冲响应，再通过自监督损失强化互易性约束。  
- **创新点**：  
  - 将物理原理转化为机器学习训练策略，显著提升稀疏数据下的泛化能力。  
  - 模型无关的Versa-ELE可适配多种神经声场模型，而Versa-SSL进一步解决非对称增益问题。  

3)  
- **任务与效果**：  
  - **声场估计任务**：在模拟（AcoustiX）和真实（MeshRIR、RAF）数据集上评估稀疏声源（≤10个）下的脉冲响应预测。  
  - **定量结果**：  
    - Versa-ELE平均提升C50指标34%、STFT指标31%；  
    - Versa-SSL在AVR模型上进一步提升C50指标24%、STFT指标48%。  
  - **感知研究**：93.3%用户认为Versa生成音频的整体相似度更高，85%用户认为Versa-SSL在音量与方向性上更优。
</div>

</details>

---

## Time-series Random Process Complexity Ranking Using a Bound on Conditional Differential Entropy
- **Authors**: Jacob Ayers, Richard Hahnloser, Julia Ulrich, Lothar Sebastian Krapp, Remo Nitschke, Sabine Stoll, Balthasar Bickel, Reinhard Furrer
- **Categories**: eess.SP, cs.IT, eess.AS, math.IT, stat.ME, stat.ML
- **arXiv**: [http://arxiv.org/abs/2510.20551v1](http://arxiv.org/abs/2510.20551v1)
- **PDF**: [http://arxiv.org/pdf/2510.20551v1](http://arxiv.org/pdf/2510.20551v1)

条件微分熵通过量化在已知历史上下文条件下未来观测值的不确定性，为时间序列复杂度的相对排序提供了直观度量。然而，对于来自未知分布的高维过程，直接计算该熵值通常难以实现。本文基于Fang等人建立的信息论预测误差边界理论，证明条件微分熵 **$h(X_k \mid X_{k-1},...,X_{k-m})$** 在任何下一步预测模型中均被预测误差协方差矩阵行列式的函数所上界。我们通过结合Hadamard不等式与协方差矩阵的半正定性，进一步强化了这一理论上界。

为验证该边界能否用于时间序列复杂度排序，我们开展了两组合成实验：（1）在加性高斯噪声的受控线性自回归过程中，将最小二乘预测误差熵代理量与各类加性噪声的真实熵值进行对比；（2）对熵值未知的仿生合成音频数据进行复杂度排序，利用神经网络预测误差还原已知的复杂度等级。

该框架基于信息论理论基础，通过使用下一步预测模型的预测误差，为时间序列复杂度排序提供了计算可行的实现路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：时间序列复杂性排序在经济学、神经科学等领域有广泛应用，现有方法主要分为三类：分形分析、非线性动力学方法和熵度量。  
- **既有方法问题**：条件微分熵虽能直观衡量时间序列复杂性，但直接计算面临两大挑战：  
  - 高维或未知分布过程中，闭式概率密度函数难以推导；  
  - 传统熵估计依赖强分布假设，实际应用中计算不可行。  

2)  
- **核心方法**：基于Fang等提出的预测误差协方差矩阵与条件熵的关联，构建可计算的熵代理指标PECEP。具体步骤包括：  
  - **理论扩展**：利用Hadamard不等式与协方差矩阵半正定性，将条件熵上界进一步放宽，得到更易计算的边界；  
  - **代理指标设计**：通过训练任意因果预测模型，计算测试集预测误差的协方差矩阵行列式，代入公式得到PECEP；  
  - **优化机制**：当预测模型捕获更多时间依赖时，PECEP趋近于条件熵的理论下界，实现复杂性排序。  
- **解决思路**：  
  - 避免直接估计概率密度，转而利用预测误差的统计特性；  
  - 通过高斯分布熵最大性，将问题转化为协方差矩阵行列式计算；  
  - 支持任意预测模型，兼容传统统计方法与神经网络。  

3)  
- **验证任务与效果**：  
  - **线性自回归实验**：在已知熵的合成数据中，OLS与Oracle模型的PECEP均收敛至理论熵下界，验证边界有效性；  
  - **生物声学音频实验**：对10种复杂度递增的合成物种，神经网络预测误差成功恢复已知排序，PECEP中位数随复杂度单调上升。  
- **实际意义**：在概率分布未知的复杂时间序列中，本方法实现了理论可靠且计算可行的复杂性排名。
</div>

</details>

---

## Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment
- **Authors**: Zhiyu Lin, Jingwen Yang, Jiale Zhao, Meng Liu, Sunzhu Li, Benyou Wang
- **Categories**: cs.SD, cs.CL, cs.LG
- **arXiv**: [http://arxiv.org/abs/2510.20513v1](http://arxiv.org/abs/2510.20513v1)
- **PDF**: [http://arxiv.org/pdf/2510.20513v1](http://arxiv.org/pdf/2510.20513v1)

当前语音到语音转换模型虽能生成清晰语音，但在自然表现力方面仍有欠缺，这主要源于缺乏可靠的评估指标。现有方法如主观平均意见得分、低层级声学特征和情感识别存在成本高昂、局限性或完整性不足等问题。为此，我们提出DeEAR框架，通过将人类对语音表现力的主观偏好转化为客观评分来解决这一难题。该框架基于语音学与心理学原理，从情感维度、韵律特征及自然度三个层面评估语音表现力，仅使用不足500个标注样本即实现与人类感知的高度对齐（斯皮尔曼等级相关系数SRCC=0.86）。除提供可靠评分外，DeEAR还能实现公平的基准测试和精准数据筛选：不仅可区分不同S2S模型的表现力差距，还成功筛选出1.4万条高表现力语音样本构建ExpressiveSpeech数据集，将S2S模型的表现力评分（百分制）从2.0提升至23.4。演示资源与代码已发布于https://github.com/FreedomIntelligence/ExpressiveSpeech

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前语音合成模型虽能生成清晰语音，但在对话场景中缺乏自然表现力，主要因缺乏可靠的客观评估指标。  
- **既有方法问题**：  
  - 主观平均意见分成本高、难以扩展；  
  - 低层级声学特征无法捕捉感知细节；  
  - 情感识别仅覆盖表现力的单一维度。  

2)  
- **核心方法**：提出DeEAR框架，将人类对语音表现力的偏好转化为客观分数，基于语音学与心理学理论分解为三个维度：  
  - **情感强度**：通过微调预训练模型量化唤醒度；  
  - **韵律丰富性**：利用大语言模型生成伪标签，解决标注数据稀缺问题；  
  - **自然度**：结合启发式规则与深度学习，评估语音的自发性。  
- **对齐策略**：  
  - 使用XGBoost融合三个维度分数，通过480条标注数据学习人类整体评分映射；  
  - 通过知识蒸馏将多模块系统压缩为单一高效模型，提升可部署性。  
- **创新点**：  
  - 模块化设计兼顾可解释性与效率；  
  - 仅需少量标注数据即可实现高人类对齐。  

3)  
- **自动化评测**：在7个主流语音模型中，DeEAR与人类评分排名相关性达0.96，显著区分模型表现力差异。  
- **数据筛选**：构建ExpressiveSpeech数据集，显著提升语音合成模型表现力评分。  
- **模型优化**：基于筛选数据微调的模型，在情感与自然度维度提升显著，人类偏好测试中胜率达78.5%。
</div>

</details>

---

## Speaking Clearly: A Simplified Whisper-Based Codec for Low-Bitrate Speech Coding
- **Authors**: Xin Zhang, Lin Li, Xiangni Lu, Jianquan Liu, Kong Aik Lee
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.20504v1](http://arxiv.org/abs/2510.20504v1)
- **PDF**: [http://arxiv.org/pdf/2510.20504v1](http://arxiv.org/pdf/2510.20504v1)

语音编解码器在连续语音信号与大语言模型之间构建桥梁，但始终面临声学保真度与语义保持之间的固有矛盾。为缓解这一矛盾，主流方法通常通过复杂的语义监督来增强声学编解码器。本研究探索了相反方向：采用语义优先策略，以具备语义能力的模型为基础，通过适配实现高保真声学重建。实证分析表明，通过针对性简化结构可释放Whisper（一种文本对齐的自动语音识别模型）的声学建模潜力。基于此发现，我们提出SimWhisper-Codec新型编解码器，其利用冻结的简化Whisper编码器，在无需外部监督的情况下实现语义与声学保真的平衡。实验结果表明，在相近码率下，本方法在语义保持度和声学质量上均优于Mimi Codec、SpeechTokenizer等语义监督式编解码器，验证了语义优先策略的有效性。代码已开源：https://github.com/ZhangXinWhut/SimWhisper-Codec。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音编解码器在连接连续语音信号与大语言模型时，面临语义保留与声学保真度的固有冲突。现有方法通常通过复杂语义监督增强声学编解码器，但增加了模型复杂性和训练难度。  
- **既有问题**：传统方法依赖外部语义模型（如HuBERT、ASR任务）进行多阶段训练，导致架构冗余，且在低比特率下难以平衡语义与声学性能。  

2)  
- **核心方法**：提出SimWhisper-Codec，采用“语义优先”思路，基于Whisper ASR模型进行架构简化，无需外部语义监督。  
  - **简化编码器**：移除Whisper编码器中的卷积前端非线性（GELU激活）和绝对位置编码，增强细粒度声学信息保留能力。  
  - **量化与解码**：结合有限标量量化（FSQ）降低比特率（1.1 kbps），并采用对称解码器进行单阶段训练。  
  - **训练目标**：通过多尺度重建损失、对抗损失和特征匹配损失优化声学重建质量。  
- **解决效果**：简化后的冻结编码器兼顾语义对齐与声学细节，显著缓解语义-声学冲突，简化训练流程。  

3)  
- **任务与效果**：在LibriSpeech测试集上，与同类低比特率编解码器（如Mimi Codec、SpeechTokenizer）相比：  
  - **语义保留**：词错误率（WER）降至3.10，优于所有基线模型。  
  - **声学质量**：说话人相似度（SIM）达0.83，可懂度（STOI）为0.91，声学指标（PESQ）接近最优。  
  - **优势**：在无需语义监督下，同时实现高语义保真度和声学重建质量。
</div>

</details>

---

## UniSE: A Unified Framework for Decoder-only Autoregressive LM-based Speech Enhancement
- **Authors**: Haoyin Yan, Chengwei Liu, Shaofei Xue, Xiaotao Liang, Zheng Xue
- **Categories**: cs.SD, cs.AI
- **arXiv**: [http://arxiv.org/abs/2510.20441v1](http://arxiv.org/abs/2510.20441v1)
- **PDF**: [http://arxiv.org/pdf/2510.20441v1](http://arxiv.org/pdf/2510.20441v1)

神经音频编解码器的发展极大促进了语言模型在语音处理与理解领域的应用。然而，自回归语言模型在统一语音增强各子任务方面的有效性尚未得到验证。本研究提出UniSE——基于解码器架构的自回归语言模型统一框架，可同时处理语音修复、目标说话人提取与语音分离三类增强任务。该框架以输入语音特征为条件，通过自回归建模生成目标语音的离散表征，从而实现多任务学习范式的兼容性。在多个基准测试上的实验表明，UniSE相较判别式与生成式基线模型均能取得具有竞争力的性能，证明了语言模型在统一语音增强任务方面的潜力。演示页面详见：https://github.com/hyyan2k/UniSE。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强涵盖语音恢复、目标说话人提取和语音分离等子任务，传统深度神经网络方法虽有效，但缺乏统一框架。  
- **既有方法问题**：现有基于语言模型的方法多局限于单一任务或失真类型，且多采用掩码生成或直接映射范式，自回归建模在多任务统一中的潜力未充分验证。  

2)  
- **核心方法**：UniSE采用仅解码器自回归语言模型，通过任务令牌统一多任务框架：  
  - **特征提取**：使用冻结的WavLM与可训练适配器提取 degraded 和参考语音的连续特征。  
  - **离散化处理**：通过BiCodec将目标语音编码为全局和语义离散令牌，实现高保真重建。  
  - **多任务统一**：定义SR、TSE和rTSE三种模式，通过任务令牌切换输入序列格式，分别优化负对数似然损失函数。  
  - **推理策略**：针对不同任务组合模式，如语音分离时依次使用SR、TSE和rTSE模式确保说话人一致性。  

3)  
- **任务与效果**：  
  - **语音恢复**：在DNS和URGENT挑战赛测试集上取得SOTA或竞争性结果，显著提升信号质量与整体评分。  
  - **目标说话人提取**：在Libri2Mix上性能与先进方法相当，支持多任务扩展。  
  - **语音分离**：在Libri2Mix和WSJ0-2mix上超越判别式和生成式基线，验证多模式推理有效性。
</div>

</details>

---

## From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era
- **Authors**: Wonil Kim, Hyeongseok Wi, Seungsoon Park, Taejun Kim, Sangeun Keum, Keunhyoung Kim, Taewan Kim, Jongmin Jung, Taehyoung Kim, Gaetan Guerrero, Mael Le Goff, Julie Po, Dongjoo Moon, Juhan Nam, Jongpil Lee
- **Categories**: cs.IR, cs.HC, cs.MA, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.20276v1](http://arxiv.org/abs/2510.20276v1)
- **PDF**: [http://arxiv.org/pdf/2510.20276v1](http://arxiv.org/pdf/2510.20276v1)

生成式人工智能正在重塑音乐创作领域，但其快速发展也暴露出署名归属、权利管理和经济模式方面的结构性缺陷。与从现场表演到录音、下载及流媒体的传统媒介变迁不同，AI技术彻底改变了音乐的全生命周期，消融了创作、分发与盈利之间的界限。当前流媒体系统采用不透明且高度集中的版税分配机制，难以应对AI驱动生产带来的规模性与复杂性挑战。本文提出基于内容的音乐AI智能体架构，通过区块级检索与智能体协同机制，将署名归属直接嵌入创作流程。该系统针对迭代式会话交互设计，将音乐解构为存储于区块数据库的细粒度组件（区块），每次使用都会触发署名层事件以实现透明溯源与实时结算。该框架将AI从生成工具升级为公平AI媒体平台的基础设施，通过实现细粒度署名、公平补偿与参与式协作，指向后流媒体时代的新范式——音乐不再作为静态目录存在，而是演变为协同创新的自适应生态系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：生成式AI正重塑音乐创作，但现有系统（如文本到音乐平台）多为单次输出模式，缺乏迭代创作和贡献者署名机制。  
- **既有问题**：  
  - 流媒体经济模型不透明，版税分配集中于少数艺术家；  
  - AI规模化生产加剧了署名与权益管理的缺失，导致创作者贡献被淹没。  

2)  
- **核心方法**：提出基于内容的音乐AI代理架构，通过三大组件实现署名与经济模型的整合：  
  - **BlockDB**：将音乐分解为细粒度组件（如音轨片段），存储元数据与创作者信息；  
  - **交互式AI代理**：基于检索增强生成（RAG）技术，根据用户指令从BlockDB检索并组合音乐块，支持分层迭代创作；  
  - **实时署名层**：记录每个音乐块的使用事件，自动分配版税，确保创作过程与署名、结算同步。  
- **解决逻辑**：  
  - 将署名嵌入创作流程，替代传统流媒体的滞后统计；  
  - 通过细粒度组件追踪，实现按实际使用比例分配收益；  
  - 结合实时结算，构建“公平AI媒体平台”基础。  

3)  
- **任务与效果**：  
  - **音乐创作**：支持用户通过会话式交互分层构建曲目，降低创作门槛；  
  - **权益管理**：在组件级别生成可审计的署名记录，替代不透明的流媒体版税模型；  
  - **经济模型**：实现基于微结算的收益分配，推动粉丝从听众转变为共创参与者。
</div>

</details>

---

## Neural Directional Filtering with Configurable Directivity Pattern at Inference
- **Authors**: Weilong Huang, Srikanth Raj Chetupalli, Emanuël A. P. Habets
- **Categories**: eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.20253v1](http://arxiv.org/abs/2510.20253v1)
- **PDF**: [http://arxiv.org/pdf/2510.20253v1](http://arxiv.org/pdf/2510.20253v1)

本文提出一种支持用户自定义指向性模式的神经定向滤波方法（UNDF），可在推理阶段根据用户设定的指向模式实现空间滤波。为实现该功能，我们设计了融合特征线性调制（FiLM）的深度神经网络架构，使自定义模式可作为条件输入参与计算。理论分析表明：基于FiLM的架构使UNDF在干扰环境下具备泛化能力，可适应训练中未出现过的高指向性模式、尺度变化及不同转向角度。通过渐进式优化训练策略，系统在模式逼近精度上得到提升，并能有效拟合不规则形状。实验对比证明，UNDF在性能上超越传统方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **传统固定波束成形**：需要大量麦克风和大孔径阵列才能保证性能，且模式改变需重新计算滤波器。  
- **参数化滤波方法**：依赖准确的到达方向估计，在多源或含非语音信号环境中表现受限。  
- **现有深度学习方法**：多数聚焦语音分离，未显式控制方向性模式，或模式在推理时无法调整。

2)  
- **FiLM-JNF架构**：基于特征线性调制，将用户定义的方向性模式向量作为条件输入，通过仿射变换调制中间特征，实现模式自适应。  
- **训练策略优化**：  
  - 使用随机差分麦克风阵列模式及其线性组合，覆盖更广的阶数和转向角度。  
  - 引入矩形模式混合训练，提升对不规则形状的拟合能力。  
- **泛化能力**：FiLM机制使模型在推理时能适应未见过的高阶模式、缩放变化及转向角度，无需重新训练。

3)  
- **方向性模式拟合**：在多种模式上优于传统参数化滤波和基线神经网络方法，包括一阶/高阶、缩放及不规则形状模式。  
- **语音增强任务**：信号失真比显著提升，最高达26.8 dB，且在混响环境和音乐信号中泛化良好。  
- **动态控制**：支持实时调整时间变化模式，即时改变输出中各声源成分比例。
</div>

</details>

---

## Vox-Evaluator: Enhancing Stability and Fidelity for Zero-shot TTS with A Multi-Level Evaluator
- **Authors**: Hualei Wang, Na Li, Chuke Wang, Shu Wu, Zhifeng Li, Dong Yu
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.20210v1](http://arxiv.org/abs/2510.20210v1)
- **PDF**: [http://arxiv.org/pdf/2510.20210v1](http://arxiv.org/pdf/2510.20210v1)

尽管基于语言模型、扩散模型与掩码生成技术的零样本文本转语音（TTS）研究在语音自然度方面取得显著进展，其稳定性与保真度仍存在关键挑战，具体表现为发音错误、可闻噪声及音质退化。为解决这些问题，我们提出Vox-Evaluator——一种多层级评估器，通过定位错误语音片段的时间边界并提供生成语音的整体质量评估，指导TTS系统进行错误修正与偏好对齐。具体而言，为优化错误片段并增强零样本TTS模型的鲁棒性，我们提出基于评估器自动识别声学错误，对错误片段进行掩码处理，并基于正确片段重新生成语音。此外，Vox-Evaluator提供的细粒度信息可引导TTS模型进行偏好对齐，从而减少合成语音中的不良案例。针对现有训练数据集的不足，我们还构建了包含细粒度发音错误与音质问题标注的合成文本-语音数据集。实验结果表明，通过语音修正机制与偏好优化，所提出的Vox-Evaluator能有效提升TTS系统的稳定性与保真度。演示内容已公开。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：零样本文本转语音（TTS）技术基于语言模型、扩散模型和掩码生成方法，在语音自然度方面取得显著进展。  
- **既有问题**：  
  - 稳定性不足，表现为发音错误、可听噪声和异常停顿。  
  - 现有方法依赖外部工具（如ASR模型）定位错误，无法检测音频质量问题。  
  - 偏好对齐方法缺乏细粒度奖励，标注成本高且流程复杂。  

2)  
- **核心方法**：提出多级评估器Vox-Evaluator，通过错误检测与修正机制及细粒度偏好对齐提升系统性能。  
  - **错误检测与修正**：  
    - 评估器自动识别错误片段的时间边界和语义不匹配。  
    - 利用动态时间规整（DTW）优化文本对齐，定位发音错误。  
    - 对错误片段进行掩码，并基于正确部分重新生成语音，迭代修正提升稳定性。  
  - **偏好对齐**：  
    - 评估器作为细粒度奖励模型，提供语义正确性和音频质量评估。  
    - 通过直接偏好优化（DPO）聚焦错误片段计算损失，避免全局过优化。  
  - **数据支持**：构建FGES数据集，包含多类错误标注和质量评分，用于训练评估器。  

3)  
- **任务与效果**：  
  - **错误检测**：在FGES测试集上，时间范围预测IOU达0.782，优于基线19.7%；文本转录WER降至2.64%。  
  - **语音修正**：在Seed-TTS和LibriSpeech数据集上，F5-TTS和VoiceCraft修正后WER分别降低21%和32%，自然度（CMOS）和相似度（SIM-o）显著提升。  
  - **偏好对齐**：F5-TTS经DPO优化后，WER从1.73%降至1.55%，主观评分持续改善。
</div>

</details>

---

## SpeechAgent: An End-to-End Mobile Infrastructure for Speech Impairment Assistance
- **Authors**: Haowei Lou, Chengkai Huang, Hye-young Paik, Yongquan Hu, Aaron Quigley, Wen Hu, Lina Yao
- **Categories**: eess.SY, cs.SD, cs.SY
- **arXiv**: [http://arxiv.org/abs/2510.20113v1](http://arxiv.org/abs/2510.20113v1)
- **PDF**: [http://arxiv.org/pdf/2510.20113v1](http://arxiv.org/pdf/2510.20113v1)

语音是人类交流的基础，但数百万人因运动障碍性构音困难、口吃、失语症等言语障碍面临社交孤立与参与度下降的问题。尽管自动语音识别与文本转语音技术取得进展，面向言语障碍群体的可访问网络及移动基础设施仍显不足，阻碍了这些技术在日常沟通中的实际应用。为此，我们提出SpeechAgent——一套端到端移动系统，通过融合大语言模型的推理能力与先进语音处理模块，为不同障碍类型提供自适应支持。为确保实际可用性，我们构建了结构化部署流程，实现在移动及边缘设备上进行实时语音处理，在保持高精度与语音质量的同时达到无感知延迟。基于真实世界障碍语音数据集的评估及边缘设备延迟测试表明，SpeechAgent兼具高效性与用户友好性，验证了其在个性化日常辅助沟通场景中的可行性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：全球数百万人受构音障碍、口吃、失语症等言语障碍困扰，导致社交孤立与沟通困难。  
- **既有方法问题**：  
  - 传统辅助工具（如AAC设备、语法校正系统）依赖预定义规则，难以处理真实场景中动态、模糊的言语输入。  
  - 临床技术侧重长期康复训练，缺乏实时对话修复能力。  
  - 现有ASR模型仅用于诊断，无法将受损语音实时转换为清晰可懂的交流内容。  

2)  
- **核心方法**：SpeechAgent构建端到端移动架构，集成多模块协同工作：  
  - **语音障碍识别（SIR）**：通过Transformer模型分析梅尔频谱图，准确分类构音障碍、口吃、失语症等类型，为后续处理提供上下文。  
  - **语音识别（ASR）**：将受损语音转录为文本，保留原始语义信息。  
  - **语义重构**：利用大语言模型（LLM）分析转录文本，结合障碍类型推断说话者意图，生成连贯、忠实的修正文本，而非简单语法校正。  
  - **语音合成（TTS）**：通过ParaStyleTTS将修正文本转换为自然语音，支持个性化语调控制。  
- **系统部署**：采用客户端-服务器架构，移动端负责录音与播放，云端处理计算密集型任务，通过RESTful API实现低延迟实时交互。  

3)  
- **任务与效果**：  
  - **障碍识别**：在包含多种障碍类型的数据集上，分类准确率超90%（AUC>0.99），显著优于通用音频-文本对齐模型。  
  - **语音修正**：  
    - 文本重构：BERT分数提升至0.95以上，语义相似度较基线提升超20%。  
    - 端到端修正：人类评估显示修正后语音清晰度显著改善，CMOS评分证实其优越性。  
  - **实时性能**：处理11.5秒语音仅需0.91秒（RTF=0.08），满足日常交互的低延迟需求。
</div>

</details>

---
