---
layout: post
title: "arXiv Daily – 2026-01-06"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-06（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-05 08:50 — 2026-01-06 08:50
- 抓取总数：9 篇 | 本页显示：9 篇（去重/过滤后）

## DARC: Drum accompaniment generation with fine-grained rhythm control
- **Authors**: Trey Brosnan
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.02357v1](https://arxiv.org/abs/2601.02357v1)
- **PDF**: [https://arxiv.org/pdf/2601.02357v1](https://arxiv.org/pdf/2601.02357v1)

在音乐创作中，快速原型构建对于探索和完善创意至关重要，然而现有生成工具往往难以同时满足用户对结构控制与风格灵活性的需求。现有的音轨到音轨生成方法虽能基于其他音轨进行条件生成，但对节奏的控制有限；而音色转换方法虽允许用户指定具体节奏，却无法结合音乐上下文进行条件生成。本文提出DARC模型，该鼓伴奏生成模型既能基于其他音轨的音乐上下文，也能根据明确的节奏提示（如口技节拍或敲击音轨）进行生成。通过参数高效微调技术，我们在保持音乐上下文感知能力的同时，为当前最先进的鼓音轨生成模型STAGE增加了细粒度节奏控制功能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在音乐创作中，快速原型设计对探索和优化想法至关重要。现有生成工具在用户需要同时控制结构和风格灵活性方面存在不足。
- **既有方法的问题**：
  - 基于音轨到音轨的生成方法（如STAGE）可以基于其他音轨的音乐上下文生成鼓点，但对节奏的控制有限，通常仅提供基于音符起始点的粗略指导。
  - 基于音色转换的方法（如TRIA）允许用户指定具体节奏（如通过敲击或Beatboxing），但无法结合音乐上下文，且需要用户提供目标音色的音频样本，限制了迭代速度。
  - 现有方法在节奏控制上粒度较粗，无法同时保留节奏提示中的音色类别信息（如区分底鼓、军鼓）。

2)  
论文提出的DARC模型通过以下核心方法解决上述问题：

- **模型基础与目标**：DARC基于先进的鼓点音轨生成模型STAGE进行参数高效微调，旨在生成既能忠实遵循用户提供的节奏提示（包括音色类别），又能与输入的无鼓音乐混音保持良好音乐一致性的鼓点伴奏。

- **细粒度节奏特征表示**：
  - 使用**非负矩阵分解（NMF）** 处理节奏提示音频（如Beatboxing或敲击音轨）。
  - NMF将频谱图分解为基矩阵（编码音色信息）和激活矩阵（编码时序信息）。
  - DARC**仅使用激活矩阵**，得到包含每个音符起始时间和音色类别索引的MIDI类表示。这有效避免了“音色泄漏”，即生成的鼓点音色不受节奏提示原始音色的影响。
  - 音色类别（如底鼓、军鼓、踩镲）通过按总能量降序排序自动推断，无需用户指定。

- **条件生成机制**：
  - **音乐上下文条件化**：沿用STAGE的**前缀条件化**方法，将无鼓混音的音频token作为前缀输入，确保生成内容与整体音乐协调。
  - **节奏提示条件化**：通过**跳跃微调**和**自适应注意力内嵌**这两种参数高效微调技术，将NMF提取的节奏特征嵌入到Transformer解码器的特定自注意力层中，实现对生成节奏的细粒度控制。

- **训练与数据**：
  - 使用Demucs从FMA数据集中提取鼓点音轨构建数据集。
  - 训练时冻结STAGE大部分参数，仅微调少量层，大幅减少可训练参数量。
  - 对音乐上下文和节奏提示进行数据增强（如变速、移调、添加噪声），以提高模型鲁棒性。

该方法的核心创新在于，**通过NMF特征在保留节奏细节（包括音色类别）的同时，利用音乐上下文自动决定合适的鼓点音色**，从而免除了用户提供音色提示的负担，实现了控制粒度与创作便捷性的平衡。

3)  
论文在以下任务上评估了DARC的效果，但结果受限于输出音频质量：

- **节奏提示遵循度**：在Tap2Drum任务上，使用AVP Beatbox数据集评估。衡量指标包括起始点F1分数（时序准确性）以及底鼓和军鼓的F1分数（音色类别准确性）。**DARC的定量结果低于基线模型TRIA和STAGE**，作者认为这主要源于模型输出音频质量差（含有噪声和伪影），影响了评估模型的准确性。

- **音乐一致性**：在MUSDB18数据集上，通过计算生成鼓点音轨与无鼓混音之间的COCOLA分数来评估。**DARC的COCOLA分数显著低于STAGE和真实鼓点音轨**。同样，音频质量问题被认为是主要原因。有趣的是，STAGE的分数甚至略高于真实音轨，这暴露出COCOLA指标可能对过度装饰的鼓点产生偏好，未能完全符合人类听感。

- **定性观察**：尽管定量结果不佳，但作者定性观察认为DARC在遵循节奏提示方面表现尚可。论文主要贡献在于提出了结合细粒度节奏控制与音乐上下文条件化的框架，并揭示了当前评估指标在应对低音频保真度时的局限性。
</div>

</details>

---

## On the Role of Spatial Features in Foundation-Model-Based Speaker Diarization
- **Authors**: Marc Deegen, Tobias Gburrek, Tobias Cord-Landwehr, Thilo von Neumann, Jiangyu Han, Lukáš Burget, Reinhold Haeb-Umbach
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.02231v1](https://arxiv.org/abs/2601.02231v1)
- **PDF**: [https://arxiv.org/pdf/2601.02231v1](https://arxiv.org/pdf/2601.02231v1)

近年来，说话人日志研究通过利用WavLM等大规模预训练基础模型，在多个数据集上取得了领先性能。以DiariZen为代表的系统虽然能够充分利用这些丰富的单通道表征，但其仅限于处理单通道音频，无法利用多通道录音中可用的空间线索。本研究通过评估多种将多通道空间特征融入模型的方法，分析了空间信息对当前先进单通道日志系统性能的影响。在会议风格数据集上的实验表明，空间信息能够提升日志性能，但整体改进幅度小于预期。这表明，WavLM各层聚合得到的特征已能充分捕捉说话人区分所需的信息，即使在语音重叠区域也是如此。这些发现揭示了利用空间线索增强基于基础模型的说话人日志系统的潜力与局限性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：说话人日志任务旨在确定“谁在何时说话”。当前最先进的系统（如DiariZen）利用基于单通道音频预训练的基础模型（如WavLM）的丰富声学特征，取得了优异性能。  
- **既有方法的问题**：这些系统仅限于处理单通道音频，无法利用多通道录音中可用的空间线索（如声源方位）。而空间信息在处理重叠语音时尤为有益，因为纯频谱系统在此场景下常表现不佳。

2)  
- **核心方法**：本文提出了一种“空间辅助网络”，将其集成到基于DiariZen的说话人日志框架中，旨在为单通道WavLM特征补充空间信息。  
- **具体实现**：
  - **空间特征提取**：从多通道输入中计算跨通道相位差（IPD）和幅度谱作为空间特征。
  - **辅助网络设计**：设计了两种变体：
    - **空间编码器**：通过共享权重的自注意力层处理空间特征，输出单通道空间嵌入。
    - **空间Conformer及空间日志网络**：在编码器后添加类似DiariZen的结构，可预训练为一个独立的空间日志模块。
  - **特征融合**：将辅助网络输出的空间嵌入通过FiLM层注入到DiariZen的Conformer模块中，实现对WavLM特征的条件调制。
- **解决思路**：该方法显式地提取并融合空间线索，使系统能同时利用WavLM的语义/声学表示和多通道的空间信息，且设计上对麦克风数量和阵列几何结构不可知，保持了通用性。

3)  
- **任务与效果**：在多个会议风格数据集（AMI、AliMeeting、AISHELL-4、NOTSOFAR-1）上评估系统性能。
- **主要结果**：
  - 使用预训练的空间日志模块作为辅助网络（DiariZen + Spatial Diarization）并联合微调后，在Oracle聚类设置下，宏观平均DER从基线12.2%降至11.6%，提升了0.6个百分点。
  - 改进在所有数据集上保持一致，且在使用VBx聚类的完整流程中同样有效。
  - 然而，改进幅度小于预期，尤其在重叠语音区域。这表明WavLM特征本身已捕获了区分说话人的关键信息，空间线索的补充增益有限。
</div>

</details>

---

## ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging
- **Authors**: Omer Nacar, Serry Sibaee, Adel Ammar, Yasser Alhabashi, Nadia Samer Sibai, Yara Farouk Ahmed, Ahmed Saud Alqusaiyer, Sulieman Mahmoud AlMahmoud, Abdulrhman Mamdoh Mukhaniq, Lubaba Raed, Sulaiman Mohammed Alatwah, Waad Nasser Alqahtani, Yousif Abdulmajeed Alnasser, Mohamed Aziz Khadraoui, Wadii Boulila
- **Categories**: cs.CL, cs.CY, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.02209v1](https://arxiv.org/abs/2601.02209v1)
- **PDF**: [https://arxiv.org/pdf/2601.02209v1](https://arxiv.org/pdf/2601.02209v1)

阿拉伯语以其丰富的地区方言为特征，这些方言在语音和词汇上存在显著差异，反映了使用者在地理和文化上的多样性。尽管已有许多多方言数据集，但将语音映射到细粒度方言来源（如城市级别）的研究仍显不足。本文介绍了ARCADE（阿拉伯语广播音频方言评估语料库），这是首个专门设计用于城市级别方言细粒度分析的阿拉伯语语音数据集。该语料库收集了来自阿拉伯世界流媒体服务的阿拉伯语广播语音。我们的数据处理流程从经过验证的广播流中截取30秒片段，涵盖现代标准阿拉伯语（MSA）及多种方言语音。为确保可靠性，每个片段由一至三位阿拉伯语母语评审员标注，包含丰富元数据，如情感、语音类型、方言类别以及方言识别任务的有效性标志。最终语料库包含6,907条标注和3,790个独立音频片段，覆盖19个国家的58个城市。这些细粒度标注支持鲁棒的多任务学习，为城市级别方言标注提供了基准。我们详细阐述了数据收集方法，评估了音频质量，并对标签分布进行了全面分析。数据集可通过以下链接获取：https://huggingface.co/datasets/riotu-lab/ARCADE-full

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：阿拉伯语方言众多，现有研究多集中于国家或区域级别的粗粒度方言识别，缺乏城市级别的细粒度标注数据。  
- **既有方法的问题**：  
  - 现有数据集（如ADI17、MGB系列）标签粒度粗，仅标注国家或大区域，无法支持城市级方言分析。  
  - 标签质量参差不齐，许多依赖弱监督（如频道元数据），引入噪声。  
  - 缺乏对同一国家内城市间方言差异的覆盖，限制了细粒度建模与应用开发。  

2)  
论文通过构建**ARCADE**语料库解决上述问题，其核心方法包括：  
- **细粒度数据收集**：  
  - 从Radio Garden等公开流媒体平台自动录制阿拉伯世界广播电台的30秒音频片段。  
  - 覆盖**19个国家、58个城市**，确保每个城市至少10段录音，实现城市级地理粒度。  
- **高质量人工标注**：  
  - 由11名母语为阿拉伯语的标注员对每段音频进行1-3次独立标注。  
  - 标注内容不仅包括方言类别（现代标准阿拉伯语、方言、混合语等），还涵盖**情感、语音类型、有效性标志及置信度**等多维度元数据。  
  - 通过定制化Gradio标注界面和多人标注机制，确保标签的可靠性与一致性。  
- **数据质量控制**：  
  - 标注过程中过滤了古兰经诵读、音乐、多人交谈等不适用于方言识别的片段。  
  - 对音频信号质量（如信噪比、动态范围）进行量化分析，验证数据可用性。  
- **语料库特点**：  
  - 包含3,790段独特音频和6,907条标注，其中65.7%的片段被保留用于方言识别任务。  
  - 首次提供城市级方言标签，支持细粒度方言建模、多任务学习及跨领域鲁棒性研究。  

3)  
- **任务**：细粒度阿拉伯语方言识别，具体为**城市级别方言标注**。  
- **效果**：  
  - 构建了首个城市级细粒度阿拉伯语语音语料库，覆盖58个城市，为方言溯源、社会语言学分析等提供了基准数据。  
  - 通过高质量人工标注（91.9%的标注为高置信度），解决了现有数据标签噪声问题。  
  - 语料库支持多任务学习（如联合预测方言、情感、语音类型），为后续模型开发奠定了基础。  
  - **注**：论文侧重于数据集的构建与描述，未报告具体识别性能指标，建模与评估留待未来工作。
</div>

</details>

---

## Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation
- **Authors**: Steffen Freisinger, Philipp Seeberger, Thomas Ranzenberger, Tobias Bocklet, Korbinian Riedhammer
- **Categories**: cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.02128v1](https://arxiv.org/abs/2601.02128v1)
- **PDF**: [https://arxiv.org/pdf/2601.02128v1](https://arxiv.org/pdf/2601.02128v1)

将语音转录文本按主题进行分段，既有利于下游处理，也能提升依赖文本获取信息的用户体验。本文提出一种新颖的层次化主题分割方法，可生成捕捉主题与子主题边界的多级目录结构。我们比较了大型语言模型的零样本提示与LoRA微调策略，并探索了高层级语音停顿特征的融合效果。在英语会议录音及多语言（葡萄牙语、德语）讲座转录文本上的实验表明，该方法相比现有主题分割基线模型有显著提升。此外，我们针对多级分割任务改进了通用评估指标，使其能够通过单一度量综合考量所有层次级别。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音转录本（如讲座、会议记录）缺乏书面文本固有的章节结构，这给用户信息查找和下游任务（如信息检索）带来困难。  
- **既有方法问题**：  
  - 传统方法多关注**线性分割**，仅按时间顺序划分段落，忽略了主题常存在的**多层次粒度**（如主主题与子主题）。  
  - 现有分割方法大多**忽视层次结构**，无法生成反映主题嵌套关系的目录，导致结构清晰度不足。  

2)  
- **核心方法**：提出 **ToC-LLM**，利用大语言模型（LLM）直接从语音转录本生成**多层次目录**，实现层次化主题分割。  
- **具体方案**：  
  - **模型与微调**：采用 **Mistral Nemo** 和 **Qwen2.5** 等LLM，通过 **LoRA（低秩适应）微调** 进行领域适配，在保持模型主体权重不变的情况下更新低秩适配器，以降低计算开销。  
  - **提示设计**：使用系统提示指令模型输出完整目录，输入按句子索引排列，输出通过点号编号（如“2.2.1”）**编码层次级别**，并包含主题名称和对应句子索引。  
  - **多模态融合**：**可选融入语音停顿特征**，在句子行中添加停顿时长标注（如“pause=0.62s”），利用停顿作为主题边界的声学线索提升检测效果。  
  - **层次化评估**：提出 **层次化边界相似度（Bhier）** 度量，通过动态规划对齐参考与假设的多个层次，综合评估嵌套分割质量，避免仅评估单一层次。  

3)  
- **任务与效果**：在**英语会议（AMI）** 和**多语言讲座（葡萄牙语VIDEOAULA、德语LECTUREDE）** 转录本上进行实验。  
  - **线性分割**：微调后的 **TOC-NEMO（融入停顿）** 在全部数据集上取得最佳F1和边界相似度（B）分数，显著超越传统基线（如TextTiling、MINISEG）及零-shot LLM方法。  
  - **层次分割**：通过 **Bhier** 验证了方法能生成有意义的嵌套结构，在层次更深的LECTUREDE数据集上，层次化指标优于线性指标，表明模型成功捕获了粗粒度主题与细粒度子主题。
</div>

</details>

---

## A Mamba-Based Model for Automatic Chord Recognition
- **Authors**: Chunyu Yuan, Johanna Devaney
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.02101v1](https://arxiv.org/abs/2601.02101v1)
- **PDF**: [https://arxiv.org/pdf/2601.02101v1](https://arxiv.org/pdf/2601.02101v1)

本研究提出了一种基于Mamba架构的高效自动和弦识别模型BMACE（双向Mamba自动和弦估计网络）。该模型通过在双向Mamba层中采用选择性结构化状态空间机制，有效建模音频信号中的时序依赖关系。实验表明，本模型在保持与前沿方法相当预测性能的同时，显著减少了参数量并降低了对计算资源的需求。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动和弦识别（ACE）是音乐信息检索（MIR）中的长期任务。尽管深度学习技术带来了显著提升，但现有方法仍面临性能瓶颈。
- **既有方法的问题**：
  - **性能天花板**：即使最先进的模型（如基于Transformer的模型）也难以突破性能上限。
  - **计算效率低**：Transformer模型因其二次复杂度，在输入序列较长时计算开销大、内存占用高，难以应用于实时或嵌入式设备等低延迟场景。

2)  
论文提出了一种名为BMACE（基于双向Mamba的自动和弦估计网络）的新模型，其核心方法通过引入选择性结构化状态空间模型（SSM）来解决上述问题，具体体现在：
- **架构创新**：
  - 采用**双向Mamba层**，通过两个方向相反的Mamba块处理序列，有效建模音频信号的前后依赖关系。
  - 用**选择性状态空间模型（SSM）** 替代Transformer中的注意力机制和MLP块。SSM能根据当前输入选择性处理信息，过滤无关数据，提升处理效率。
  - 模型结构更简洁，仅使用统一的SSM块，减少了参数数量和计算复杂度。
- **效率优化**：
  - **线性复杂度**：Mamba架构使其计算复杂度随序列长度线性增长，避免了Transformer的二次复杂度问题。
  - **硬件感知并行**：采用专门设计的并行算法优化循环操作，提升硬件效率，进一步加速推理。
  - **轻量化设计**：通过残差连接和全连接层保持信息流，在减少参数的同时维持模型表达能力。

3)  
- **任务**：在自动和弦识别（ACE）任务上，使用`uspops2002`数据集，评估了25标签（maj-min）和170标签（大词汇表）两种和弦类型。
- **效果**：
  - **性能**：BMACE在多项指标（如Root、Maj-min、Triads等）上达到或略微超过最先进的CRNN和Transformer（BTC）模型，性能相当。
  - **效率**：所有Mamba变体（包括BMACE）的参数数量仅为Transformer模型的约1/25，计算量（GFlops）显著降低，同时模型尺寸比CRNN小约1/3。
</div>

</details>

---

## BeatlesFC: Harmonic function annotations of Isophonics' The Beatles dataset
- **Authors**: Ji Yeoung Sim, Rebecca Moranis, Johanna Devaney
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.02099v1](https://arxiv.org/abs/2601.02099v1)
- **PDF**: [https://arxiv.org/pdf/2601.02099v1](https://arxiv.org/pdf/2601.02099v1)

本文介绍了BeatlesFC——一套针对Isophonics披头士数据集的和声功能标注系统。该标注体系将和弦标记归类为稳定功能（主功能）或不稳定功能（下属功能、属功能），其标注层级聚焦于乐句层面，在具体和弦标记与更高层级的曲式结构之间建立起分析桥梁。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：Isophonics披头士数据集已包含和弦、节拍、调性、曲式等多层标注，但缺乏连接低层（和弦）与高层（曲式）的**和声功能**标注。  
- **既有方法问题**：和声功能分析在西方古典音乐符号数据集中已有应用，但在基于音频的流行音乐数据集中尚未广泛包含，导致难以分析流行音乐中从稳定到不稳定再回归稳定的和声进行模式。  

2)  
- **核心方法**：基于Nobile针对摇滚音乐的**功能环路**理论进行标注，将和弦功能分为稳定主功能、不稳定下属功能与属功能。  
- **解决思路**：  
  - **适配流行音乐**：该理论根植于申克分析，但专为摇滚音乐设计，允许非传统和弦（不限于V级）承担属功能，反映摇滚和声实践。  
  - **连接多层次**：在Isophonics原有和弦、调性、结构标注基础上，为每个和弦标注功能，使其成为连接低层和弦与高层曲式的桥梁。  
  - **标注流程**：由两位音乐理论博士生使用Sonic Visualizer，结合音频与原有标注，为179首可辨调性的歌曲逐和弦标注功能，并通过交叉校验保证质量。  
  - **处理特殊案例**：排除无明确调性的歌曲，但保留三首受拉格影响的歌曲以探索理论边界，并保留标注者分歧版本供研究。  

3)  
- **任务**：为Isophonics披头士数据集提供和声功能标注。  
- **效果**：  
  - 完成了179首歌曲（共180首，排除1首）的14,132个功能标注，主功能占比70.3%，下属功能16.5%，属功能13.2%，符合摇滚和声从稳定到不稳定再回归的环路特征。  
  - 标注数据已公开，包含时间边界与分歧版本，支持音乐信息检索、和声分析与音乐教育等任务。
</div>

</details>

---

## Towards Prosodically Informed Mizo TTS without Explicit Tone Markings
- **Authors**: Abhijit Mohanta, Remruatpuii, Priyankoo Sarmah, Rohit Sinha, Wendy Lalhminghlui
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.02073v1](https://arxiv.org/abs/2601.02073v1)
- **PDF**: [https://arxiv.org/pdf/2601.02073v1](https://arxiv.org/pdf/2601.02073v1)

本文介绍了一种面向米佐语的文本转语音（TTS）系统开发工作。米佐语是一种资源稀缺、具有声调特征的藏缅语系语言，主要使用于印度米佐拉姆邦。该TTS系统仅使用5.18小时数据构建，但通过主客观评估表明，其合成语音在感知可接受度和清晰度方面均表现良好。研究首先基于Tacotron2构建基线模型，随后使用相同数据构建了基于VITS的TTS模型。在主客观评估中，VITS模型均优于Tacotron2模型，尤其在声调合成方面，VITS模型的声调错误率显著更低。本研究证明，基于非自回归的端到端框架能够实现感知质量与清晰度俱佳的语音合成效果。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：米佐语是一种低资源、有声调的藏缅语系语言，其正字法不显式标记声调。开发高质量的文本转语音系统面临数据稀缺和声调建模的挑战。
- **既有方法问题**：传统的自回归TTS模型（如Tacotron2）需要大量数据且推理速度慢，在低资源场景下难以有效学习复杂的声调模式，导致合成语音的韵律和声调准确性不足。

2)  
- **采用VITS框架**：论文使用基于变分推理和对抗学习的端到端非自回归模型VITS。该框架直接生成波形，无需额外声码器，并接受原始文本输入，简化了处理流程。
- **关键优势**：
  - **低资源适应性**：非自回归结构使其在仅5.18小时数据上也能高效训练，推理速度快。
  - **声调建模能力**：通过联合优化声学表示和波形生成，结合对抗训练和多尺度判别器，VITS能更好地捕捉米佐语的声调特征，即使没有显式声调标注。
  - **技术细节**：使用Conformer文本编码器、后验编码器、标准化流和HiFi-GAN解码器，并融合对抗损失、梅尔谱损失、特征匹配损失等，提升合成语音的自然度和声调准确性。
- **对比基线**：与需要两阶段流程（声学模型+声码器）的Tacotron2相比，VITS的端到端设计减少了误差传播，在有限数据下实现了更优的韵律和声调合成。

3)  
- **任务与效果**：在米佐语TTS任务上，使用5.18小时数据训练，通过主客观评估验证效果：
  - **客观指标**：VITS在DNSMOS（3.90 vs. 3.81）、MCD（2.32 dB vs. 2.53 dB）和声调错误率（5.67% vs. 12.93%）上均优于Tacotron2。
  - **主观评估**：35名母语者评分显示，VITS的MOS得分（3.46）显著高于Tacotron2（2.63），且41%的VITS合成句被误判为自然语音，接近自然语音的76%。
- **结论**：VITS在低资源条件下能合成感知质量可接受、可懂度高的米佐语语音，显著提升了声调准确性。
</div>

</details>

---

## MORE: Multi-Objective Adversarial Attacks on Speech Recognition
- **Authors**: Xiaoxue Gao, Zexin Li, Yiming Chen, Nancy F. Chen
- **Categories**: eess.AS, cs.AI, cs.LG
- **arXiv**: [https://arxiv.org/abs/2601.01852v1](https://arxiv.org/abs/2601.01852v1)
- **PDF**: [https://arxiv.org/pdf/2601.01852v1](https://arxiv.org/pdf/2601.01852v1)

随着Whisper等大规模自动语音识别（ASR）模型的出现，其在各类实际应用中的普及度显著提升。因此，确保模型在面对微小输入扰动时仍保持鲁棒性，对于在实时环境中维持可靠性能至关重要。现有研究主要关注对抗攻击下的识别准确率下降，而模型在效率方面的鲁棒性尚未得到充分探索。这种局限性导致对ASR模型脆弱性的理解尚不全面。为弥补这一空白，本研究对多种攻击场景下的ASR鲁棒性进行了系统性分析。我们提出了MORE（多目标重复倍增激励攻击），该方法通过分层递进的排斥-锚定机制，同步降低识别准确率与推理效率。具体而言，我们将多目标对抗优化重构为分层框架，依次实现双重攻击目标。为进一步增强攻击效果，我们提出了一种新颖的重复激励倍增目标，在维持准确率下降的同时，通过周期性倍增预测序列长度，诱导模型生成重复文本。总体而言，MORE能够通过单一对抗样本，迫使ASR模型以显著更高的计算成本产生错误转写结果。实验表明，与现有基线方法相比，MORE在保持高词错误率的同时，持续生成显著更长的转写文本，充分证明了其在多目标对抗攻击中的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：以Whisper为代表的大规模自动语音识别模型应用广泛，但其在现实环境中的鲁棒性至关重要。现有研究主要关注对抗攻击下的**识别准确率**下降，而**推理效率**（如解码时间、输出长度）的鲁棒性在很大程度上被忽视。这种单一维度的评估无法全面揭示ASR模型的脆弱性。
- **既有方法的问题**：现有攻击方法（如PGD、MI-FGSM等）大多仅优化准确率下降这一目标；唯一专注于效率攻击的SlothSpeech则未联合考虑准确率退化，也未系统探索对抗输出的结构化模式，导致对ASR模型的多维鲁棒性评估不足。

2)  
论文提出MORE方法，通过**分层多目标优化**解决上述问题。其核心机制如下：

- **分层两阶段攻击框架**：
    - **排斥阶段**：首先使用基于交叉熵的梯度攻击最大化词错误率，破坏模型识别准确率，为后续攻击准备“不稳定”的解码轨迹。
    - **锚定阶段**：在保持高错误率的基础上，专门针对效率进行优化，诱导模型生成极长的转录文本。

- **效率攻击的核心组件**：
    - **EOS抑制**：通过降低结束符的概率并提升次优候选词的概率，阻止解码提前终止。
    - **重复鼓励倍增目标**：受Transformer模型重复循环现象启发，REDO周期性地将已解码片段复制并作为目标，通过交叉熵损失鼓励模型输出重复的语义内容，从而结构化地延长输出序列。
    - **非对称交错机制**：将长序列生成任务分解为课程式子问题，仅在特定周期更新倍增目标，其余步骤进行稳定优化，避免了长时域优化的不稳定性。

- **统一优化**：算法将两阶段与上述组件整合，通过分层策略避免了准确率与效率梯度在同步优化时的竞争，实现了稳定且高效的多目标对抗攻击。

3)  
- **任务**：在**白盒设置**下对Whisper系列模型进行对抗攻击评估，使用了LibriSpeech和LJ-Speech数据集。
- **效果**：
    - **效率攻击**：MORE生成的转录文本平均长度显著超过所有基线（例如，在Whisper-large上达到约300词，是仅关注准确率的基线的约10倍，也是SlothSpeech的约3.8倍）。
    - **准确率攻击**：在产生超长输出的同时，保持了与先进准确率攻击方法相当的高词错误率。
    - **综合表现**：MORE首次实现了对ASR模型**准确率与效率**的联合有效破坏，揭示了模型的双重脆弱性。
</div>

</details>

---

## HyperCLOVA X 8B Omni
- **Authors**: NAVER Cloud HyperCLOVA X Team
- **Categories**: cs.LG, cs.AI, cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.01792v1](https://arxiv.org/abs/2601.01792v1)
- **PDF**: [https://arxiv.org/pdf/2601.01792v1](https://arxiv.org/pdf/2601.01792v1)

本报告介绍了HyperCLOVA X 8B Omni，这是HyperCLOVA X系列中首个支持文本、音频和视觉任意模态输入输出的全模态模型。该模型将多模态理解与生成任务整合于单一架构，而非依赖独立的模态专用流程，从而成为面向实用化任意模态交互助手的80亿参数级全路径探索基准。在架构层面，模型通过共享的跨模态序列下一词元预测接口实现模态统一，同时利用视觉与音频编码器注入连续嵌入以支持细粒度理解与语义对齐。实证评估表明，在涵盖韩语与英语的文本、音频及视觉多样化输入输出组合任务中，该模型均展现出与同规模模型相当的性能竞争力。我们预计HyperCLOVA X 8B Omni的开放权重发布将为广泛的研究与应用部署提供支持。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现实应用需要AI能跨文本、音频、视觉模态进行理解与生成。现有方法通常基于大型语言模型（LLM），通过依次添加各模态的编码器/解码器来扩展，但这种方法在多模态训练中容易导致LLM主干发生**灾难性遗忘**，丢失原有的文本知识。  
- **既有问题**：这种分阶段、分模态的扩展策略难以实现真正的跨模态语义组合，且无法在一个统一框架内支持任意的输入-输出模态组合。

2)  
论文提出了 **HyperCLOVA X 8B Omni**，这是一个统一的、支持文本、音频、视觉任意输入输出的全模态模型。其核心方法通过以下设计解决上述问题：  

- **统一的序列建模框架**：采用仅解码器的Transformer主干，将不同模态的离散token和连续嵌入在**交织的多模态序列**中共同处理。所有模态共享同一个**下一token预测**接口，实现了跨模态的语义组合与生成。  
- **双路径表示**：  
  - **离散token**：将视觉和音频的编码器输出量化为离散语义token，与文本token共同构成扩展的词表，使模型能通过自回归方式生成多模态序列。  
  - **连续嵌入**：通过视觉和音频编码器注入细粒度的连续嵌入，以增强感知理解和对齐（grounding）。  
- **模态特定编解码器**：  
  - **编码器**：视觉采用ViT架构，音频基于Whisper-large-v3，输出连续特征并与主干嵌入空间对齐。  
  - **解码器**：视觉生成使用基于扩散的decoder，从语义token重建像素；音频生成使用Unit-BigVGAN，从离散音频token合成波形。两者均与主干解耦，但通过共享序列表示连接。  
- **渐进式训练策略**：  
  - 先进行文本预训练，再分阶段引入多模态离散token和连续编码器，通过精心设计的数据混合比例和损失掩码，**有效缓解了灾难性遗忘**，同时建立了跨模态的协同能力。  

该方法将理解与生成统一在单一模型中，实现了端到端的任意模态到任意模态的转换。

3)  
模型在以下任务上取得了具有竞争力的效果：  
- **文本任务**：在韩语（KMMLU-pro、HAERAE）和英语（MMLU、GSM8K）的理解与推理基准上显著优于同类规模模型。  
- **视觉任务**：在韩语（KoNET、K-MMBench）和英语（SEED-IMG、LLaVA-W）的视觉问答、文档理解等任务上表现优异；在文本到图像生成（GenEval）和图像编辑（ImgEdit）任务上也取得领先或接近最佳的结果。  
- **音频任务**：在韩语和英语的语音识别（WER）、音频描述（SPIDEr）、语音翻译（ASR-BLEU）等任务上达到先进水平；在文本到语音（TTS）的人类评估中，尤其在韩语上获得了高自然度评分。  
模型是**唯一支持全部输入-输出模态组合**的8B规模模型，并在多语言环境下验证了其有效性。
</div>

</details>

---
