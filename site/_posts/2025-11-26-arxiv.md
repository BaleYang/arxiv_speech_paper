---
layout: post
title: "arXiv Daily – 2025-11-26"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-11-26（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-11-25 08:50 — 2025-11-26 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model
- **Authors**: Genís Plaja-Roglans, Yun-Ning Hung, Xavier Serra, Igor Pereira
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2511.20470v1](https://arxiv.org/abs/2511.20470v1)
- **PDF**: [https://arxiv.org/pdf/2511.20470v1](https://arxiv.org/pdf/2511.20470v1)

从音乐混合信号中提取独立声源是音乐制作与实践的重要工具。当前主流方法采用经优化的神经网络对混合声谱进行掩码处理或将其转换为独立声源，但音乐信号中存在的声源重叠与相关性构成了固有挑战。此外，获取混合信号中全部声源对于训练此类系统至关重要，却存在实际困难。现有生成式方法虽尝试解决这些难题，但在分离性能与推理效率方面仍存在局限。本研究探索扩散模型在填补这一空白方面的潜力，聚焦于仅需孤立人声与混合信号配对数据即可训练的生成式人声分离方法。为契合创作流程，我们采用潜在扩散机制：系统在压缩的潜在空间中生成编码样本，随后将其解码为音频。该设计实现了高效优化与快速推理。本系统完全基于开放数据训练，在多项信号质量指标与干扰消除任务上超越现有生成式分离系统，并达到与非生成式系统相当的水平。通过对潜在编码器进行噪声鲁棒性研究，揭示了其在该任务中的应用潜力。我们同步发布模块化工具包以促进后续研究。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**  
- 音乐源分离任务中，传统方法依赖神经网络对混合音频的频谱进行掩码或变换，但面临以下挑战：  
  - 音乐信号中源之间的重叠与相关性导致分离性能受限。  
  - 训练需获取混合音频中所有源的独立数据，成本高昂且复杂。  
- 现有生成式方法（如GAN、VAE）在分离性能和推理效率上表现不足，难以满足实际应用需求。  

2)  
**论文核心方法如何解决上述问题**  
- **潜在扩散模型（LDM）框架**：  
  - 在EnCodec神经编解码器的潜在空间中进行扩散过程，将高维音频压缩为紧凑的潜在表示，显著提升训练和推理效率。  
  - 使用v-目标扩散方法，通过预测速度参数（velocity）稳定训练，并利用DDIM采样器减少迭代步骤（T=50），加快生成速度。  
- **条件生成机制**：  
  - 以混合音频的潜在编码作为条件信号，引导扩散过程生成目标歌声，降低伴奏干扰。  
  - 采用两阶段训练：先冻结条件编码器预训练生成器，再微调编码器以优化分离效果。  
- **模型架构设计**：  
  - 使用一维卷积U-Net处理潜在向量，避免二维卷积引入的伪影，增强时序依赖性建模。  
  - 通过残差连接、注意力机制和多尺度条件注入，平衡局部特征与全局上下文信息。  
- **数据与效率优化**：  
  - 仅需成对的歌声与混合音频数据（如musdb18hq），无需全部分离源标签。  
  - 潜在空间操作减少计算资源需求，推理速度优于其他生成式基线（如MSDM）。  

3)  
**在哪些任务上取得了怎样的效果**  
- **任务**：歌声分离（从音乐混合音频中提取人声）。  
- **效果**：  
  - 客观指标：在LSD、Mel-MAE和基频误差上优于生成式基线（如InstGlow、MSDM），与非生成式方法（如BS-RNN）性能接近。  
  - 主观评估：在干扰去除方面表现最佳（MOS≈4.0），但语音清晰度（MOS≈2.76）和信号质量（MOS≈3.43）略低于非生成式模型，因高频伪影和解码误差存在改进空间。  
  - 效率：推理速度显著提升（1.74秒），适用于音乐创作场景。
</div>

</details>

---

## Differentiable Attenuation Filters for Feedback Delay Networks
- **Authors**: Ilias Ibnyahya, Joshua D. Reiss
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2511.20380v1](https://arxiv.org/abs/2511.20380v1)
- **PDF**: [https://arxiv.org/pdf/2511.20380v1](https://arxiv.org/pdf/2511.20380v1)

本文提出一种基于反馈延迟网络（FDN）的数字音频混响系统衰减滤波器设计新方法。通过将二阶节无限冲激响应滤波器配置为参量均衡器结构，实现了对频率相关混响衰减特性的精细控制。与传统图示均衡器方案需为每条延迟线配置大量滤波器不同，本方法采用可扩展架构，滤波器数量可灵活调整。所有延迟线共享频率、增益和品质因数参数，仅根据延迟长度调整增益值。该设计不仅显著减少了优化参数数量，还保持完全可微特性，兼容基于梯度的学习框架。借鉴模拟滤波器设计原理，本方法通过监督学习实现高效精确的滤波器拟合，在显著降低计算成本的同时，提供了灵活可微的设计方案，并达到当前最优性能水平。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：FDN是人工混响常用技术，传统方法使用图形均衡器控制频域衰减，但存在以下问题：
  - 图形均衡器在长延迟线中匹配效果差，且DC与Nyquist频率处响应失配。
  - 现有方法参数空间维度高，难以实现端到端可微分优化。
  - 基于深度学习的IIR滤波器设计缺乏对全频带衰减时间的统一控制机制。

2.  
- **核心方法**：提出基于参数均衡器的可微分衰减滤波器设计，具体包括：
  - **滤波器结构**：使用二阶节构建参数均衡器，包含钟形、低架和高架滤波器；滤波器数量可调，显著减少计算量。
  - **参数共享**：频率、Q值为全局共享参数，仅增益根据延迟线长度调整，大幅降低优化参数数量。
  - **可微分优化**：在PyTorch中实现全可微分模型，通过监督回归（MSE损失）联合优化所有滤波器参数，支持梯度下降训练。
  - **频谱精度**：基于模拟滤波器原型和双线性变换，改进高频精度，避免Nyquist频率附近失真。
  - **计算效率**：例如8阶FDN中，12波段设计仅需96个双二阶滤波器，比31波段图形均衡器减少约2/3。

3.  
- **任务与效果**：
  - **任务**：在房间脉冲响应数据集上匹配频率相关的混响时间曲线。
  - **效果**：
    - 12波段PEQ在512,000个数据点上达到接近31波段TSAF的精度（MSE 1.7×10⁻³），误差分布集中于零附近。
    - 计算成本降低62%，参数数量与TSAF相当（36 vs 33），适合实时和嵌入式应用。
</div>

</details>

---

## DUO-TOK: Dual-Track Semantic Music Tokenizer for Vocal-Accompaniment Generation
- **Authors**: Rui Lin, Zhiyue Wu, Jiahe Le, Kangdi Wang, Weixiong Chen, Junyu Dai, Tao Jiang
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2511.20224v1](https://arxiv.org/abs/2511.20224v1)
- **PDF**: [https://arxiv.org/pdf/2511.20224v1](https://arxiv.org/pdf/2511.20224v1)

DUO-TOK是一种面向人声-伴奏音乐的源感知双码本分词器，旨在解决现代歌词到歌曲生成系统中重建质量与语言模型可学习性之间日益突出的矛盾。现有编解码器要么采用难以建模的声学令牌以追求高保真重建，要么通过压缩为语义令牌实现语言模型友好但损失精度的表征，且鲜有分词器能显式感知双轨结构。本文提出以自监督学习为核心的四阶段流程：首先在大规模音频上预训练BEST-RQ风格编码器；接着通过高斯替换噪声与多任务监督对表征进行稳定化与因子分解；随后冻结编码器，基于SimVQ框架学习具有硬路由机制的人声与伴奏双码本；最终在离散令牌上训练潜在扩散解码器。在0.75 kbps码率下，DUO-TOK重构了经验性重建-生成的帕累托边界，在保持与顶尖音乐分词器相当重建质量的同时，取得了最优的音乐标签平均精度，并在对比编解码器中实现了最低的词汇标准化语言模型困惑度。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现代歌词到歌曲生成系统面临重建质量与语言模型可学习性之间的权衡。现有音频编解码器要么侧重高保真重建但产生难以建模的声学令牌，要么过度压缩为语义令牌虽易建模但信息损失严重。  
- **既有问题**：  
  - 单码本设计导致人声与伴奏语义纠缠，缺乏对双轨结构的显式感知。  
  - 低码率下难以平衡重建质量与LM友好性，形成“重建-生成帕累托边界”僵局。  

2)  
- **四阶段流水线设计**：  
  - **阶段1**：基于BEST-RQ的SSL预训练，通过掩码预测学习音乐语义表示。  
  - **阶段2**：引入高斯替换噪声与多任务监督（ASR、Mel谱、Chroma、MSS掩码），稳定并分解表征，抑制量化敏感成分。  
  - **阶段3**：冻结编码器，基于SimVQ训练双码本，通过硬路由分离人声与伴奏的离散化过程。  
  - **阶段4**：在离散令牌上训练潜在扩散解码器，实现高保真重建。  
- **核心创新**：  
  - 高斯噪声注入作为正则化器，提升令牌的LM可预测性。  
  - 多任务监督作为语义护栏，保留音乐结构与跨轨关联。  
  - 双码本设计支持源感知的分离建模，增强可控性。  

3)  
- **任务与效果**：  
  - **音乐标注**：在MagnaTagATune上取得最高平均精度（AP 0.35），优于对比编解码器。  
  - **语言建模**：在0.75 kbps码率下，词汇归一化困惑度（PPL@1024）降至4.75，显著低于基线。  
  - **重建质量**：保持与先进音乐令牌化器相当的PESQ、STOI与Mel L1距离，同时推动重建-生成帕累托边界优化。
</div>

</details>

---

## Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach
- **Authors**: Huu Tuong Tu, Ha Viet Khanh, Tran Tien Dat, Vu Huan, Thien Van Luong, Nguyen Tien Cuong, Nguyen Thi Thu Trang
- **Categories**: cs.CL, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.20107v1](https://arxiv.org/abs/2511.20107v1)
- **PDF**: [https://arxiv.org/pdf/2511.20107v1](https://arxiv.org/pdf/2511.20107v1)

【中文摘要】
发音错误检测与诊断在语言学习及言语治疗中具有关键作用。不同于需构建评分模型或训练音素级模型的传统方法，本文提出一种无需训练的创新框架，通过结合预训练自动语音识别模型与检索技术实现功能。该方法无需针对特定音素建模或进行额外任务适配训练，仍能精准实现发音错误的检测与诊断。在L2-ARCTIC数据集上的实验表明，本方法在避免模型训练复杂度的同时，取得了69.60%的优异F1分数。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：误读检测与诊断（MDD）在计算机辅助发音训练中至关重要，传统方法依赖音素级建模或训练评分模型，如基于GOP或端到端ASR的方法。  
- **既有问题**：  
  - GOP方法缺乏详细诊断反馈；  
  - 端到端模型常忽略参考文本信息，导致预测与标准音素序列不匹配；  
  - 现有方法需复杂训练或音素特定建模，增加计算成本与实现难度。  

2)  
- **核心方法**：提出无训练检索框架PER-MDD，利用预训练ASR模型提取语音嵌入，通过检索策略实现MDD。  
- **解决步骤**：  
  - **嵌入池构建**：从标注数据中提取帧级音素嵌入，采用中帧池化减少计算量；  
  - **推理过程**：  
    - 对测试语音提取查询嵌入，基于余弦相似度从池中检索Top-k近邻；  
    - 应用阈值过滤低相似度候选，通过众数投票分配音素标签；  
    - 后处理（如去重）生成最终音素序列，与标准序列对齐以检测错误。  
- **优势**：  
  - 无需额外训练或音素建模，降低复杂度；  
  - 利用ASR模型内置语言信息，避免数据不匹配问题；  
  - 通过检索灵活整合多样本知识，提升错误诊断鲁棒性。  

3)  
- **任务与效果**：在L2-ARCTIC数据集上评估非母语英语发音任务。  
- **关键成果**：  
  - F1分数达69.60%，优于多数基线模型；  
  - 误拒率（FRR）仅4.43%，显著降低学习者挫败感；  
  - 正确性（COR）达90.42%，表明预测与人工标注高度一致。
</div>

</details>

---

## BERT-APC: A Reference-free Framework for Automatic Pitch Correction via Musical Context Inference
- **Authors**: Sungjae Kim, Kihyun Na, Jinyoung Choi, Injung Kim
- **Categories**: eess.AS, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.20006v1](https://arxiv.org/abs/2511.20006v1)
- **PDF**: [https://arxiv.org/pdf/2511.20006v1](https://arxiv.org/pdf/2511.20006v1)

自动音高修正技术通过将演唱中的音高偏差对齐至目标音符以提升人声录音质量。然而现有系统或依赖参考音高而限制实际应用，或采用简易音高估计算法导致难以保持演唱表现力与自然度。本文提出BERT-APC——一种无需参考音高的创新框架，可在修正音高误差的同时保持人声表演的自然表现力。该框架包含三个核心组件：新型稳态音高预测器首先从失谐人声中估计各音符的感知音高；基于音乐语境推理的音符音高预测器通过重构的音乐语言模型推断目标音高序列；音符级修正算法在保留情感表达所需故意音高偏差的同时修正误差。此外，我们提出可学习的数据增强策略，通过模拟真实失谐模式提升音乐语言模型的鲁棒性。与两种最新歌声转录模型相比，BERT-APC在音符音高预测任务中表现优异，在严重失谐样本上的原始音高准确率较次优模型ROSVOT提升10.49%。MOS测试中，BERT-APC以$4.32 \pm 0.15$的得分显著优于商用工具AutoTune（$3.22 \pm 0.18$）和Melodyne（$3.08 \pm 0.18$），且在保持表现力细微差异方面达到相当水平。据我们所知，这是首个利用音乐语言模型实现符号化音乐语境推理的无参考音高修正系统。修正音频样本已在线发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**
- 现有自动音高校正系统主要分为两类：
  - 基于参考的系统依赖乐谱或专业人声，实际应用受限；
  - 无参考系统（如AutoTune）采用简单量化方法，忽略高级音乐上下文，导致校正结果不自然。
- 传统歌声转录模型依赖统计方法或纯声学特征分类，对严重音高偏差鲁棒性差，且无法利用音乐语境消除歧义。

2. **论文核心方法如何解决上述问题**
- **整体框架**：提出BERT-APC，首个结合符号音乐语言模型的无参考音高校正框架，通过三阶段流程实现：
  - 音符级特征提取：使用音符分割器与平稳音高预测器，在存在过渡段与装饰音的情况下准确估计感知音高；
  - 上下文感知音符音高预测：基于MusicBERT构建预测器，通过插值音高嵌入表示连续音高，融合音乐语境推断目标音高；
  - 音符级音高校正：计算音符级误差并统一调整帧级音高，保留表达性细微变化。
- **关键技术贡献**：
  - 可学习的平稳音高预测器：通过加权平均与多目标正则化，精准估计音符平稳区域音高；
  - 音乐语言模型适配：利用插值嵌入解决连续音高与离散符号的模态差异，增强音乐一致性推断；
  - 可学习失调器数据增强：模拟真实失调模式，提升模型对严重音高偏差的鲁棒性。
- **优势**：无需外部参考即可实现音乐语境驱动的音高纠偏，同时保持演唱情感表达的自然性。

3. **在哪些任务上取得了怎样的效果**
- **音符音高预测任务**：在严重失调测试集上，原始音高准确率比次优模型ROSVOT提升10.49%，达到89.24%。
- **音高校正质量评估**：MOS测试中，音高准确度得分4.32±0.15，显著优于AutoTune（3.22）与Melodyne（3.08），且表达保持能力相当。
- **技术验证**：在包含过渡音、颤音等复杂场景中，成功校正超过半音的严重音高偏差，证实音乐语境推理的有效性。
</div>

</details>

---

## Continual Audio Deepfake Detection via Universal Adversarial Perturbation
- **Authors**: Wangjie Li, Lin Li, Qingyang Hong
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.19974v1](https://arxiv.org/abs/2511.19974v1)
- **PDF**: [https://arxiv.org/pdf/2511.19974v1](https://arxiv.org/pdf/2511.19974v1)

随着语音合成与声纹转换技术的快速发展，多媒体取证领域面临严峻安全挑战。现有检测模型虽表现优异，却难以应对持续演变的深度伪造攻击。若依赖历史训练数据持续微调模型，又会带来高昂的计算与存储成本。为突破这些局限，我们提出一种创新框架，将通用对抗扰动技术融入音频深度伪造检测领域，使模型无需直接接触历史数据即可掌握过往伪造特征的分布规律。该方法在微调过程中将通用对抗扰动与预训练自监督音频模型无缝集成。大量实验验证了本方案的有效性，展现了其在音频深度伪造持续学习方面的应用潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音合成与转换技术快速发展，导致音频伪造攻击日益复杂，现有检测模型难以持续应对新型攻击。  
- **既有问题**：  
  - 模型在增量学习过程中易出现灾难性遗忘，即学习新攻击类型时丢失历史知识。  
  - 传统方法需存储历史数据并联合训练，计算与存储成本高，且存在数据泄露风险。  
  - 现有持续学习策略通过梯度修正优化权重，但限制了模型对新攻击的适应能力。  

2)  
- **核心方法**：提出一种基于通用对抗扰动（UAP）的持续学习框架，通过以下步骤解决历史知识保留问题：  
  - **UAP生成与存储**：  
    - 利用历史模型对真实音频特征生成UAP，使其被误判为伪造样本。  
    - 仅存储UAP向量而非原始数据，显著降低存储需求。  
  - **增量微调机制**：  
    - 在新任务训练时，从UAP池中随机采样历史扰动，与当前真实特征叠加生成伪伪造样本。  
    - 结合当前真实与伪造数据，微调预训练自监督音频模型（如WavLM）。  
  - **分布一致性保持**：  
    - 引入特征级知识蒸馏，通过均方误差损失约束当前模型与历史模型对真实特征和伪伪造特征输出的一致性。  
    - 损失函数融合交叉熵与蒸馏损失，平衡新旧知识学习。  
  - **优势**：  
    - 无需历史数据，通过UAP近似历史伪造分布。  
    - 特征级UAP比波形级UAP更有效，因预训练特征冗余度低、一致性更高。  

3)  
- **任务与效果**：  
  - 在ASVspoof 2019 LA、CFAD和ASVspoof 5等跨语言、多攻击类型数据集上验证。  
  - 相比基线序列微调，UAP方法在历史任务上平均错误率降低22%-48%，显著缓解灾难性遗忘。  
  - 特征级UAP在跨域评估中优于波形级UAP，更稳定保留历史分布。
</div>

</details>

---

## It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models
- **Authors**: Xiangyu Zhao, Yaling Shen, Yiwen Jiang, Zimu Wang, Jiahe Liu, Maxmartwell H Cheng, Guilherme C Oliveira, Robert Desimone, Dominic Dwyer, Zongyuan Ge
- **Categories**: cs.MM, cs.CV, cs.LG, eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.19877v1](https://arxiv.org/abs/2511.19877v1)
- **PDF**: [https://arxiv.org/pdf/2511.19877v1](https://arxiv.org/pdf/2511.19877v1)

抑郁症是全球最普遍的心理健康障碍之一。近年来，语音、视频和文本转录等多模态数据被广泛应用于开发AI辅助的抑郁评估系统。大语言模型凭借其强大的语言理解与泛化能力，进一步推动了该领域发展。然而，传统大语言模型仍以文本为中心，无法有效处理音频与视觉模态中丰富的非语言线索——这些线索在心理健康评估中具有关键作用。尽管多模态大语言模型展现出潜力，但鲜有专门针对心理学应用的模型。本研究提出一种新型多模态大语言模型框架用于抑郁检测：通过在音频语言模型中融合视觉理解能力，并实现音视频特征在时间戳层级的细粒度对齐。这种对齐方式不仅能提升跨模态时序动态建模能力，还可减少对大量训练数据与计算资源的需求。在DAIC-WoZ数据集上的实验表明，本模型性能优于单模态方法及既往多模态方法。此外，该框架可扩展至整合更多生理信号，为拓展至心理健康领域之外的临床应用铺平道路。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**  
- **背景**：抑郁症是全球高发的心理健康问题，传统诊断依赖人工评估，成本高且效率低。AI辅助系统利用多模态数据（如语音、视频和文本）进行自动化检测，大型语言模型因其强大的语言理解和泛化能力被引入该领域。  
- **问题**：  
  - 传统LLM仅处理文本，无法利用音频和视觉中的非语言线索（如面部表情、语调），而这些对心理健康评估至关重要。  
  - 现有多模态LLM多针对静态图像设计，缺乏处理时序数据（如视频）的能力，且抑郁症数据集规模小，易导致过拟合和计算资源需求高。  

2)  
**论文核心方法如何解决上述问题**  
- **框架设计**：基于预训练音频语言模型（如Qwen2-Audio），集成视觉编码器，构建文本、音频和视频多模态LLM。通过三阶段训练逐步融合模态：  
  - **自监督视觉预训练**：采用掩码自编码器学习视觉特征的时序依赖，提升表示能力并减少对原始视频数据的依赖。  
  - **语句级音视频对齐**：通过对比学习对齐音频和视觉嵌入，确保跨模态时间同步，仅更新视觉编码器上层参数以保持稳定性。  
  - **多模态指令微调**：使用参数高效微调技术（如LoRA）整合视觉模块，降低计算开销，避免大规模预训练需求。  
- **关键技术**：  
  - **时间戳级对齐**：利用音频和视频的共享时序结构，通过元素级加法融合特征，保持与预训练LLM的兼容性。  
  - **数据增强**：基于子对话分割和重采样，解决数据不平衡和长度问题，并移除访谈者片段以聚焦参与者行为线索。  
- **优势**：  
  - 细粒度时序建模能捕捉抑郁相关细微行为变化（如面部僵硬、语调单调）。  
  - 模块化设计提升训练效率，适用于小规模数据集，并支持扩展至其他生理信号。  

3)  
**在哪些任务上取得了怎样的效果**  
- **任务**：在DAIC-WOZ抑郁症检测数据集上评估，涵盖文本、音频和视频多模态输入。  
- **效果**：  
  - 在开发集上F1分数达0.844，超越单模态方法（如纯文本模型最高0.636、纯音频模型最高0.720）和先前多模态方法（如音视频融合模型最高0.79）。  
  - 在测试集上F1分数为0.825，优于最新多模态基准（如HiQuE的0.79），验证了其鲁棒性。  
  - 仅使用7B参数模型，性能超过部分13B参数模型，凸显计算效率优势。
</div>

</details>

---
