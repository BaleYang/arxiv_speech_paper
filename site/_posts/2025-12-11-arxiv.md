---
layout: post
title: "arXiv Daily – 2025-12-11"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-11（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-10 08:50 — 2025-12-11 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## TinyDéjàVu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers
- **Authors**: Zhaolan Huang, Emmanuel Baccelli
- **Categories**: cs.LG, cs.PF, cs.SD, eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2512.09786v1](https://arxiv.org/abs/2512.09786v1)
- **PDF**: [https://arxiv.org/pdf/2512.09786v1](https://arxiv.org/pdf/2512.09786v1)

常开传感器日益需要搭载多种微型神经网络，并持续对其感知的时间序列数据进行推理。为满足电池供电场景下的寿命与能耗要求，此类硬件通常采用内存资源极有限的微控制器（如128kB RAM）。在此背景下，优化神经网络层间的数据流成为关键。本文提出TinyDéjàVu——一个专为典型微控制器硬件设计的全新框架与算法，可大幅降低传感器数据时间序列在多种微型机器学习模型推理过程中所需的RAM占用。我们以开源形式发布TinyDéjàVu的实现，并在硬件上进行可复现的基准测试。实验表明，TinyDéjàVu能够节省超过60%的RAM使用量，并在重叠滑动窗口输入场景下消除高达90%的冗余计算。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2512.09786v1）
</div>

</details>

---

## Robust Speech Activity Detection in the Presence of Singing Voice
- **Authors**: Philipp Grundhuber, Mhd Modar Halimeh, Martin Strauß, Emanuël A. P. Habets
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.09713v1](https://arxiv.org/abs/2512.09713v1)
- **PDF**: [https://arxiv.org/pdf/2512.09713v1](https://arxiv.org/pdf/2512.09713v1)

语音活动检测系统常将歌唱误判为语音，导致对话增强和自动语音识别等应用性能下降。本文提出歌唱鲁棒性语音活动检测方法，该神经网络旨在歌唱存在环境下实现稳健的语音检测。核心贡献包括：1）采用语音与歌唱样本比例可控的训练策略以提升判别能力；2）构建计算高效模型，在降低推理时延的同时保持稳健性能；3）提出适用于混合语音-歌唱场景的SAD鲁棒性评估新指标。跨多音乐流派的挑战性数据集实验表明，该方法在有效抑制歌唱误检的同时，仍保持高语音检测精度（AUC=0.919）。通过显式学习语音与歌唱的区分特征，本方法为混合语音-歌唱场景提供了更可靠的语音活动检测方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2512.09713v1）
</div>

</details>

---

## DMP-TTS: Disentangled multi-modal Prompting for Controllable Text-to-Speech with Chained Guidance
- **Authors**: Kang Yin, Chunyu Qiang, Sirui Zhao, Xiaopeng Wang, Yuzhe Liang, Pengfei Cai, Tong Xu, Chen Zhang, Enhong Chen
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.09504v1](https://arxiv.org/abs/2512.09504v1)
- **PDF**: [https://arxiv.org/pdf/2512.09504v1](https://arxiv.org/pdf/2512.09504v1)

可控文本转语音（TTS）系统在实现说话人音色与说话风格的独立调控方面面临显著挑战，常因二者属性间的耦合而受限。本文提出DMP-TTS，一种具备显式解耦与多模态提示能力的隐式扩散变换器框架。基于CLAP的风格编码器（Style-CLAP）将参考音频与描述性文本的线索对齐至共享空间，并通过对比学习与风格属性的多任务监督进行训练。为实现推理过程中的细粒度控制，我们引入基于分层条件丢弃训练的解耦式无分类器引导机制，支持对内容、音色及风格引导强度进行独立调节。此外，采用表征对齐技术将预训练Whisper模型的声学-语义特征蒸馏至扩散变换器的中间表示中，从而稳定训练并加速收敛。实验表明，DMP-TTS在保持优异可懂度与自然度的同时，相比开源基线模型展现出更强的风格可控能力。代码与演示页面详见https://y61329697.github.io/DMP-TTS/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：可控文本转语音系统旨在独立操纵音色与说话风格，但现有方法常导致两者纠缠。例如，使用参考音频控制风格时，音色信息会泄露，导致合成语音偏离目标说话人。  
- **既有问题**：  
  - **属性纠缠**：风格与音色难以分离，影响独立控制。  
  - **模态单一**：主流系统仅支持单一模态（音频或文本）提示，灵活性不足。  
  - **扩展性局限**：如ControlSpeech等方法与特定骨干网络紧密耦合，难以迁移至其他架构。

2)  
- **核心方法**：DMP-TTS基于潜在扩散Transformer，通过以下机制解决属性纠缠与多模态控制问题：  
  - **统一多模态风格编码器**：  
    - 基于CLAP构建Style-CLAP，将参考音频与描述文本映射到共享嵌入空间。  
    - 通过对比学习与多任务监督（预测情感、能量、语速等属性）增强风格表征判别力。  
    - 文本标签刻意排除年龄、性别等与音色相关的描述，减少信息重叠。  
  - **链式无分类器引导**：  
    - 训练时采用分层条件丢弃策略：先随机丢弃风格条件，再按概率丢弃音色条件，最后在两者均缺失时丢弃文本条件。  
    - 推理时通过独立调节文本、音色、风格的引导强度，实现细粒度独立控制。  
  - **表征对齐**：  
    - 使用REPA将Whisper模型的声学-语义特征蒸馏到DiT中间层，提升训练稳定性并加速收敛。  
  - **整体架构**：  
    - 条件输入包括文本编码器、说话人编码器、风格编码器，配合时长预测器调节韵律对齐。

3)  
- **任务与效果**：  
  - **风格控制**：在中文数据集上，文本提示的情感/能量/语速控制准确率达0.64/0.85/0.73，音频提示达0.55/0.82/0.74，均超越开源基线。  
  - **语音质量**：音频提示的自然度MOS（3.82）接近真实语音（3.86），文本提示可懂度WER（0.038）接近最优基线。  
  - **音色保持**：跨说话人风格迁移时，音色相似度达0.71–0.72，音频提示未出现音色泄露。  
  - **消融验证**：多任务监督显著提升风格控制，REPA有效降低WER并加速模型收敛。
</div>

</details>

---

## UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking
- **Authors**: Xuangeng Chu, Ruicong Liu, Yifei Huang, Yun Liu, Yichen Peng, Bo Zheng
- **Categories**: cs.CV, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.09327v1](https://arxiv.org/abs/2512.09327v1)
- **PDF**: [https://arxiv.org/pdf/2512.09327v1](https://arxiv.org/pdf/2512.09327v1)

生成逼真的对话式虚拟形象不仅需要模拟独立的说话者，更需建模说话与倾听之间动态、互动的双向关系。然而，对倾听者的建模尤为困难：直接基于音频驱动的训练方法会失败，产生僵硬、静态的倾听动作。这一问题的根源在于根本性的不平衡：说话者的动作由语音音频强烈驱动，而倾听者的动作主要遵循内在的动作先验，仅受外部语音的松散引导。这一挑战导致现有方法大多仅聚焦于说话生成。此前唯一尝试联合生成的研究需依赖额外的说话者动作来生成倾听者，该设计并非端到端，因而阻碍了实时应用。

为突破这一局限，我们提出了UniLS——首个仅通过双轨音频驱动的端到端框架，用于生成统一的说话-倾听表情。本方法引入了一种新颖的两阶段训练范式：第一阶段首先训练一个无需音频的自回归生成器，学习自然面部动作的自发动态特性，从而捕捉内在动作先验；第二阶段引入双轨音频，对生成器进行微调，使其能够基于外部语音线索调整已学习的动作先验。大量实验评估表明，UniLS在说话准确性上达到了最优水平。更重要的是，其在倾听相关指标上实现了高达44.1%的提升，生成的倾听表情显著更具多样性和自然度。这有效缓解了动作僵硬问题，为交互式数字人提供了一种实用且高保真的音频驱动解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：构建逼真的对话式虚拟人需要同时建模说话和倾听的动态交互。然而，现有方法大多只专注于单向的说话生成，忽略了倾听行为。
- **既有问题**：  
  - 直接进行端到端的音频驱动训练会导致生成的倾听动作僵硬、静态，缺乏自然变化。  
  - 唯一尝试联合生成说话与倾听的方法（DualTalk）依赖于预先生成的说话者面部序列，并非真正的端到端，阻碍了实时应用。

2)  
论文提出 **UniLS**，首个仅由双轨音频驱动的、端到端的统一说话-倾听表情生成框架。其核心方法是**两阶段训练范式**，旨在解决倾听动作僵硬及非端到端的问题：  

- **第一阶段：学习内部运动先验**  
  - 移除音频输入，在无配对的多场景视频数据上训练一个**自回归的自由运动生成器**。  
  - 该生成器仅根据过去的面部运动预测未来运动，从而学习到自然面部动态的**内部先验**（如眨眼、微表情、头部细微运动的自发模式）。  

- **第二阶段：音频驱动的微调**  
  - 在配对的对话数据上，对第一阶段预训练的生成器进行微调。  
  - 通过为每个Transformer块**新增两个交叉注意力层**，分别关注说话者A和说话者B的音频特征，将双轨音频作为条件输入。  
  - 使用**LoRA**技术微调主干网络权重，使模型能基于外部语音线索**调制**已学到的内部运动先验，生成音频驱动的说话与倾听动作。  

- **关键设计**：  
  - 两阶段设计将“内部运动模式学习”与“外部音频调制”解耦，解决了倾听行为与音频相关性弱导致的模型崩溃问题。  
  - 仅需音频输入即可同步生成双方的面部运动，实现了真正的端到端，支持实时应用。

3)  
UniLS在**说话-倾听联合生成任务**上取得了显著效果：  
- **说话质量**：在唇部同步误差（LVE）、头部平均距离（MHD）等指标上达到最优，生成了高精度、与语音对齐的说话表情。  
- **倾听质量**：在倾听动作的动态偏差（FDD, PDD, JDD）和分布自然度（F-FID, P-FID）指标上，相比之前方法提升最高达**44.1%**，生成了更多样、更自然的倾听表情，有效缓解了僵硬问题。  
- **综合表现**：用户研究表明，在唇同步、表情自然度、倾听反应自然度等方面均显著优于基线模型。同时，模型支持**实时生成**（560.6 FPS），为交互式数字人提供了实用方案。
</div>

</details>

---

## VABench: A Comprehensive Benchmark for Audio-Video Generation
- **Authors**: Daili Hua, Xizhi Wang, Bohan Zeng, Xinyi Huang, Hao Liang, Junbo Niu, Xinlong Chen, Quanqing Xu, Wentao Zhang
- **Categories**: cs.CV, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.09299v1](https://arxiv.org/abs/2512.09299v1)
- **PDF**: [https://arxiv.org/pdf/2512.09299v1](https://arxiv.org/pdf/2512.09299v1)

近期视频生成领域进展显著，模型已能生成视觉吸引力强且音画同步的视频。现有视频生成基准虽在视觉质量评估方面较为全面，但缺乏对音画生成效果、特别是针对同步音画输出模型的可靠评估方法。为填补这一空白，我们提出了VABench——一个全面、多维的基准框架，旨在系统评估同步音画生成能力。VABench涵盖三大任务类型：文本到音画生成、图像到音画生成以及立体声音画生成，并构建了两大评估模块，共包含15个评估维度。这些维度专门评估文本-视频、文本-音频、视频-音频的成对相似性，音画同步性，唇语一致性，以及精心设计的音视频问答对等。此外，VABench覆盖七大内容类别：动物、人声、音乐、环境声、同步物理声、复杂场景和虚拟世界。我们通过系统化分析与可视化呈现评估结果，旨在为具备同步音频能力的视频生成模型建立新的评估标准，并推动该领域的全面进步。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音视频生成技术快速发展，但现有评估基准主要关注视觉质量，缺乏对音视频同步生成模型的系统性评估。  
- **既有方法的问题**：  
  - 现有基准（如JAVISDiT）评估维度有限，场景覆盖不足。  
  - 忽视音视频生成特有的多模态耦合现象（如多普勒效应、情感协同表达）。  
  - 缺乏对立体声音频空间声学属性的专门评估。  
  - 依赖人工评估，可扩展性差，且缺乏对高阶一致性（如物理逻辑、情感连贯性）的量化指标。

2)  
VABench通过构建一个全面、多维的基准框架来解决上述问题，具体包括：  
- **任务与内容设计**：  
  - 涵盖三种核心任务：文本到音视频（T2AV）、图像到音视频（I2AV）和立体声音频生成。  
  - 定义七大类内容类别（如动物、人声、音乐、复杂场景等），确保评估覆盖真实世界逻辑与多样性。  
- **精细化评估体系**：  
  - 提出15个细粒度指标，分为基于专家模型和基于多模态大语言模型（MLLM）的两类评估模块。  
  - **专家模型指标**：评估单模态质量（如语音清晰度、音频美学）、跨模态语义对齐（文本-视频、文本-音频、音视频对齐）以及时序同步性（如唇语同步、去同步化偏移）。  
  - **MLLM指标**：通过宏观（如对齐度、艺术性、真实感）和微观（音视频QA对）评估，模拟人类对复杂语义的理解。  
- **立体声评估创新**：  
  - 引入针对立体声音频的专用测试用例与九项声学指标，评估空间成像质量（如声场宽度、成像稳定性）和信号完整性（如相位一致性、单声道兼容性），填补了现有基准在空间音频渲染评估上的空白。  
- **数据构建与验证**：  
  - 采用双路径策略（T2AV/I2AV）构建高质量数据集，结合LLM/VLM生成提示与QA对，并经过人工验证确保语义准确性与一致性。  
  - 通过用户研究验证了基准评分与人类偏好之间的强相关性，确保了评估的可靠性与人性化对齐。

3)  
- **评估任务**：在T2AV、I2AV和立体声音频生成任务上，对多种模型（如Sora2、Veo3、Wan2.5及V+A组合模型）进行了系统评估。  
- **取得的效果**：  
  - **整体性能**：端到端AV模型（如Veo3）在跨模态语义对齐和音频质量上表现最佳，凸显了联合训练的优势；V+A模型中Kling+MMAudio组合表现最强。  
  - **细分能力**：模型在音乐、动物等弱相关性内容上表现较好，但在人声、复杂场景等高同步性任务上仍有挑战；立体声生成方面，现有模型均未实现可靠的文本到空间音频生成，但Veo3和Sora2显示出一定的空间化潜力。  
  - **评估有效性**：VABench的指标与人类评价高度相关，成功揭示了模型在语义一致性、同步性和真实感之间的权衡难题，为领域提供了可靠的评估标准。
</div>

</details>

---

## Who Speaks What from Afar: Eavesdropping In-Person Conversations via mmWave Sensing
- **Authors**: Shaoying Wang, Hansong Zhou, Yukun Yuan, Xiaonan Zhang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.09285v1](https://arxiv.org/abs/2512.09285v1)
- **PDF**: [https://arxiv.org/pdf/2512.09285v1](https://arxiv.org/pdf/2512.09285v1)

多参与者会议广泛存在于商务谈判、医疗会诊等场景，常涉及商业秘密、经营策略、患者病情等敏感信息。已有研究表明，攻击者可通过毫米波雷达在室外探测室内物体因语音引起的微振动，从而窃听会议内容。然而，此类窃听攻击在多参与者会议中无法区分具体发言内容所属的个人，易导致信息误解与决策失误。本文旨在解决“何人发言内容为何”的问题。通过利用环境中普遍存在的物体所带来的空间多样性，我们提出一种攻击系统，使攻击者能够在无需预先获知参与者身份、人数或座位布局等先验信息的情况下，远程窃听现场对话。由于现场会议中的参与者通常位于不同位置，其语音会在邻近物体上激发不同的振动模式。为此，我们设计了一种抗噪声的无监督方法，通过在频域检测语音引起的振动差异来区分不同参与者。同时，本文探索了一种基于深度学习的框架，用于融合多物体信号以提升语音质量。我们通过大量实验验证了该概念性攻击在语音分类与信号增强方面的有效性。实验结果表明，在会议室存在多名参与者的情况下，本攻击方案可实现高达 $0.99$ 的语音分类准确率。此外，在不同雷达与物体距离的真实场景中，本方案均能持续提升语音质量。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：多参与者会议常涉及敏感内容，窃听需同时获取语音内容与说话人身份。现有方法利用监督学习重建高频语音特征以区分说话人，但依赖配对的高质量音频数据进行训练，这在现实窃听场景中往往不可得。  
- **既有方法问题**：监督方法需要攻击者事先获取干净的语音数据及说话人身份标签，不适用于实际场景；且毫米波信号带宽窄、信噪比低，缺乏区分说话人所必需的高频语音信息（如共振峰、音色线索），导致说话人区分困难。

2)  
论文提出一种**无监督、基于多物体毫米波传感的窃听攻击**，核心方法通过以下模块解决上述问题：  
- **目标物体检测与振动提取**：  
  - 利用距离-角度图识别并选择振动响应最强的多个日常物体（如纸袋、日历）作为目标。  
  - 设计**语音感知校准方案**：分析静态干扰模型，通过分步圆拟合（先拟合非语音段圆，再以该半径为约束拟合语音段圆），消除静态物体反射带来的干扰，准确提取语音引起的振动信号。  
- **说话人区分**：  
  - **频率响应增强**：采用三阶段信号处理流程——①带通滤波保留语音相关低频分量；②谱减法抑制毫米波硬件自噪声；③计算功率谱进一步抑制残留噪声，从而放大微弱的、说话人特有的频率响应模式。  
  - **无监督聚类**：从增强后的频谱中提取频谱包络作为特征，融合多个物体的特征后，使用高斯混合模型进行聚类，自动估计并区分不同说话人，无需任何标签数据。  
- **信号增强**：  
  - **去噪网络**：采用编码器-解码器架构，在时频域去除硬件噪声，提升信号质量。  
  - **波束成形**：基于LSTM网络估计全频带掩码，指导MVDR波束成形融合多个物体的振动信号，聚合互补的语音成分，从而重建出更清晰的语音。

3)  
- **说话人区分任务**：在多种真实场景（包括不同物体布局、说话人排列、传感距离）下，成功率达到**0.99**（2-4人）及以上，即使在5人肩并肩的挑战性布局中仍超过0.9。  
- **信号增强任务**：融合多物体信号后，信噪比（SNR）平均提升至**10 dB以上**，峰值信噪比（PSNR）超过**20 dB**，优于仅使用最强振动信号的基线方法。  
- **语音恢复任务**：增强后的信号输入卷积神经网络进行数字识别，整体准确率达到**0.85**，证实了恢复语音的可理解性。
</div>

</details>

---

## Human perception of audio deepfakes: the role of language and speaking style
- **Authors**: Eugenia San Segundo, Aurora López-Jareño, Xin Wang, Junichi Yamagishi
- **Categories**: eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2512.09221v1](https://arxiv.org/abs/2512.09221v1)
- **PDF**: [https://arxiv.org/pdf/2512.09221v1](https://arxiv.org/pdf/2512.09221v1)

音频深度伪造技术已达到以假乱真的水平，使得区分人声与合成声音日益困难，这带来了身份盗用或虚假信息传播等风险。尽管存在这些担忧，目前对人类识别深度伪造能力的研究仍较为有限，多数研究集中于英语场景，且极少探讨听者感知判断背后的原因。本研究通过一项感知实验填补了这一空白：54名听者（28名西班牙语母语者、26名日语母语者）对声音进行自然或合成的分类，并阐述判断依据。实验包含80条刺激样本（50%为合成声音），按三个变量组织：语言（西班牙语/日语）、说话风格（有声书/访谈）及对声音的熟悉度（熟悉/陌生）。研究旨在探究这些变量如何影响检测效果，并定性分析听者感知决策背后的逻辑。实验结果显示平均识别准确率为59.11%，其中对真实样本的识别表现更优。对声音自然度的判断依赖于语言与非语言线索的结合。通过对比日语与西班牙语听者的表现，定性分析进一步揭示了听者在理解语音“人性化”特征时既存在共性线索，也存在显著的跨语言差异。总体而言，参与者主要依赖超音段特征及高层/语言外特征——如语调、节奏、流畅度、停顿、语速、呼吸声与笑声——而非音段特征进行判断。这些发现凸显了人类区分自然与合成语音时感知策略的复杂性，部分印证了先前研究强调韵律及自发语音典型现象（如不流畅特征）重要性的观点。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频深度伪造技术已达到高度逼真水平，难以区分真伪，带来身份盗窃、虚假信息传播等风险。  
- **既有问题**：现有研究多集中于英语，对人类识别能力的探索有限，且很少探究听者判断背后的原因。此外，多数研究依赖朗读语音，缺乏对自发语音（如访谈）的考察，对语音熟悉度等因素的影响也研究不足。

2)  
- **核心方法**：本研究通过感知实验，让54名听者（28名西班牙语母语者，26名日语母语者）对80段音频（50%为合成）进行真伪分类，并说明判断理由。实验设计系统考察了三个变量：
  - **语言**：西班牙语 vs. 日语。
  - **说话风格**：有声书（朗读） vs. 访谈（自发）。
  - **语音熟悉度**：熟悉 vs. 不熟悉。
- **解决问题**：
  - 通过对比不同语言和说话风格，弥补了以往研究多限于英语和朗读语音的不足。
  - 引入定性分析，探究听者判断所依赖的声学线索，如超音段特征（语调、节奏）和非语言特征（呼吸、笑声）。
  - 使用混合效应模型量化各变量对检测准确率的影响，并分析听者反馈的文本，按语音成分（音段、超音段、高层特征）分类，揭示跨语言感知策略的异同。

3)  
- **任务与效果**：在音频深度伪造检测任务中，听者平均准确率为59.11%，对真实样本的识别率更高。
  - **语言熟悉度**：母语样本检测更准确（支持假设1）。
  - **说话风格**：西班牙语听者对访谈（自发语音）的检测优于有声书（支持假设2）；日语听者模式不一致。
  - **语音熟悉度**：未发现显著影响（假设3未得到支持）。
- **定性发现**：听者主要依赖超音段和高层特征（如语调、流畅度、呼吸声）进行判断，揭示了人类感知策略的复杂性。
</div>

</details>

---
