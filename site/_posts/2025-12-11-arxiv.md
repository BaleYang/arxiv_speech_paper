---
layout: post
title: "arXiv Daily – 2025-12-11"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-11（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-10 08:50 — 2025-12-11 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## TinyDéjàVu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers
- **Authors**: Zhaolan Huang, Emmanuel Baccelli
- **Categories**: cs.LG, cs.PF, cs.SD, eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2512.09786v1](https://arxiv.org/abs/2512.09786v1)
- **PDF**: [https://arxiv.org/pdf/2512.09786v1](https://arxiv.org/pdf/2512.09786v1)

常开传感器正日益需要搭载多种微型神经网络，并对其感知的时序数据持续进行推理。为满足电池供电场景下的寿命与能耗要求，此类硬件通常采用内存资源极其有限的微控制器（如128kB RAM）。在此背景下，优化神经网络层间的数据流成为关键。本文提出TinyDéjàVu——一个专为典型微控制器硬件设计的全新框架与算法，可大幅降低传感器时序数据在多种微型机器学习模型推理过程中所需的RAM占用。我们以开源形式发布TinyDéjàVu的实现，并在硬件上进行可复现的基准测试。实验表明，TinyDéjàVu可节省超过60%的RAM使用量，并在重叠滑动窗口输入中消除高达90%的冗余计算。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在电池供电的始终在线（always-on）传感器设备上，需部署微型神经网络对传感器数据流进行持续实时推理。此类设备通常使用内存预算极小的微控制器（如128kB RAM），对内存占用和能耗有严苛限制。
- **既有方法问题**：现有方法在处理时间序列时，通常采用重叠滑动窗口作为输入，导致在推理阶段对重叠部分进行大量冗余计算。同时，传统神经网络层在处理流式数据时需缓存整个窗口的中间激活值，造成较高的峰值内存占用，难以满足微控制器的资源约束。

2)  
论文提出的 **TinyDéjàVu** 框架通过以下核心方法解决上述问题：
- **将时序算子转化为状态空间模型（SSM）**：
    - 对神经网络计算图进行时序分析，识别出具有局部感受野的时序算子（如卷积、池化）。
    - 将这些算子形式化地等价转换为SSM。SSM仅需维护固定大小的隐藏状态（对应算子感受野），而非整个输入窗口，从而将内存占用从 O(N) 降至 O(τ)（τ为感受野大小）。
- **基于SSM的流式推理与冗余计算消除**：
    - 将网络划分为 **SSM子图**（局部时序算子）和 **GTA子图**（全局时序聚合器）。
    - 在SSM子图中，算子被转换为级联的SSM。每个新数据点到来时，SSM仅更新其隐藏状态并计算输出，实现增量处理，避免了为每个滑动窗口重新计算整个网络。
    - 对于重叠滑动窗口，框架通过分析窗口重叠率，确定需要更新的隐藏状态数量，仅计算新数据影响的部分，最高可消除90%的冗余计算。
- **全局池化优化**：
    - 针对全局池化这类全局感受野算子，设计了一种两级SSM等效结构，将内存复杂度从 O(N) 降至 O(N/s)（s为窗口步长）。
- **可选BF16精度支持**：
    - 提供使用BF16格式存储SSM隐藏状态的选项，可在可接受的精度损失下，将状态存储内存再降低约一半。

3)  
- **任务与模型**：在多种用于时间序列任务的混合架构模型上进行了验证，包括TC-CNN、TEMPONet、ResTCN、CET-S和TC-TFM，涵盖分类、生成等任务。
- **效果**：
    - **内存占用**：相比原始模型，**TinyDéjàVu 平均减少超过60%的RAM使用**，在TC-TFM等模型上节省尤为显著。
    - **推理速度**：在重叠率高的滑动窗口流式推理中，**消除了大量冗余计算**。例如在WaveNet生成任务中，流式阶段单样本推理速度比预热阶段提升近200倍。
    - **准确性**：启用BF16优化后，输出误差（RMSE）仅在1%-3%之间，影响甚微。
</div>

</details>

---

## Robust Speech Activity Detection in the Presence of Singing Voice
- **Authors**: Philipp Grundhuber, Mhd Modar Halimeh, Martin Strauß, Emanuël A. P. Habets
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.09713v1](https://arxiv.org/abs/2512.09713v1)
- **PDF**: [https://arxiv.org/pdf/2512.09713v1](https://arxiv.org/pdf/2512.09713v1)

语音活动检测系统常将歌唱误判为语音，导致对话增强和自动语音识别等应用性能下降。本文提出歌唱鲁棒性语音活动检测方法，该神经网络旨在歌唱存在环境下稳健检测语音。核心贡献包括：1）采用语音与歌唱样本比例可控的训练策略以提升区分能力；2）构建计算高效模型，在降低推理时延的同时保持稳健性能；3）提出适用于混合语音-歌唱场景的SAD鲁棒性评估新指标。在多音乐流派挑战数据集上的实验表明，该方法在有效抑制歌唱的同时保持高语音检测精度（AUC=0.919）。通过显式学习语音与歌唱的区分特征，本方法为混合语音-歌唱场景提供了更可靠的语音活动检测方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音活动检测（SAD）是音频处理中的关键预处理步骤，广泛应用于对话增强、自动语音识别等场景。然而，现有SAD系统在遇到歌唱声音时，常将其误判为语音，导致下游任务性能下降。  
- **既有方法的问题**：  
  - 传统SAD方法（如ECAPA-TDNN、AS-pVAD）主要针对纯净语音或噪声环境优化，未专门处理歌唱与语音的相似性。  
  - 歌唱检测领域的研究侧重于从伴奏中分离人声，而非区分歌唱与语音。  
  - 现有方法缺乏对混合语音-歌唱场景的鲁棒性评估指标，且训练数据通常未包含可控比例的歌唱样本。

2)  
论文提出**歌唱鲁棒语音活动检测（SR-SAD）**，通过以下核心方法解决上述问题：  
- **改进的训练策略**：  
  - 在训练数据生成中，引入参数 \(p_s\) 控制选择语音或歌唱样本的概率。实验表明，包含20%-30%歌唱样本的训练能显著提升模型对歌唱的区分能力。  
  - 使用动态混合生成训练样本：语音与噪声混合，歌唱与其对应器乐混合，从而独立控制歌唱-音乐比例。  
  - 采用多种音频增强技术（如SNR调整、频带滤波、幅度缩放等），提升模型对多样声学条件的适应性。  
- **高效的模型架构**：  
  - **SR-SAD**：基于双向门控循环单元（GRU）的循环神经网络，输入为80维梅尔频谱图，通过跳跃连接保留帧级信息，擅长建模时序模式。  
  - **SR-SAD-LC（低复杂度变体）**：在时间轴上进行下采样（使用步进卷积），减少GRU参数和计算量，再通过转置卷积上采样恢复分辨率，在保持性能的同时大幅降低推理成本。  
- **专用评估指标**：  
  - 提出 **AUCSiRR** 指标，同时衡量语音检测的召回率（TPR）和歌唱拒绝率（SiRR），专门评估混合场景下的SAD鲁棒性。

3)  
- **任务与效果**：  
  - **语音检测与歌唱抑制**：在包含多音乐流派、混合语音-歌唱的挑战性测试集上，SR-SAD在语音检测（AUC = 0.919）和歌唱拒绝（AUCSiRR = 0.726）上均取得最佳性能，显著优于AS-pVAD、STA-VAD等基线模型。  
  - **流派泛化性**：在纯歌唱数据集（MoisesDB）上，SR-SAD对大多数音乐流派（如流行、摇滚）的歌唱抑制准确率高达97.86%，但在说唱（rap）和蓝调（blues）上因人声接近语音而表现受限。  
  - **计算效率**：SR-SAD-LC将计算量降至15.6 M MACs，实时因子（RTF）达275，在保持性能的同时实现80%的计算缩减。
</div>

</details>

---

## DMP-TTS: Disentangled multi-modal Prompting for Controllable Text-to-Speech with Chained Guidance
- **Authors**: Kang Yin, Chunyu Qiang, Sirui Zhao, Xiaopeng Wang, Yuzhe Liang, Pengfei Cai, Tong Xu, Chen Zhang, Enhong Chen
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.09504v1](https://arxiv.org/abs/2512.09504v1)
- **PDF**: [https://arxiv.org/pdf/2512.09504v1](https://arxiv.org/pdf/2512.09504v1)

可控文本转语音（TTS）系统在实现说话人音色与说话风格的独立调控方面面临显著挑战，常因二者属性间的纠缠而受限。本文提出DMP-TTS，一种具备显式解耦与多模态提示能力的隐式扩散变换器框架。基于CLAP的风格编码器（Style-CLAP）将参考音频与描述性文本的线索对齐至共享空间，并通过对比学习结合风格属性的多任务监督进行训练。为实现推理过程中的细粒度控制，我们引入基于层级条件丢弃训练的解耦式无分类器引导机制，支持对内容、音色及风格引导强度进行独立调节。此外，采用表征对齐技术将预训练Whisper模型的声学-语义特征蒸馏至扩散变换器的中间表示中，从而稳定训练并加速收敛。实验表明，DMP-TTS在保持可懂度与自然度竞争力的同时，相比开源基线模型展现出更优的风格可控性。代码与演示页面详见 https://y61329697.github.io/DMP-TTS/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：可控文本转语音系统旨在独立操控音色与说话风格，但现有方法常导致二者纠缠。例如，使用参考音频控制风格时，音色信息会泄露，导致合成语音偏离目标说话人。  
- **既有方法问题**：主流方法多依赖单一模态提示（仅音频或文本），灵活性不足；现有解耦方案（如ControlSpeech）与特定模型架构强耦合，扩展性差，且文本提示可能包含身份信息，阻碍严格解耦。

2)  
- **核心方法**：DMP-TTS基于隐扩散Transformer，通过三个关键技术实现解耦与多模态控制。  
- **Style-CLAP编码器**：基于CLAP构建统一多模态风格编码器，将参考音频与描述文本映射到共享嵌入空间，并通过对比学习与多任务监督（预测情感、能量、语速）增强风格表征的判别性，避免音色泄露。  
- **链式无分类器引导**：训练时采用分层条件丢弃策略，按语义（文本）、音色、风格的层级随机丢弃条件；推理时通过独立调节三个引导强度，实现内容、音色、风格的精细独立控制。  
- **表征对齐**：使用Whisper模型作为教师，将其中间声学-语义特征与DiT中间层对齐，通过余弦相似度损失注入先验知识，稳定训练并加速收敛。

3)  
- **任务与效果**：在中文语音数据集上评估，DMP-TTS在风格控制性上超越开源基线。  
  - 文本提示下，情感/能量/语速准确率达0.64/0.85/0.73；音频提示下达0.55/0.82/0.74。  
  - 可懂度接近最优基线（WER为0.038-0.043），自然度MOS接近真实语音（3.82-3.83）。  
  - 支持跨说话人风格迁移，音色相似度达0.71-0.72，证实了音色与风格的有效解耦。
</div>

</details>

---

## UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking
- **Authors**: Xuangeng Chu, Ruicong Liu, Yifei Huang, Yun Liu, Yichen Peng, Bo Zheng
- **Categories**: cs.CV, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.09327v1](https://arxiv.org/abs/2512.09327v1)
- **PDF**: [https://arxiv.org/pdf/2512.09327v1](https://arxiv.org/pdf/2512.09327v1)

生成逼真的对话式虚拟形象不仅需要模拟独立的说话者，更需建模说话与倾听之间动态、互动的双向过程。然而，对倾听者进行建模尤为困难：直接基于音频驱动的训练方法会失败，产生僵硬、静态的倾听动作。这一问题的根源在于根本性的不平衡：说话者的动作由语音音频强烈驱动，而倾听者的动作主要遵循内在的动作先验，仅受外部语音的松散引导。这一挑战导致现有方法大多仅聚焦于说话生成。此前唯一尝试联合生成的研究需依赖额外的说话者动作来生成倾听者，这种设计非端到端，因而阻碍了实时应用。为突破这一局限，我们提出了UniLS——首个仅通过双轨音频驱动的端到端框架，用于生成统一的说话-倾听表情。本方法引入了一种新颖的两阶段训练范式：第一阶段首先通过训练无音频的自回归生成器来学习内在动作先验，捕捉自然面部动作的自发动态；第二阶段引入双轨音频，对生成器进行微调，使其能够基于外部语音线索调节已学习的动作先验。大量评估表明，UniLS在说话准确性上达到了最先进水平。更重要的是，其在倾听指标上实现了高达44.1%的提升，生成了显著更多样、更自然的倾听表情。这有效缓解了僵硬问题，为交互式数字人提供了一种实用且高保真的音频驱动解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：构建逼真的对话式虚拟人需要同时建模说话和倾听的动态交互。然而，现有方法大多只专注于单向的说话生成，忽略了倾听行为。
- **既有问题**：  
  - 直接端到端训练倾听行为会失败，导致生成僵硬、静态的倾听动作。  
  - 根本原因在于音频-运动关联的不平衡：说话者的运动由自身语音强烈驱动，而倾听者的运动主要遵循内部运动先验，仅被外部语音微弱引导。  
  - 此前唯一尝试联合生成的方法（DualTalk）依赖额外的说话者运动序列来生成倾听者，这使其非端到端，阻碍了实时应用。

2.  
论文提出 **UniLS**，首个仅由双轨音频驱动的端到端统一说话-倾听生成框架。其核心方法是**两阶段训练范式**，旨在解决倾听僵硬问题并实现端到端实时生成。

- **阶段一：学习内部运动先验**  
  - **目标**：捕获自然面部动态的自发模式（如眨眼、微表情、头部细微运动），这些模式独立于语音。  
  - **方法**：在**无音频输入**的情况下，使用未配对的多场景视频数据（如新闻、访谈）训练一个自回归“自由生成器”。该生成器仅根据过去运动块和风格嵌入，预测下一块运动，从而建立起对内部运动规律的先验知识。

- **阶段二：引入音频引导进行微调**  
  - **目标**：将阶段一学到的内部运动先验与外部语音线索相结合，生成受音频调制的说话与倾听运动。  
  - **方法**：在配对的对话数据上微调阶段一的生成器。关键设计是**引入两个交叉注意力层**：  
    - 一个关注**自身音频**（用于驱动说话行为）。  
    - 另一个关注**对方音频**（用于调制倾听行为）。  
  - 通过**LoRA**技术微调主干网络权重，高效地使模型适应音频条件，而不覆盖已学到的内部运动先验。  
  - 最终，模型仅需双轨音频即可同步生成双方自然、协调的面部运动。

- **解决思路总结**：将倾听行为重新诠释为“内部运动先验”与“外部音频线索”的结合，通过两阶段训练分别学习这两个组件，从而克服了音频-运动关联弱导致的模型崩溃（生成静态脸）问题，实现了高质量、端到端的统一生成。

3.  
- **任务**：在**说话-倾听联合面部动画生成**任务上进行了评估。
- **效果**：  
  - **说话质量**：在唇部同步误差（LVE）、平均头部距离（MHD）等指标上达到**最先进水平**，生成准确的口型与面部表情。  
  - **倾听质量**：在倾听动态偏差（FDD、PDD、JDD）和分布自然度（F-FID、P-FID）等指标上，相比之前方法取得了**高达44.1%的提升**，生成了显著更多样、更自然的倾听表情，有效缓解了僵硬问题。  
  - **综合性能**：用户研究表明，在唇同步、表情自然度、倾听反应自然度等所有方面均被显著偏好。同时，模型支持**实时生成**（560.6 FPS），具备实际应用潜力。
</div>

</details>

---

## VABench: A Comprehensive Benchmark for Audio-Video Generation
- **Authors**: Daili Hua, Xizhi Wang, Bohan Zeng, Xinyi Huang, Hao Liang, Junbo Niu, Xinlong Chen, Quanqing Xu, Wentao Zhang
- **Categories**: cs.CV, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.09299v1](https://arxiv.org/abs/2512.09299v1)
- **PDF**: [https://arxiv.org/pdf/2512.09299v1](https://arxiv.org/pdf/2512.09299v1)

近期视频生成领域进展显著，模型已能生成视觉表现力强且音画同步的视频内容。现有视频生成基准虽在视觉质量评估方面提供了全面的度量体系，但缺乏对音画生成效果——尤其是针对同步音画输出模型——的有效评估方法。为填补这一空白，我们提出了VABench：一个多维度综合基准框架，旨在系统评估同步音画生成能力。该框架涵盖三大任务类型：文本到音画生成、图像到音画生成以及立体声音画生成，并构建了包含15个维度的两大评估模块。这些维度专门评估文本-视频、文本-音频、视频-音频的成对相似性，音画同步性，唇语-语音一致性，以及精心设计的音视频问答对等。此外，VABench覆盖七大内容类别：动物声、人声、音乐、环境声、同步物理声、复杂场景及虚拟世界。我们通过系统化的结果分析与可视化呈现，旨在为具备同步音频能力的视频生成模型建立新的评估标准，推动该领域的全面进步。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音视频生成技术快速发展，但现有评估基准主要关注视觉质量，缺乏对同步音视频生成的系统性评估。  
- **既有方法问题**：  
  - 现有基准（如JAVISDiT）评估维度有限，场景覆盖不足。  
  - 忽视音视频耦合现象（如多普勒效应、情感协同表达）。  
  - 缺乏对立体声音频空间声学特性的专门评估。  
  - 依赖人工评估，难以扩展；且缺少对高阶物理与情感一致性的量化指标。

2)  
VABench通过构建一个多维度、自动化的基准框架，系统性解决上述评估缺口：  
- **任务与内容覆盖**：  
  - 涵盖三大生成任务：文本到音视频（T2AV）、图像到音视频（I2AV）、立体声音频生成。  
  - 设计七大类内容（动物、人声、音乐、环境声、同步物理声、复杂场景、虚拟世界），确保评估场景的多样性与真实性。  
- **精细化评估体系**：  
  - **专家模型评估**：基于8个专用模型指标，量化单模态质量（如语音清晰度、音频美学）、跨模态语义对齐（文本-视频、文本-音频、音视频对齐）及时间同步性（口型同步、去同步偏移）。  
  - **MLLM评估**：基于7个多模态大语言模型指标，模拟人类对宏观维度（如对齐度、艺术性、表现力、真实性）和微观细粒度QA（音频与视觉QA对）的判断，实现自动化、可扩展的高阶语义评估。  
- **立体声评估创新**：  
  - 引入九项声学指标，分为空间成像质量（如声场宽度、成像稳定性）与信号完整性（如相位一致性、单声道兼容性），专门评估生成音频的空间感知与声场渲染能力。  
- **数据构建与验证**：  
  - 采用LLM/VLM生成提示与QA对，并结合人工校验，确保数据语义准确性与音视频一致性。  
  - 通过用户研究验证了基准评分与人类偏好的强相关性，确保评估结果可靠且符合人类感知。

3)  
- **评估任务**：在T2AV、I2AV和立体声音频生成任务上，对多种端到端音视频模型（如Veo3、Sora2、Wan2.5）及解耦的V+A模型进行了系统评估。  
- **取得效果**：  
  - 揭示了模型在语义一致性、同步性和真实性之间的权衡挑战；端到端模型整体优于解耦模型，尤其在跨模态协同方面表现更优。  
  - 细粒度QA分析显示，当前模型在音乐、动物等弱关联类别表现较好，但在人声、复杂场景等同步要求高的类别仍有不足。  
  - 立体声评估表明现有模型尚无法稳定生成具有语义区分度的立体声，但部分模型（如Veo3、Sora2）已展现出一定的空间音频线索生成能力。
</div>

</details>

---

## Who Speaks What from Afar: Eavesdropping In-Person Conversations via mmWave Sensing
- **Authors**: Shaoying Wang, Hansong Zhou, Yukun Yuan, Xiaonan Zhang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.09285v1](https://arxiv.org/abs/2512.09285v1)
- **PDF**: [https://arxiv.org/pdf/2512.09285v1](https://arxiv.org/pdf/2512.09285v1)

多参与者会议广泛存在于商务谈判、医疗咨询等场景，常涉及商业秘密、经营策略、患者病情等敏感信息。已有研究表明，攻击者可通过毫米波雷达在室外探测物体因语音引起的微振动，从而窃听会议内容。然而，此类窃听攻击在多参与者会议中无法区分具体发言者与其语音内容的对应关系，易导致信息误解与决策失误。本文针对“谁在说什么”这一问题展开研究，利用环境中普遍存在的物体所带来的空间多样性，提出一种无需预先获知参与者身份、人数或座位布局等先验知识的远程窃听攻击系统。由于线下会议参与者通常位于不同位置，其语音会使邻近物体产生差异化的振动模式。为此，我们设计了一种抗噪声的无监督方法，通过在频域检测语音引起的振动差异来区分不同发言者。同时，探索基于深度学习的框架，融合多物体信号以提升语音质量。我们通过大量实验验证了该概念性攻击在语音分类与信号增强方面的有效性。实验结果表明，在会议室存在多名参与者的情况下，本攻击可实现高达$0.99$的语音分类准确率；同时，在包括雷达与物体间不同距离在内的多种真实场景中，均能实现稳定的语音质量提升。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：多参与者会议常涉及敏感内容，传统窃听方法依赖高质量音频数据训练监督模型，以重建高频语音特征并区分说话人。  
- **既有问题**：实际场景中，攻击者难以获取配对的高质量音频数据；且毫米波信号带宽窄、信噪比低，缺乏区分说话人所需的高频信息（如共振峰细节），导致现有方法实用性受限。  

2)  
- **核心方法**：提出一种无监督、基于多物体毫米波感知的窃听攻击，无需干净音频或说话人身份信息，通过分析室内日常物体（如纸袋、日历）受语音激发的微小振动实现“谁说了什么”的推断。  
- **技术方案**：  
  - **振动分辨率增强**：设计语音感知校准方案，基于静态干扰理论模型，从混合信号中分离语音引起的振动。  
  - **频率响应放大**：采用抗噪声信号处理流程，通过抑制毫米波硬件自噪声，放大弱语音频率响应。  
  - **多物体信号增强**：提出深度学习框架，聚合不同物体上互补的语音成分，提升信号质量。  
  - **说话人区分**：提取物体振动频谱包络作为特征，结合高斯混合模型进行无监督聚类，利用说话人与物体相对位置的空间差异实现区分。  

3)  
- **任务与效果**：  
  - **说话人区分**：在真实隔音房间实验中，成功率达0.99（2-4人场景），即使5人肩并肩布局仍超过0.9。  
  - **信号增强**：在多说话人场景下，信噪比平均超过10dB，峰值信噪比超20dB，优于仅使用最强振动信号的基线方法。  
  - **语音恢复**：增强后的信号在数字识别任务中准确率达85%，验证了语音内容可恢复性。  
  - **鲁棒性**：方法在不同物体材料、说话人性别、距离（80-280cm）及布局下均表现一致。
</div>

</details>

---

## Human perception of audio deepfakes: the role of language and speaking style
- **Authors**: Eugenia San Segundo, Aurora López-Jareño, Xin Wang, Junichi Yamagishi
- **Categories**: eess.AS, eess.SP
- **arXiv**: [https://arxiv.org/abs/2512.09221v1](https://arxiv.org/abs/2512.09221v1)
- **PDF**: [https://arxiv.org/pdf/2512.09221v1](https://arxiv.org/pdf/2512.09221v1)

音频深度伪造技术已达到以假乱真的水平，使得区分人声与合成声音日益困难，这带来了身份盗用或虚假信息传播等风险。尽管存在这些担忧，目前对人类识别深度伪造能力的研究仍十分有限，大多数研究集中于英语，且极少探讨听者感知判断背后的原因。本研究通过一项感知实验填补了这一空白：54名听者（28名西班牙语母语者、26名日语母语者）对声音进行自然或合成的分类，并说明判断依据。实验包含80条刺激样本（50%为合成声音），按三个变量组织：语言（西班牙语/日语）、说话风格（有声书/访谈）及对声音的熟悉度（熟悉/不熟悉）。研究旨在探究这些变量如何影响检测效果，并定性分析听者感知决策背后的逻辑。结果显示平均识别准确率为59.11%，其中对真实样本的识别表现更优。对声音自然度的判断依赖于语言与非语言线索的结合。通过对比日语与西班牙语听者，定性分析进一步揭示了听者在理解语音“人性化”特征时既存在共同依据，也存在显著的跨语言差异。总体而言，参与者主要依赖超音段特征及更高层次或语言外特征——如语调、节奏、流畅度、停顿、语速、呼吸声和笑声——而非音段特征。这些发现凸显了人类区分自然与合成语音的感知策略的复杂性，部分印证了先前研究强调韵律及自发语音典型现象（如不流畅表达）重要性的观点。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频深度伪造技术已达到高度逼真水平，难以区分真伪，带来身份盗窃、虚假信息传播等风险。  
- **既有问题**：现有研究多集中于英语，对人类识别能力的探索有限，且很少探究听者判断背后的原因。此外，多数研究依赖朗读语音，缺乏对自发语音（如访谈）的考察，对语音熟悉度等因素的影响也研究不足。

2)  
- **实验设计**：研究通过感知实验，让54名听者（西班牙语和日语母语者）对80段音频（50%为合成）进行真伪分类并说明理由。实验控制了三个变量：语言（西班牙语/日语）、说话风格（有声书/访谈）、语音熟悉度（熟悉/不熟悉）。  
- **核心方法**：  
  - **定量分析**：使用广义线性混合模型检验语言、说话风格和熟悉度对分类准确率的影响。  
  - **定性分析**：对听者的开放式回答进行主题分析，依据语音成分理论（音段、超音段、高层特征）对反馈进行分类，探究判断依据。  
- **解决思路**：  
  - 通过跨语言比较，揭示语言熟悉度对检测的影响。  
  - 对比不同说话风格（脚本化 vs. 自发），评估AI复制自发语音特征（如不流利现象）的难度。  
  - 结合定量与定性分析，不仅评估检测准确率，还深入理解听者依赖的声学线索（如语调、节奏、呼吸声）。

3)  
- **任务与效果**：在音频深度伪造检测任务中，听者整体准确率为59.11%，对真实样本的检测表现更好。  
- **关键发现**：  
  - 语言熟悉度提升检测准确率（母语表现更优）。  
  - 访谈风格（自发语音）的伪造音频更易被识别（西班牙语听者中显著）。  
  - 听者主要依赖超音段特征（语调、节奏）和高层/非语言特征（呼吸、笑声），而非音段细节。  
  - 定性分析揭示了跨语言共享的线索（如不流利现象）以及语言特异性差异（如日语听者关注音高重音）。
</div>

</details>

---
