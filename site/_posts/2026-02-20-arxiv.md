---
layout: post
title: "arXiv Daily – 2026-02-20"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-20（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-19 08:50 — 2026-02-20 08:50
- 抓取总数：5 篇 | 本页显示：5 篇（去重/过滤后）

## Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment
- **Authors**: Ivan Rinaldi, Matteo Mendula, Nicola Fanelli, Florence Levé, Matteo Testi, Giovanna Castellano, Gennaro Vessio
- **Categories**: cs.CV, cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.17599v1](https://arxiv.org/abs/2602.17599v1)
- **PDF**: [https://arxiv.org/pdf/2602.17599v1](https://arxiv.org/pdf/2602.17599v1)

通过多模态深度学习的进步，音乐生成技术已能基于文本乃至图像合成音频。然而，现有的图像条件生成系统存在两个根本性局限：（1）通常基于自然照片训练，难以捕捉艺术品更丰富的语义、风格与文化内涵；（2）大多依赖图像到文本的转换阶段，将语言作为语义捷径，虽简化了条件控制，却阻碍了视觉到音频的直接学习。为弥补这些不足，我们构建了ArtSound数据集——一个包含105,884组艺术品-音乐配对的大规模多模态数据集，通过扩展ArtGraph与Free Music Archive并配以双模态描述文本实现。进一步，我们提出ArtToMus框架，这是首个专为直接实现艺术品到音乐生成而设计的系统，无需经过图像到文本转换或基于语言的语义监督，即可将数字化艺术品映射为音乐。该框架将视觉嵌入向量投影至潜在扩散模型的条件空间中，实现完全由视觉信息引导的音乐合成。实验结果表明，ArtToMus生成的音乐在听觉上连贯、风格一致，并能反映源艺术品的显著视觉特征。尽管在去除语言监督后任务难度显著增加，其绝对对齐分数仍低于文本条件系统（符合预期），但ArtToMus在感知质量与跨模态对应关系方面均达到可竞争水平。本研究确立了视觉到音乐直接生成这一独特且富有挑战性的研究方向，并为多媒体艺术、文化遗产保护及AI辅助创作等应用提供了资源支持。代码与数据集将在论文录用后公开发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
   - **数据局限性**：现有图像到音乐生成系统通常在自然照片上训练，难以捕捉艺术品更丰富的语义、风格和文化内涵。
   - **方法瓶颈**：多数方法依赖“图像→文本→音乐”的流程，将语言作为语义捷径。这虽然简化了条件控制，但阻碍了直接视觉到音频的学习，并过滤掉了难以用语言描述的风格和细节信息。

2) **论文核心方法如何解决上述问题**
   - **构建大规模数据集**：为解决数据瓶颈，论文构建了**ArtSound**数据集，包含105,884个艺术品-音乐配对，并辅以高质量的双模态（图像和音频）描述。这为直接学习视觉-音频对应关系提供了基础。
   - **提出直接生成框架**：论文提出了**Art2Mus**框架，这是首个为**直接艺术品到音乐生成**设计的系统。其核心创新在于绕过了文本中介，具体实现如下：
     - **视觉条件提取器**：使用预训练的视觉编码器（如CLIP或ImageBind）提取艺术品图像的嵌入表示。
     - **图像对齐器**：通过一个投影层，将视觉嵌入映射到GPT-2的“音频语言”嵌入空间。这个空间原本是为文本到音频生成设计的，Art2Mus通过学习将其与视觉信息对齐。
     - **利用预训练扩散模型**：框架冻结了AudioLDM 2的潜在扩散模型（包括去噪UNet和GPT-2翻译器）作为生成先验，仅训练图像对齐器。这使得模型能够利用强大的音频生成能力，同时仅通过视觉嵌入进行条件控制。
   - **核心优势**：这种方法迫使模型直接从视觉表示中学习跨模态对应关系，保留了可能被文本描述过滤掉的风格、构图等非语言化视觉线索，从而生成更忠实反映源艺术品特质的音乐。

3) **在哪些任务上取得了怎样的效果**
   - **任务**：在**直接艺术品到音乐生成**任务上进行了评估。
   - **效果**：
     - **客观指标**：在感知质量（FAD）和跨模态一致性（ImageBind Score）上取得了有竞争力的结果。虽然绝对对齐分数低于依赖文本监督的基线模型（这是移除语言监督后任务难度增加的预期结果），但Art2Mus在更困难的设定下实现了良好的权衡。
     - **主观评估**：人工评估表明，Art2Mus生成的音乐在音质、表现力以及与艺术品的相关性和对齐性方面均获得认可，能够产生音乐连贯、风格一致且反映源艺术品显著视觉线索的输出。
     - **贡献**：这项工作确立了直接视觉到音乐生成作为一个独立且具有挑战性的研究方向。
</div>

</details>

---

## The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?
- **Authors**: Jayadev Billa
- **Categories**: cs.CL, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.17598v1](https://arxiv.org/abs/2602.17598v1)
- **PDF**: [https://arxiv.org/pdf/2602.17598v1](https://arxiv.org/pdf/2602.17598v1)

当前语音大语言模型主要执行隐式语音识别：在可通过文本转录解决的任务中，其行为机制与简单的Whisper→大语言模型级联系统完全等效。我们通过首次采用统一骨干网络的对比测试验证了这一结论，覆盖四种语音大语言模型和六项任务。Ultravox模型与其对应级联系统在统计上无法区分（κ=0.93）；对数透镜技术显示其隐藏状态中直接浮现文本信息；LEACE概念擦除实验证实，在测试的两种架构中文本表征均具有因果必要性，消除后模型准确率骤降至接近零。Qwen2-Audio模型则表现出实质性差异，表明级联等效性具有架构依赖性而非普适规律。对于多数实际应用场景，当前语音大语言模型实为高成本级联系统，且在噪声环境下性能更劣——在0dB信噪比条件下，其洁净环境优势会发生高达7.6%的性能逆转。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：端到端语音大语言模型（Speech LLMs）旨在直接处理音频，以捕捉转录文本之外的副语言线索（如韵律、情感）。  
- **既有问题**：现有评估存在两个主要局限：  
  - 缺乏对**每样本行为一致性**的分析，无法区分真实架构差异与表面精度相似性。  
  - **跨模型比较未控制LLM骨干网络**，导致观察到的差异可能源于骨干网络的不同推理能力，而非音频处理方式的差异。  

2)  
- **核心方法**：论文提出**匹配骨干网络的行为测试**，以隔离架构效应与骨干网络效应。具体包括：  
  - **构建匹配的级联系统**：为每个语音LLM（如Ultravox、Qwen2-Audio）构建使用相同LLM骨干的Whisper→LLM级联，作为直接比较基准。  
  - **定义级联等价假设**：在文本信息足够的任务上（即转录本携带全部任务相关信息），语音LLM应与使用相同骨干的级联在行为上无法区分。  
  - **多层面验证**：  
    - **行为层面**：通过每样本一致性指标（如Cohen's κ）、条件错误重叠和McNemar检验，量化语音LLM与匹配级联的相似性。  
    - **机制层面**：  
      - **探测分析**：使用线性探针分析隐藏状态中的声学（能量、基频）和文本（字符序列、词袋）信息编码。  
      - **Logit Lens**：通过模型自身的解嵌入矩阵，可视化隐藏状态中文本信息的涌现过程。  
      - **LEACE概念擦除**：因果干预实验，通过线性擦除文本预测子空间，验证文本表示是否是任务性能的必要条件。  
  - **噪声鲁棒性测试**：在多种信噪比条件下评估，比较语音LLM与级联系统在噪声下的性能退化。  

3)  
- **任务与效果**：  
  - **文本足够任务**（如AG News主题分类、SST-2情感分析、CommonsenseQA常识推理）：  
    - Ultravox与匹配级联行为高度一致（κ最高达0.93），机制分析显示其内部构建了类转录本的文本表示。  
    - Qwen2-Audio则表现出显著差异（κ为0.54–0.85），其交叉注意力架构编码文本的方式不同。  
  - **文本不足任务**（如MELD情感识别、MUStARD讽刺检测）：所有模型的κ均下降，但匹配骨干分析表明，骨干网络的混淆效应仍占部分差异。  
  - **噪声条件**：在所有测试的文本足够任务上，基于Whisper的级联系统比所有端到端模型更鲁棒（在0 dB信噪比下性能下降更小）。
</div>

</details>

---

## Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks
- **Authors**: Nuno Saavedra, Pedro Ribeiro, André Coelho, Rui Campos
- **Categories**: cs.NI, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.17394v1](https://arxiv.org/abs/2602.17394v1)
- **PDF**: [https://arxiv.org/pdf/2602.17394v1](https://arxiv.org/pdf/2602.17394v1)

无人机辅助网络正日益被视为应急响应领域的一种前景广阔的技术路径，能够在陆地基础设施受损或缺失的环境中提供快速、灵活且具备韧性的通信支持。在此类场景中，语音无线电通信因其鲁棒性仍是救援人员不可或缺的通信手段；然而，其非结构化特性阻碍了其与无人机辅助网络自动化管理系统的直接集成。本文提出SIREN——一种基于人工智能的语音驱动感知框架，旨在实现无人机辅助网络的语音信息理解。该框架通过融合自动语音识别技术、基于大语言模型的语义提取以及自然语言处理验证，将应急语音通信内容转化为结构化的机器可读信息，涵盖响应单位、位置指向、事件紧急程度及服务质量需求等关键要素。研究采用合成应急场景对SIREN进行评估，通过控制语言类型、说话者数量、背景噪声及信息复杂度等变量进行测试。结果表明，该系统在不同操作条件下均能实现稳定的语音转写与可靠的语义提取，同时揭示出说话人分离与地理指代歧义是当前主要的技术瓶颈。这些发现验证了语音驱动态势感知在无人机辅助网络中的可行性，并为应急响应行动中的人机协同决策支持与自适应网络管理奠定了实践基础。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：无人机辅助网络在应急响应中前景广阔，但地面基础设施受损时，救援人员仍依赖语音无线电通信。然而，语音通信是非结构化的，难以与自动化无人机网络管理直接集成，限制了协同决策和态势感知能力。  
- **既有方法问题**：现有无人机网络管理方案多基于用户位置、流量需求等假设，在动态应急环境中可能不成立；且现有感知方法主要依赖计算机视觉，在遮挡、光照变化或恶劣条件下性能下降，而语音驱动的语义感知作为网络管理直接输入的研究尚属空白。

2)  
论文提出了名为 **SIREN** 的AI驱动框架，通过多阶段模块化流水线将非结构化应急语音转换为机器可读的结构化信息，以解决上述问题：  
- **自动语音识别（ASR）阶段**：支持轻量本地模型（用于离线处理）和云端API（用于高精度转录），并集成说话人日志和情感分析等高级功能，以适应不同操作环境（如噪声条件）。  
- **信息提取阶段**：  
  - 使用**大语言模型（LLM）** 进行语义提取，通过模式约束提示定义输出字段（如位置、单位、紧急程度、QoS需求），确保结构化输出。  
  - 结合**自然语言处理（NLP）验证**：包括命名实体识别（NER）验证地理位置、说话人日志辅助单位归属、情感分析校准紧急程度，以此提高可靠性并减少幻觉。  
- **结构化输出**：生成JSON格式数据，包含位置、紧急程度、响应单位及其QoS需求等语义类别，可直接用于无人机网络管理（如无人机定位、资源分配）或人机协同决策支持。  
- **关键设计**：框架分离了语义提取与地理解析，强调可解释性（如要求明确的需求理由），并通过交互式地图界面提供可视化态势感知。

3)  
SIREN在**合成应急通信场景**中进行了评估，任务与效果如下：  
- **语音转录任务**：使用云端API时，在清洁和噪声条件下均保持较低词错误率（WER约11.34%-12.30%），优于本地Whisper模型（噪声下WER升至19.26%）。  
- **语义提取任务**：在多数场景中成功提取位置（成功率最高达100%）、响应单位（成功率60%-100%）和QoS需求（成功率40%-100%），但高复杂度场景中位置提取因地理歧义下降至0%。  
- **局限性**：说话人日志在语音相似时失效（部分场景成功率0%），地理编码在名称模糊时易出错，且处理时间随语义复杂度增加（LLM分析占主导）。整体表明框架在多种操作条件下具有可行性，为应急网络中的态势感知提供了实践基础。
</div>

</details>

---

## CC-G2PnP: Streaming Grapheme-to-Phoneme and prosody with Conformer-CTC for unsegmented languages
- **Authors**: Yuma Shirahata, Ryuichi Yamamoto
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.17157v1](https://arxiv.org/abs/2602.17157v1)
- **PDF**: [https://arxiv.org/pdf/2602.17157v1](https://arxiv.org/pdf/2602.17157v1)

本文提出CC-G2PnP，一种流式字素到音素与韵律（G2PnP）模型，用于以流式方式连接大语言模型与文本转语音系统。该模型基于Conformer-CTC架构，通过逐块处理输入字素标记，实现对音素与韵律（PnP）标签的流式推理。通过为每个输入标记保证最小前瞻长度，模型能够考虑各标记的未来上下文，从而获得稳定的PnP标签预测。与以往依赖显式词边界的流式方法不同，CC-G2PnP中的CTC解码器在训练中有效学习字素与音素间的对齐关系，使其适用于无分词标记的语言。在无显式词边界的日语数据集上的实验表明，CC-G2PnP在PnP标签预测准确率上显著优于基线流式G2PnP模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在语音对话系统中，级联方法（ASR-LLM-TTS）因其鲁棒性而受关注。为降低延迟，需对LLM输出的文本进行流式处理，这要求G2PnP（字素到音素与韵律）模型也支持流式推理。  
- **既有方法问题**：  
  - 简单分块处理无法有效利用上下文，性能不稳定。  
  - 现有流式方法（如LLM2PnP）依赖显式词边界，无法直接应用于日语、汉语等无分词语言。  

2)  
论文提出 **CC-G2PnP** 模型，基于Conformer-CTC架构，通过以下机制解决上述问题：  
- **流式架构**：采用分块感知流式（Chunk-aware streaming）处理输入字素序列，每个块内的令牌可关注同一块及固定数量过去上下文的令牌，实现可控的向前看（look-ahead）并保持低延迟。  
- **CTC解码器**：将G2PnP建模为序列标注问题，CTC在训练中动态学习字素与音素/韵律标签的对齐，无需预定义词边界或对齐信息，从而适用于无分词语言。  
- **最小向前看（MLA）**：针对分块末尾令牌因无未来上下文而易预测错误的问题，MLA允许第一层自注意力额外参考当前块外的M个未来令牌，确保所有令牌至少有一个令牌的向前看，提升块边界的预测一致性。  
- **自条件CTC**：在中间Conformer层引入自条件CTC，缓解CTC的条件独立性假设，通过多任务损失（中间与最终CTC损失）提升模型性能。  

3)  
- **任务**：在日语（无分词语言）的流式G2PnP任务上进行评估，包括音素与韵律标签预测。  
- **效果**：  
  - **客观指标**：在CER（字符错误率）和SER（句子错误率）上显著优于基线流式方法（如分块Dict-DNN）。最佳配置（块大小5，MLA=1）接近非流式模型性能。  
  - **主观听感**：与TTS系统结合后，合成语音的自然度MOS评分（4.02）远超流式基线，接近非流式系统与真实标签水平。  
  - **延迟**：在保持高性能的同时，仅需6个令牌的延迟即可启动预测，优于需20个令牌的基线方法。
</div>

</details>

---

## AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing
- **Authors**: William Chen, Prem Seetharaman, Rithesh Kumar, Oriol Nieto, Shinji Watanabe, Justin Salamon, Zeyu Jin
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.17097v1](https://arxiv.org/abs/2602.17097v1)
- **PDF**: [https://arxiv.org/pdf/2602.17097v1](https://arxiv.org/pdf/2602.17097v1)

尽管近期取得突破性进展，音频基础模型在处理复杂多源声学场景时仍面临困难。我们将这一挑战性领域称为音频故事，其可能包含多个说话人及背景/前景音效。与传统音频处理任务相比，音频故事在语义、时序和物理层面引入了新的复杂性。为应对这一挑战，我们提出AudioChat框架，用于开发能够生成、编辑和理解音频故事的音频基础模型。AudioChat引入了一种新范式：基于大语言模型的工具调用代理模拟用户与系统间的交互，并将这些模拟对话作为训练数据。我们还提出新颖的音频融合强制训练目标，使模型能通过结构化思维链推理分解高级指令，同时执行交互式多轮音频理解与生成。为评估生成与编辑性能，我们开发了三种直接衡量任务表现的新指标，而非依赖基于分布的打分方法。我们强烈建议读者访问演示页面以更深入理解AudioChat的能力：https://wanchichen.github.io/audiochat/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频基础模型在处理包含多说话人、背景/前景音效的复杂多源声学场景（即“音频故事”）时面临挑战。这类场景在语义、时间和物理层面引入了新的复杂性，超越了传统语音合成、文本到音频和音频描述等任务。
- **既有方法问题**：
  - 现有模型主要针对单一声音的理解与生成，难以处理多源场景。
  - 早期方法多为基于级联代理的系统，将多个单任务模型（如TTS、T2A）作为工具组合，但无法访问和编辑已生成的音频，且延迟高、扩展性差。
  - 现有音频编辑模型通常仅支持固定的操作集（如插入、删除），依赖于简单随机混合的音频片段，无法处理复杂的开放词汇编辑。

2)  
论文提出的AudioChat框架通过一系列创新方法解决上述问题：

- **数据生成范式创新**：为解决训练数据匮乏问题，开发了**AudioCopilot**，一个基于LLM的工具调用代理。它模拟用户与AI音效设计师之间的多轮对话，从零开始生成用户指令和对应的多源音频场景。该方法生成了600万次对话作为训练数据，提供了细粒度的音频故事标注。

- **核心训练目标**：提出了**音频Transfusion Forcing**目标，将**结构化思维链推理**与**多轮扩散生成**统一在一个端到端模型中。
  - **结构化推理**：模型将抽象的用户指令（如“创建一个紧张的场景”）分解为包含时间戳、音量、声像等参数的单个音效描述（思维链），实现可解释的细粒度控制。
  - **Transfusion Forcing**：结合了因果语言建模损失和经过改进的多轮扩散损失。关键的**扩散强迫**技术为对话中的每一轮音频独立采样不同的扩散时间步，防止模型在训练时简单地复制输入音频，从而解决了多轮编辑中条件信息高度相关导致的训练崩溃和泛化差的问题。

- **模型架构**：采用**自级联Transformer**架构。
  - 使用单一的连续音频分词器处理48kHz立体声音频。
  - 将Transformer层在理解任务（前U层，仅用LM头）和生成任务（剩余K-U层，同时用LM头和扩散目标）之间顺序划分。这种设计比需要复制整个LLM参数的MoT等架构更简单、灵活，且提升了音频生成质量。

- **统一建模**：通过上述方法，AudioChat成为首个能够通过结构化推理，在单一模型中统一完成音频故事**生成、编辑和理解**的模型，实现了对复杂声学场景的开放式、交互式控制。

3)  
论文在构建的评估集StoryGen-Eval上验证了AudioChat的效果：

- **音频编辑**：在添加、移除、调整声像/音量、更改声音、开放式编辑等六项任务上，AudioChat在音频质量、与原始音频的一致性、指令跟随性方面均优于所有基线模型（如DiT、扩散LLM、级联系统）。
- **音频故事生成**：在语义一致性（multiFLAM）上表现优异，同时推理延迟（32秒）远低于基于代理的WavJourney系统（628秒）。
- **音频理解**：在细粒度的语音理解（时间约束置换词错误率，tcpWER）和非语音音频理解（multiFLAM）任务上，性能接近甚至优于专门的专家系统（如WhisperX），证明了其在复杂声学环境下的鲁棒性。
</div>

</details>

---
