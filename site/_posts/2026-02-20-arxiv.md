---
layout: post
title: "arXiv Daily – 2026-02-20"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-20（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-19 08:50 — 2026-02-20 08:50
- 抓取总数：5 篇 | 本页显示：5 篇（去重/过滤后）

## Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment
- **Authors**: Ivan Rinaldi, Matteo Mendula, Nicola Fanelli, Florence Levé, Matteo Testi, Giovanna Castellano, Gennaro Vessio
- **Categories**: cs.CV, cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.17599v1](https://arxiv.org/abs/2602.17599v1)
- **PDF**: [https://arxiv.org/pdf/2602.17599v1](https://arxiv.org/pdf/2602.17599v1)

基于多模态深度学习的音乐生成技术已取得显著进展，使得模型能够从文本乃至图像合成音频。然而，现有的图像条件生成系统存在两个根本性局限：（1）它们通常在自然摄影图像上训练，难以捕捉艺术品中更丰富的语义、风格与文化内涵；（2）多数方法依赖图像到文本的转换阶段，将语言作为语义捷径，虽简化了条件控制，却阻碍了直接的视觉到音频学习。针对这些不足，本研究首先构建了ArtSound大规模多模态数据集，包含105,884组艺术品-音乐配对数据，并通过扩展ArtGraph与Free Music Archive为其增补了双模态描述文本。在此基础上，我们提出ArtToMus框架——首个专为直接实现艺术品到音乐生成而设计的系统。该框架将数字化艺术品直接映射为音乐，无需经过图像到文本的转换或基于语言的语义监督。其核心机制是将视觉嵌入向量投影至潜在扩散模型的条件空间中，实现完全由视觉信息引导的音乐合成。实验结果表明，ArtToMus生成的音乐在听觉上连贯且风格统一，能够反映源艺术品的显著视觉特征。尽管在绝对对齐分数上仍低于文本条件系统（这在去除语言监督后难度显著增加的情况下符合预期），但ArtToMus在感知质量与跨模态对应关系方面表现出竞争力。本研究确立了直接视觉到音乐生成这一独特且富有挑战性的研究方向，并为多媒体艺术、文化遗产保护及AI辅助创作等应用提供了资源支持。代码与数据集将在论文录用后公开发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
   - **数据局限性**：现有图像到音乐的生成系统通常在自然照片上训练，难以捕捉艺术品更丰富的语义、风格和文化内涵。
   - **方法瓶颈**：多数方法依赖“图像→文本→音乐”的转换流程，将语言作为语义中介。这虽然简化了条件控制，但：
     - 丢失了难以用语言精确描述的视觉细节（如纹理、风格）。
     - 阻碍了模型直接学习视觉特征与音乐结构之间的跨模态对应关系。

2) **论文核心方法如何解决上述问题**
   - **构建大规模数据集 ArtSound**：为解决数据稀缺问题，论文整合了 ArtGraph 和 Free Music Archive，构建了包含 105,884 个“艺术品-音乐”配对的大规模数据集 ArtSound。每个配对都通过多模态大模型生成了高质量的双模态（图像和音频）描述，并设计了 ICScore 和 ACScore 两个指标来评估描述质量。
   - **提出直接生成框架 Art2Mus**：这是首个为**直接**从艺术品生成音乐而设计的框架，完全绕过了文本中介。
     - **架构核心**：基于预训练的 AudioLDM 2（一个文本到音频的潜在扩散模型），但对其条件接口进行了根本性重构。
     - **视觉条件提取器**：使用预训练的视觉编码器（CLIP 或 ImageBind）提取艺术品图像的嵌入向量。
     - **图像对齐器**：通过一个可学习的投影层，将视觉嵌入向量映射到 AudioLDM 2 所使用的“音频语言”嵌入空间。这使得扩散模型能够直接理解并利用视觉信息进行条件生成。
     - **训练策略**：在训练过程中，**仅更新图像对齐器的参数**，而冻结 AudioLDM 2 的所有其他组件（包括扩散去噪器和 GPT-2 条件翻译器）。这有效利用了预训练模型的强大生成先验，同时专注于学习从视觉到音频语义空间的跨模态对齐。
   - **意义**：该方法迫使模型直接从视觉表示中学习跨模态对应关系，避免了语言抽象带来的信息损失，从而能够保留并利用艺术品的构图、风格等非语言化线索来指导音乐生成。

3) **在哪些任务上取得了怎样的效果**
   - **任务**：艺术品到音乐的跨模态生成。
   - **效果**：
     - **客观评估**：在 ArtSound 数据集上的实验表明，Art2Mus 生成的音乐在感知质量（FAD分数）和与源艺术品的跨模态一致性（ImageBind Score）上具有竞争力。虽然其绝对对齐分数仍低于依赖文本语义监督的基线模型（这是移除语言中介后任务难度增加的预期结果），但它实现了良好的权衡。
     - **主观评估**：人工评估显示，Art2Mus 生成的音乐在音质、表现力以及与艺术品的相关性和对齐度方面均获得认可，证明了其生成**音乐连贯、风格一致**且能反映源艺术品显著视觉线索的能力。
     - **贡献**：本研究确立了直接视觉到音乐生成作为一个独立且具有挑战性的研究方向，并为多媒体艺术、文化遗产和AI辅助创意实践提供了资源（数据集和框架）。
</div>

</details>

---

## The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?
- **Authors**: Jayadev Billa
- **Categories**: cs.CL, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.17598v1](https://arxiv.org/abs/2602.17598v1)
- **PDF**: [https://arxiv.org/pdf/2602.17598v1](https://arxiv.org/pdf/2602.17598v1)

当前语音大语言模型主要执行隐式语音识别：在可通过文本转录解决的任务中，其行为机制与简单的Whisper→大语言模型级联系统完全等效。我们通过首次采用统一骨干网络的对比测试验证了这一结论，涵盖四种语音大语言模型和六项任务。Ultravox模型与其对应级联系统的表现统计无差异（κ=0.93）；对数透镜技术显示其隐藏状态中直接浮现文本表征；LEACE概念擦除实验证实，在测试的两种架构中文本表征均具有因果必要性，消除后模型准确率骤降至接近零。Qwen2-Audio则展现出实质性差异，表明级联等效性具有架构依赖性而非普适规律。对于多数实际应用场景，当前语音大语言模型实质是计算成本高昂的级联系统，且在噪声环境下性能更差——其在纯净条件下的优势在0 dB信噪比时会发生高达7.6%的性能逆转。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：端到端语音大语言模型（Speech LLMs）旨在直接处理音频，以捕捉传统ASR丢弃的副语言线索（如韵律、情感）。  
- **既有问题**：现有研究多关注聚合准确率，缺乏对模型内部处理机制的深入分析。具体问题包括：  
  - 未控制LLM主干网络的影响，导致比较结果混淆了音频处理架构与LLM推理能力的差异。  
  - 缺乏基于样本级别的行为一致性分析，无法区分真实架构差异与表面准确率相似性。  
  - 在仅需文本即可解决的任务上，语音LLMs是否真正利用了音频信息尚不明确。  

2)  
- **核心方法**：论文提出“匹配主干行为测试”方法，并形式化“级联等价假设”，通过多层面分析验证语音LLMs与ASR→LLM级联的等价性。  
  - **匹配主干测试**：为每个语音LLM构建使用相同LLM主干的Whisper→LLM级联，以隔离架构效应与主干效应。  
  - **行为指标**：使用Cohen's κ衡量样本级别一致性、条件错误重叠分析共享失败模式、McNemar检验检测系统性偏差。  
  - **机制分析**：  
    - **层间探测**：训练声学与文本线性探针，分析隐藏状态中声学特征与文本可解码性的变化。  
    - **Logit Lens**：通过模型自身的解嵌入矩阵可视化隐藏状态中文本的涌现过程。  
    - **LEACE概念擦除**：因果干预，通过线性擦除文本预测子空间，验证文本表示是否是行为驱动的必要条件。  
  - **噪声鲁棒性测试**：在多种信噪比条件下评估模型性能，分析分布偏移下的行为变化。  
- **解决思路**：该方法首次系统性地控制了主干网络变量，并通过行为与机制证据的结合，揭示了不同语音LLM架构在“级联等价性”上的光谱分布及其原因。  

3)  
- **任务与效果**：在六项任务上评估了四种语音LLM与五种级联系统。  
  - **文本充足任务**（如AG News主题分类、SST-2情感分析、CommonsenseQA）：  
    - Ultravox与其匹配级联行为高度一致（κ最高达0.93），机制上显示其内部构建了类文本表示。  
    - Qwen2-Audio则表现出真实架构差异（κ较低），其交叉注意力编码器已提前完成语音到文本转换。  
  - **文本不足任务**（如MELD情感识别、MUStARD讽刺检测）：所有模型的κ均下降，但匹配主干分析表明主干混淆效应显著。  
  - **噪声条件**：在0 dB信噪比下，基于Whisper的级联性能下降更小（0.5–4.2%），而端到端模型下降更大（5.9–12.7%），级联展现出更优的噪声鲁棒性。
</div>

</details>

---

## Voice-Driven Semantic Perception for UAV-Assisted Emergency Networks
- **Authors**: Nuno Saavedra, Pedro Ribeiro, André Coelho, Rui Campos
- **Categories**: cs.NI, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.17394v1](https://arxiv.org/abs/2602.17394v1)
- **PDF**: [https://arxiv.org/pdf/2602.17394v1](https://arxiv.org/pdf/2602.17394v1)

无人机辅助网络正日益被视为应急响应中极具前景的技术路径，能够在陆地基础设施受损或缺失的环境中提供快速、灵活且鲁棒的通信支持。在此类场景中，语音无线电通信因其强健性仍是救援人员的关键通信手段；然而，其非结构化特性阻碍了其与无人机辅助网络自动化管理系统的直接集成。本文提出SIREN——一种基于人工智能的语音驱动感知框架，旨在实现无人机辅助网络的语音信息理解。该框架通过融合自动语音识别技术、基于大语言模型的语义提取以及自然语言处理验证，将应急语音通信内容转化为结构化的机器可读信息，包括响应单位、位置参照、紧急程度及服务质量需求等关键要素。研究采用合成应急场景对SIREN进行评估，通过控制语言类型、说话者数量、背景噪声及信息复杂度等变量进行测试。结果表明，该系统在不同操作条件下均能实现稳定的语音转写与可靠的语义提取，同时指出说话人分离与地理信息歧义是当前主要限制因素。这些发现验证了语音驱动态势感知在无人机辅助网络中的可行性，为应急响应行动中的人机协同决策支持与自适应网络管理奠定了实践基础。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：无人机辅助网络在应急响应中前景广阔，但地面基础设施受损时，救援人员仍依赖语音无线电通信。  
- **既有问题**：语音通信是非结构化的，无法直接集成到自动化的无人机网络管理中，限制了协同决策和态势感知能力。

2)  
- **核心方法**：论文提出SIREN框架，通过多阶段AI流水线将应急语音转换为结构化信息。  
  - **自动语音识别**：支持本地轻量模型或云端API，实现语音到文本转换，适应不同环境。  
  - **信息提取**：利用大语言模型进行语义提取，通过模式约束提示输出特定字段（如位置、单位、紧急程度、QoS需求）。  
  - **验证与增强**：结合命名实体识别验证地理位置，使用说话人日记化进行单位归属，通过情感分析校准紧急程度，减少幻觉。  
  - **结构化输出**：生成JSON格式数据，包含位置、紧急程度、单位及QoS需求，可直接用于无人机网络管理决策支持。  
- **解决思路**：通过整合ASR、LLM和NLP技术，将非结构化语音转化为机器可读的语义感知层，弥补传统遥感的不足，为自适应网络管理提供上下文感知输入。

3)  
- **评估任务**：在合成应急场景中测试，涵盖不同语言、说话人数量、背景噪声和消息复杂度。  
- **效果**：  
  - 转录准确率高，云端API模型在噪声环境下表现更稳健。  
  - 语义提取可靠，能成功识别位置、单位和QoS需求，但在说话人日记化（声音相似时）和地理歧义（地名解析）方面存在局限。  
  - 执行时间随场景复杂度增加，LLM处理是主要耗时环节，证明了语音驱动感知在无人机应急网络中的可行性。
</div>

</details>

---

## CC-G2PnP: Streaming Grapheme-to-Phoneme and prosody with Conformer-CTC for unsegmented languages
- **Authors**: Yuma Shirahata, Ryuichi Yamamoto
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.17157v1](https://arxiv.org/abs/2602.17157v1)
- **PDF**: [https://arxiv.org/pdf/2602.17157v1](https://arxiv.org/pdf/2602.17157v1)

本文提出CC-G2PnP，一种流式字素到音素及韵律模型，用于以流式方式连接大语言模型与文本转语音系统。该模型基于Conformer-CTC架构，通过逐块处理输入字素标记，实现对音素与韵律标签的流式推理。通过为每个输入标记保证最小前瞻长度，模型能够考虑各标记的未来上下文，从而获得稳定的音素与韵律标签预测。与以往依赖显式词边界的流式方法不同，CC-G2PnP中的CTC解码器在训练过程中有效学习字素与音素间的对齐关系，使其适用于无分词标记的语言。在无显式词边界的日语数据集上的实验表明，CC-G2PnP在音素与韵律标签预测准确率上显著优于基线流式G2PnP模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在语音对话系统中，级联方法（ASR-LLM-TTS）因其鲁棒性而受关注。为降低延迟，需对LLM输出的文本进行流式处理，这要求G2PnP（字素到音素及韵律）模型也支持流式推理。  
- **既有方法问题**：  
  - 简单分块方法：将输入文本分块后使用非流式G2PnP模型处理，但G2PnP高度依赖上下文，导致性能不稳定。  
  - 基于Transformer的流式方法（如LLM2PnP）：通过注意力掩码实现流式，但依赖显式的词边界，无法直接应用于日语、汉语等无分词语言。

2)  
论文提出 **CC-G2PnP** 模型，基于 **Conformer-CTC** 架构，通过以下机制解决上述问题：  
- **整体架构**：采用Conformer编码器与CTC解码器。CTC无需预定义对齐或词边界，可动态学习字素与音素/韵律标签的映射，从而适用于无分词语言。  
- **流式实现**：  
  - **分块感知流式**：将输入序列划分为固定大小的块（chunk），每个块内的token可相互关注，并允许关注过去固定数量的上下文。这保证了有限的前瞻（look-ahead），同时控制延迟不随层数线性增长。  
  - **最小前瞻**：针对分块中最后一个token无前瞻易导致预测不一致的问题，提出MLA机制，允许第一层自注意力额外关注当前块之外的M个未来token，确保所有token至少拥有M个token的前瞻，提升边界处预测稳定性。  
- **训练优化**：引入自条件CTC，在中间Conformer层添加辅助CTC损失，以缓解CTC的条件独立性假设，提升模型性能。

3)  
在 **日语G2PnP任务** 上进行了实验评估：  
- **客观指标**：在自建评测集6D-Eval上，CC-G2PnP（块大小5，MLA=1）在音素与韵律标签预测的CER/SER上显著优于流式基线模型（Dict-DNN），性能接近非流式模型。  
- **主观听感**：结合TTS合成语音，CC-G2PnP在自然度MOS上大幅超过流式基线，最佳流式模型得分（4.02）与非流式模型及真实标签接近。  
- **延迟**：在保持高性能的同时，仅需6个token的等待即可开始输出，实现了低延迟流式推理。
</div>

</details>

---

## AudioChat: Unified Audio Storytelling, Editing, and Understanding with Transfusion Forcing
- **Authors**: William Chen, Prem Seetharaman, Rithesh Kumar, Oriol Nieto, Shinji Watanabe, Justin Salamon, Zeyu Jin
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.17097v1](https://arxiv.org/abs/2602.17097v1)
- **PDF**: [https://arxiv.org/pdf/2602.17097v1](https://arxiv.org/pdf/2602.17097v1)

尽管近期取得突破性进展，音频基础模型在处理复杂多源声学场景时仍面临挑战。我们将这一具有挑战性的领域称为音频故事，其可能包含多个说话人及背景/前景音效。与传统音频处理任务相比，音频故事引入了语义、时序和物理层面的新复杂度。为应对这一挑战，我们提出AudioChat框架，用于开发能够生成、编辑和理解音频故事的音频基础模型。AudioChat引入了一种新范式：基于大语言模型的工具调用代理模拟用户与系统间的交互，并将这些模拟对话作为训练数据。我们还提出新颖的音频融合强制训练目标来训练AudioChat模型，使其能通过结构化思维链推理同时分解高层指令，并执行交互式多轮音频理解与生成。为评估生成与编辑性能，我们开发了三种直接衡量任务表现的新指标，而非依赖基于分布相似度的评分。我们强烈建议读者访问演示页面以更深入理解AudioChat的能力：https://wanchichen.github.io/audiochat/。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频基础模型在处理包含多个说话者和背景/前景音效的复杂多源声学场景（即“音频故事”）时存在困难。这类场景在语义、时间和物理层面引入了新的复杂性，超越了传统语音合成、文本到音频和音频描述等任务。
- **既有方法问题**：
  - 现有模型主要针对单一声音的理解与生成，难以处理多源场景。
  - 基于智能体的级联系统延迟高、可扩展性差，且无法对生成的音频进行细粒度控制和编辑。
  - 现有音频编辑模型通常只支持固定的操作集（如插入、删除），依赖于简单随机混合的音频片段，无法处理复杂的开放词汇编辑任务。
  - 缺乏能够直接衡量任务性能（而非基于分布相似性）的评估指标。

2)  
论文提出的AudioChat框架通过一系列创新方法解决上述问题：

- **数据生成范式创新**：为解决训练数据匮乏的问题，提出了**AudioCopilot**，一个基于大语言模型的工具调用智能体。它通过模拟用户与AI音效设计师之间的多轮对话，从零开始生成包含用户指令和期望输出音频的合成数据。该方法生成了600万次对话作为训练数据，为模型提供了细粒度的、语义连贯的多源音频场景。

- **核心训练目标**：提出了**音频Transfusion Forcing**目标，使模型能够在一个端到端模型中同时执行结构化推理和音频生成/编辑。
  - **Transfusion**：结合了因果语言建模损失和扩散模型损失，使模型具备联合理解与生成能力。
  - **Diffusion Forcing**：针对多轮音频编辑任务，为每一轮对话独立采样不同的扩散时间步，对输入音频上下文和目标音频分别添加噪声。这避免了模型在训练时简单地复制输入音频，解决了训练崩溃和训练-测试不匹配的问题，并支持并行训练任意轮次的编辑样本。

- **模型架构**：采用**自级联Transformer**架构，将Transformer层顺序分配给理解和生成任务。前U层仅用于理解（通过语言建模头优化），剩余层则同时用于生成（通过扩散目标优化）。这种设计比需要复制整个LLM参数的方法更简单、灵活，并提高了音频生成质量。

- **结构化推理**：模型利用**结构化思维链**推理，将抽象的用户指令分解为单个音效及其时间、空间（声像、响度）参数。这种细粒度的分解提供了可解释的处理过程和精确的控制能力。

- **统一建模**：AudioChat是首个通过结构化推理，统一处理包含语音和非语音音频的音频故事生成、编辑和理解任务的模型。

3)  
论文在以下任务上评估了AudioChat，并取得了显著效果：
- **音频编辑**：在包含开放编辑、添加/移除声音、调整声像/音量、更换声音等6个子任务的StoryGen-Eval基准上，AudioChat在音频质量、与原始音频的一致性以及指令跟随方面均优于所有基线模型（包括DiT、仅扩散LLM、级联系统）。
- **音频故事生成**：在语义一致性（multiFLAM）上表现优异，同时推理延迟（32秒）远低于基于智能体的基线WavJourney（628秒）。
- **音频理解**：在细粒度语音理解（时间约束置换词错误率）和非语音音频理解（multiFLAM）任务上，性能大幅优于专用语音识别系统WhisperX，并与在合成故事数据上微调的Whisper-Story模型表现相当。
</div>

</details>

---
