---
layout: post
title: "arXiv Daily – 2026-01-30"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-30（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-29 08:50 — 2026-01-30 08:50
- 抓取总数：14 篇 | 本页显示：14 篇（去重/过滤后）

## TidyVoice 2026 Challenge Evaluation Plan
- **Authors**: Aref Farhadipour, Jan Marquenie, Srikanth Madikeri, Teodora Vukovic, Volker Dellwo, Kathy Reid, Francis M. Tyers, Ingo Siegert, Eleanor Chodroff
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21960v1](https://arxiv.org/abs/2601.21960v1)
- **PDF**: [https://arxiv.org/pdf/2601.21960v1](https://arxiv.org/pdf/2601.21960v1)

说话人验证系统在语言不匹配情况下性能显著下降，这一关键挑战因该领域对英语中心化数据的依赖而加剧。为此，我们提出面向跨语言说话人验证的TidyVoice挑战赛。本挑战赛采用新型TidyVoice基准中的TidyVoiceX数据集——该数据集源自Mozilla Common Voice的大规模多语言语料库，并经过专门设计以隔离约40种语言间切换带来的影响。参赛者需构建对此类不匹配具有鲁棒性的系统，主要评估指标为跨语言测试中的等错误率。通过提供标准化数据、开源基线系统和严谨的评估协议，本挑战赛旨在推动研究向更公平、包容且语言无关的说话人识别技术发展，这与Interspeech 2026“共语未来”的主题高度契合。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前说话人验证系统严重依赖以英语为中心的数据进行训练和评估。当测试语言与训练语言不匹配时，系统性能会显著下降，这限制了其在多语言环境下的公平性和实用性。
- **既有方法的问题**：现有基准和数据集未能有效隔离和控制语言切换的影响，导致难以专门评估和提升系统在跨语言场景下的鲁棒性。此外，缺乏一个标准化的、大规模多语言的评估框架来推动语言无关的说话人识别研究。

2)  
本论文通过组织 **TidyVoice 2026 挑战赛** 并提出一套完整的评估方案来解决上述问题，其核心方法体现在以下几个方面：

- **构建专用数据集与评估框架**：
    - 挑战赛基于全新的 **TidyVoice 基准**，并专门使用了其 **TidyVoiceX 数据集**。该数据集源自 Mozilla Common Voice，但经过精心策划，包含了约 40 种语言的语音数据。
    - 关键设计在于**严格隔离语言切换效应**：数据组织确保可以清晰构建同一说话人不同语言、不同说话人同种语言等多种对比试验对，以精确衡量语言不匹配带来的影响。

- **设计严格的泛化能力测试**：
    - 训练集和开发集包含 40 种语言，而最终的评估集则包含 **38 种全新的、未在训练中出现的语言**。
    - 评估阶段包含两个试验列表：
        - `tv26_eval-A`：注册语音来自已见语言，测试语音来自未见语言。
        - `tv26_eval-U`：注册和测试语音均来自未见语言。
    - 此设计强制系统必须学习**语言无关的说话人特征**，并测试其向完全陌生语言泛化的能力。

- **提供标准化工具与基线**：
    - 提供**开源基线系统**（基于 SimAM-ResNet34 架构，在 VoxBlink2 和 VoxCeleb2 上预训练，并在 TidyVoiceX 上微调），为参与者提供起点和可复现的参考。
    - 制定清晰的**评估协议、性能指标（主要使用等错误率 EER）和提交格式**，确保比较的公平性和一致性。
    - 通过**禁止使用除指定分区外的 Mozilla Common Voice 数据**等规则，防止数据泄露，保证评估的纯净度。

- **引导研究方向**：整个挑战赛作为一个推动性平台，旨在激励社区开发对语言变化不敏感、更公平和包容的说话人验证技术。

3)  
- **评估任务**：任务聚焦于**跨语言说话人验证**。系统需要在注册语音和测试语音语言相同或不同的各种条件下，判断两者是否属于同一说话人。
- **取得的效果**：
    - 论文提出的评估方案（含数据集和协议）本身为衡量跨语言鲁棒性提供了**标准化基准**。
    - 官方基线系统在开发集上的整体 EER 为 3.07%，但其详细分析揭示出现有方法对语言信息存在不当依赖（例如，当目标对语言不同而非目标对语言相同时，EER 恶化至 5.19%），这**凸显了该挑战的必要性和所提出评估方法的有效性**，即能够精准暴露系统在语言不匹配时的弱点。
    - 该挑战赛预期将推动社区在此基准上取得更好的、真正语言无关的说话人验证性能。
</div>

</details>

---

## DisContSE: Single-Step Diffusion Speech Enhancement Based on Joint Discrete and Continuous Embeddings
- **Authors**: Yihui Fu, Tim Fingscheidt
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21940v1](https://arxiv.org/abs/2601.21940v1)
- **PDF**: [https://arxiv.org/pdf/2601.21940v1](https://arxiv.org/pdf/2601.21940v1)

基于离散音频编解码特征的扩散语音增强方法因其卓越的语音成分重建能力而备受关注。然而，由于需要多次反向迭代，这类方法通常面临推理计算复杂度高的问题。此外，它们在非侵入式指标上表现优异，但在侵入式指标上效果欠佳，往往难以准确重建音素。本文提出DisContSE，一种基于联合离散编解码标记与连续嵌入的高效扩散语音增强模型。我们的贡献包括三个方面：首先，我们构建了分别作用于离散音频编解码标记和连续嵌入的离散与连续增强模块，以同时提升语音的保真度和可懂度；其次，引入语义增强模块以优化音素准确性；第三，通过一种新颖的量化误差掩码初始化策略，实现了推理过程中高效的单步反向过程——据我们所知，这是首个基于音频编解码的成功实现单步扩散语音增强的方法。在URGENT 2024语音增强挑战赛的数据划分上进行训练与评估，所提出的DisContSE在PESQ、POLQA、UTMOS等客观指标及ITU-T P.808主观听力测试中均显著优于当前最优的时域与频域扩散基线方法，综合表现位居榜首。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于扩散模型的语音增强方法在提升语音质量和可懂度方面受到关注。现有方法主要分为两类：在频域复值表示上进行扩散，或在离散音频编解码器特征上进行扩散。
- **既有问题**：
  - **推理效率低**：两类方法通常依赖多步反向迭代过程，导致计算复杂度高。
  - **性能不平衡**：基于离散特征的方法在非侵入式指标上表现好，但在侵入式指标（如音素准确性）上较差，难以准确重建音素；而基于连续嵌入的方法虽在音素保真度上有优势，但同样面临高计算成本。

2)  
论文提出 **DisContSE** 模型，通过联合离散与连续嵌入的混合架构解决上述问题，核心方法包括三个模块和一个高效推理策略：

- **模块设计**：
  - **离散增强模块**：基于预训练的Descript音频编解码器（DAC）提取的离散令牌进行操作，采用掩码语言模型（如MaskGIT）进行训练，通过交叉熵损失优化，专注于提升语音的保真度和整体质量。
  - **连续增强模块**：作为判别式增强模块，对DAC编码的连续嵌入进行增强，使用连续语言模型并通过均方误差（MAE）损失优化。该模块提供可靠的初始状态，有助于实现高效的单步扩散。
  - **语义增强模块**：基于WavLM编码的语义特征进行增强，同样使用MAE损失优化，旨在提升音素准确性，减少幻觉现象。

- **训练策略**：
  - 采用联合损失函数，结合离散模块的交叉熵损失、自批判采样（self-critic）的二元交叉熵损失，以及连续和语义模块的MAE损失。
  - 为提升参数效率，离散增强模块与连续增强模块中的嵌入层共享权重。

- **高效推理**：
  - **单步反向过程**：在推理时，仅执行一次反向迭代，大幅降低计算复杂度。
  - **量化误差掩码初始化**：提出一种新颖的掩码生成策略，基于DAC量化误差矩阵选择掩码位置（而非完全随机或完全掩码），使模型能更准确地聚焦于需要修复的令牌，从而在单步内实现有效增强。
  - 该策略与连续增强模块提供的预增强特征相结合，确保了单步扩散的可靠性。

3)  
- **任务**：在URGENT 2024语音增强挑战数据集上进行了训练和评估，任务为通用语音增强（去噪、去混响等）。
- **效果**：
  - 在多项指标上超越现有先进的时域和频域扩散基线模型。
  - **侵入式指标**：在PESQ和POLQA上取得最佳性能。
  - **非侵入式指标**：在UTMOS上排名第一，在主观ITU-T P.808听力测试中获得最高平均意见得分（MOS）。
  - **可懂度与音素保真度**：在ESTOI和LPS（音素相似性）上位列第二，表明模型在保持低幻觉的同时具有良好的可懂度。
  - **综合排名**：在包含七种对比方法的综合排名中总体位列第一。
</div>

</details>

---

## Localizing Speech Deepfakes Beyond Transitions via Segment-Aware Learning
- **Authors**: Yuchen Mao, Wen Huang, Yanmin Qian
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21925v1](https://arxiv.org/abs/2601.21925v1)
- **PDF**: [https://arxiv.org/pdf/2601.21925v1](https://arxiv.org/pdf/2601.21925v1)

局部深度伪造音频的定位——即仅部分语音片段被篡改的情况——由于篡改痕迹细微且分散，仍具挑战性。现有方法通常依赖帧级预测来识别伪造片段，近期一些研究通过聚焦真实与伪造音频间的过渡区域来提升性能。然而，我们发现这些模型往往过度依赖边界伪影，而忽略了后续被篡改的内容。我们认为，有效的定位需要理解完整片段，而不仅仅是检测过渡区域。为此，我们提出片段感知学习框架，该框架引导模型关注片段的内部结构。SAL包含两项核心技术：片段位置标注——根据片段内相对位置提供细粒度帧级监督；以及跨片段混合——一种生成多样化片段模式的数据增强方法。在多个深度伪造定位数据集上的实验表明，SAL在域内和跨域场景中均表现优异，在非边界区域提升显著，且降低了对过渡伪影的依赖。代码已开源：https://github.com/SentryMao/SAL。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音深度伪造检测面临部分伪造（仅片段被篡改）的挑战，其篡改痕迹细微且分散。  
- **既有方法问题**：现有方法多依赖帧级预测，近期方法虽通过关注真假音频间的过渡区域提升性能，但模型过度依赖边界伪影，而忽略了对篡改内容本身的识别，导致泛化能力受限。

2)  
论文提出**Segment-Aware Learning (SAL)**框架，旨在使模型关注音频片段的内部特征，而非仅依赖边界。其核心方法包括：  
- **Segment Positional Labeling (SPL)**：  
  - 在传统二分类（真/假）基础上，为每一帧添加其在所属连续片段内的相对位置标签（如起始、中间、结束、单帧）。  
  - 通过多任务损失（结合二分类损失与位置分类损失）训练模型，迫使模型学习片段的内部结构与声学连贯性，减少对边界线索的依赖。  
- **Cross-Segment Mixing (CSM)**：  
  - 作为一种数据增强技术，随机切割并混合不同语音样本的片段，生成具有多样长度、位置和边界类型的新训练样本。  
  - 通过暴露模型于更丰富的片段模式，打破其对固定过渡模式的记忆，提升对未见场景的泛化能力。  
- **整体框架**：以预训练SSL模型（如Wav2Vec2-XLSR、WavLM）为前端，结合轻量级Conformer模块，通过SPL与CSM共同优化，使模型从“过渡检测”转向“片段感知”。

3)  
SAL在多个部分伪造定位任务上取得优异效果：  
- **PartialSpoof数据集**：取得最高F1分数（97.09%），EER（3.00%）与最优方法相当。  
- **HAD数据集**：EER（0.05%）和F1分数（99.99%）均达到最新最优水平。  
- **跨数据集泛化**：在未见过的LlamaPartialSpoof数据集上，EER（35.52%）和F1分数（56.09%）显著优于基线，显示出强泛化能力。  
- **关键改进**：通过可视化与误差分析证实，SAL在非边界区域（尤其是中间片段）的检测准确率大幅提升，有效减少了基于边界伪影的捷径学习。
</div>

</details>

---

## Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts
- **Authors**: Michael Kuhlmann, Alexander Werning, Thilo von Neumann, Reinhold Haeb-Umbach
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21886v1](https://arxiv.org/abs/2601.21886v1)
- **PDF**: [https://arxiv.org/pdf/2601.21886v1](https://arxiv.org/pdf/2601.21886v1)

大量研究从语句或系统层面探讨语音自动评估。此类方法虽能有效评判整体质量，却难以解释特定评分背后的具体原因。帧级评分虽能提供更好的可解释性，但由于训练过程中缺乏强标注目标，预测帧级评分的模型往往难以优化和正则化。本研究证明，通过在语句级语音质量预测模型中引入基于片段的连续性约束，可显著降低帧级预测的随机性。进而，我们展示了帧级评分的两项应用：局部伪造语音场景检测，以及两种前沿文本转语音系统中的合成伪影识别。针对后者，我们通过听觉实验证实：在帧级评分较低定义的语音片段集合中，听者判定为低质量的频率显著高于随机对照组。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动语音质量评估模型通常从语句或系统层面预测整体质量分数，但无法解释分数背后的具体原因，缺乏可解释性。  
- **既有方法问题**：  
  - 现有帧级质量预测模型（如Quality-Net）在训练时缺乏强监督目标，难以优化和正则化。  
  - 局部语音伪影（如合成失真）会导致相邻帧的分数被“污染”，降低检测精度。  
  - 主观听测方法（如人工标记错误）虽能定位问题，但效率低、一致性差，且缺乏大规模数据集支持监督训练。

2)  
- **核心方法**：提出一种基于一致性约束的帧级语音质量预测框架，通过正则化提升局部伪影的定位能力。  
- **具体解决策略**：  
  - **嵌入一致性损失**：在训练中随机切割音频片段，通过最小化完整上下文编码与片段编码的均方误差，约束局部嵌入仅编码局部信息，减少长上下文依赖。  
  - **分数一致性损失**：对非线性解码器（如BLSTM）额外添加帧级分数的一致性约束，使用平均绝对误差确保片段与完整上下文的帧分数一致。  
  - **模型配置**：基于WavLM编码器，结合线性或BLSTM解码器，通过调整损失权重（λemb、λscores）平衡质量预测与一致性约束。  
- **效果提升机制**：  
  - 一致性约束显著降低帧分数的波动性（volatility），使分数变化更平滑。  
  - 在部分伪造检测任务中，将检测精度从31.7%提升至62.3%，证明其能更精确地定位局部失真。

3)  
- **任务与效果**：  
  - **部分伪造语音检测**：在PartialSpoof数据集上，使用一致性约束的模型（配置8）达到62.3%的检测精度，显著优于未正则化模型（31.7%）。  
  - **TTS合成伪影定位**：应用于StyleTTS2和F5-TTS系统，通过阈值化帧分数定位低质量片段。听测实验证实，检测到的片段中72%（StyleTTS2）和66%（F5-TTS）被标记为非自然语音，远高于随机对照组的35%和25%，有效识别了速度异常、非语音噪声等伪影类型。
</div>

</details>

---

## MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding
- **Authors**: Meng Yang, Jon McCormack, Maria Teresa Llano, Wanchao Su, Chao Lei
- **Categories**: cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21740v1](https://arxiv.org/abs/2601.21740v1)
- **PDF**: [https://arxiv.org/pdf/2601.21740v1](https://arxiv.org/pdf/2601.21740v1)

近期面向音频音乐的多模态大语言模型（MLLM）在音乐理解方面展现出强大能力，但作为音乐结构基础表征的符号音乐尚未得到探索。本研究提出了MIDI-LLaMA——首个面向符号音乐理解的指令跟随型MLLM。我们通过特征对齐与指令微调两阶段流程，将MIDI编码器MusicBERT与Llama-3-8B进行对齐。为支持训练，我们设计了可扩展的标注流程，为GiantMIDI-Piano数据集添加细粒度元数据注释，构建出MIDI-文本数据集。与在相同指令微调流程下仅训练MIDI转ABC记谱法的基线模型相比，MIDI-LLaMA在描述生成和问答任务中的语义对齐方面均显著优于基线。人工评估进一步证实了MIDI-LLaMA在音乐理解、情感识别、创造力和整体偏好方面的优势。这些发现表明，将符号音乐整合到大语言模型中能有效增强其音乐理解能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐信息检索（MIR）传统上依赖预定义标签，限制了开放交互。多模态大语言模型（MLLM）在音频音乐理解上取得进展，但**符号音乐**（如MIDI）这一能清晰表达音乐结构、噪声更低、更易编辑的模态尚未被探索。  
- **既有方法问题**：  
  - 先前研究多集中于音频，或将符号音乐（如MIDI）转换为ABC记谱法作为文本输入，**丢失了节奏、复调等细节**，限制了音乐理解的深度。  
  - **缺乏大规模符号音乐-文本配对数据集**，制约了多模态模型在该领域的发展。

2)  
论文核心方法 **MIDI-LLaMA** 通过一个两阶段训练流程，将符号音乐编码器与大语言模型对齐，以解决上述问题：  
- **模型架构**：  
  - 采用 **MusicBERT** 作为冻结的符号音乐编码器，它专为音乐结构设计，能捕捉音高、节奏、和声等核心特征。  
  - 采用 **Llama-3-8B** 作为冻结的语言模型骨干。  
  - 通过一个**可训练的线性投影层**，将MusicBERT提取的音乐嵌入映射到LLM的文本嵌入空间，生成“音乐令牌”并与文本令牌拼接，使LLM能联合处理符号音乐和语言。  
- **两阶段训练流程**：  
  1. **特征对齐阶段**：冻结两个编码器，仅训练投影层，使音乐嵌入能被LLM理解。  
  2. **指令微调阶段**：冻结音乐编码器，使用LoRA技术微调LLM，并继续更新投影层，使模型能遵循指令完成多样化的音乐理解任务。  
- **数据构建**：为解决数据稀缺问题，设计了一个**可扩展的自动标注流程**：  
  - 基于GPT-4o，从古典音乐网站爬取的文本中挖掘并提取**流派、风格、情感、创作背景与表达意图**等细粒度元数据。  
  - 人工验证显示标注质量高（89%-93%接受率）。  
  - 将GiantMIDI-Piano数据集中的每首乐曲分段，并利用GPT-4o生成大量问答对，最终构建了包含约2.3百万问答对的大规模符号音乐-文本数据集用于训练。

3)  
在以下任务上进行了评估，并取得了显著效果：  
- **音乐理解问答**：在语义对齐（ROUGE-L, BERTScore）上优于将MIDI转为ABC记谱法的文本基线（ABC-LLaMA），表明其能更准确地回答关于风格、情感等具体问题。  
- **音乐描述生成（字幕）**：在BLEU、METEOR、ROUGE-L和BERTScore所有自动指标上均大幅超越基线。  
- **人工评估**：在音乐理解准确性、情感识别、创造性和整体偏好上均获得明显优势。例如，在100个样本的比较中，MIDI-LLaMA在58个样本上被整体偏好，而基线仅为22个。
</div>

</details>

---

## Representation-Regularized Convolutional Audio Transformer for Audio Understanding
- **Authors**: Bing Han, Chushu Zhou, Yifan Yang, Wei Wang, Chenda Li, Wangyou Zhang, Yanmin Qian
- **Categories**: eess.AS, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21612v1](https://arxiv.org/abs/2601.21612v1)
- **PDF**: [https://arxiv.org/pdf/2601.21612v1](https://arxiv.org/pdf/2601.21612v1)

基于自举的自监督学习在音频理解领域已取得显著进展。然而，现有方法通常仅在单一粒度上操作，限制了其对复杂音频信号中多样化时频结构的建模能力。此外，从零开始自举表示的计算成本高昂，往往需要大量训练才能收敛。本研究提出卷积音频变换器，这是一个为解决上述挑战而设计的统一框架。首先，为捕捉层次化音频特征，该框架引入了多分辨率模块，用于聚合不同粒度的信息。其次，为提升训练效率，我们提出了表示正则化目标。该辅助任务借鉴生成式建模思想，通过将学生模型的预测与来自冻结的预训练外部编码器的高质量语义表示对齐，从而指导模型学习。实验结果表明，该框架在音频理解基准测试中显著优于基线方法。值得注意的是，其在AudioSet 20k数据集上取得了具有竞争力的性能，且收敛速度比现有方法快5倍。代码与模型检查点将于近期发布于https://github.com/realzhouchushu/CAT。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频自监督学习（SSL）在音频理解中取得进展，但现有方法存在两大问题。
- **既有方法的问题**：
  - **单粒度建模**：现有方法通常在单一固定粒度上处理音频，难以有效捕捉音频信号中多样的时频多尺度结构。
  - **训练效率低下**：基于自举（bootstrap）的方法从零开始学习表示，计算成本高，需要大量训练才能收敛。

2)  
论文提出**卷积音频变换器（CAT）**，通过两个核心设计解决上述问题：

- **多分辨率块（Multi-resolution Block）**：
  - 取代传统的单层补丁嵌入，使用分层卷积层在不同时间与频率尺度上提取和聚合特征。
  - 通过多个分辨率块（如分辨率{4,8,16}）捕获从细粒度纹理到粗粒度语义的层次化音频结构，从而解决单粒度建模的局限性。

- **表示正则化（Representation Regularization）**：
  - 将掩码预测任务视为一种隐式生成任务，引入一个辅助的表示正则化目标（\(L_r\)）。
  - 该目标利用冻结的、预训练的外部音频编码器（如CLAP、Audio-MAE）提取的高质量语义表示作为指导，对齐学生模型中间层的特征。
  - 这为表示学习提供了稳定的语义先验，有效“缩短”了从零开始的引导阶段，显著提升了训练效率和表示质量。

- **整体框架**：
  - CAT沿用师生自举范式，整体损失结合了补丁级损失（\(L_p\)）、全局损失（\(L_g\)）和表示正则化损失（\(L_r\)）。
  - 多分辨率块与表示正则化相辅相成，前者提供结构归纳偏置以捕获多尺度特征，后者提供语义引导以加速收敛和提升表示判别力。

3)  
CAT在多个音频理解基准任务上取得了显著效果：

- **AudioSet**：在AS-2M全集上达到50.2% mAP，在平衡子集AS-20K上达到47.8% mAP，均创下新SOTA，尤其在AS-20K上相比之前最佳方法提升显著。
- **ESC-50**：达到98.6%的准确率，展现了优秀的跨领域泛化能力。
- **Speech Commands V2**：达到98.3%的准确率，保持了竞争力。
- **训练效率**：仅需20k次迭代即可在AS-20K上达到37.9% mAP，相比基线方法实现**5倍加速收敛**，大幅降低了计算成本。
</div>

</details>

---

## Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs
- **Authors**: Jun Xue, Yi Chai, Yanzhen Ren, Jinshen He, Zhiqiang Tang, Zhuolin Yi, Yihuan Huang, Yuankun Xie, Yujie Chen
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.21463v1](https://arxiv.org/abs/2601.21463v1)
- **PDF**: [https://arxiv.org/pdf/2601.21463v1](https://arxiv.org/pdf/2601.21463v1)

语音编辑通过对原始语音进行细粒度的片段级操作，在保持整体感知自然度的同时实现语义反转。现有检测研究主要关注具有明显拼接痕迹的人工编辑语音，难以应对新兴的端到端神经语音编辑技术所生成的无缝声学过渡。为应对这一挑战，本研究首先构建了一个大规模双语数据集 AiEdit，该数据集利用大语言模型驱动精确的语义篡改逻辑，并采用多种先进神经语音编辑方法进行数据合成，从而填补了高质量语音编辑数据集的空白。在此基础上，我们提出了 PELM（先验增强音频大语言模型），这是首个通过将语音编辑检测与内容定位统一为音频问答任务的大模型框架。为缓解现有音频大模型中存在的固有伪造偏差和语义优先偏差，PELM 引入词级概率先验以提供显式声学线索，并进一步设计了基于质心聚合的声学一致性感知损失，以显式强化对细微局部分布异常的建模。大量实验结果表明，PELM 在 HumanEdit 和 AiEdit 数据集上均显著优于现有最优方法，分别实现了 0.57% 和 9.28%（定位任务）的等错误率。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音编辑技术通过精细的片段级操作实现语义反转，同时保持全局感知自然性。新兴的端到端神经语音编辑方法能生成无缝的声学过渡，隐蔽性极强。
- **既有方法问题**：现有检测研究主要针对具有明显拼接痕迹的人工编辑语音，难以应对神经编辑技术。相关高质量数据集稀缺，且现有方法（如基于边界感知或特征相似性的模型）过度依赖低层级信号痕迹，无法有效检测无拼接痕迹的编辑。

2)  
论文提出PELM框架，首次将语音编辑检测与内容定位统一为音频问答任务，通过两项核心设计解决上述问题：
- **引入词级概率先验**：利用一个帧级检测器生成帧级篡改概率，按词边界聚合为词级概率序列，作为明确的声学线索注入音频大语言模型。这为模型提供了外部声学证据，有效校准了模型的推理边界，缓解了音频大模型中常见的“伪造偏差”（过度预测伪造）问题。
- **设计基于质心聚合的声学一致性感知损失**：该损失函数在特征空间施加约束。对于真实语音，强制所有帧特征紧密聚集在质心周围；对于编辑语音，则最大化质心与异常帧（偏离最大的Top-K%帧）之间的距离。这明确迫使模型建模细微的局部分布异常，缓解了模型过度关注语义内容而忽视低层级声学信号的“语义优先偏差”。
- **统一框架优势**：通过将任务形式化为结构化音频问答，并整合上述先验与损失，PELM能够同时利用大语言模型的强大推理能力和明确的低层级声学线索，实现对无缝神经编辑的高精度检测与定位。

3)  
PELM在以下任务上取得了显著优于现有方法的效果：
- **在HumanEdit数据集上**：在内容定位任务上取得了0.57%的等错误率（EER），检测准确率达99.62%。
- **在更具挑战性的AiEdit数据集上**：在内容定位任务上取得了9.28%的EER，检测F1分数达97.19%，显著优于所有基线模型。
- **综合性能**：在两个数据集的平均（Pool）结果上，其检测与定位的EER分别低至4.46%和4.93%，确立了新的性能标杆。
</div>

</details>

---

## SemanticAudio: Audio Generation and Editing in Semantic Space
- **Authors**: Zheqi Dai, Guangyan Zhang, Haolin He, Xiquan Li, Jingyu Li, Chunyat Wu, Yiwen Guo, Qiuqiang Kong
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21402v1](https://arxiv.org/abs/2601.21402v1)
- **PDF**: [https://arxiv.org/pdf/2601.21402v1](https://arxiv.org/pdf/2601.21402v1)

近年来，文本到音频生成技术取得了显著进展，为声音创作者提供了将文本灵感转化为生动音频的强大工具。然而，现有模型主要在变分自编码器（VAE）的声学隐空间中进行操作，往往导致生成的音频与文本描述之间的对齐效果欠佳。本文提出SemanticAudio，一种直接在高层语义空间中进行音频生成与编辑的新框架。我们将该语义空间定义为一种紧凑表示，用于捕捉声音事件的全局身份与时序结构，而非细粒度的声学细节。SemanticAudio采用两阶段流匹配架构：语义规划器首先生成紧凑的语义特征以勾勒全局语义布局，随后声学合成器基于该语义规划生成高保真声学隐变量。借助这种解耦设计，我们进一步提出了一种无需训练的文本引导编辑机制，能够对通用音频进行精确的属性级修改，而无需重新训练模型。具体而言，该方法通过源文本与目标文本提示所导出的速度场差异，引导语义生成轨迹。大量实验表明，SemanticAudio在语义对齐方面优于现有主流方法。演示页面见：https://semanticaudio1.github.io/

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本到音频生成技术发展迅速，但现有主流模型通常在变分自编码器的声学潜空间直接建模。  
- **既有问题**：这种设计虽能保持低层声学保真度，却难以实现高层语义对齐。模型常无法准确捕捉文本意图，导致生成的音频事件在存在性和时序上与文本描述匹配不足。

2)  
论文提出 **SemanticAudio**，一个两阶段流匹配框架，通过在高层语义空间进行生成和编辑来解决上述问题。  

- **核心方法**：  
  - **语义空间定义**：将音频语义定义为描述声音事件身份、出现及时序的紧凑高层表示，与细粒度声学细节分离。  
  - **两阶段架构**：  
    1. **语义规划器**：从文本生成低维语义特征，勾勒全局事件布局。  
    2. **声学合成器**：基于语义规划，生成高保真声学潜表示，供VAE解码为音频。  
  - **训练策略**：先联合训练声学合成器与语义投影头，再固定投影头训练语义规划器，确保语义特征包含足够重建信息。  
  - **免训练编辑机制**：利用语义规划器学习的速度场，通过源文本与目标文本条件对应的速度场差异，在语义空间中引导生成轨迹，实现属性级修改，无需额外训练或复杂反转步骤。  

- **解决思路**：将生成过程解耦为先进行高层语义规划，再合成声学细节，使模型能更专注于捕捉文本的语义意图，从而改善对齐。

3)  
- **任务与效果**：  
  - **文本到音频生成**：在AudioCaps测试集上，取得了最先进的语义对齐分数（CLAP得分0.354），显著优于基线模型（如TangoFlux的0.318），证明了其在捕捉文本意图方面的优势。  
  - **免训练音频编辑**：在构建的“困难”编辑集上，实现了高语义一致性（CLAP得分0.3539），能够根据文本指令精确修改音频属性（如音色、氛围），且在没有源文本的情况下仍表现稳健。
</div>

</details>

---

## Understanding Frechet Speech Distance for Synthetic Speech Quality Evaluation
- **Authors**: June-Woo Kim, Dhruv Agarwal, Federica Cerina
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.21386v1](https://arxiv.org/abs/2601.21386v1)
- **PDF**: [https://arxiv.org/pdf/2601.21386v1](https://arxiv.org/pdf/2601.21386v1)

合成语音质量的客观评估仍是一项关键挑战。人工听音测试虽为黄金标准，但成本高昂且难以大规模实施。弗雷歇距离已成为一种有潜力的替代方案，但其可靠性高度依赖于嵌入表示的选择与实验设置。本研究系统评估了在不同嵌入表示及实验条件下弗雷歇语音距离及其变体语音最大均值差异的性能，并结合人工听音测试、TTS可懂度评估以及基于合成语音训练的ASR词错误率，验证了这些指标与感知质量的相关性。实验结果表明，采用WavLM Base+特征时，指标与人工评分具有最稳定的相关性。尽管弗雷歇语音距离及其变体无法完全替代主观评估，但研究表明它们可作为补充性、高性价比且可复现的度量标准，尤其适用于大规模或直接听音评估不可行的情况。代码已开源：https://github.com/kaen2891/FrechetSpeechDistance。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：合成语音质量的客观评估至关重要。人耳听音测试（MOS）是黄金标准，但成本高、难以规模化，且存在主观偏差。现有客观指标如词错误率（WER）无法全面衡量语音质量，尤其是在多说话人或噪声场景下，WER可能因自动语音识别（ASR）模型适应性问题而失真。  
- **既有方法问题**：弗雷歇语音距离（FSD）作为一种有潜力的客观指标，但其可靠性严重依赖于所选的语音嵌入表示和实验设置（如参考数据集、噪声条件）。先前研究使用不同的嵌入和设置，导致结果难以比较和复现，限制了FSD的实用性和一致性。

2)  
- **核心方法**：本文系统性地评估了FSD及其变体语音最大平均差异（SMMD），通过多维度实验设计来解决嵌入选择和设置敏感性问题。  
- **嵌入全面比较**：测试了五种语音嵌入（wav2vec2 Base、HuBERT Base、WavLM Base+、Whisper Base、ECAPA-TDNN），分析它们在干净及噪声条件下的表现，发现WavLM Base+特征与人类评分最稳定对齐。  
- **引入SMMD作为补充**：SMMD基于高斯核函数，不依赖嵌入服从正态分布的假设，提供了对分布差异的另一种稳健度量，尤其在嵌入非正态时可作为FSD的有效补充。  
- **系统化验证框架**：  
  - 通过添加高斯噪声和真实环境噪声（MS-SNSD），在不同信噪比下检验FSD/SMMD对质量下降的敏感性。  
  - 进行样本效率分析，验证FSD/SMMD在随机和基于说话人抽样下的收敛行为，证明其只需约3小时语音数据即可稳定。  
  - 结合多种客观指标（TTS可懂度、基于合成数据训练的ASR的WER）和主观人类MOS测试，综合评估FSD/SMMD的感知相关性。  
- **解决思路**：通过大规模实验确定最佳嵌入（WavLM Base+），并明确SMMD的适用场景，为研究者提供了可靠、可复现的配置建议，降低了因实验设置不同导致的混淆。

3)  
- **评估任务与效果**：在LibriSpeech数据集上测试了多个TTS模型（XTTS、YourTTS、Tacotron2、VITS）。  
- **质量评估**：FSD（使用WavLM嵌入）和SMMD与合成训练的ASR WER趋势一致，且与人类MOS评分呈正相关，能有效区分不同系统的质量。  
- **噪声鲁棒性**：在多数嵌入下，FSD/SMMD分数随噪声增加而上升（质量下降），符合预期；但Whisper和ECAPA嵌入表现不稳定。  
- **实际效用**：FSD和SMMD可作为低成本、可复现的辅助指标，尤其在大规模评估或无法直接进行听音测试时，能有效追踪模型性能，但不能完全替代主观评价。
</div>

</details>

---

## Towards Robust Dysarthric Speech Recognition: LLM-Agent Post-ASR Correction Beyond WER
- **Authors**: Xiuwen Zheng, Sixun Dong, Bornali Phukon, Mark Hasegawa-Johnson, Chang D. Yoo
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21347v1](https://arxiv.org/abs/2601.21347v1)
- **PDF**: [https://arxiv.org/pdf/2601.21347v1](https://arxiv.org/pdf/2601.21347v1)

尽管自动语音识别（ASR）通常以词错误率（WER）作为评估基准，但实际应用最终取决于语义的准确性。这种不匹配在构音障碍语音识别中尤为突出，因为发音不精确和不流畅可能导致严重的语义失真。为弥合这一差距，我们提出了一种基于大语言模型（LLM）的代理方法，用于ASR后修正：该代理作为“法官-编辑”系统，对ASR生成的top-k候选假设进行处理，保留高置信度片段，重写不确定部分，并支持零样本和微调两种模式。同时，我们发布了目前最大的构音障碍语音修正基准数据集SAP-Hypo5，以促进研究的可复现性和未来探索。在多维度评估中，我们的代理方法实现了14.51%的WER降低，并显著提升了语义准确性——在挑战性样本上，MENLI指标提升7.59个百分点，Slot Micro F1指标提升7.66个百分点。进一步分析表明，WER对领域偏移高度敏感，而语义指标与下游任务性能具有更强的相关性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：针对构音障碍语音，传统自动语音识别（ASR）系统常因发音不精确、不流畅等问题，产生语义扭曲的转录结果。  
- **既有方法问题**：现有研究多仅优化词错误率（WER），但实际应用（如字幕生成、口语理解）更依赖语义保真度；WER与语义保真度存在不匹配，尤其在领域偏移时WER不够鲁棒。

2)  
- **核心框架**：提出基于大语言模型（LLM）的智能体框架“Judge–Editor Agent”，对ASR输出的top-k假设进行后处理修正。该框架无需修改声学模型，支持零样本提示或轻量微调。  
- **工作流程**：  
  - **Judge角色**：分析多个假设之间的不一致性，识别高置信度（跨假设一致）的文本片段予以保留。  
  - **Editor角色**：对不确定的片段进行重写或融合，以更好地捕捉说话者意图。  
- **关键技术**：  
  - 采用轻量级自监督微调（如LoRA），仅更新<0.25%的参数，适应构音障碍语音特点。  
  - 设计重复短语截断算法，减少ASR幻觉对评估的干扰。  
- **解决思路**：通过联合Judge与Editor的决策，在保留可靠内容的同时修正语义失真，从而在WER降低之外，显著提升语义保真度与下游任务可用性。

3)  
- **任务与效果**：在发布的构音障碍语音基准SAP-Hypo5上评估：  
  - **WER降低**：微调后的智能体在错误样本上实现约14.51%的相对WER下降（从21.98%降至18.79%）。  
  - **语义指标提升**：MENLI得分提升7.59个百分点，Q-Emb、BERTScore等语义相似度指标均显著改善。  
  - **下游任务提升**：在口语理解任务中，意图准确率提升约3个百分点，槽位微平均F1提升7.66个百分点。
</div>

</details>

---

## Qwen3-ASR Technical Report
- **Authors**: Xian Shi, Xiong Wang, Zhifang Guo, Yongqi Wang, Pei Zhang, Xinyu Zhang, Zishan Guo, Hongkun Hao, Yu Xi, Baosong Yang, Jin Xu, Jingren Zhou, Junyang Lin
- **Categories**: cs.CL, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21337v1](https://arxiv.org/abs/2601.21337v1)
- **PDF**: [https://arxiv.org/pdf/2601.21337v1](https://arxiv.org/pdf/2601.21337v1)

本报告介绍了Qwen3-ASR系列模型，包含两款一体化语音识别模型与一款创新的非自回归语音强制对齐模型。Qwen3-ASR-1.7B与Qwen3-ASR-0.6B是支持52种语言与方言的语音识别模型，兼具语种识别功能。二者均基于大规模语音训练数据及其基础模型Qwen3-Omni强大的音频理解能力构建。除开源基准测试外，我们进行了全面的内部评估，因为语音识别模型在开源基准上的得分可能差异微小，但在实际场景中往往表现出显著的质量差距。实验表明，1.7B版本在开源语音识别模型中达到最优性能，与最强的商业API竞争力相当；而0.6B版本则在准确性与效率间取得最佳平衡，其平均首字延迟可低至92毫秒，并在128并发条件下实现1秒内完成2000秒语音转写。Qwen3-ForcedAligner-0.6B是基于大语言模型的非自回归时间戳预测器，可对11种语言的文本-语音对进行对齐。时间戳精度实验显示，该模型在准确率上超越三款主流强制对齐模型，并在效率与泛化能力方面更具优势。为加速语音识别与音频理解领域的社区研究，本系列模型均以Apache 2.0协议开源发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动语音识别正从传统端到端范式转向大型音频语言模型范式。后者能利用大语言模型的语言建模能力和世界知识，更自然地处理长文本转录、噪声鲁棒性、多语言/方言覆盖等挑战。
- **既有方法问题**：传统ASR模型在复杂真实场景（如噪声、口音、歌唱识别）中表现受限；现有时间戳预测方法通常作为后处理步骤（如CTC/CIF），在精度、速度和跨语言灵活性上存在不足。

2)  
论文提出了Qwen3-ASR模型家族，其核心方法通过以下设计解决上述问题：
- **架构与训练策略**：
  - 基于Qwen3-Omni基础模型，具备强大的音频理解能力。
  - 采用四阶段训练：AuT编码器预训练（使用4000万小时伪标签数据）、Omni多模态预训练、ASR监督微调（SFT，支持52种语言/方言）、ASR强化学习（提升噪声鲁棒性与稳定性）。
  - 引入动态注意力窗口（1秒至8秒），统一支持流式与离线推理。
- **创新模型组成**：
  - **Qwen3-ASR-1.7B/0.6B**：一体化ASR模型，支持语言识别与多语言/方言识别，在复杂声学环境（如噪声、歌唱）中表现鲁棒。
  - **Qwen3-ForcedAligner-0.6B**：首个基于LLM的非自回归语音强制对齐模型，将时间戳预测重构为槽填充任务，支持11种语言、任意粒度（词/句/段落）的时间戳预测，且无需语言特定音素集。
- **关键技术优势**：
  - 通过大规模多语言数据训练和因果训练策略，提升跨语言泛化能力与时间戳一致性。
  - 非自回归推理实现高效率，在长音频和跨语言场景下保持精度。

3)  
- **多语言ASR**：在30种语言和22种中文方言上达到领先水平，在公开基准（如LibriSpeech、Fleurs）和内部测试中优于多数开源模型，并与最强商业API竞争。
- **复杂场景识别**：在噪声环境、口音英语、儿童/老人语音、绕口令等内部测试中表现最佳；歌唱和带背景音乐歌曲的识别准确率超越主流基线。
- **时间戳预测**：Qwen3-ForcedAligner-0.6B在MFA标注和人工标注测试集上，累计平均偏移降低67%~77%，优于Montreal Forced Aligner等现有方法，且支持长音频和跨语言场景。
- **效率**：Qwen3-ASR-0.6B实现92ms平均首词延迟，在128并发下每秒处理2000秒音频，兼顾精度与部署效率。
</div>

</details>

---

## Evaluating Spatialized Auditory Cues for Rapid Attention Capture in XR
- **Authors**: Yoonsang Kim, Swapnil Dey, Arie Kaufman
- **Categories**: cs.HC, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21264v1](https://arxiv.org/abs/2601.21264v1)
- **PDF**: [https://arxiv.org/pdf/2601.21264v1](https://arxiv.org/pdf/2601.21264v1)

在时间紧迫的扩展现实（XR）场景中，用户在执行主要任务的同时需快速将注意力重新定向至危险、警报或指令，空间音频能够在不占用视觉带宽的情况下提供即时方向提示。然而，此类场景通常仅允许短暂的听觉暴露，要求用户无需长时间聆听或依赖头部转动优化即可快速判断声音方向。本文报告了一项关于XR中快速空间音频定位的受控探索性研究。通过使用HRTF渲染的宽带刺激声，从听者周围半密集方向呈现，我们量化了用户仅凭短暂音频推断粗略方向的准确度。进一步研究了短期视听反馈训练作为轻量级校准机制的效果。研究结果表明，简短的空间提示能够传递粗略方向信息，且即使短期校准也能提升用户对听觉信号的感知能力。尽管这些结果凸显了空间音频在快速注意力引导方面的潜力，但也表明仅凭听觉提示可能无法为复杂或高风险任务提供足够精度，空间音频在与其他感官模态或视觉提示互补、且不依赖头部转动优化时可能最为有效。本研究以空间音频为切入点，对可穿戴XR设备（如VR头戴显示器与AR智能眼镜）的初级注意力引导通道进行了初步探索，并为时间敏感场景下的刺激声选择与校准提供了设计参考。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在时间紧迫的XR场景（如工业维护、户外导航）中，用户需快速将注意力转向危险或目标，但视觉带宽有限。  
- **既有方法问题**：现有空间音频研究多依赖**长时间听觉暴露**或**头部转动细化**来精确定位，缺乏对**即时、短暂音频提示**在无视觉辅助下**粗粒度定向能力**的实证理解，且未充分探索**短期校准训练**对快速感知的影响。

2)  
- **核心方法**：本研究通过一项受控探索性实验（N=17），系统评估了在时间紧迫条件下，仅凭短暂空间音频提示进行快速定向的可行性。  
- **解决上述问题的具体设计**：  
  - **刺激设计**：使用HRTF渲染的**宽带音频刺激**（500-9000 Hz），覆盖ITD、ILD和频谱线索，以最大化空间线索可用性。  
  - **实验条件**：在音频播放时**固定头部朝向**并**移除视觉线索**，模拟“一次性”听觉提示场景，防止头部转动细化。  
  - **校准机制**：引入**短期视听反馈训练阶段**，让用户在听到空间音频的同时看到声源位置的视觉指示，以研究短期学习对校准听觉空间感知的影响。  
  - **评估指标**：量化**三维角距离误差**、**方向依赖性混淆**（如前-后、左-右、上-下）以及**用户信心**，并与随机基线比较。  
- **如何解决问题**：该方法直接测试了**单次短暂音频暴露**下的粗定位能力，明确了空间音频作为**即时注意力引导信号**的效能边界，并验证了短期校准对提升感知一致性的作用，为XR中快速音频通知的设计提供了实证基础。

3)  
- **任务与效果**：在**快速空间音频定位任务**中，参与者仅凭短暂（3秒）音频提示推断声源方向。  
- **主要效果**：  
  - **粗粒度定向可行**：定位准确性显著高于随机基线（p<0.001），例如，校准后约33%的响应落在目标45度锥角内。  
  - **方向依赖性明显**：左-右区分最准确（混淆率最低~6%），前-后（混淆率~46%）和上-下（混淆率~40%）混淆严重。  
  - **短期校准有效**：校准后整体定位误差显著降低（平均减少约3.8度），用户信心提升，但未能消除固有的方向混淆模式。
</div>

</details>

---

## Music Plagiarism Detection: Problem Formulation and a Segment-based Solution
- **Authors**: Seonghyeon Go, Yumin Kim
- **Categories**: cs.SD, cs.AI, cs.LG, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21260v1](https://arxiv.org/abs/2601.21260v1)
- **PDF**: [https://arxiv.org/pdf/2601.21260v1](https://arxiv.org/pdf/2601.21260v1)

近年来，音乐抄袭问题已成为日益紧迫的社会议题。随着音乐信息检索研究的深入，针对音乐抄袭相关问题的研究日益增多。然而，包括我们先前工作在内的许多研究，均未明确定义音乐抄袭检测任务的具体内涵。这种定义的缺失不仅阻碍了研究进展，也导致研究成果难以应用于实际场景。为解决这一问题，我们明确了音乐抄袭检测与其他音乐信息检索任务的区别，并系统阐述了该任务需要解决的核心问题。为此，我们构建了"相似音乐对"数据集以支持这一新定义的任务。此外，我们提出了一种基于片段转录的解决方案作为实现该任务的可行路径。演示系统与数据集已公开于 https://github.com/Mippia/ICASSP2026-MPD。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐抄袭已成为日益严重的社会问题。现有研究（包括作者先前工作）普遍缺乏对“音乐抄袭检测”任务的明确定义，导致研究进展缓慢且难以应用于实际场景。
- **既有方法的问题**：
  - 多数研究依赖人工构建的数据集，模型容易过拟合，泛化能力差。
  - 不同研究使用的数据集和评估指标不一致，难以直接比较。
  - 现有方法（如音频指纹、翻唱识别）通常关注整体歌曲相似性或声学特征，而抄袭可能仅涉及部分片段或特定音乐元素（如旋律），因此不适用于精确的抄袭检测。

2)  
论文提出了一种基于**音乐片段转录**的框架来解决上述问题，其核心流程分为三个阶段：

- **音乐片段转录**：
  - 将原始音频转换为结构化的音乐表示。使用一系列模型进行源分离、结构分析、人声转录、旋律转录与和弦转录。
  - 每个片段包含关键音乐信息（如时间范围、调性、和弦、歌词、曲式结构），形成可搜索的音乐片段库。

- **片段级相似性分析**（对应任务2：定位相似部分）：
  - 采用两种方法衡量片段相似性：
    - **基于音乐知识的算法方法**：计算钢琴卷帘、节奏起始点、和弦等元素的相似性，加权组合得分。该方法可解释性强，能说明具体哪些音乐元素相似。
    - **基于深度学习的方法**：使用孪生网络架构，分别微调音频预训练模型（MERT）和钢琴卷帘CNN模型，并尝试了双编码器交叉融合的多模态方法，以自动学习抄袭模式。

- **歌曲级过滤与检索**（对应任务1：从库中找出抄袭歌曲）：
  - 通过片段相似性结果，应用加权多数投票的过滤算法，聚合片段级匹配以确定歌曲级相似性，从而从大规模库中检索出可能抄袭的歌曲。

- **整体贡献**：
  - 明确定义了音乐抄袭检测任务，并区分于其他MIR任务。
  - 通过片段转录将音频转化为结构化表示，专注于**部分相似性**和**选择性音乐元素**的比较，克服了现有方法在局部抄袭检测上的不足。
  - 构建了包含真实抄袭案例的SMP数据集，支持任务评估。

3)  
- **任务**：在**片段级抄袭检测**（任务2）和**歌曲级检索**（任务1）上进行了评估。
- **效果**：
  - **片段级检测**：在SMP数据集上，最佳模型（MERT）在严格指标（Rec.1s@1）上达到25.6%的召回率。基于音乐知识的方法在不同数据规模下表现更稳定。
  - **歌曲级检索**：在Covers80数据集上，歌曲级检索性能（mAP 0.475）虽低于SOTA的翻唱识别模型，但系统能**精确定位相似片段的时间位置并提供解释**（如指出旋律或节奏相似），这是传统翻唱识别模型不具备的能力。
  - 系统在**可解释性**方面表现突出，即使在错误案例中也能提供具体的音乐理由。
</div>

</details>

---

## Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling
- **Authors**: Eunjung Yeo, Julie M. Liss, Visar Berisha, David R. Mortensen
- **Categories**: cs.CL, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21205v1](https://arxiv.org/abs/2601.21205v1)
- **PDF**: [https://arxiv.org/pdf/2601.21205v1](https://arxiv.org/pdf/2601.21205v1)

随着与构音障碍相关的神经系统疾病日益普遍，开发适用于多语言的自动化可懂度评估方法显得尤为重要。然而，现有方法大多局限于单一语言，或未能捕捉影响可懂度的语言特异性因素。本文提出一种多语言音素产出评估框架，该框架结合了通用音素识别与语言特异性音素解析，通过对比性音系特征距离实现音素到音位的映射及序列对齐。该框架可生成三项评估指标：音素错误率（PER）、音系特征错误率（PFER）以及新提出的无需对齐的度量——音素覆盖度（PhonCov）。在英语、西班牙语、意大利语和泰米尔语上的实验表明：PER受益于映射与对齐的结合，PFER仅需对齐即可优化，而PhonCov则通过映射得到提升。进一步分析证明，该框架能够捕捉具有临床意义的可懂度下降模式，与构音障碍语音的已知观察结果一致。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经系统疾病（如帕金森病）常导致构音障碍，影响言语清晰度。临床评估依赖言语治疗师的主观感知评分，耗时且难以规模化。  
- **既有方法问题**：现有自动评估方法多为单语言，或虽有多语言模型但仅依赖通用表征，忽略了语言特定的音位对比等结构因素，导致跨语言性能不佳。

2)  
- **核心框架**：提出两阶段多语言音位产生评估框架。  
- **阶段一：通用音素识别**：使用通用音素识别器将语音转写为与语言无关的国际音标序列，提供跨语言的语音基础表征。  
- **阶段二：语言特定音位解释**：基于每种语言的音位库存，利用对比性音系特征距离进行两项处理：  
  - **音素到音位映射**：将识别出的音素映射到目标语言中最接近的音位类别，模拟母语者的感知归类效应。  
  - **序列对齐**：使用加权Needleman-Wunsch算法，以音系特征距离作为替换成本，对预测序列和参考序列进行对齐，能区分细微的语言学差异。  
- **产出指标**：通过上述处理计算三个可解释的音位层面指标：  
  - **音位错误率**：衡量整体音位错误频率。  
  - **音系特征错误率**：衡量音位在特征空间上的偏差程度。  
  - **音位覆盖率**：无需对齐，衡量说话者实现的音位库存广度。  
- **解决思路**：该框架将语言通用表征与语言特定解释解耦，既利用了UPR的跨语言可扩展性，又通过音系学驱动的映射和对齐，捕捉了塑造清晰度的语言特定因素，从而兼顾了可扩展性与评估准确性。

3)  
- **评估任务与效果**：在英语、西班牙语、意大利语和泰米尔语的构音障碍语音数据集上，评估所提指标与临床清晰度评分的相关性。  
- **主要效果**：  
  - 所提的三个音位产生指标均显著优于基线声学特征和ASR词错误率。  
  - 语言特定处理（映射和/或对齐）普遍提升了指标与清晰度的相关性，其中PER受益于两者结合，PFER主要受益于对齐，PhonCov受益于映射。  
  - 分析表明，这些指标能捕捉到与临床观察一致的、随严重程度恶化的音位产生模式。
</div>

</details>

---
