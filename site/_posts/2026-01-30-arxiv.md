---
layout: post
title: "arXiv Daily – 2026-01-30"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-30（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-29 08:50 — 2026-01-30 08:50
- 抓取总数：10 篇 | 本页显示：10 篇（去重/过滤后）

## MIDI-LLaMA: An Instruction-Following Multimodal LLM for Symbolic Music Understanding
- **Authors**: Meng Yang, Jon McCormack, Maria Teresa Llano, Wanchao Su, Chao Lei
- **Categories**: cs.MM, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21740v1](https://arxiv.org/abs/2601.21740v1)
- **PDF**: [https://arxiv.org/pdf/2601.21740v1](https://arxiv.org/pdf/2601.21740v1)

近期面向音频音乐的多模态大语言模型（MLLM）在音乐理解方面展现出强大能力，但作为音乐结构基础表征的符号音乐尚未得到探索。本研究提出了MIDI-LLaMA——首个面向符号音乐理解的指令跟随型多模态大语言模型。我们通过特征对齐与指令微调两阶段流程，将MIDI编码器MusicBERT与Llama-3-8B进行对齐。为支持训练，我们设计了可扩展的标注流程，为GiantMIDI-Piano数据集添加细粒度元数据注释，构建出MIDI-文本数据集。在相同指令微调流程下，相较于将MIDI转换为ABC记谱法的基线模型，MIDI-LLaMA在描述生成与问答语义对齐方面表现显著更优。人工评估进一步证实了MIDI-LLaMA在音乐理解、情感识别、创造性及整体偏好方面的优势。这些发现表明，将符号音乐整合到大语言模型中能有效增强其音乐理解能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐信息检索（MIR）传统上依赖预定义标签，限制了开放交互。多模态大语言模型（MLLM）在音频音乐理解上取得进展，但**符号音乐**（如MIDI）作为音乐结构的基础表示，尚未被探索。  
- **既有问题**：  
  - 缺乏大规模符号音乐-文本配对数据集，制约了多模态研究。  
  - 现有方法常将MIDI转换为ABC记谱法作为文本输入，但**丢失了节奏、复调等细节**，限制了音乐理解的深度。  
  - 视觉或音频的MLLM无法直接处理符号音乐这一独特模态。

2)  
论文提出**MIDI-LLaMA**，首个面向符号音乐（MIDI）的指令跟随多模态大语言模型，通过以下方法解决上述问题：  
- **模型架构**：  
  - 采用**冻结的符号音乐编码器MusicBERT**（基于OctupleMIDI表示预训练）和**冻结的语言模型Llama-3-8B**。  
  - 通过一个**可训练的线性投影层**将MusicBERT输出的音乐嵌入映射到LLM的文本嵌入空间，生成“音乐令牌”并与文本嵌入拼接，使LLM能联合处理符号音乐和语言。  
- **两阶段训练流程**：  
  - **特征对齐阶段**：冻结编码器和LLM，仅训练投影层，使音乐嵌入可被LLM理解。  
  - **指令微调阶段**：冻结音乐编码器，使用LoRA微调LLM，并进一步更新投影层，使模型能执行多样化的音乐理解任务。  
- **数据构建**：  
  - 设计**基于GPT-4o的自动标注流程**，结合人工验证，为GiantMIDI-Piano数据集（古典钢琴MIDI）标注流派、风格、情感、表达意图等细粒度元数据，构建首个符号音乐-文本数据集。  
  - 从每首乐曲中分段提取20秒片段，并利用标注生成问答对，最终得到约2.3百万个QA对，用于指令微调。  
- **核心优势**：  
  - 直接处理MIDI符号表示，**保留了完整的音乐结构信息**（如节奏、和声）。  
  - 通过指令微调实现**开放式的、基于自然语言的音乐交互**，超越了传统基于标签的封闭系统。

3)  
在以下任务上评估，MIDI-LLaMA均优于文本基线（ABC-LLaMA）：  
- **音乐理解问答**：在ROUGE-L和BERTScore上表现更优，表明其答案与参考在**语义上更对齐**。  
- **音乐描述生成**：在BLEU、METEOR、ROUGE-L和BERTScore**全部指标上显著提升**，显示其能生成更准确、全面的描述。  
- **人工评估**：在音乐理解准确性、情感识别、创造性和整体偏好上**均获得明显偏好**（例如，在整体偏好上以58:22领先）。结果验证了符号音乐嵌入对于提升音乐理解的有效性。
</div>

</details>

---

## Representation-Regularized Convolutional Audio Transformer for Audio Understanding
- **Authors**: Bing Han, Chushu Zhou, Yifan Yang, Wei Wang, Chenda Li, Wangyou Zhang, Yanmin Qian
- **Categories**: eess.AS, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21612v1](https://arxiv.org/abs/2601.21612v1)
- **PDF**: [https://arxiv.org/pdf/2601.21612v1](https://arxiv.org/pdf/2601.21612v1)

基于自举的自监督学习在音频理解领域取得了显著进展。然而，现有方法通常仅在单一粒度上操作，限制了其对复杂音频信号中多样化时频结构的建模能力。此外，从零开始自举表示的计算成本高昂，往往需要大量训练才能收敛。本研究提出卷积音频变换器，这是一个旨在应对上述挑战的统一框架。首先，为捕捉层次化音频特征，该框架引入了多分辨率模块，以聚合不同粒度的信息。其次，为提升训练效率，我们提出了表示正则化目标。该辅助任务借鉴生成式建模思想，通过将学生模型的预测与来自冻结的预训练外部编码器的高质量语义表示对齐，从而指导学生模型的学习。实验结果表明，该框架在音频理解基准测试中显著优于基线方法。值得注意的是，其在AudioSet 20k数据集上取得了具有竞争力的性能，且收敛速度比现有方法快5倍。代码与模型检查点将于近期发布于https://github.com/realzhouchushu/CAT。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频自监督学习（SSL）在音频理解领域取得显著进展，其中基于自举（bootstrap）的方法（如data2vec、EAT）通过师生框架学习表征。  
- **既有问题**：  
  - **粒度单一**：现有方法通常在单一固定粒度上处理音频，难以有效建模音频信号中多样的时频多尺度结构。  
  - **训练低效**：从零开始自举表征计算成本高，需要大量训练时间和资源才能收敛。

2)  
论文提出**卷积音频Transformer（CAT）**，通过以下核心方法解决上述问题：  
- **多分辨率块（Multi-resolution Block）**：  
  - 替代传统的单层补丁嵌入，使用分层卷积层提取并聚合不同时间与频率尺度的特征。  
  - 通过多个分辨率块（如{4,8,16}）捕获从细粒度纹理到粗粒度语义的层次化音频结构，适应音频的多尺度特性。  
- **表征正则化（Representation Regularization）**：  
  - 将掩码预测任务视为隐式生成过程，引入辅助损失项（Lr）对齐学生模型中间特征与外部预训练编码器（如CLAP、Audio-MAE）的高质量语义表征。  
  - 通过冻结的外部编码器提供稳定语义指导，加速训练早期阶段的表征学习，避免“冷启动”问题。  
- **整体框架**：  
  - 沿用师生自举范式，结合补丁级损失（Lp）、全局损失（Lg）与表征正则化损失（Lr）进行联合优化。  
  - 多分辨率块增强特征提取能力，表征正则化提升训练效率，两者互补，共同提升模型性能。

3)  
CAT在多个音频理解任务上取得显著效果：  
- **AudioSet**：在AS-2M上达到50.2% mAP，在AS-20K上达到47.8% mAP，超越此前最佳方法（如ASDA），尤其在数据量较小的AS-20K上提升显著（+6.3% mAP）。  
- **ESC-50**：取得98.6%的准确率，创下新基准。  
- **Speech Commands V2**：保持竞争性性能（98.3%准确率）。  
- **训练效率**：仅需20k次迭代即可达到37.9% mAP（AS-20K），相比基线方法（需100k次迭代）实现**5倍加速收敛**，大幅降低计算成本。
</div>

</details>

---

## Unifying Speech Editing Detection and Content Localization via Prior-Enhanced Audio LLMs
- **Authors**: Jun Xue, Yi Chai, Yanzhen Ren, Jinshen He, Zhiqiang Tang, Zhuolin Yi, Yihuan Huang, Yuankun Xie, Yujie Chen
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.21463v1](https://arxiv.org/abs/2601.21463v1)
- **PDF**: [https://arxiv.org/pdf/2601.21463v1](https://arxiv.org/pdf/2601.21463v1)

语音编辑通过对原始话语进行细粒度片段级操控，在保持全局感知自然度的同时实现语义反转。现有检测研究主要关注具有显式拼接痕迹的人工编辑语音，难以应对新兴的端到端神经语音编辑技术所生成的无缝声学过渡。为应对这一挑战，我们首先构建了大规模双语数据集AiEdit，利用大语言模型驱动精确的语义篡改逻辑，并采用多种先进神经语音编辑方法进行数据合成，从而填补了高质量语音编辑数据集的空白。在此基础上，我们提出PELM（先验增强音频大语言模型），这是首个通过将语音编辑检测与内容定位统一建模为音频问答任务的大模型框架。为缓解现有音频大模型中存在的固有伪造偏差和语义优先偏差，PELM引入词级概率先验以提供显式声学线索，并进一步设计基于质心聚合的声学一致性感知损失，以显式强化对细微局部分布异常的建模。大量实验结果表明，PELM在HumanEdit和AiEdit数据集上均显著优于现有最优方法，分别实现了0.57%和9.28%（定位任务）的等错误率。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音编辑技术通过精细的片段级操作实现语义反转，同时保持全局感知自然性。新兴的端到端神经语音编辑方法能生成无缝的声学过渡，隐蔽性极强。
- **既有方法问题**：现有检测研究主要针对具有明显拼接痕迹的人工编辑语音，难以应对无缝的神经编辑。相关高质量数据集稀缺，且现有方法（如基于边界感知或特征相似性的模型）过度依赖低层级声学不连续性，无法有效检测高度一致的AI编辑语音。

2)  
论文提出PELM框架，首次将语音编辑检测与内容定位统一为音频问答任务，并通过两项核心设计解决上述问题：
- **引入词级概率先验**：利用帧级检测器生成帧级篡改概率，按词边界聚合为词级概率序列，作为明确的声学线索注入音频大语言模型。这为模型提供了外部声学证据，有效校准了其推理边界，缓解了音频大模型中常见的“伪造偏差”（过度预测篡改的倾向）。
- **设计基于质心聚合的声学一致性感知损失**：该损失函数在特征空间施加聚类约束。
    - 对于真实语音，强制所有帧特征紧密聚集在质心周围，优化全局紧凑性。
    - 对于编辑语音，则最大化质心与Top-K异常帧的分离度，以放大细微的局部声学分布异常。
- **统一任务建模**：通过结构化提示词，将检测与定位任务转化为可解析的文本生成问题，充分利用了大语言模型的推理与生成能力。

3)  
PELM在以下任务上取得了显著优于现有方法的效果：
- **检测任务**：在HumanEdit和AiEdit数据集上，检测准确率分别达到99.62%和95.2%，均优于基线模型。
- **内容定位任务**：在HumanEdit和AiEdit数据集上，定位的等错误率（EER）分别低至0.57%和9.28%，显著提升了对于无缝神经编辑的定位精度。综合评估表明，PELM在应对传统人工编辑和先进AI编辑攻击时均表现出优越的鲁棒性。
</div>

</details>

---

## SemanticAudio: Audio Generation and Editing in Semantic Space
- **Authors**: Zheqi Dai, Guangyan Zhang, Haolin He, Xiquan Li, Jingyu Li, Chunyat Wu, Yiwen Guo, Qiuqiang Kong
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21402v1](https://arxiv.org/abs/2601.21402v1)
- **PDF**: [https://arxiv.org/pdf/2601.21402v1](https://arxiv.org/pdf/2601.21402v1)

近年来，文本到音频生成技术取得了显著进展，为声音创作者提供了将文本灵感转化为生动音频的强大工具。然而，现有模型主要在变分自编码器（VAE）的声学隐空间中进行操作，往往导致生成的音频与文本描述之间的对齐效果欠佳。本文提出SemanticAudio，一种直接在高层语义空间中进行音频生成与编辑的新框架。我们将该语义空间定义为一种紧凑表示，用于捕捉声音事件的全局身份与时序结构，而非细粒度的声学细节。SemanticAudio采用两阶段流匹配架构：语义规划器首先生成紧凑的语义特征以勾勒全局语义布局，随后声学合成器基于该语义规划生成高保真声学隐变量。借助这种解耦设计，我们进一步引入无需训练的文本引导编辑机制，能够对通用音频实现精确的属性级修改，而无需重新训练模型。具体而言，该方法通过源文本与目标文本提示所导出的速度场差异来引导语义生成轨迹。大量实验表明，SemanticAudio在语义对齐方面优于现有主流方法。演示页面见：https://semanticaudio1.github.io/

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本到音频生成技术发展迅速，但主流方法通常在变分自编码器的声学隐空间直接建模。  
- **既有问题**：这种设计虽能保持低层声学保真度，却难以实现高层语义对齐，导致生成的音频事件在存在性和时序上与文本描述匹配不足。

2)  
- **核心方法**：提出SemanticAudio，一个两阶段流匹配框架，将生成过程解耦为高层语义规划与底层声学合成。  
- **语义规划器**：首先在紧凑的语义空间中，根据文本生成全局事件布局的语义特征。该空间使用预训练的感知编码器提取帧级语义表示，捕获声音事件的时序结构。  
- **声学合成器**：随后以语义特征为条件，在VAE隐空间中合成高保真声学细节。  
- **训练策略**：先联合训练声学合成器与投影头，确保语义特征保留足够信息；再固定投影头，训练语义规划器。  
- **免训练编辑机制**：利用语义规划器的速度场，通过源与目标文本提示的速度场差分，在语义空间中引导生成轨迹，实现属性级音频编辑，无需额外训练或复杂反转。

3)  
- **任务与效果**：  
  - **文本到音频生成**：在AudioCaps测试集上，取得了最先进的语义对齐分数（CLAP得分0.354），显著优于基线模型。  
  - **免训练音频编辑**：在构建的“困难”编辑集上，实现了高语义一致性（CLAP得分0.3539），能精准修改音色、氛围等属性，且无需源文本时仍表现稳健。
</div>

</details>

---

## Understanding Frechet Speech Distance for Synthetic Speech Quality Evaluation
- **Authors**: June-Woo Kim, Dhruv Agarwal, Federica Cerina
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2601.21386v1](https://arxiv.org/abs/2601.21386v1)
- **PDF**: [https://arxiv.org/pdf/2601.21386v1](https://arxiv.org/pdf/2601.21386v1)

合成语音质量的客观评估仍是一个关键挑战。人工听音测试虽为黄金标准，但成本高昂且难以大规模实施。弗雷歇距离已成为一种有前景的替代方案，但其可靠性高度依赖于嵌入特征的选择与实验设置。本研究在不同嵌入特征和条件下，对弗雷歇语音距离及其变体语音最大平均差异进行了全面评估。我们进一步结合人工听音测试、TTS可懂度测试以及基于合成语音训练的ASR词错误率，验证了这些指标与感知质量的相关性。实验结果表明，采用WavLM Base+特征时，指标与人工评分的一致性最为稳定。尽管弗雷歇语音距离和语音最大平均差异无法完全替代主观评估，但研究表明它们可作为补充性、高性价比且可复现的度量标准，尤其适用于大规模或直接听音评估难以实施的情况。代码已开源：https://github.com/kaen2891/FrechetSpeechDistance。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：合成语音质量的客观评估至关重要。人耳听测（如MOS）是黄金标准，但成本高、难以规模化。现有客观指标（如词错误率WER）无法全面衡量语音质量，且对说话人多样性或噪声敏感。
- **既有方法问题**：  
  - 弗雷歇语音距离（FSD）等指标虽被提出，但其可靠性严重依赖于**嵌入特征的选择**（如wav2vec2、Whisper等）和**实验设置**（如参考数据集、噪声条件）。  
  - 不同研究使用不同的嵌入和设置，导致结果难以比较和复现，限制了FSD作为通用基准的有效性。

2)  
- **核心方法**：本文系统评估了FSD及其变体语音最大平均差异（SMMD），通过多维度实验确定可靠配置，并验证其感知相关性。  
- **解决上述问题的具体做法**：  
  - **嵌入特征全面比较**：测试了五种语音嵌入（wav2vec2 Base、HuBERT Base、WavLM Base+、Whisper Base、ECAPA-TDNN），分析其对FSD/SMMD分数的影响。发现**WavLM Base+** 在多种条件下表现最稳定。  
  - **引入SMMD作为补充**：SMMD基于高斯核函数，不依赖嵌入特征的正态分布假设，提供了对FSD偏差问题的补充视角。  
  - **多条件验证**：  
    - **噪声鲁棒性测试**：在合成语音中添加高斯噪声和真实环境噪声（MS-SNSD），观察FSD/SMMD随信噪比变化的行为，确认其能反映质量下降。  
    - **样本效率分析**：通过随机采样和基于说话人采样，证明FSD/SMMD仅需约3小时语音数据即可收敛，对说话人多样性敏感，实用性强。  
  - **感知相关性验证**：  
    - 结合**人工MOS评测**、**TTS可懂度（WER）** 以及**基于合成语音训练的ASR的WER（synthetic-WER）**，综合评估FSD/SMMD与人类听感的一致性。  
    - 结果表明，FSD（尤其使用WavLM嵌入）与synthetic-WER趋势一致，且与MOS评分存在正相关，可作为成本低廉的替代评估方案。

3)  
- **评估任务与效果**：  
  - **合成语音质量评估**：在LibriSpeech数据集上测试了多个TTS模型（XTTS、YourTTS、Tacotron2、VITS）。使用WavLM嵌入的FSD能有效区分不同模型的质量，其得分与人工MOS趋势一致（如XTTS的FSD最低、MOS最高）。  
  - **噪声环境下的评估**：FSD/SMMD分数随噪声增加而升高（除Whisper和ECAPA嵌入外），表明其能捕捉语音质量退化。  
  - **效果总结**：FSD和SMMD**无法完全替代主观评测**，但可作为**高效、可复现的补充指标**，尤其在大规模评估或无法直接进行听感测试时具有实用价值。WavLM Base+被确定为最稳定的嵌入选择。
</div>

</details>

---

## Towards Robust Dysarthric Speech Recognition: LLM-Agent Post-ASR Correction Beyond WER
- **Authors**: Xiuwen Zheng, Sixun Dong, Bornali Phukon, Mark Hasegawa-Johnson, Chang D. Yoo
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.21347v1](https://arxiv.org/abs/2601.21347v1)
- **PDF**: [https://arxiv.org/pdf/2601.21347v1](https://arxiv.org/pdf/2601.21347v1)

尽管自动语音识别（ASR）通常以词错误率（WER）作为评估基准，但实际应用最终取决于语义的准确性。这一差异在构音障碍语音识别中尤为突出，因为发音不精确和不流畅可能导致严重的语义失真。为弥合这一差距，我们提出了一种基于大语言模型（LLM）的代理方法，用于ASR后修正：该代理作为“法官-编辑”系统，对ASR生成的top-k假设进行处理，保留高置信度片段，重写不确定部分，并支持零样本和微调两种模式。同时，我们发布了目前最大的构音障碍语音修正基准数据集SAP-Hypo5，以促进研究的可复现性与未来探索。在多维度评估中，我们的代理方法实现了14.51%的WER降低，并显著提升了语义准确性——在挑战性样本上，MENLI指标提升7.59个百分点，Slot Micro F1指标提升7.66个百分点。进一步分析表明，WER对领域偏移高度敏感，而语义指标与下游任务性能具有更强的相关性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：针对构音障碍语音，传统自动语音识别系统通常以词错误率为主要优化目标。然而，构音障碍语音因发音不精确、不流畅和韵律异常，常导致ASR输出在词级上看似合理但语义严重失真，无法满足真实应用对语义保真度的需求。  
- **既有方法问题**：现有基于大语言模型的ASR后处理方法大多仅优化词错误率，忽略了语义保真度；且缺乏针对构音障碍语音的公开基准数据集，评估体系单一，难以全面衡量系统在语义理解和下游任务上的实际表现。

2)  
论文提出一个基于大语言模型的**法官-编辑智能体**框架，用于对ASR输出的top-k候选假设进行后处理校正，以提升语义保真度。该方法的核心创新与解决思路如下：  
- **问题重构**：将后处理校正重新定义为智能体决策问题。LLM作为智能体，综合分析多个候选假设之间的不一致性，执行跨假设的置信度判断与编辑决策。  
- **双角色协同机制**：  
  - **法官角色**：评估候选假设中跨假设一致性高的可靠片段，予以保留。  
  - **编辑角色**：针对不确定的片段，进行重写或融合，以更好地捕捉说话者意图。  
- **轻量级部署模式**：  
  - 支持**零样本提示**，通过精心设计的指令模板让LLM直接执行法官-编辑任务。  
  - 支持**轻量微调**，使用LoRA等技术在构音障碍数据集上微调少量参数，使模型内化领域特定模式。  
- **技术细节增强**：  
  - 采用**重复短语截断算法**，自动检测并截断ASR可能产生的幻觉循环，确保评估可靠性。  
  - 模型训练时仅对输出令牌计算损失，加速收敛。  
- **方法优势**：  
  - **模型无关**：无需修改底层声学模型。  
  - **训练轻量**：参数更新比例极低，计算效率高。  
  - **以语义为中心**：决策核心是最大化输出与真实语义的对应，而非单纯最小化词级错误。

3)  
论文在构建的**SAP-Hypo5**构音障碍语音后处理基准上进行了评估，主要效果如下：  
- **整体性能**：经微调的法官-编辑智能体在测试集上实现了**14.51%的词错误率相对降低**，同时在语义和下游任务指标上获得显著提升。  
- **语义保真度**：在具有挑战性的错误样本子集上，语义指标大幅改善，其中**MENLI提升7.59个百分点**，Q-Emb和BERTScore也持续上升。  
- **下游任务效用**：下游口语理解任务表现同步增强，**意图准确率**和**槽位微平均F1分数**分别获得提升，其中**Slot Micro F1提升7.66个百分点**，证明了校正后文本在实际应用中的更好可用性。
</div>

</details>

---

## Qwen3-ASR Technical Report
- **Authors**: Xian Shi, Xiong Wang, Zhifang Guo, Yongqi Wang, Pei Zhang, Xinyu Zhang, Zishan Guo, Hongkun Hao, Yu Xi, Baosong Yang, Jin Xu, Jingren Zhou, Junyang Lin
- **Categories**: cs.CL, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21337v1](https://arxiv.org/abs/2601.21337v1)
- **PDF**: [https://arxiv.org/pdf/2601.21337v1](https://arxiv.org/pdf/2601.21337v1)

本报告介绍了Qwen3-ASR系列模型，包含两款一体化语音识别模型与一款创新的非自回归语音强制对齐模型。Qwen3-ASR-1.7B与Qwen3-ASR-0.6B是支持52种语言与方言的语音识别模型，兼具语种识别功能。二者均基于大规模语音训练数据及其基础模型Qwen3-Omni强大的音频理解能力构建。除开源基准测试外，我们进行了全面的内部评估，因为语音识别模型在开源基准上的得分可能差异微小，但在实际场景中往往表现出显著的质量差距。实验表明：1.7B版本在开源语音识别模型中达到最优性能，并与最强的商业API表现相当；0.6B版本则实现了最佳的精度-效率平衡，其平均首字响应时间可低至92毫秒，在128并发条件下能以1秒完成2000秒语音的转写。Qwen3-ForcedAligner-0.6B是基于大语言模型的非自回归时间戳预测器，可对11种语言的文本-语音对进行对齐。时间戳精度实验显示，该模型性能超越当前三种最强的强制对齐模型，并在效率与泛化能力方面更具优势。为加速语音识别与音频理解领域的社区研究，本系列模型均基于Apache 2.0协议开源发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自动语音识别正从传统端到端范式转向大型音频语言模型范式，以更好地利用语言模型的世界知识和建模能力，解决长音频转录、噪声鲁棒性、多语言/方言覆盖等挑战。
- **既有方法问题**：传统ASR模型在复杂真实场景（如噪声、口音、歌唱）中表现受限；现有时间戳预测方法多为后处理（如CTC/CIF），精度和灵活性不足，且缺乏统一的多语言解决方案。

2)  
论文核心方法是通过构建Qwen3-ASR模型家族来解决上述问题，具体包括：
- **模型架构与训练**：
  - 基于Qwen3-Omni基础模型，结合预训练的AuT编码器（支持动态注意力窗口，实现流式/离线统一推理）。
  - 采用四阶段训练：AuT预训练（大规模伪标签数据）、Omni预训练（多模态理解）、ASR有监督微调（多语言数据，支持语音检测与上下文利用）、强化学习（提升噪声鲁棒性与转录稳定性）。
- **关键创新**：
  - **Qwen3-ASR-1.7B/0.6B**：一体化ASR模型，支持52种语言/方言的语音识别与语种识别，在复杂环境（噪声、歌唱、长音频）中表现鲁棒。
  - **Qwen3-ForcedAligner-0.6B**：首个基于LLM的非自回归强制对齐模型，将时间戳预测重构为槽位填充任务，支持11种语言、任意粒度（词/句/段落）的时间戳预测，且无需语言特定音素集。
- **解决效果**：
  - 通过大规模数据训练与多阶段优化，显著提升了多语言覆盖、噪声鲁棒性及长音频处理能力。
  - 强制对齐模型采用非自回归推理，在保持高精度的同时大幅提升效率，并统一了多语言时间戳预测方案。

3)  
- **语音识别任务**：在公开与内部基准测试中，Qwen3-ASR-1.7B在开源ASR模型中达到SOTA，并与最强商业API竞争；0.6B版本在精度与效率间取得最佳平衡。具体包括：
  - 中英文识别领先；支持30种语言、22种方言，在口音、噪声、儿童/老人语音等复杂场景中鲁棒。
  - 歌唱与带背景音乐歌曲的转录准确率超越多数基线。
  - 支持流式推理，延迟低（0.6B版平均首词延迟92ms），吞吐量高（每秒处理2000秒音频）。
- **强制对齐任务**：Qwen3-ForcedAligner-0.6B在11种语言上时间戳预测准确率显著优于主流方法（如MFA、NFA），累计平均偏移降低67%~77%，且支持跨语言场景与长音频（最长300秒）。
</div>

</details>

---

## Evaluating Spatialized Auditory Cues for Rapid Attention Capture in XR
- **Authors**: Yoonsang Kim, Swapnil Dey, Arie Kaufman
- **Categories**: cs.HC, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21264v1](https://arxiv.org/abs/2601.21264v1)
- **PDF**: [https://arxiv.org/pdf/2601.21264v1](https://arxiv.org/pdf/2601.21264v1)

在时间紧迫的扩展现实（XR）场景中，用户在执行主要任务的同时需快速将注意力重新定向至危险、警报或指令，空间音频能够在不占用视觉带宽的情况下提供即时方向提示。然而，此类场景通常仅允许短暂的听觉暴露，要求用户无需长时间聆听或依赖头部转动优化即可快速判断声音方向。本文报告了一项关于XR中快速空间音频定位的受控探索性研究。通过使用HRTF渲染的宽带刺激，从听者周围半密集方向呈现，我们量化了用户仅凭简短音频推断粗略方向的准确度。进一步研究了短期视听反馈训练作为轻量级校准机制的效果。研究结果表明，简短的空间提示能够传递粗略的方向信息，且即使短期校准也能提升用户对听觉信号的感知能力。尽管这些结果凸显了空间音频在快速注意力引导方面的潜力，但也表明仅凭听觉提示可能无法为复杂或高风险任务提供足够精度，空间音频在与其他感官模态或视觉提示互补、且不依赖头部转动优化时可能最为有效。本研究以空间音频为切入点，对可穿戴XR设备（如VR头戴显示器与AR智能眼镜）的初级注意力引导通道进行了初步探索，并为时间敏感场景下的刺激选择与校准提供了设计参考。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在时间紧迫的XR场景中，用户需快速将注意力转向危险或目标，但视觉带宽有限。空间音频可作为非侵入式定向提示，但现有方法通常依赖长时间聆听或头部转动来精确定位，这在即时场景中不可行。  
- **既有问题**：缺乏对用户在时间受限、仅通过短暂音频暴露（无视觉辅助或头部细化）下，如何快速理解粗略方向的经验研究。现有工作未充分考察刺激特性与短期校准对快速空间音频感知的影响。

2)  
- **核心方法**：本研究通过一项受控探索性实验，量化用户在短暂音频暴露下对粗略方向的感知能力，并考察短期视听反馈训练的效果。  
- **具体设计**：  
  - **刺激与呈现**：使用HRTF渲染的宽带音频刺激，覆盖低、中、高频段以最大化ITD、ILD和频谱线索。音频从听者周围90个方向（方位角20°间隔，俯仰角30°间隔）呈现。  
  - **实验条件**：在预校准和后校准阶段，参与者仅通过短暂音频推断声源方向，期间头部方向固定且无视觉线索，以隔离听觉空间解释能力。  
  - **校准机制**：在预校准与后校准阶段之间，引入短期视听反馈训练阶段，将空间音频与声源位置的视觉指示配对，让参与者校准听觉感知。  
  - **测量指标**：包括方位角与俯仰角偏差、三维角距离、方向信心及感知混淆报告。  
- **解决问题**：  
  - 通过系统量化短暂暴露下的定位性能，为空间音频作为即时注意力引导机制提供了实证基础。  
  - 明确了不同方向区域（如前-后、左-右、上-下）的感知差异，揭示了方向依赖的模糊性模式。  
  - 证明了短期校准能改善整体方向推断和用户信心，尽管无法完全消除固有的感知混淆。

3)  
- **任务与效果**：在快速空间音频定位任务中，参与者仅通过短暂音频暴露进行方向判断。  
  - **定位准确性**：后校准阶段的三维角距离平均误差为65.38°，显著优于随机基线，表明空间音频能有效传达粗略方向信息。  
  - **方向依赖性**：左-右判断最稳定（混淆率最低），而前-后和上-下区分存在显著模糊性（混淆率较高）。  
  - **校准效果**：短期视听反馈训练使整体定位误差显著降低（平均减少3.81°），并提高了用户信心，尤其改善了前区和左区的定位准确性。
</div>

</details>

---

## Music Plagiarism Detection: Problem Formulation and a Segment-based Solution
- **Authors**: Seonghyeon Go, Yumin Kim
- **Categories**: cs.SD, cs.AI, cs.LG, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21260v1](https://arxiv.org/abs/2601.21260v1)
- **PDF**: [https://arxiv.org/pdf/2601.21260v1](https://arxiv.org/pdf/2601.21260v1)

近年来，音乐抄袭问题已成为愈发紧迫的社会议题。随着音乐信息检索研究的深入，针对音乐抄袭相关问题的研究日益增多。然而，包括我们先前工作在内的许多研究，均未明确定义音乐抄袭检测任务的具体内涵。这种定义的缺失不仅延缓了研究进展，也导致研究成果难以应用于实际场景。为改善这一状况，我们明确了音乐抄袭检测与其他音乐信息检索任务的区别，并系统阐述了该任务所需解决的核心问题。为此，我们构建了"相似音乐对"数据集以支持这一新定义的任务。此外，我们提出了一种基于片段转录的解决方案作为实现该任务的可行路径。演示系统与数据集已公开于 https://github.com/Mippia/ICASSP2026-MPD。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐抄袭已成为日益严重的社会问题。现有研究虽多，但普遍缺乏对“音乐抄袭检测”任务的明确定义，导致研究进展缓慢且难以应用于实际场景。
- **既有方法的问题**：
  - 任务定义模糊：许多研究未清晰界定该任务，使其与音频指纹识别、翻唱歌曲识别等其他音乐信息检索任务混淆。
  - 数据集不真实：多数方法依赖人工构建的数据集，模型容易过拟合，泛化到真实抄袭案例的能力有限。
  - 评估不一致：不同研究使用的数据集和评估指标各异，难以直接比较方法优劣。

2)  
论文提出了一种基于**片段转录**的解决方案，其核心流程分为三个阶段，以应对抄袭检测的独特挑战：

- **音乐片段转录**：
  - 将原始音频通过一系列模型（如源分离、结构分析、旋律转录、和弦转录等）转换为结构化的音乐表示。
  - 每个片段包含关键音乐信息（如旋律、和弦、歌词、曲式结构），形成可索引的音乐片段库。这解决了抄袭可能仅涉及**特定音乐元素**（如仅旋律相同）而非整体音频信号的问题。

- **片段级相似性分析**：
  - 采用两种方法衡量片段相似性：
    1.  **基于音乐知识的算法**：计算钢琴卷帘、节奏、和弦等元素的相似性，并进行加权组合。该方法能**解释**抄袭的具体原因（如旋律相似），对应任务3。
    2.  **基于深度学习的模型**：使用Siamese网络架构，分别微调音频预训练模型（MERT）和钢琴卷帘CNN模型，并尝试了双编码器交叉融合的多模态方法。这些模型旨在自动学习抄袭模式。
  - 该阶段专注于**片段级**的精确比对，解决了抄袭常表现为**局部相似**而非整曲相似的核心难题，对应任务2。

- **歌曲级过滤与检索**：
  - 通过一个过滤算法，对片段级匹配结果进行加权多数投票，从而从大型数据库中检索出可能抄袭的歌曲（任务1）。
  - 系统最终能输出抄袭的具体时间段及其相似的音乐元素原因，实现了对抄袭行为的精确定位和解释。

3)  
- **任务与效果**：
  - **片段级抄袭检测**：在提出的SMP数据集上，评估模型在给定查询片段后，能否在大型片段库中精确检索到相似片段（时间误差在1秒内）。最佳模型（MERT）在较小规模库上取得了最高51.0%的Recall@10，但性能随数据库规模扩大而下降，显示了大规模检索的挑战。
  - **歌曲级抄袭检测**：在Covers80数据集上，评估系统从歌曲库中找出抄袭歌曲的能力。其mAP（0.475）虽低于顶尖的翻唱识别模型，但**优势在于能提供精确的片段级匹配位置和可解释的抄袭理由**（如“人声旋律相似”），这是传统翻唱识别模型所不具备的，更贴合实际抄袭检测的司法与鉴定需求。
</div>

</details>

---

## Multilingual Dysarthric Speech Assessment Using Universal Phone Recognition and Language-Specific Phonemic Contrast Modeling
- **Authors**: Eunjung Yeo, Julie M. Liss, Visar Berisha, David R. Mortensen
- **Categories**: cs.CL, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.21205v1](https://arxiv.org/abs/2601.21205v1)
- **PDF**: [https://arxiv.org/pdf/2601.21205v1](https://arxiv.org/pdf/2601.21205v1)

随着与构音障碍相关的神经系统疾病日益普遍，开发适用于多语言的自动化可懂度评估方法显得尤为重要。然而，现有方法大多局限于单一语言，或未能捕捉影响可懂度的语言特异性因素。本文提出一种多语言音素产出评估框架，该框架结合了通用音素识别与语言特异性音素解析，通过对比性音系特征距离实现音素到音位的映射与序列对齐。该框架产出三项指标：音素错误率（PER）、音系特征错误率（PFER）以及新提出的无需对齐的度量——音素覆盖度（PhonCov）。在英语、西班牙语、意大利语和泰米尔语上的实验表明：PER受益于映射与对齐的结合，PFER仅需对齐即可提升效果，而PhonCov则通过映射得到优化。进一步分析证明，该框架能够捕捉具有临床意义的可懂度下降模式，与构音障碍语音的已知观察结果一致。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：构音障碍与多种神经系统疾病相关，其全球患病率上升，亟需可跨语言应用的自动化可懂度评估工具。  
- **既有方法问题**：  
  - 现有临床评估依赖言语治疗师的主观感知评分，耗时且难以扩展。  
  - 多数自动评估方法仅针对单一语言（如英语），或未能捕捉影响可懂度的语言特异性因素（如音位对比）。  
  - 仅基于通用表征的多语言模型常表现不佳，且缺乏临床可解释性。

2)  
论文提出一个两阶段多语言音位产出评估框架，核心方法如下：  
- **通用音素识别**：使用通用音素识别器将语音转录为语言无关的国际音标序列，为跨语言分析提供基础。  
- **语言特异性音位解释**：  
  - **音位对比建模**：基于每种语言的音位库存，利用PanPhon识别对比性语音特征（如浊音性、发音部位），并计算加权的语音特征距离。  
  - **音素到音位映射**：将UPR输出的音素映射到目标语言中最接近的音位，模拟母语者的感知范畴化效应。  
  - **序列对齐**：采用加权Needleman-Wunsch算法，以语音特征距离作为替换成本，对预测序列和参考序列进行对齐，以反映音位相似性。  
- **产出可解释指标**：  
  - **音位错误率**：衡量音位替换、插入和删除的比例。  
  - **语音特征错误率**：基于对齐计算音位间的特征距离，捕捉细微的发音偏差。  
  - **音位覆盖率**：无需对齐，计算说话者产出的音位库存覆盖比例，反映音位多样性减少。  
该方法通过结合通用识别与语言特异性解释，在保持可扩展性的同时，使评估更贴合每种语言的音位结构。

3)  
- **评估任务**：在英语、西班牙语、意大利语和泰米尔语的构音障碍语音数据集上，评估所提音位产出指标与临床可懂度评分的相关性。  
- **取得效果**：  
  - 所提指标（PER、PFER、PhonCov）与临床评分的相关性均优于基线声学特征和ASR词错误率。  
  - 语言特异性处理（映射和/或对齐）显著提升了非英语语言的关联强度，其中PER受益于映射与对齐结合，PFER主要受益于对齐，PhonCov受益于映射。  
  - 分析表明，指标能捕捉与临床观察一致的构音障碍严重度模式，如错误类型分布、发音偏差增大及复杂音位更易受损。
</div>

</details>

---
