---
layout: post
title: "arXiv Daily – 2026-01-21"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-21（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-20 08:50 — 2026-01-21 08:50
- 抓取总数：4 篇 | 本页显示：4 篇（去重/过滤后）

## Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification
- **Authors**: HyeYoung Lee
- **Categories**: cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.13589v1](https://arxiv.org/abs/2601.13589v1)
- **PDF**: [https://arxiv.org/pdf/2601.13589v1](https://arxiv.org/pdf/2601.13589v1)

本文提出一种多智能体人工智能系统，能够基于音频衍生的情感信号实时生成面向响应的媒体内容。与传统语音情感识别研究主要关注分类精度不同，我们的方法强调通过结构化专用智能体流水线，将推断出的情感状态转化为安全、适龄且可控的响应内容。该系统包含四个协同工作的智能体：(1) 采用基于CNN的声学特征提取的情感识别智能体，(2) 负责将情感映射至响应模式的响应策略决策智能体，(3) 生成媒体控制参数的内容参数生成智能体，以及(4) 强制执行适龄性与刺激约束的安全验证智能体。我们引入了显式安全验证循环机制，在输出前对生成内容进行过滤，确保其符合预设规则。在公开数据集上的实验结果表明，该系统在保持低于100毫秒推理延迟（适合端侧部署）的同时，实现了73.2%的情感识别准确率、89.4%的响应模式一致性以及100%的安全合规率。模块化架构赋予系统可解释性与可扩展性，使其可应用于儿童相关媒体、治疗场景及情感响应型智能设备。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.13589v1）
</div>

</details>

---

## LongSpeech: A Scalable Benchmark for Transcription, Translation and Understanding in Long Speech
- **Authors**: Fei Yang, Xuanfan Ni, Renyi Yang, Jiahui Geng, Qing Li, Chenyang Lyu, Yichao Du, Longyue Wang, Weihua Luo, Kaifu Zhang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.13539v1](https://arxiv.org/abs/2601.13539v1)
- **PDF**: [https://arxiv.org/pdf/2601.13539v1](https://arxiv.org/pdf/2601.13539v1)

近期音频-语言模型在短片段语音任务上取得了显著进展。然而，实际应用场景如会议转录、口语文档理解与会话分析等，均需要能够处理并推理长音频的鲁棒模型。为此，我们提出LongSpeech——一个专为评估和提升语音模型在长时音频处理能力而设计的大规模可扩展基准。LongSpeech包含超过10万个语音片段，每段时长约10分钟，并提供了丰富的标注信息，涵盖自动语音识别、语音翻译、摘要生成、语言检测、说话人计数、内容分割及问答等多个任务。我们提出了一套可复现的构建流程，能够从多样化数据源中构建长语音基准，支持未来扩展。基于当前前沿模型的初步实验表明，现有模型存在显著的性能差距：它们往往仅擅长单一任务而难以兼顾其他，且在高层推理任务上表现欠佳。这些发现凸显了本基准的挑战性。我们将公开此基准，以促进研究社区的发展。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2601.13539v1）
</div>

</details>

---

## ICASSP 2026 URGENT Speech Enhancement Challenge
- **Authors**: Chenda Li, Wei Wang, Marvin Sach, Wangyou Zhang, Kohei Saijo, Samuele Cornell, Yihui Fu, Zhaoheng Ni, Tim Fingscheidt, Shinji Watanabe, Yanmin Qian
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.13531v1](https://arxiv.org/abs/2601.13531v1)
- **PDF**: [https://arxiv.org/pdf/2601.13531v1](https://arxiv.org/pdf/2601.13531v1)

ICASSP 2026 URGENT挑战赛延续系列传统，聚焦于能够处理多种失真类型、领域及输入条件的通用语音增强系统。本概述论文详细阐述了挑战赛的设立动机、任务定义、数据集、基线系统、评估方案及结果。挑战赛分为两个互补赛道：赛道一专注于通用语音增强，赛道二则引入针对增强语音的质量评估任务。本次赛事吸引了超过80支团队注册，其中29支提交了有效结果，显示出学术界对鲁棒语音增强技术的高度关注。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强旨在提升受噪声、混响等干扰的语音的清晰度与质量。尽管神经网络已推动其发展，但现有研究大多在匹配条件和有限失真类型下评估，导致系统在**未见过的声学环境、多样的说话人特征或未知失真类型**上表现不佳。
- **既有方法的问题**：这种**泛化能力不足**阻碍了实际部署，因为现实场景中语音在内容、语言和风格上差异巨大。先前方法通常依赖大规模但未经筛选的数据，可能带来收益递减，且未能充分涵盖年龄、口音、歌唱/耳语等多种语音多样性。

2)  
- **核心方法**：ICASSP 2026 URGENT挑战赛通过两个互补的赛道推动通用、鲁棒的语音增强与质量评估。
    - **赛道1（通用语音增强）**：要求构建单一系统处理多种失真、领域和输入格式。关键创新在于：
        - **强调数据筛选**：鼓励使用先进数据选择技术（如基于MOS的过滤），从海量语料中精选高质量训练数据，而非盲目使用全部数据，以提升数据效率和模型性能。
        - **扩展语音多样性**：在训练数据中纳入多种语言，并引入年龄、口音、情感、歌唱及耳语语音等真实世界变异，以增强模型对未见场景的适应能力。
        - **采用混合生成-判别范式**：领先系统多结合生成模型（用于鲁棒的语音恢复）和判别网络（用于精确信号保持），形成双分支或多阶段流水线，以协同处理多样失真。
    - **赛道2（语音质量评估）**：新引入该赛道，专注于评估经语音增强系统处理后的语音的感知质量。其方法核心是：
        - **利用多样化的MOS标注数据集**：包括合成语音、真实退化语音及来自以往挑战赛的增强语音，训练模型预测平均意见得分。
        - **评估协议**：通过比较预测MOS与人工标注在语句级和系统级的相关性（如线性相关系数、斯皮尔曼等级相关系数）来评估模型性能。

3)  
- **任务与效果**：
    - **在通用语音增强任务（赛道1）上**：吸引了23个有效提交。领先的混合生成-判别模型在客观指标（如PESQ、ESTOI）和主观听测（MOS、CMOS）上均超越了单独的生成或判别基线，显著提升了对多样失真和未见语言（如印地语、韩语）的处理能力，同时保持了说话人和语言特征的完整性。
    - **在语音质量评估任务（赛道2）上**：收到了6个有效提交。顶级系统通过改进模型架构和训练策略，在语句级和系统级的MOS预测上与人工评分达到了高度相关，有效评估了增强语音的感知质量。
</div>

</details>

---

## Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels
- **Authors**: Noriyuki Tonami, Wataru Kohno, Yoshiyuki Yajima, Sakiko Mishima, Yumi Arai, Reishi Kondo, Tomoyuki Hino
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.13513v1](https://arxiv.org/abs/2601.13513v1)
- **PDF**: [https://arxiv.org/pdf/2601.13513v1](https://arxiv.org/pdf/2601.13513v1)

分布式多通道声学传感（DMAS）能够实现大规模声音事件分类（SEC），但在测试时传感器布局与训练布局不同、且多通道严重退化时，其性能会显著下降。本文提出一种基于逆时偏移（RTM）的无学习、物理信息驱动的修复前端方法。该方法首先利用解析格林函数将观测到的多通道频谱图在三维网格上进行反向传播，生成场景一致的空间图像，随后通过前向投影重建修复后的信号，再进行对数梅尔特征提取和基于Transformer的分类。我们在ESC-50数据集上使用50个传感器和三种布局（圆形、线性、直角）进行评估，各通道信噪比（SNR）在-30至0 dB范围内采样。与AST基线、缩放稀疏最大值通道选择以及通道交换增强等方法相比，所提出的RTM前端在所有布局中均取得最优或具有竞争力的分类准确率，尤其在直角布局上将准确率从9.7%提升至22.8%（相对提升13.1个百分点）。相关性分析表明，空间权重与信噪比的相关性强于与通道-声源距离的相关性，且更高的信噪比-权重相关性对应更高的SEC准确率。这些结果证明，在布局开放配置和严重通道退化的条件下，基于物理的“先重建再投影”预处理方法能够有效弥补纯学习型方法在DMAS任务中的不足。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：分布式多通道声学传感（DMAS）支持大规模声学事件分类（SEC），但在实际部署中面临三大挑战：
  - 部分传感通道因噪声、硬件差异或削波而性能严重退化。
  - 测试时的传感器布局可能与训练布局不同，导致模型泛化能力下降。
  - 传感器与声源间距离较大（数米以上），引发同步、传播延迟和空间混叠等问题。

- **既有方法问题**：现有基于机器学习的方法（如AST基线、通道选择或交换增强）严重依赖训练布局，对布局变化和通道退化敏感，尤其在右角布局等非训练布局上性能显著下降。

2)  
论文提出一种**基于物理信息修复（Physics-informed Inpainting）的前端处理方法**，核心是利用**逆时偏移（Reverse Time Migration, RTM）** 将传感器坐标依赖的观测转换为几何无关的物理表示。其解决步骤包括：

- **逆向传播成像**：
  - 将观测到的多通道声谱图通过三维自由空间格林函数（解析解）逆向传播，在公共三维网格上生成场景一致的声场图像。
  - 该步骤利用波动方程物理约束，将传感器位置信息融入传播算子，减弱对具体传感器坐标的依赖。

- **前向投影修复**：
  - 从重建的声场图像前向投影至所有传感器位置，生成修复后的多通道信号。
  - 此操作本质上是一个基于物理的空间滤波器，能均衡各通道质量，补全缺失或低信噪比通道的数据。

- **特征提取与分类**：
  - 修复后的信号转换为对数梅尔谱特征，输入基于Transformer的分类器（如AST）进行事件分类。
  - 同时引入可学习的空间权重，通过分析权重与信噪比的相关性，验证模型对高质量通道的利用效率。

- **方法优势**：
  - **无需学习**：前端处理完全基于物理模型，不依赖机器学习，避免对训练布局过拟合。
  - **布局鲁棒性**：通过物理成像统一不同布局的表示，提升对未见布局的适应能力。
  - **通道质量均衡**：利用波动传播原理修复退化通道，提升整体信号质量。

3)  
- **任务**：在模拟的分布式多通道声学传感场景中，使用ESC-50数据集，在三种传感器布局（圆形、线性、右角）下进行声学事件分类，并模拟通道信噪比随机退化（-30至0 dB）。

- **效果**：
  - 在右角布局上，相比AST基线（9.7%准确率），所提方法将准确率提升至22.8%，**相对提升13.1个百分点**。
  - 在所有布局的平均准确率上，该方法达到23.6%，优于通道选择（18.1%）和通道交换增强（17.3%）等对比方法。
  - 分析显示，模型的空间权重与通道信噪比高度相关（而非与声源距离），且信噪比-权重相关性越高，分类准确率越高，证明该方法能有效利用高质量通道信息。
</div>

</details>

---
