---
layout: post
title: "arXiv Daily – 2026-02-25"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-25（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-24 08:50 — 2026-02-25 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## 823-OLT @ BUET DL Sprint 4.0: Context-Aware Windowing for ASR and Fine-Tuned Speaker Diarization in Bengali Long Form Audio
- **Authors**: Ratnajit Dhar, Arpita Mallik
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.21183v1](https://arxiv.org/abs/2602.21183v1)
- **PDF**: [https://arxiv.org/pdf/2602.21183v1](https://arxiv.org/pdf/2602.21183v1)

尽管孟加拉语是全球使用最广泛的语言之一，其在长时语音技术领域仍存在明显不足，尤其是在涉及转写和说话人归属的系统中。本研究提出了面向长时孟加拉语语音的智能处理框架，分别采用基于Whisper Medium模型的自动语音识别系统和基于微调分割模型的说话人日志系统。语音识别流程融合了人声分离、语音活动检测以及间隙感知的窗口化策略，以构建保持上下文信息的语音片段，从而实现稳定的解码。在说话人日志方面，通过在官方竞赛数据集（源自BUET CSE Fest组织的DL Sprint 4.0竞赛）上微调预训练的说话人分割模型，使其能更有效地捕捉孟加拉语对话特征。最终系统实现了对长时音频的高效转写和说话人感知的转写，为低资源语言提供了可扩展的语音技术解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：孟加拉语作为全球主要语言之一，在长音频语音技术（如转录和说话人识别）中资源匮乏，缺乏针对长时、多说话人、含噪声的音频处理方案。  
- **既有问题**：传统方法常采用固定长度分段，导致句子被截断、上下文断裂；同时，现有系统对背景噪声、音乐干扰和说话人变异的鲁棒性不足，且缺乏针对孟加拉语对话模式的优化。

2)  
- **整体方案**：构建模块化处理流程，将自动语音识别（ASR）与说话人日志（Diarization）作为独立但互补的组件分别优化。  
- **ASR核心方法**：  
  - **语音增强**：使用Demucs进行人声分离，抑制背景音乐与噪声。  
  - **语音活动检测（VAD）**：采用Silero VAD检测语音区间，避免静音段干扰。  
  - **间隙感知窗口化**：基于VAD结果合并语音段为约20秒的窗口，若静音间隙超过5秒则不合并，以保持语句连续性。  
  - **上下文填充**：在窗口两端添加1秒静音，减少解码边界伪影。  
  - **解码模型**：使用微调后的孟加拉语Whisper-Medium模型进行自回归解码，并辅以文本规范化。  
- **说话人日志核心方法**：  
  - **基线方法**：尝试了基于ECAPA-TDNN嵌入与谱聚类（含VBx优化）的无训练流程，但性能有限。  
  - **微调策略**：在竞赛数据集上对预训练的pyannote分割模型进行微调，优化学习率、批次大小等超参数，使其适应孟加拉语对话的声学与说话人模式。  
  - **集成流程**：结合语音增强、微调后的神经分割模型及神经聚类后端，提升说话人边界检测与归属准确性。

3)  
- **自动语音识别任务**：在长音频转录中，最佳配置（使用Demucs及默认VAD阈值）在私有测试集上取得0.362的词错误率（WER），优于未使用语音增强或VAD阈值不当的配置。  
- **说话人日志任务**：微调后的神经分割模型在私有测试集上达到0.18758的说话人日志错误率（DER），显著优于无训练方法（如谱聚类DER为0.42535）及未微调的预训练模型。
</div>

</details>

---

## Training-Free Intelligibility-Guided Observation Addition for Noisy ASR
- **Authors**: Haoyang Li, Changsong Liu, Wei Rao, Hao Shi, Sakriani Sakti, Eng Siong Chng
- **Categories**: eess.AS, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.20967v1](https://arxiv.org/abs/2602.20967v1)
- **PDF**: [https://arxiv.org/pdf/2602.20967v1](https://arxiv.org/pdf/2602.20967v1)

在嘈杂环境下，自动语音识别（ASR）性能会严重下降。尽管语音增强（SE）前端能有效抑制背景噪声，但常会引入损害识别性能的伪影。观测融合方法通过结合带噪语音与SE增强语音，在不修改SE或ASR模型参数的情况下提升了识别效果。本文提出一种基于可懂度指导的观测融合方法，其融合权重直接通过后端ASR系统生成的可懂度估计值计算得出。与以往基于训练神经预测器的方法不同，本方法无需训练，既降低了复杂度，又增强了泛化能力。通过在多种SE-ASR组合及数据集上的大量实验，本方法展现出优于现有观测融合基线的鲁棒性和性能提升。对基于可懂度指导的切换替代方案、帧级与语句级融合的进一步分析，也验证了所提设计的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：噪声环境下，自动语音识别性能严重下降。语音增强作为前端处理能抑制噪声，但常引入损害识别的伪影。
- **既有方法问题**：
  - 联合训练SE与ASR计算成本高，且可能损害语音质量，在模型无法集成时不适用。
  - 观测融合方法虽无需修改模型，但其融合权重S′的确定依赖信号质量预测或需训练神经网络预测器，前者未考虑SE伪影，后者需真实转录计算WER/CER，增加了复杂性和泛化问题。

2)  
论文提出了一种**免训练的、以可懂度为指导的观测融合方法**，核心是利用后端ASR系统直接提供的置信度来动态计算融合权重S′，从而平衡带噪语音y和增强语音ˆx。

- **核心设计**：
  - **可懂度指导**：直接使用ASR的识别置信度作为语音可懂度的代理指标，避免了依赖信号质量分数或需要真实转录。
  - **权重计算**：S′ = conf(y) / [conf(y) + conf(ˆx)]。该公式基于y和ˆx的置信度，为可懂度更高的信号分配更高权重。
  - **免训练**：无需为S′训练专门的神经网络预测器，所有计算在推理时由冻结的ASR系统完成，显著降低了系统复杂性和对标注数据的依赖。

- **针对不同ASR的适配**：论文为Whisper、Parakeet和Wav2Vec2-CTC三种主流ASR系统设计了具体的置信度计算方法，确保了方法的通用性。
- **对比方案**：论文还探讨了基于置信度的硬切换策略和帧级融合方案，通过对比进一步验证了所提出的**话语级、基于置信度的加权融合**是最优策略。

3)  
- **任务**：在带噪语音识别任务上，使用多种SE模型（Demucs, GR-KAN MP-SENet）和ASR系统（Whisper, Parakeet, Wav2Vec2），在VoiceBank-DEMAND（域内）和CHiME-4（域外）数据集上进行了评估。
- **效果**：
  - 提出的Conf-OA方法在绝大多数实验设置下取得了最低的词错误率，显著优于基于SNR、DNSMOS质量分数或分类器预测的现有OA基线。
  - 在域外真实噪声场景（CHiME-4 Real）中表现尤其稳健，证明了其良好的泛化能力。
  - 分析表明，该方法在ASR置信度与WER错配的情况下，能通过加权融合有效补偿，性能优于简单的硬切换策略。
</div>

</details>

---

## Geometric Analysis of Speech Representation Spaces: Topological Disentanglement and Confound Detection
- **Authors**: Bipasha Kashyap, Pubudu N. Pathirana
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.20823v1](https://arxiv.org/abs/2602.20823v1)
- **PDF**: [https://arxiv.org/pdf/2602.20823v1](https://arxiv.org/pdf/2602.20823v1)

基于语音的临床工具在多语言环境中应用日益广泛，但病理语音特征与口音变异在几何空间上是否保持可分离性仍不明确。现有系统可能误判健康非母语者，或遗漏多语言患者的病理特征。本文提出一种四指标聚类框架，用于评估情感、语言及病理语音特征在六个语料库、八种数据集组合中的几何解耦效果。结果显示一致的层级结构：情感特征聚类最紧密（轮廓系数0.250），病理特征次之（0.141），语言特征最分散（0.077）。混淆分析表明病理与语言特征的重叠度始终低于0.21，虽高于随机置换基线，但仍处于临床部署可接受范围。可信度分析验证了嵌入表示的有效性与几何结论的稳健性。本框架为构建面向多元群体的公平可靠语音健康系统提供了可操作的指导原则。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音承载着情感、语言和病理信息，但现有自监督模型（如wav2vec 2.0）生成的嵌入表示几何结构不透明，难以保证不同特征在表示空间中可分离。  
- **既有问题**：在多语言临床环境中，病理特征（如帕金森病语音）可能与口音变异在几何上重叠，导致健康非母语者被误判为患病，或多语言患者的病理被误认为口音差异，引发诊断偏差。

2)  
- **方法框架**：提出一个四指标聚类框架，用于评估语音表示空间的几何解缠结。具体步骤包括：  
  - **特征提取**：基于语音产生的源-滤波器模型，手工提取三组结构化特征（情感R28、语言R33、病理R16）。  
  - **流形学习**：使用t-SNE将高维特征非线性降维至2D，以保留局部邻域结构并评估聚类可分性。  
  - **聚类质量评估**：综合使用四种互补指标量化聚类结构：  
    - **轮廓系数**：衡量样本内聚与分离程度。  
    - **戴维森-堡丁指数**：量化最坏情况下的聚类重叠。  
    - **卡林斯基-哈拉巴斯指数**：评估类间与类内方差比。  
    - **自助法稳定性**：通过调整兰德指数评估子采样下的聚类鲁棒性。  
  - **混淆检测**：在共享PCA子空间中计算病理与语言特征的几何重叠度，并与置换检验生成的零分布对比，以区分真实混淆与随机接近。  
- **解决思路**：该框架通过多指标系统量化不同语音维度在表示空间中的聚类紧密度与分离度，从而揭示特征间的几何关系；混淆分析则直接评估病理与语言特征的重叠程度，为临床部署提供边界明确的参考。

3)  
- **评估任务与效果**：在六个语音库（涵盖情感、语言、病理数据）的八种组合上进行了评估：  
  - **聚类质量**：发现一致的层次结构——情感特征聚类最紧密（平均轮廓系数0.250），病理特征次之（0.141），语言特征最分散（0.077）。  
  - **混淆控制**：病理-语言特征重叠度被限制在0.21以下，虽高于随机基线，但仍处于临床可接受范围。  
  - **鲁棒性验证**：t-SNE嵌入的保真度（可信度>0.80）和聚类稳定性（情感最高）均得到确认，证明结论可靠。
</div>

</details>

---

## Assessing the Impact of Speaker Identity in Speech Spoofing Detection
- **Authors**: Anh-Tuan Dao, Driss Matrouf, Nicholas Evans
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2602.20805v1](https://arxiv.org/abs/2602.20805v1)
- **PDF**: [https://arxiv.org/pdf/2602.20805v1](https://arxiv.org/pdf/2602.20805v1)

语音欺骗检测系统通常使用多说话人的多样化录音进行训练，并假设生成的嵌入向量独立于说话人身份。然而，这一假设尚未得到验证。本文研究了说话人信息对欺骗检测系统的影响，并在说话人不变多任务框架中提出了两种方法：一种在嵌入向量中建模说话人身份，另一种则将其去除。该框架通过集成梯度反转层，实现了说话人识别与欺骗检测的联合多任务学习。在四个数据集上的评估表明，与基线相比，说话人不变模型的平均等错误率降低了17%，在面对最具挑战性的攻击（如A11）时，错误率最高可降低48%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音欺骗检测系统通常使用多说话人的多样化录音进行训练，并假设生成的嵌入表示与说话人身份无关。然而，这一假设在训练数据中说话人数量或多样性有限时可能不成立，其影响尚未得到验证。  
- **既有方法问题**：先前的研究虽探索了结合说话人识别的多任务学习来提升检测性能，但主要应用于欺骗感知的说话人验证任务，在独立欺骗检测中关注较少。此外，早期数据集（如ASVspoof 2019）说话人数量有限，限制了模型对说话人相关信息的鲁棒学习。

2)  
论文提出了**说话人不变多任务框架**，基于自监督学习模型构建，通过两种配置灵活处理说话人信息的影响：  

- **说话人感知配置**：联合训练欺骗检测和说话人识别任务，不引入梯度反转层。模型利用说话人相关线索，在欺骗音频与真实说话人特征不匹配的场景中提升检测能力。  
- **说话人不变配置**：在特征提取器与说话人分类器之间插入梯度反转层。在反向传播时，该层对说话人分类梯度取反，对抗性地最小化特征中的说话人可区分性，迫使模型聚焦于欺骗相关的通用特征。  

- **核心机制**：  
  - 使用XLSR编码器提取上下文感知的帧级嵌入。  
  - 采用双MHFA分类器，分别处理欺骗检测和说话人识别。  
  - 通过梯度反转层的启停，在同一骨干网络上无缝切换两种配置，从而直接比较两种策略。  
- **解决思路**：该框架不再假设嵌入与说话人无关，而是主动探索说话人信息的两种处理方式——整合或抑制，以优化模型对不同数据分布的适应性。

3)  
- **任务与数据集**：在四个欺骗检测数据集上评估，包括ASVspoof 5评估集、In-the-Wild、ASVspoof 2021 LA和DF隐藏子集。  
- **效果**：  
  - 说话人不变模型相比基线MHFA平均等错误率降低17.2%，在最具挑战性的攻击类型上提升显著，如对A11攻击降低48%。  
  - 两种配置均优于基线，其中说话人不变模型在未见说话人的跨域数据集上表现更优。  
- **可视化验证**：t-SNE显示说话人不变模型成功抑制了说话人聚类，而说话人感知模型则增强了说话人特征分离。
</div>

</details>

---

## Voices of the Mountains: Deep Learning-Based Vocal Error Detection System for Kurdish Maqams
- **Authors**: Darvan Shvan Khairaldeen, Hossein Hassani
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.20744v1](https://arxiv.org/abs/2602.20744v1)
- **PDF**: [https://arxiv.org/pdf/2602.20744v1](https://arxiv.org/pdf/2602.20744v1)

麦嘎姆（Maqam）作为一种歌唱形式，是库尔德音乐的重要组成部分。传统上，麦嘎姆歌手通过面对面传授或自学进行训练。自动歌唱评估（ASA）利用机器学习（ML）技术对演唱风格的准确性进行评价，并通过错误检测帮助学习者提升表现。目前已有的ASA工具均遵循西方音乐规则，其要求音符在整个演唱过程中必须严格保持在预期音高范围内。由于这类系统无法检测微音程和音高滑移，导致即使歌手按照传统规则演唱库尔德麦嘎姆，仍会被判定为错误。库尔德麦嘎姆的演唱错误识别需在微分音空间中进行，这超出了西方十二平均律的范畴。本研究首次尝试填补这一空白。尽管演唱中可能出现多种错误类型，我们重点关注巴亚提-库尔德（Bayati-Kurd）语境下的音高、节奏和调式稳定性错误。我们收集了13位歌手演唱的50首歌曲（约2-3小时），标注了221个错误片段（含150个精细音高错误、46个节奏错误、25个调式漂移错误）。数据被分割为15,199个重叠窗口并转换为对数梅尔频谱图。我们开发了一种带注意力机制的双头CNN-BiLSTM模型，用于判断窗口是否包含错误，并根据选定错误类型进行分类。模型经过20轮训练（第10轮早停），验证集宏观F1分数达到0.468。在全部50首歌曲以0.750为阈值的评估中，召回率为39.4%，精确率为25.8%。在检测到的窗口内，错误类型的宏观F1分数为0.387，其中精细音高错误F1为0.492，节奏错误为0.536，调式漂移错误为0.133；调式漂移的召回率仅为8.0%。常见错误类型的较好表现证明了方法的有效性，而调式漂移的低召回率则表明需要更多数据及样本平衡优化。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：库尔德木卡姆是一种包含微分音、滑音等独特演唱技巧的音乐传统，其教学主要依赖口传心授，缺乏便捷的自动评估工具。
- **既有方法的问题**：
  - 现有的自动歌唱评估系统大多基于西方平均律音乐规则，将微分音、滑音等库尔德木卡姆中的合法技巧误判为音高错误。
  - 这些系统通常依赖固定的参考旋律进行对齐（如动态时间规整），无法有效处理库尔德音乐中富有表现力的节奏和音高变化。
  - 缺乏针对库尔德木卡姆的标注数据集和专门设计的评估模型。

2) **论文核心方法如何解决上述问题**
- **构建专用数据集**：首次创建了针对库尔德木卡姆（Bayati-Kurd）的歌唱错误检测数据集。包含13位歌手演唱的50首歌曲（总长约2.5小时），并由专家标注了221个错误片段，错误类型分为**细微音高错误**、**节奏错误**和**调式漂移错误**。
- **设计专用模型架构**：提出了一种双头深度神经网络模型。
  - **特征提取**：将音频转换为对数梅尔频谱图作为输入。
  - **主干网络**：采用**CNN-BiLSTM with Attention**架构。
    - **CNN**：用于从频谱图中提取局部时频模式。
    - **BiLSTM**：用于建模更长的时序和音乐上下文依赖关系。
    - **Attention机制**：帮助模型聚焦于包含关键信息的时间帧。
  - **双任务输出头**：
    - **检测头**（二分类）：判断当前时间窗口是否包含任何错误。
    - **分类头**（三分类）：若包含错误，则进一步判断其具体类型（细微音高、节奏或调式漂移）。
- **应对数据挑战**：
  - 针对严重的**类别不平衡**问题（如调式漂移错误仅25例），采用了加权采样、焦点损失函数等技术。
  - 使用了数据增强（如时间拉伸、音高偏移、添加噪声等）来扩充训练数据，特别是针对稀有错误类型。

3) **在哪些任务上取得了怎样的效果**
- **任务**：在Bayati-Kurd木卡姆演唱中，进行**错误检测**（判断是否有错）与**错误分类**（判断错误类型）。
- **效果**：
  - **整体检测**：在全部50首歌的评估中，模型检测的召回率为39.4%，精确率为25.8%（F1分数为0.311）。这表明从大量正常片段中发现错误窗口仍具挑战。
  - **错误分类**：在模型成功检测到的错误窗口内，分类性能更优。三类错误的宏观平均F1为0.387，其中：
    - **细微音高错误**：F1为0.492，分类精确率高达89.5%。
    - **节奏错误**：F1为0.536，表现最佳。
    - **调式漂移错误**：F1仅为0.133，召回率仅8.0%，主要受限于训练数据极度稀缺。
  - **结论**：模型能有效识别和分类常见的音高与节奏错误，为库尔德木卡姆的自动评估提供了首个可行的基线系统，但调式漂移的检测仍需更多数据支持。
</div>

</details>

---

## Quantifying Dimensional Independence in Speech: An Information-Theoretic Framework for Disentangled Representation Learning
- **Authors**: Bipasha Kashyap, Björn W. Schuller, Pubudu N. Pathirana
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.20592v1](https://arxiv.org/abs/2602.20592v1)
- **PDF**: [https://arxiv.org/pdf/2602.20592v1](https://arxiv.org/pdf/2602.20592v1)

语音信号在共享的声学通道中编码了情感、语言及病理信息；然而，解耦效果通常仅通过下游任务性能间接评估。本文提出一种信息论框架，通过结合有界神经互信息估计与非参数验证，量化手工声学特征中的跨维度统计依赖性。在六个语料库上的实验表明，跨维度互信息始终处于较低水平（估计边界严格，< 0.15 纳特），说明所考察数据中统计耦合较弱；而源-滤波器互信息则显著更高（0.47 纳特）。归因分析（定义为总互信息中源成分与滤波器成分的贡献比例）显示，情感维度以源成分为主导（80%），而语言维度和病理维度则分别以滤波器成分为主导（各占60%和58%）。这些发现为量化语音中的维度独立性提供了理论框架。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音信号在共享的声学通道中同时编码情感、语言和病理信息。解耦这些维度对下游任务至关重要，但现有方法缺乏对解耦质量的原理性度量。
- **既有方法问题**：现有研究主要关注成对分离问题，且依赖下游任务性能作为解耦的间接评估指标，未能全面评估情感、语言和病理三维信息流之间的统计独立性。

2)  
- **核心方法**：论文提出一个信息论框架，通过结合有界的神经互信息估计与非参数验证，量化手工声学特征间的跨维度统计依赖性。
- **具体技术**：
    - **有界神经MI估计**：联合使用MINE（提供变分下界）和CLUB（提供对比上界）进行互信息估计，并通过指数移动平均稳定化和方差钳制来提升估计稳定性。
    - **非参数验证**：引入KSG估计器进行训练无关的验证，使用k近邻统计提供独立参考。
    - **自适应加权融合**：根据上下界之间的不确定性，自适应地加权融合神经估计与KSG估计，得到最终MI值。
    - **源-滤波器归因分析**：基于KSG估计，将每个语义维度与源特征、滤波器特征的互信息进行分解，计算归因比例，以揭示不同维度的声学生理基础。
- **如何解决问题**：该框架首次为语音维度独立性提供了原理性的量化工具。它直接估计了特征集之间的互信息，而非依赖下游任务代理指标，从而能够客观评估解耦的理论可行性。通过严格的上下界估计和跨数据集验证，确保了结果的可靠性。

3)  
- **评估任务**：在涵盖情感、语言和病理维度的六个英语语音语料库上，评估了跨维度（情感-语言、情感-病理、语言-病理）以及源-滤波器特征之间的互信息。
- **取得效果**：
    - 跨维度互信息极低（最终估计均值0.10-0.12 nats），表明在所使用的特征空间中，这些维度近乎统计独立。
    - 源-滤波器互信息较高（0.47 nats），但低于经典声学理论预测，支持了源-滤波器分解的假设。
    - 归因分析显示，情感维度信息主要源于声源特征（80%），而语言和病理维度信息则主要源于滤波器特征（分别占60%和58%）。
</div>

</details>

---

## Memory-guided Prototypical Co-occurrence Learning for Mixed Emotion Recognition
- **Authors**: Ming Li, Yong-Jin Liu, Fang Liu, Huankun Sheng, Yeying Fan, Yixiang Wei, Minnan Luo, Weizhan Zhang, Wenping Wang
- **Categories**: cs.LG, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.20530v1](https://arxiv.org/abs/2602.20530v1)
- **PDF**: [https://arxiv.org/pdf/2602.20530v1](https://arxiv.org/pdf/2602.20530v1)

基于多模态生理与行为信号的情感识别在情感计算中具有关键作用，但现有模型大多局限于在受控实验室环境下预测单一情感。相比之下，真实世界中的人类情感体验往往同时包含多种情感状态，这促使混合情感识别作为情感分布学习问题受到广泛关注。然而，当前方法通常忽略了共存情感之间固有的效价一致性与结构化关联。为克服这一局限，本文提出一种记忆引导的原型共现学习框架，显式建模情感共现模式。具体而言，我们首先通过多尺度关联记忆机制融合多模态信号；为捕捉跨模态语义关系，构建情感特定的原型记忆库以生成丰富的生理与行为表征，并采用原型关系蒸馏确保潜在原型空间中的跨模态对齐。此外，受人类认知记忆系统启发，引入记忆检索策略以提取跨情感类别的语义级共现关联。通过这种自底向上的层次化抽象过程，模型学习到具有情感信息量的表征，从而实现精确的情感分布预测。在两个公开数据集上的综合实验表明，所提方法在混合情感识别任务中定量与定性评估均持续优于现有先进方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：真实世界的人类情感体验通常是多种情感状态同时存在的混合情绪，而现有模型大多局限于预测单一情绪，且通常在受控实验室环境下进行。
- **既有方法的问题**：当前方法（如多标签分类或情绪分布学习）往往将情绪类别视为独立，忽略了共存情绪之间固有的效价一致性和结构化相关性。它们主要依赖数据驱动的统计关联，未能结合情感空间中效价一致性和互斥性等心理学先验，导致无法捕捉原始多模态数据中内在的共现关系，也难以对情绪交互关系和潜在组合结构进行建模。

2)  
论文提出的**记忆引导的原型共现学习（MPCL）框架**通过以下核心方法解决上述问题：

- **多尺度关联记忆融合**：
    - 采用多尺度关联记忆策略，通过可调节的逆温度参数（β）控制信息聚合的粒度，将多模态生理信号（如EEG、GSR、PPG）融合为统一表示，捕获内在跨模态关联和互补信息。

- **原型对齐与共现学习**：
    - **构建原型记忆库**：为生理和行为模态分别构建可学习的细粒度情绪原型记忆库。通过将输入特征重建为这些原型的加权组合，获得语义丰富且结构一致的表示。
    - **原型关系蒸馏**：通过计算原型之间的语义相关性矩阵，并利用KL散度进行跨模态蒸馏，强制两个模态在潜在原型空间中的结构对齐，确保跨模态语义一致性。
    - **原型共现学习**：受人类联想记忆机制启发，设计记忆检索策略。将原型增强的特征作为查询，从存储的样本模式中检索关联特征，强化在查询和存储模式间表现出语义一致性的特征。通过一个对比学习目标（SemLOOB损失）来促进跨模态语义共现一致性，从而在语义层面揭示不同情绪之间的潜在结构关联。

- **分层语义压缩与分布预测**：
    - 通过堆叠的压缩块（每个块包含原型抽象层和自注意力层），以自底向上的方式逐步将细粒度原型压缩为高度抽象的语义表示。原型记忆库用于初始化压缩过程，语义槽容量逐层减少，最终将表示映射到情绪概率空间进行分布预测。

3)  
- **任务**：在两个公开的多模态混合情绪识别数据集（DMER和WESAD）上，将任务形式化为情绪分布学习问题。
- **效果**：在主体依赖和主体独立两种实验设置下，MPCL在六项标准评估指标（如Chebyshev距离、KL散度、余弦相似度等）上均**一致优于**所有先进的基线方法（包括EmotionDict、HeLo等），取得了最先进的性能。可视化分析也表明，MPCL能更准确地恢复情绪标签之间的内在相关结构，预测分布与真实分布更吻合。
</div>

</details>

---
