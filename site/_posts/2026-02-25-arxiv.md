---
layout: post
title: "arXiv Daily – 2026-02-25"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-25（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-24 08:50 — 2026-02-25 08:50
- 抓取总数：6 篇 | 本页显示：6 篇（去重/过滤后）

## Training-Free Intelligibility-Guided Observation Addition for Noisy ASR
- **Authors**: Haoyang Li, Changsong Liu, Wei Rao, Hao Shi, Sakriani Sakti, Eng Siong Chng
- **Categories**: eess.AS, cs.AI, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.20967v1](https://arxiv.org/abs/2602.20967v1)
- **PDF**: [https://arxiv.org/pdf/2602.20967v1](https://arxiv.org/pdf/2602.20967v1)

在嘈杂环境下，自动语音识别（ASR）性能会显著下降。尽管语音增强（SE）前端能有效抑制背景噪声，但常会引入损害识别效果的伪影。观测融合方法通过结合带噪语音与SE增强语音，在不修改SE或ASR模型参数的情况下提升了识别性能。本文提出一种基于可懂度指导的观测融合方法，其融合权重直接通过后端ASR系统生成的可懂度估计值计算得出。与以往基于训练神经预测器的方法不同，本方法无需训练，既降低了复杂度又增强了泛化能力。通过在多种SE-ASR组合及数据集上的大量实验表明，该方法具有强鲁棒性，且性能优于现有观测融合基线。对基于可懂度指导的切换替代方案、帧级与语句级融合的进一步分析，也验证了所提设计的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：噪声环境下，自动语音识别性能严重下降。语音增强作为前端处理能抑制噪声，但常引入损害识别的伪影。
- **既有方法问题**：
  - 联合训练SE与ASR计算成本高，且可能牺牲语音质量，在实际部署中常不可行。
  - 传统的观测值加法方法依赖信号质量分数（如SNR、DNSMOS）或需额外训练的神经网络来预测融合权重，前者未考虑SE伪影的严重性，后者需真实转录计算WER/CER，增加了工程复杂度与泛化问题。

2)  
- **核心方法**：提出一种免训练的、以可懂度为指导的观测值加法框架。该方法利用后端ASR系统直接估计的置信度分数，动态融合带噪语音和增强语音。
- **解决思路**：
  - **可懂度替代指标**：在真实场景中无法获取真实转录来计算WER，因此采用ASR系统输出的置信度作为语音可懂度的实用近似。置信度反映了ASR模型内部的识别不确定性。
  - **权重计算**：融合权重S'通过公式 \( S' = \frac{conf(y)}{conf(y) + conf(\hat{x})} \) 计算，其中 \( conf(y) \) 和 \( conf(\hat{x}) \) 分别是带噪语音和增强语音的ASR置信度。该公式赋予可懂度更高（即置信度更高）的信号更大权重。
  - **通用性设计**：针对不同ASR系统（如Whisper、Parakeet、Wav2Vec2-CTC），论文设计了相应的置信度计算方法，确保框架的广泛适用性。
  - **对比方案**：论文还探讨了基于可懂度的硬切换策略和帧级OA作为对比，进一步验证了所提 utterance-level OA 设计的有效性。
- **优势**：
  - **免训练**：无需为权重预测训练额外的神经网络，降低了系统复杂性和对标注数据的依赖。
  - **泛化性强**：直接利用现有ASR系统的输出，使其能轻松应用于不同的SE-ASR组合和数据集。
  - **针对性优化**：权重基于与ASR性能直接相关的可懂度（置信度）估计，而非与识别关联较弱的信号级质量指标，能更好地平衡噪声抑制与伪影引入。

3)  
- **任务与效果**：在多种SE模型（Demucs, GR-KAN MP-SENet）、ASR系统（Whisper, Parakeet, Wav2Vec2）和数据集（VoiceBank-DEMAND, CHiME-4）上进行了评估。
- **主要效果**：
  - 所提的置信度引导OA方法在大多数测试场景下取得了最低的词错误率，显著优于基于SNR、DNSMOS以及需要训练的神经网络分类器等基线OA方法。
  - 在域外（CHiME-4）的挑战性场景中表现出更强的鲁棒性和性能提升。
  - 分析表明，在ASR置信度与WER错位（Miscalibrated）的情况下，OA相比硬切换策略能带来更显著的性能改善。
</div>

</details>

---

## Geometric Analysis of Speech Representation Spaces: Topological Disentanglement and Confound Detection
- **Authors**: Bipasha Kashyap, Pubudu N. Pathirana
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.20823v1](https://arxiv.org/abs/2602.20823v1)
- **PDF**: [https://arxiv.org/pdf/2602.20823v1](https://arxiv.org/pdf/2602.20823v1)

基于语音的临床工具在多语言环境中的应用日益广泛，但病理语音特征是否能在几何上与口音变异保持可分离性仍不明确。现有系统可能误判健康的非母语者，或遗漏多语言患者的病理特征。本文提出一种基于四类度量的聚类框架，用于评估情感、语言及病理语音特征在六个语料库和八种数据集组合中的几何解耦效果。结果显示出一致的层次结构：情感特征形成最紧密的聚类（轮廓系数0.250），其次是病理特征（0.141）和语言特征（0.077）。混淆分析表明病理与语言特征的重叠度始终低于0.21，虽高于随机置换的零假设基准，但仍处于临床部署可接受范围内。可信度分析验证了嵌入表示的有效性及几何结论的稳健性。本框架为构建面向多样化人群的公平、可靠的语音健康系统提供了可操作的指导原则。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音承载着情感、语言和病理信息，但现有自监督模型（如wav2vec 2.0）生成的嵌入表示几何结构不透明，无法保证不同特征在表示空间中几何可分。  
- **既有问题**：在多语言临床环境中，病理特征（如帕金森病语音）与口音变异可能共享声学特性，导致系统错误分类——健康非母语者被误判为患病，或多语言患者的病理被误认为口音差异。

2)  
论文提出一个四指标聚类框架，系统评估语音表示空间中情感、语言和病理特征的几何解缠。核心方法如下：  
- **特征提取与降维**：  
  - 基于语音产生的源-滤波器模型，提取三组结构化特征：情感特征（28维）、语言特征（33维）、病理特征（16维）。  
  - 使用t-SNE将高维特征非线性降维至2D，以保留局部邻域结构，便于可视化与分析。  
- **四指标聚类质量评估**：  
  - **轮廓系数**：衡量样本与其所属簇的紧密度及与其他簇的分离度。  
  - **戴维森堡丁指数**：量化最坏情况下的簇间重叠，值越低分离越好。  
  - **卡林斯基-哈拉巴斯指数**：评估簇间方差与簇内方差的比率，值越高表示聚类越紧凑、分离越清晰。  
  - **Bootstrap稳定性**：通过自助采样计算调整兰德指数，评估聚类对数据扰动的鲁棒性。  
- **混淆检测**：  
  - 在共享PCA子空间中，计算病理样本落入语言簇2σ范围内的比例，量化几何重叠。  
  - 通过置换检验生成零分布，区分真实混淆与随机接近。  
- **验证嵌入保真度**：  
  - 计算t-SNE嵌入的信任度，确认高维邻居关系在低维空间中得以保留，排除投影伪影影响。  
该方法通过多指标、多数据组合的稳健分析，揭示了特征空间的层次结构，并量化了病理与语言特征间的有限重叠。

3)  
- **任务**：在跨六个语音库（情感、语言、病理各两个）的八种组合上，评估了三类语音特征的几何解缠与混淆。  
- **效果**：  
  - 建立了一致的聚类质量层次：情感特征聚类最紧（平均轮廓系数0.250），病理特征次之（0.141），语言特征最分散（0.077）。  
  - 病理-语言特征重叠被量化在0.135至0.206之间，虽高于置换零基线（~0.06），但有界且可控。  
  - 所有嵌入的信任度均高于0.79，证实了几何结论的可靠性，为跨人群公平语音健康系统提供了设计依据。
</div>

</details>

---

## Assessing the Impact of Speaker Identity in Speech Spoofing Detection
- **Authors**: Anh-Tuan Dao, Driss Matrouf, Nicholas Evans
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2602.20805v1](https://arxiv.org/abs/2602.20805v1)
- **PDF**: [https://arxiv.org/pdf/2602.20805v1](https://arxiv.org/pdf/2602.20805v1)

语音欺骗检测系统通常使用多说话人的多样化录音进行训练，并默认生成的嵌入向量与说话人身份无关。然而，这一假设尚未得到验证。本文研究了说话人信息对欺骗检测系统的影响，并在说话人不变多任务框架中提出了两种方法：一种在嵌入向量中建模说话人身份，另一种则将其去除。该框架通过集成梯度反转层，实现了说话人识别与欺骗检测的联合多任务学习。在四个数据集上的评估表明，与基线相比，说话人不变模型将平均等错误率降低了17%，对最具挑战性的攻击（如A11）的等错误率降低幅度最高可达48%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音欺骗检测系统通常使用多说话人的数据进行训练，并假设生成的语音表征与说话人身份无关。然而，这一假设在训练数据中说话人数量或多样性有限时可能不成立，其影响尚未得到验证。  
- **既有方法的问题**：先前的研究虽然探索了将说话人识别作为辅助任务的多任务学习（MTL）来提升检测性能，但主要集中在欺骗感知的说话人验证（SASV）任务上，在独立的欺骗检测任务中关注较少。此外，像ASVspoof 2019这样的欺骗数据集说话人数量有限，限制了模型从说话人相关信息中学习互补线索的能力。

2)  
论文提出了**说话人不变多任务（SInMT）框架**，基于自监督学习（SSL）模型构建，通过两种配置灵活处理说话人信息，以解决上述问题：  

- **架构核心**：  
  - 使用预训练的XLSR编码器作为特征提取器。  
  - 配备两个结构相同的多头部因子化注意力（MHFA）分类器头：一个用于二分类欺骗检测，另一个用于说话人识别。  
  - 关键组件是**梯度反转层（GRL）**，插入在特征提取器和说话人分类器头之间。  

- **两种配置及解决思路**：  
  - **说话人感知（MHFA-spk）配置**：  
    - **方法**：同时优化欺骗检测和说话人识别任务（多任务学习），不使用GRL。  
    - **解决逻辑**：主动利用说话人相关的线索。当欺骗音频与目标说话人特征不匹配时，这些线索可作为互补信息，帮助模型更好地区分真实和欺骗语音。  
  - **说话人不变（MHFA-IVspk）配置**：  
    - **方法**：在训练时激活GRL。在反向传播中，GRL将通往说话人分类器的梯度乘以负值（-λ），对抗性地最小化特征中的说话人可区分性。  
    - **解决逻辑**：强制模型学习与说话人身份无关的表征。这促使特征提取器专注于欺骗本身的固有线索（如合成伪影），而非说话人特质，从而提升模型在面对未见说话人时的泛化能力。  

- **统一与灵活性**：SInMT框架在统一的SSL骨干网络上，仅通过激活或停用GRL即可在两种策略间无缝切换，便于直接比较和根据任务需求（是否已知说话人）选择最佳处理方式。

3)  
- **评估任务与数据集**：在四个欺骗检测数据集上进行了评估：ASVspoof 5评估集、In-the-Wild（ITW）、ASVspoof 2021 LA和DF隐藏子集。  
- **取得的效果**：  
  - **整体性能**：两种SInMT配置均优于基线MHFA模型。其中，说话人不变模型（MHFA-IVspk）将平均等错误率（EER）降低了17.2%。  
  - **具体提升**：在最具挑战性的攻击类型上效果显著，例如在ASVspoof 2021 LA数据集的A11攻击上，EER降低了48%。  
  - **可视化验证**：t-SNE可视化表明，MHFA-IVspk成功抑制了表征中的说话人聚类信息，证实了其学习说话人不变特征的能力。
</div>

</details>

---

## Voices of the Mountains: Deep Learning-Based Vocal Error Detection System for Kurdish Maqams
- **Authors**: Darvan Shvan Khairaldeen, Hossein Hassani
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2602.20744v1](https://arxiv.org/abs/2602.20744v1)
- **PDF**: [https://arxiv.org/pdf/2602.20744v1](https://arxiv.org/pdf/2602.20744v1)

"木卡姆"作为一种歌唱形式，是库尔德音乐的重要组成部分。传统上，木卡姆歌手通过面对面教学或自学进行训练。自动歌唱评估技术利用机器学习分析演唱风格的准确性，可通过错误检测帮助学习者提升表现。目前已有的自动歌唱评估工具均遵循西方音乐规则，其系统要求所有音符自始至终保持在预期音高范围内，无法识别微分音与滑音，导致即使歌手按传统规则演唱，系统仍会判定库尔德木卡姆演唱为错误。库尔德木卡姆的演唱错误检测需在微分音空间中进行识别，这已超出西方十二平均律的范畴。本研究首次尝试填补这一空白。尽管演唱中存在多种错误类型，我们重点关注巴亚提-库尔德调式中的音高、节奏与调式稳定性错误。我们收集了13位歌手演唱的50首歌曲（总时长2-3小时），标注了221个错误片段（含150个精细音高错误、46个节奏错误、25个调式漂移错误）。数据被分割为15,199个重叠时间窗并转换为对数梅尔频谱图。我们开发了带注意力机制的双头CNN-BiLSTM模型，用于判断时间窗是否包含错误，并根据选定错误类型进行分类。模型训练20轮（第10轮早停），验证集宏观F1值达0.468。在0.750阈值下对全部50首歌曲进行评估，召回率为39.4%，精确率为25.8%。在检测到错误的窗口内，类型宏观F1值为0.387，其中精细音高错误F1为0.492，节奏错误为0.536，调式漂移错误为0.133（召回率仅8.0%）。模型对常见错误类型的较好表现证明了方法的有效性，而调式漂移的低召回率则表明需要更多数据与类别平衡优化。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：库尔德马卡姆（Maqam）是一种重要的歌唱传统，学习者通常依赖面对面或自学。自动歌唱评估（ASA）工具能帮助检测错误，但现有系统主要基于西方音乐规则。
- **既有问题**：西方ASA系统要求音符严格遵循十二平均律的固定音高范围，无法识别微音程和滑音等库尔德马卡姆中的特色表现。这导致系统常将符合传统的演唱误判为错误，缺乏对微音空间和非西方调音体系的适应性。

2)  
- **核心方法**：本文针对库尔德马卡姆中的巴亚提-库尔德（Bayati-Kurd）变体，构建了一个基于深度学习的检测与分类模型，具体包括：
  - **数据构建**：收集了13位歌手的50首歌曲（总时长约2.5小时），由专家标注出221个错误片段（包括150个细微音高错误、46个节奏错误、25个调式漂移错误），并处理了严重的类别不平衡问题。
  - **特征处理**：音频被分割为15,199个重叠窗口（10秒/1秒步长及3秒/0.5秒步长用于增强），并转换为对数梅尔频谱图作为输入。
  - **模型设计**：采用双头CNN-BiLSTM注意力模型。一个头负责检测窗口中是否存在错误，另一个头对错误进行分类（细微音高、节奏、调式漂移）。
  - **训练策略**：使用加权采样、焦点损失（Focal Loss）等技术缓解类别不平衡，训练20个周期并在第10周期提前停止。

3)  
- **任务与效果**：模型在全部50首歌曲上评估，检测阈值设为0.750。
  - **错误检测**：召回率39.4%，精确率25.8%，F1分数0.311。
  - **错误分类**：在检测到的窗口中，分类宏平均F1为0.387。其中，细微音高错误F1为0.492，节奏错误F1为0.536，调式漂移错误F1仅为0.133（召回率8.0%）。
  - **结论**：模型对常见错误类型（音高、节奏）表现出可行性能，但调式漂移因数据稀缺而效果较差，未来需更多数据与平衡处理。
</div>

</details>

---

## Quantifying Dimensional Independence in Speech: An Information-Theoretic Framework for Disentangled Representation Learning
- **Authors**: Bipasha Kashyap, Björn W. Schuller, Pubudu N. Pathirana
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.20592v1](https://arxiv.org/abs/2602.20592v1)
- **PDF**: [https://arxiv.org/pdf/2602.20592v1](https://arxiv.org/pdf/2602.20592v1)

语音信号在共享的声学通道中编码情感、语言及病理信息；然而，解耦效果通常仅通过下游任务性能间接评估。本文提出一种信息论框架，通过结合有界神经互信息估计与非参数验证，量化手工声学特征中的跨维度统计依赖性。在六个数据集上的实验表明，跨维度互信息始终处于较低水平（估计边界严格小于0.15纳特），说明所考察数据中统计耦合较弱；而源-滤波器互信息则显著更高（0.47纳特）。归因分析（定义为源成分与滤波器成分对总互信息的贡献比例）显示，情感维度以源成分为主导（80%），而语言维度和病理维度分别以滤波器成分为主导（各占60%和58%）。这些发现为量化语音维度独立性提供了理论框架。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音信号在共享的声学通道中同时编码情感、语言和病理信息。解耦这些维度对下游任务至关重要，但现有方法缺乏对解耦质量的原理性度量。
- **既有方法问题**：现有研究主要关注成对分离问题，且依赖下游任务性能作为解耦的间接评估指标，未能全面评估情感、语言和病理三维信息流之间的统计独立性。

2)  
- **核心方法**：论文提出一个信息论框架，通过结合有界的神经互信息估计与非参数验证，量化手工声学特征间的跨维度统计依赖性。
    - **有界神经MI估计**：整合MINE（提供下界）和CLUB（提供上界）两种神经估计器，并采用指数移动平均稳定化和方差截断技术，以控制估计方差。
    - **非参数验证**：使用KSG估计器进行无训练验证，提供独立的MI基准。
    - **自适应加权融合**：根据上下界之间的不确定性，自适应地加权神经估计与KSG估计的结果，提高估计的鲁棒性。
    - **源-滤波器归因分析**：基于KSG估计，将每个语义维度与源特征、滤波器特征的互信息进行分解，计算归因比例，揭示不同维度的声学生理基础。

3)  
- **评估任务与效果**：在涵盖情感、语言和病理的六个语音语料库上评估。
    - **跨维度独立性量化**：情感-语言、情感-病理、语言-病理三对维度间的互信息均低于0.15纳特，表明在考察的特征空间中接近统计独立。
    - **源-滤波器耦合分析**：源特征与滤波器特征间的互信息较高（0.47纳特），但低于经典声学理论预测，支持源-滤波器分解假设。
    - **归因分析结果**：情感维度信息主要源于源特征（占80%），而语言和病理维度信息则主要源于滤波器特征（分别占60%和58%）。
</div>

</details>

---

## Memory-guided Prototypical Co-occurrence Learning for Mixed Emotion Recognition
- **Authors**: Ming Li, Yong-Jin Liu, Fang Liu, Huankun Sheng, Yeying Fan, Yixiang Wei, Minnan Luo, Weizhan Zhang, Wenping Wang
- **Categories**: cs.LG, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.20530v1](https://arxiv.org/abs/2602.20530v1)
- **PDF**: [https://arxiv.org/pdf/2602.20530v1](https://arxiv.org/pdf/2602.20530v1)

基于多模态生理与行为信号的情感识别在情感计算中具有关键作用，但现有模型大多局限于在受控实验室环境下预测单一情感。相比之下，真实世界中的人类情感体验往往同时包含多种情感状态，这促使混合情感识别作为情感分布学习问题受到广泛关注。然而，当前方法通常忽略了共存情感之间固有的效价一致性与结构化关联。为克服这一局限，本文提出一种记忆引导的原型共现学习框架，显式建模情感共现模式。具体而言，我们首先通过多尺度关联记忆机制融合多模态信号；为捕捉跨模态语义关系，构建情感特定的原型记忆库以生成丰富的生理与行为表征，并采用原型关系蒸馏确保潜在原型空间中的跨模态对齐。此外，受人类认知记忆系统启发，引入记忆检索策略以提取跨情感类别的语义级共现关联。通过这种自底向上的层次化抽象过程，模型学习到具有情感信息量的表征，从而实现准确的情感分布预测。在两个公开数据集上的综合实验表明，所提方法在定量与定性评估中均持续优于当前最先进的混合情感识别方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现实世界的人类情感体验通常是多种情感状态同时存在的混合情感，而现有模型大多局限于预测单一情感，且通常在受控实验室环境下进行。
- **既有方法的问题**：
  - 现有方法（如多标签分类或情感分布学习）常将情感类别视为独立，忽略了共存情感之间固有的效价一致性和结构化相关性。
  - 它们主要依赖数据驱动的统计关联，未能融入情感空间中效价一致性（如同效价情感更可能共现）这一心理学先验。
  - 原始多模态数据中的低层特征往往与噪声纠缠，缺乏语义解耦，直接建模共现模式可能捕获表面的数据相关性而非有意义的心理情感结构。

2)  
论文提出的**记忆引导的原型共现学习（MPCL）框架**通过以下核心方法解决上述问题：

- **多尺度关联记忆融合（MSAF）**：
  - 利用现代Hopfield网络的关联记忆机制，以多尺度（不同β值）融合多模态生理信号（如EEG、GSR、PPG）。
  - 较小的β促进全局上下文聚合，抑制模态特定噪声；较大的β强调局部特征对齐，从而捕获内在相关性和互补信息。

- **原型对齐与共现学习**：
  - **原型记忆库构建**：为生理和行为模态分别构建可学习的原型记忆库。输入特征被重建为这些原型的加权组合，得到语义丰富的嵌入表示。
  - **原型关系蒸馏（PRD）**：通过KL散度损失，强制跨模态原型在拓扑结构上对齐，确保异构模态间的语义一致性。
  - **原型共现学习（PCL）**：受人类联想记忆机制启发，设计记忆检索策略。将原型增强的特征作为查询，从存储的样本中检索关联特征，通过对比学习目标（SemLOOB损失）强化跨模态语义共现一致性。这能在语义层面放大频繁共现的情感原型，抑制虚假关联。

- **分层语义压缩与分布预测**：
  - 通过堆叠的压缩块（每块包含Hopfield原型抽象层和自注意力层），以自底向上的方式将细粒度原型逐步压缩为高度抽象的情感概念表示。
  - 最终通过分类器预测混合情感的概率分布。

整个框架通过**关联记忆融合、原型语义对齐与共现学习、分层抽象**这一流程，显式地建模了情感共现模式，并确保了跨模态的语义结构一致性，从而能够学习到更具信息量的情感表示以进行准确的情感分布预测。

3)  
- **任务**：在多模态混合情感识别任务上，将其建模为情感分布学习问题。
- **数据集**：在两个公开数据集DMER（含EEG、GSR、PPG和面部视频）和WESAD（含ECG、EMG、EDA和加速度计）上进行了评估。
- **效果**：
  - 在**主体依赖**和**主体独立**两种实验设置下，MPCL在全部六个评估指标（如Chebyshev距离↓、KL散度↓、余弦相似度↑等）上均**优于所有基线方法**，取得了最先进的性能。
  - 可视化分析表明，MPCL预测的情感分布与真实分布更接近，并能更好地恢复情感标签间的内在相关结构，证明了其捕获复杂情感共现模式的有效性。
</div>

</details>

---
