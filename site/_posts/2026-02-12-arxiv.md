---
layout: post
title: "arXiv Daily – 2026-02-12"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-12（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-11 08:50 — 2026-02-12 08:50
- 抓取总数：9 篇 | 本页显示：9 篇（去重/过滤后）

## SCRAPL: Scattering Transform with Random Paths for Machine Learning
- **Authors**: Christopher Mitcheltree, Vincent Lostanlen, Emmanouil Benetos, Mathieu Lagrange
- **Categories**: cs.SD, cs.LG, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.11145v1](https://arxiv.org/abs/2602.11145v1)
- **PDF**: [https://arxiv.org/pdf/2602.11145v1](https://arxiv.org/pdf/2602.11145v1)

小波散射变换系数（即路径）间的欧氏距离，在计算机视觉、语音与音频处理的深度逆问题中，为感知质量评估提供了信息丰富的梯度。然而，由于路径数量众多，这些变换作为可微损失函数用于随机梯度下降时计算成本高昂，严重限制了其在神经网络训练中的应用。针对此问题，我们提出“用于机器学习的随机路径散射变换”（SCRAPL）：一种高效评估多变量散射变换的随机优化方案。我们为联合时频散射变换（JTFS）实现了SCRAPL，该变换可在多尺度与多速率下解调谱时域模式，从而精细刻画间歇性听觉纹理。我们将SCRAPL应用于可微分数字信号处理（DDSP），具体包括颗粒合成器与Roland TR-808鼓机的无监督声音匹配任务。此外，我们提出一种基于重要性采样的初始化启发式方法，使SCRAPL能够自适应数据集的感知内容，从而提升神经网络收敛速度与评估性能。我们公开了代码与音频样本，并将SCRAPL封装为Python软件包提供使用。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在计算机视觉、语音和音频处理中，小波散射变换（ST）的欧氏距离能提供感知质量评估的有效梯度。然而，作为可微损失函数用于随机梯度下降时，其路径数量众多，计算成本高昂，限制了在神经网络训练中的应用。  
- **既有方法问题**：现有方法（如完整ST）计算开销大，内存和操作成本高；而简单随机采样路径虽能加速，但梯度估计方差大，导致数值精度不足，影响训练稳定性。

2)  
论文提出**SCRAPL**（随机路径散射变换），通过随机优化方案高效评估多变量散射变换，解决计算成本与精度问题。核心方法包括：  
- **随机路径采样**：每次训练迭代仅随机采样单一路径计算损失梯度，大幅降低计算开销。  
- **优化算法增强**：  
  - **P-Adam**：针对ST路径梯度非独立同分布特性，为每条路径维护自适应矩估计，调整平滑时间常数。  
  - **P-SAGA**：引入路径记忆机制，存储历史梯度以加速收敛并减少方差。  
- **θ-重要性采样**：基于任务感知的初始化启发式方法，根据参数对路径梯度的敏感度分配采样概率，使优化更关注信息丰富的路径，提升收敛速度和性能。  
- **整体流程**：结合上述技术，SCRAPL在保持梯度无偏估计（均匀采样时）的同时，通过方差控制与任务适配采样，在计算效率与模型精度间取得平衡。

3)  
在**无监督声音匹配**任务上评估效果：  
- **颗粒合成器**：SCRAPL在合成器参数误差上接近完整JTFS（差距2倍内），计算成本接近多尺度谱损失（MSS）（差距2倍内），显著优于其他感知损失方法（如MSS、MS-CLAP）。  
- **Chirplet合成器**：θ-重要性采样进一步提升参数预测精度（AM误差降低25–55%，FM误差降低14–80%），加速收敛23–50%。  
- **Roland TR-808鼓机**：在时间对齐/非对齐设置下，SCRAPL均能有效匹配瞬态部分，性能优于MSS，但弱于完整JTFS；在音频距离指标（如FAD）上表现稳健。
</div>

</details>

---

## Simultaneous Speech-to-Speech Translation Without Aligned Data
- **Authors**: Tom Labiausse, Romain Fabre, Yannick Estève, Alexandre Défossez, Neil Zeghidour
- **Categories**: cs.CL, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.11072v1](https://arxiv.org/abs/2602.11072v1)
- **PDF**: [https://arxiv.org/pdf/2602.11072v1](https://arxiv.org/pdf/2602.11072v1)

同步语音翻译需要在实时处理非单调词汇依赖的同时，将源语音转换为目标语言。传统方法依赖于词级对齐数据的监督训练，这类数据难以大规模收集，因此通常采用针对特定语言的启发式方法生成次优的合成对齐数据。本文提出Hibiki-Zero方法，完全摒弃了对词级对齐数据的需求。这从根本上简化了训练流程，使其能够无缝扩展到具有不同语法结构的多样化语言，消除了设计语言特定对齐启发式方法的瓶颈。我们首先利用句子级对齐数据进行训练，以学习高延迟下的语音翻译，随后采用基于GRPO的新型强化学习策略，在保持翻译质量的同时优化延迟。Hibiki-Zero在五项跨语言至英语翻译任务中，在翻译准确度、延迟、音色迁移和自然度方面均达到最先进水平。此外，我们证明该模型仅需不足1000小时的语音数据即可适配新的输入语言。我们提供了示例、模型权重、推理代码，并发布了包含45小时多语言数据的语音翻译评估基准。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：实时语音翻译需在源语音输入时同步生成目标语言输出，并处理非单调的词序依赖。传统方法依赖词级对齐数据进行监督训练。
- **既有问题**：词级对齐数据难以大规模收集，现有系统通常依赖基于语言特定启发式方法生成的合成对齐数据，这种方法次优且限制了模型在不同语法结构语言间的扩展。

2)  
论文提出的Hibiki-Zero方法通过以下核心步骤解决了上述问题：
- **消除词级对齐依赖**：仅使用句子级对齐数据（通过标点符号等易于获取）训练基础翻译模型，极大简化了数据准备流程，并支持跨语言扩展。
- **两阶段训练流程**：
    - **第一阶段**：在句子级对齐数据上训练，学习高延迟下的语音翻译，建立基本的翻译能力。
    - **第二阶段**：应用新颖的强化学习策略优化延迟。该方法基于GRPO算法，并设计了基于BLEU分数的过程奖励机制。
- **核心创新——过程奖励与GRPO**：
    - 在翻译过程中，模型会生成多个输出。奖励信号不仅基于最终翻译的BLEU分数，还基于多个中间时间点的部分翻译与对应源语句子参考翻译的BLEU分数（通过参数α平衡）。
    - 这种细粒度的“过程奖励”提供了更丰富的优化信号，使模型能学习在翻译质量和延迟之间取得更好平衡的决策策略，而无需任何词级或人工解释数据。
- **架构优势**：模型基于多流RQ-Transformer架构，能同步处理源语音并生成目标语音和文本流，保持了实时推理和高保真语音生成的能力。

3)  
Hibiki-Zero在多项多语言到英语的语音翻译任务上取得了领先效果：
- **翻译质量与延迟**：在长文本和短文本测试集上，其ASR-BLEU、COMET等翻译质量指标优于或媲美Seamless和Hibiki等基线模型，同时实现了更低的端到端偏移和平均滞后。
- **语音保真度**：在说话人相似性、音频质量和语音自然度的人类评估中大幅领先，特别是在跨语言说话人身份保留方面提升显著。
- **语言扩展性**：仅使用不足1000小时的语音数据对模型进行微调，即可成功适配新的输入语言（如意大利语），并保持优异的性能。
</div>

</details>

---

## MOSS-Audio-Tokenizer: Scaling Audio Tokenizers for Future Audio Foundation Models
- **Authors**: Yitian Gong, Kuangwei Chen, Zhaoye Fei, Xiaogui Yang, Ke Chen, Yang Wang, Kexin Huang, Mingshu Chen, Ruixiao Li, Qingyuan Cheng, Shimin Li, Xipeng Qiu
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.10934v1](https://arxiv.org/abs/2602.10934v1)
- **PDF**: [https://arxiv.org/pdf/2602.10934v1](https://arxiv.org/pdf/2602.10934v1)

离散音频分词器是赋予大语言模型原生音频处理与生成能力的基础。尽管近期取得进展，现有方法通常依赖预训练编码器、语义蒸馏或异构的基于CNN的架构。这些设计引入了固定的归纳偏置，限制了重建保真度并阻碍了有效的规模化扩展。本文主张离散音频分词应通过同质且可扩展的架构进行完全端到端学习。为此，我们首先提出CAT（基于Transformer的因果音频分词器），这是一种纯Transformer架构，通过从头联合优化编码器、量化器和解码器来实现高保真重建。基于CAT架构，我们开发了MOSS-Audio-Tokenizer——一个拥有16亿参数的大规模音频分词器，在300万小时多样化通用音频数据上进行预训练。研究表明，这种基于同质因果Transformer模块构建的简单、完全端到端方法能够优雅地扩展，并在多音频领域支持高保真重建。在语音、环境声和音乐任务中，MOSS-Audio-Tokenizer在广泛码率范围内持续超越现有编解码器，同时展现出随规模扩大可预测的性能提升。值得注意的是，利用本模型的离散令牌，我们开发了首个纯自回归TTS模型，其性能超越先前的非自回归与级联系统。此外，MOSS-Audio-Tokenizer无需辅助编码器即可实现具有竞争力的ASR性能。我们的研究确立了CAT架构作为新一代原生音频基础模型的统一可扩展接口。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：为赋能大语言模型处理音频，需将连续音频离散化为令牌。现有方法多依赖预训练编码器、语义蒸馏或混合CNN架构，引入了固定的归纳偏置。
- **既有问题**：这些设计限制了重建保真度，阻碍了模型、数据和量化能力的统一扩展，难以满足未来音频基础模型对简单、可扩展架构的需求。

2)  
论文提出**CAT架构**及在其基础上构建的**MOSS-Audio-Tokenizer**，通过以下方式解决问题：
- **同质化可扩展架构**：采用纯因果Transformer块构建编码器和解码器，消除CNN等固定偏置，形成简单、统一的架构，便于端到端联合优化与规模扩展。
- **完全端到端训练**：在单一训练流程中，联合优化编码器、量化器、解码器和判别器，不依赖预训练组件或分阶段优化，确保所有模块协同适应大规模数据。
- **统一音频建模**：
    - **语义建模**：通过连接仅解码器LLM并执行音频到文本任务（如ASR、音频描述），使离散表示富含语义并与文本对齐。
    - **声学建模**：采用多尺度梅尔谱图损失和对抗训练，确保跨语音、声音和音乐的高保真重建。
    - **量化优化**：使用残差向量量化并支持量化器丢弃，实现宽比特率范围内的鲁棒建模。
- **可控音频生成**：基于CAT构建纯自回归TTS系统，并引入**渐进序列丢弃**训练策略，使单个模型能通过控制RVQ深度实现可变比特率合成，无需修改架构。

3)  
- **音频重建**：在语音、声音和音乐上，于宽比特率范围（0.75-4 kbps）均取得SOTA重建质量，指标（如SIM、PESQ）优于Encodec等先前编解码器。
- **语音生成**：基于CAT令牌构建的纯自回归TTS模型（CAT-TTS），在Seed-TTS-Eval基准上，其说话人相似度（SIM）超越所有开源对比系统（包括非自回归和级联模型），首次实现纯自回归系统性能领先。
- **语音理解**：在不使用辅助音频编码器的情况下，支持具有竞争力的自动语音识别性能，匹配或超越了依赖专用编码器结合LLM的模型。
</div>

</details>

---

## Self-Supervised Learning for Speaker Recognition: A study and review
- **Authors**: Theo Lepage, Reda Dehak
- **Categories**: eess.AS, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.10829v1](https://arxiv.org/abs/2602.10829v1)
- **PDF**: [https://arxiv.org/pdf/2602.10829v1](https://arxiv.org/pdf/2602.10829v1)

在监督学习环境下训练的深度学习模型已彻底改变了音频与语音处理领域。然而，其性能本质上依赖于人工标注数据的数量，导致模型扩展成本高昂，且在未见条件下泛化能力较差。为应对这些挑战，自监督学习作为一种新兴范式，通过利用大量无标注数据学习有效表征而展现出巨大潜力。自监督学习在自动语音识别领域的应用已得到广泛研究，但在其他下游任务——尤其是说话人识别——中的研究仍处于早期阶段。本文系统阐述了最初为计算机视觉设计的自监督学习实例不变性框架（如SimCLR、MoCo和DINO）及其在说话人识别中的适配方法，同时介绍了基于这些框架提出的各类说话人识别自监督学习方法。通过系统性评述：（1）探究了自监督学习框架关键超参数的影响；（2）分析了自监督学习组件的作用（如数据增强、投影器、正样本采样）；（3）在统一实验设置下，使用领域内与跨领域数据评估了各框架在说话人识别任务上的表现，并对现有方法进行了全面比较。研究发现：DINO在下游任务中表现最优，且能有效建模说话人内部差异性，但其对超参数和训练条件高度敏感；而SimCLR与MoCo作为稳健的替代方案，能有效捕捉说话人间差异性且更不易出现表征坍缩。本研究旨在揭示该领域的最新趋势与进展，并指出现阶段面临的挑战。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
**研究背景与既有方法的问题**  
- **背景**：说话人识别（SR）是语音处理的核心任务，传统方法依赖监督深度学习，性能受限于大量人工标注数据，成本高且泛化能力有限。  
- **问题**：  
  - 监督方法需大规模标注数据（如VoxCeleb、NIST SRE），数据获取困难且昂贵。  
  - 自监督学习（SSL）在自动语音识别（ASR）中广泛应用，但在SR领域研究仍处于早期阶段，缺乏系统分析。  
  - 现有SSL方法（如SimCLR、MoCo、DINO）多源自计算机视觉，其适应SR的机制、超参数影响及组件作用尚不明确。

2)  
**论文核心方法如何解决上述问题**  
本文通过系统性综述与实验，全面评估SSL在SR中的应用，具体解决方式如下：  
- **系统比较主流SSL框架**：  
  - 在统一实验设置下，对比了基于实例不变性的SSL框架（包括对比学习、聚类、信息最大化和自蒸馏四类），如SimCLR、MoCo、DINO、SwAV、VICReg等。  
  - 使用两种编码器（轻量级Fast ResNet-34和大型ECAPA-TDNN），在VoxCeleb基准上评估性能。  
- **分析关键超参数与组件**：  
  - **超参数影响**：探究了负样本数量、温度参数、动量更新系数等对性能的影响（例如MoCo中负样本数量影响有限，DINO对训练设置高度敏感）。  
  - **组件作用**：  
    - **数据增强**：SSL性能严重依赖数据增强（如混响和噪声添加），以学习对信道变化鲁棒的表征；禁用增强会导致性能平均下降约199%。  
    - **投影头**：对比学习框架（SimCLR、MoCo）无需投影头性能更优，而非对比框架（如VICReg、DINO）需投影头以避免前置任务与下游任务不匹配。  
    - **正样本采样**：默认同语句采样易编码信道信息；使用监督正样本（同一说话人的不同录音）可显著提升性能，凸显了改进采样策略的必要性。  
- **评估单阶段与多阶段方法**：  
  - **单阶段方法**：直接基于SSL目标训练，DINO表现最佳，能有效建模说话人内部变化，但对超参数敏感；SimCLR和MoCo更稳定，擅长捕获说话人间差异。  
  - **多阶段方法**：利用SSL生成伪标签进行监督训练，可进一步提升性能，但计算成本更高且需估计说话人类别数。  
- **开源工具**：提供了`sslsv`工具包，便于复现实验与进一步研究。

3)  
**在哪些任务上取得了怎样的效果**  
- **任务**：说话人验证（SV），主要在VoxCeleb1（O/E/H）基准上进行评估，并测试了跨域泛化能力（SITW、VOiCES数据集）。  
- **效果**：  
  - **最佳性能**：DINO框架在ECAPA-TDNN编码器上达到最优，在VoxCeleb1-O上EER为2.82%（监督基线为1.34%），显著优于其他SSL方法（如SimCLR EER 6.41%）。  
  - **跨域泛化**：DINO在SITW和VOiCES上同样领先（EER分别为4.92%和1.85%），展现了强泛化能力。  
  - **框架对比**：SimCLR和MoCo计算效率更高且性能稳定，是DINO的有效替代；多阶段方法通过伪标签训练可逼近监督性能，但需更多计算资源。  
  - **系统融合**：融合多个SSL框架（SimCLR、MoCo、SwAV、VICReg、DINO）可将EER进一步降至2.60%，表明方法间存在互补性。
</div>

</details>

---

## Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity
- **Authors**: Hugo L. Hammer, Vajira Thambawita, Pål Halvorsen
- **Categories**: cs.SD, cs.AI, cs.CL
- **arXiv**: [https://arxiv.org/abs/2602.10735v1](https://arxiv.org/abs/2602.10735v1)
- **PDF**: [https://arxiv.org/pdf/2602.10735v1](https://arxiv.org/pdf/2602.10735v1)

有声电子书将同步音频与数字文本相结合，在播放时高亮显示当前朗读的词语或句子。这种形式不仅有助于早期识字教育，辅助有阅读障碍的人群，也能让普通读者在阅读与听读之间无缝切换。随着自然流畅的神经文本转语音（TTS）技术的兴起，已有多种商业服务利用该技术将标准文本电子书转换为高质量的有声电子书。然而，目前尚无开源解决方案能够实现这一功能。本文提出Calliope，一个旨在填补这一空白的开源框架。该方法利用前沿的开源TTS技术，将文本电子书转换为符合EPUB 3媒体叠加格式的有声电子书。本方法包含多项创新步骤：音频时间戳在TTS过程中直接捕获，确保朗读与文本高亮精确同步；严格保留出版商的原始排版、样式及嵌入媒体；整个流程可离线运行。离线能力消除了持续的API成本，缓解了隐私顾虑，并避免了基于云服务可能涉及的版权合规问题。该框架目前支持前沿开源TTS系统XTTS-v2和Chatterbox。一种潜在的替代方案是首先通过TTS生成朗读音频，再通过强制对齐实现与文本的同步。然而，尽管本方法能确保精确同步，实验表明强制对齐会导致音频与文本高亮之间产生明显偏移，从而降低阅读体验。源代码及使用说明详见https://github.com/hugohammer/TTS-Narrated-Ebook-Creator.git。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：有声电子书结合同步音频与文本高亮，能辅助阅读学习。随着神经TTS技术发展，已有商业服务将文本电子书转换为高质量有声书，但缺乏开源方案。
- **既有方法问题**：
  - 现有开源方案（如Syncabook、Storyteller）依赖强制对齐，易产生音画同步漂移，影响阅读体验。
  - 部分方案（如Audible-epub3-maker）依赖云端API，带来持续成本、隐私风险及版权合规问题。
  - 多数方案会重建电子书布局，破坏原出版商的版式与样式保真度。

2)  
Calliope通过一个全离线、开源框架解决上述问题，其核心方法包括三个阶段：

- **阶段1：文本提取与布局保留**
  - 解析EPUB源文件的DOM，提取文本块并规范化。
  - 通过递归文本分割算法处理长句/短句，适配TTS模型的上下文窗口限制。
  - 在原文中注入带唯一ID的`<span>`标签作为同步锚点，严格保留原始CSS样式与布局。

- **阶段2：TTS生成与音画同步**
  - 集成两种先进开源TTS模型（XTTS-v2与Chatterbox），在本地生成音频。
  - **关键创新**：在TTS合成过程中直接捕获时间戳，而非事后强制对齐。每个句子的音频时长在生成时即被记录，通过公式累加计算每个句子的起止时间，确保同步精确性。
  - 对音频进行淡出处理并添加静音间隔，以消除边界伪影，同时通过时间戳连续计算实现“无间隙”高亮，避免视觉闪烁。

- **阶段3：打包与集成**
  - 根据计算出的时间戳生成SMIL同步文件，映射文本锚点与音频区间。
  - 更新OPF包文件，注入媒体时长、高亮激活类等元数据。
  - 注入自适应CSS规则，支持深色/浅色模式下的高亮颜色切换。
  - 最终将所有资源打包为符合EPUB 3 Media Overlay标准的电子书，整个过程完全离线。

- **整体优势**：
  - **精确同步**：从源头避免强制对齐带来的漂移问题。
  - **隐私与成本**：本地运行，无云端API依赖，保护隐私并免去费用。
  - **布局保真**：对源文件进行“外科手术式”修改，完整保留原书排版、样式及嵌入媒体。

3)  
- **核心任务**：将静态EPUB文本电子书转换为具备精确音画同步功能的EPUB 3 Media Overlay有声电子书。
- **取得效果**：
  - **同步精度**：实现了音画高亮的完全精确同步（漂移为0）。实验对比显示，强制对齐方法（如Syncabook）在超过30%的句子上存在大于50ms的滞后漂移，足以影响阅读体验，而Calliope无此问题。
  - **格式兼容性**：生成的文件可在Thorium Reader、BookFusion等标准阅读器上完美播放。
  - **实用性**：框架已开源，支持在消费级硬件上运行，并提供了详细的安装与使用指南。
</div>

</details>

---

## RE-LLM: Refining Empathetic Speech-LLM Responses by Integrating Emotion Nuance
- **Authors**: Jing-Han Chen, Bo-Hao Su, Ya-Tse Wu, Chi-Chun Lee
- **Categories**: eess.AS, cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.10716v1](https://arxiv.org/abs/2602.10716v1)
- **PDF**: [https://arxiv.org/pdf/2602.10716v1](https://arxiv.org/pdf/2602.10716v1)

随着生成式人工智能的快速发展，人机交互中的共情能力变得至关重要。现有研究多集中于情感反射，而作为深化互动关键的情感探索却常被忽视。当前大型语言模型主要依赖文本信息，难以捕捉细腻的情感层次。为此，我们提出RE-LLM模型，通过融合维度情感嵌入与辅助学习机制，构建了具备语音理解能力的大型语言模型。实验结果表明，在三个数据集上，该模型在共情指标上均取得统计显著性提升：在ESD数据集上，相较于纯文本基线及语音-LLM基线，情感反应分数分别相对提升14.79%和6.76%；在情感探索维度，IEMOCAP数据集上提升35.42%和3.91%，ESD数据集上提升139.28%和9.83%，MSP-PODCAST数据集上提升60.95%和22.64%。同时，在语音情感识别任务中，未加权准确率在IEMOCAP、ESD和MSP-PODCAST数据集上分别提升5.4%、2.3%和6.9%。这些成果充分验证了RE-LLM在增强情感理解与提升共情响应生成能力方面的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：随着生成式AI的发展，人机交互中的共情能力变得至关重要。现有研究主要关注情感反射（即匹配用户情绪），但忽视了情感探索（通过提问引导更深层情感表达），而后者是深度共情互动的关键。  
- **既有方法的问题**：当前共情大语言模型（LLM）主要依赖文本输入，但文本仅能捕捉有限的情感细微差别。现有方法通过文本描述或自动语音识别（ASR）嵌入来捕捉副语言信息，这忽略了语音信号中丰富的情感细节，导致模型无法进行有效的情感探索和适应性回应。

2)  
论文提出的RE-LLM通过整合语音情感嵌入和辅助学习任务来解决上述问题，具体方法如下：  
- **架构设计**：以BLSP-Emo为骨干网络，在标准语音LLM（包含语音编码器和模态适配器）基础上，增加了一个**情感细微差别模块**。  
- **情感嵌入增强**：  
  - 使用预训练的wav2vec 2.0模型（冻结参数）提取**维度情感嵌入**（包含效价、唤醒度和支配度），与Whisper提取的原始语音嵌入在时间步上拼接，形成更丰富的情感表征。  
- **多视角辅助任务**：  
  - 在训练阶段引入两个辅助任务：**分类情感分类**（四类主要情感）和**维度情感回归**（三个维度）。  
  - 通过交叉熵损失和均方误差损失联合优化，使模型能更精细地理解情感信息。  
- **训练流程**：  
  - **预处理阶段**：基于情感标签生成期望回应，构建语音-回应配对数据集。  
  - **对齐阶段**：通过KL散度损失使模型生成的回应与期望回应对齐，同时结合辅助任务损失进行多任务学习。  
- **核心优势**：  
  - 无需复杂的大模型微调，通过简单的嵌入拼接和辅助任务即可增强情感理解。  
  - 同时利用分类和维度情感信息，更全面地捕捉语音中的情感细微差别，从而提升共情回应的探索能力。

3)  
- **任务与数据集**：在三个语音情感数据集（IEMOCAP、ESD、MSP-PODCAST）上评估了共情回应生成和语音情感识别任务。  
- **共情效果**：  
  - **情感反应（ER）分数**：在ESD上相对纯文本基线和语音LLM基线分别提升14.79%和6.76%；在MSP-PODCAST上也显著提升。  
  - **探索（Ex）分数**：提升尤为显著，在IEMOCAP、ESD和MSP-PODCAST上相对纯文本基线分别提升35.42%、139.28%和60.95%，表明模型更能通过提问引导深层情感表达。  
- **语音情感识别**：未加权准确率（UA）在三个数据集上分别提升5.4%、2.3%和6.9%，证明模型的情感感知能力增强。
</div>

</details>

---

## From Diet to Free Lunch: Estimating Auxiliary Signal Properties using Dynamic Pruning Masks in Speech Enhancement Networks
- **Authors**: Riccardo Miccini, Clément Laroche, Tobias Piechowiak, Xenofon Fafoutis, Luca Pezzarossa
- **Categories**: eess.AS, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.10666v1](https://arxiv.org/abs/2602.10666v1)
- **PDF**: [https://arxiv.org/pdf/2602.10666v1](https://arxiv.org/pdf/2602.10666v1)

音频设备中的语音增强常依赖语音活动检测、信噪比估计或声学场景分类等辅助模块，以实现鲁棒的上下文感知与流畅用户体验。与语音增强类似，这些任务通常采用深度学习技术，但在设备端部署额外模型会带来计算负担，而基于云端的推断则会引入额外延迟并损害隐私。先前研究采用动态通道剪枝技术，通过根据当前输入自适应关闭特定通道来降低计算量。本文探讨是否可从这些内部剪枝掩码中提取有效的信号属性，从而避免使用独立模型。研究表明，通过简单可解释的预测器，在语音活动检测任务中准确率可达93%，噪声分类达84%，基频估计的R²达0.86。使用二值掩码时，预测可简化为加权求和，产生的开销可忽略不计。本研究的贡献在于：一方面通过下游预测任务分析动态通道剪枝模型的涌现行为，揭示其学习机制；另一方面将动态通道剪枝重新定位为一种高效语音增强与信号属性同步估计的集成解决方案。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音增强（SE）系统常需辅助模块（如VAD、SNR估计）以实现上下文感知，但为每个任务部署独立模型在设备端计算成本高，云端推理则存在延迟和隐私问题。  
- **既有方法问题**：现有方法或需额外专用模型（计算开销大），或依赖显式估计辅助线索（增加系统复杂度），缺乏高效、一体化的解决方案。

2)  
- **核心方法**：利用动态通道剪枝（DynCP）SE模型内部的**二进制剪枝掩码**作为特征，通过简单线性/逻辑回归模型估计辅助信号属性，无需额外专用模型。  
- **解决思路**：  
  - **特征来源**：DynCP的剪枝掩码由门控子网生成，其动态激活模式隐式编码了输入信号的语音/噪声特性（如噪声水平、语音活动）。  
  - **特征处理**：筛选具有足够方差（σ > τ）的掩码作为特征，去除恒定或无信息通道，得到紧凑特征集（如202维）。  
  - **预测模型**：采用轻量级回归/分类模型，利用二进制掩码将点积简化为求和操作，计算开销极低。  
  - **优势**：  
    - 从DynCP的“副产品”中提取信息，实现多任务学习效果；  
    - 特征线性可分，简单模型即可达到高精度；  
    - 几乎零额外计算成本，适合边缘设备部署。

3)  
- **任务与效果**：在多个分类和回归任务上评估，使用剪枝掩码特征显著优于基线（STFT频谱、抑制掩码）。  
  - **分类任务**：VAD准确率达93%，噪声分类准确率84%，性别分类准确率高，但口音分类接近随机（因SE模型对此不敏感）。  
  - **回归任务**：F0估计R²为0.86，输入PESQ估计归一化MAE为0.2，输入SI-SDR估计MAE为3.2 dB。  
  - **计算效率**：所有任务（21个目标）仅需每帧4242次额外运算，占总计算量0.6%-0.93%，适合实时边缘应用。
</div>

</details>

---

## AudioRAG: A Challenging Benchmark for Audio Reasoning and Information Retrieval
- **Authors**: Jingru Lin, Chen Zhang, Tianrui Wang, Haizhou Li
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.10656v1](https://arxiv.org/abs/2602.10656v1)
- **PDF**: [https://arxiv.org/pdf/2602.10656v1](https://arxiv.org/pdf/2602.10656v1)

随着大型音频语言模型在声音、语音及音乐相关任务中展现出卓越性能，相关评测基准的构建日益受到关注。现有基准大多仅关注模型内部知识的推理能力，忽视了现实场景中对外部信息关联的需求。为弥补这一不足，我们提出了AudioRAG——一个面向真实网络环境、基于信息检索增强的音频推理能力评估基准。该基准包含由大语言模型生成和人工标注的问答对。实验表明，即使当前最先进的音频语言模型也难以有效回答这些问题。为此，我们设计了一种融合音频推理与检索增强生成的智能体流程，为后续研究提供了更具参考价值的基线。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：大型音频-语言模型（LALMs）在音频理解任务上表现优异，但现有基准主要评估模型基于内部知识的推理能力，忽略了现实场景中常需结合外部信息的需求。  
- **既有问题**：现有音频推理基准未涵盖需结合信息检索的复杂多跳推理任务，导致模型在缺乏外部知识时易产生幻觉（错误回答），无法满足真实用户查询（如基于新闻音频查询比赛比分）。

2)  
论文提出一个**基于LLM的智能体管道**，通过整合音频处理与信息检索来解决上述问题。其核心方法如下：  
- **管道架构**：采用“思考-调用-回答”的自主策略，由文本LLM作为推理引擎，协调整个流程。  
- **工具集成**：  
  - **音频处理工具**：调用LALM（如Qwen-Omni系列）从音频上下文中提取关键属性（如音乐流派、语音转文本）。  
  - **搜索工具**：基于WebThinker实现，通过搜索引擎API检索外部知识（如实时信息、事实数据）。  
- **工作流程**：  
  1. 接收用户查询（文本或音频问题+音频上下文）。  
  2. 推理LLM分解多跳问题，决定何时调用音频工具提取信息，或调用搜索工具获取外部知识。  
  3. 迭代执行工具调用与中间推理，最终生成答案。  
- **优势**：  
  - 弥补了LALMs在文本指令遵循和复杂推理上的不足。  
  - 通过检索增强减少知识幻觉，提升事实准确性。  
  - 结构化多步推理改善了复杂问题的理解和求解能力。

3)  
- **评估任务**：在AudioRAG基准上测试，该基准包含500个需结合音频推理与信息检索的多跳问答样本。  
- **效果**：  
  - 原始LALMs表现不佳，最佳模型（Gemini-2.5-Flash）准确率仅45%。  
  - 智能体管道显著提升性能：例如，Qwen3-Omni作为音频工具时，准确率从37.0%提升至46.2%（相对提升24.9%）。  
  - 错误分析显示，管道大幅减少了推理错误和知识错误，但无效回答略有增加。
</div>

</details>

---

## AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning
- **Authors**: Liyang Chen, Hongkai Chen, Yujun Cai, Sifan Li, Qingwen Ye, Yiwei Wang
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.10439v1](https://arxiv.org/abs/2602.10439v1)
- **PDF**: [https://arxiv.org/pdf/2602.10439v1](https://arxiv.org/pdf/2602.10439v1)

大型音频语言模型在音频理解与推理方面展现出强大能力，但其在细粒度听觉感知任务上的表现仍不稳定，现有方法主要依赖数据密集型训练来内化感知能力。本文提出AudioRouter——一种基于强化学习的框架，使大型音频语言模型能够通过学习何时及如何使用外部音频工具来提升音频理解能力。与将工具使用和音频推理紧密耦合的传统方式不同，AudioRouter将工具调用建模为显式的决策问题，在保持底层推理模型冻结的同时优化轻量级路由策略。实验结果表明，AudioRouter在标准音频理解基准上取得显著性能提升，且学习工具调用所需的训练数据量相比传统训练范式最多可减少600倍。这些发现表明，学习有效的工具使用策略为大型音频语言模型提供了一条数据高效、可扩展的感知能力增强路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：大型音频语言模型在语义理解上表现出色，但在音高估计、事件计数等细粒度听觉感知任务上仍不可靠。  
- **既有方法的问题**：主流方法依赖大规模端到端训练，通过海量标注数据内化感知能力，这面临两大挑战：高质量音频标注成本高昂，且感知能力提升并不总是与数据量成正比。此外，现有工具调用方法存在**表面关键词偏差**（仅根据语义关键词而非功能有效性选择工具）和**工具能力边界幻觉**（调用超出工具能力范围）等失败模式。

2)  
论文提出了**AudioRouter**，一个基于强化学习的框架，通过解耦决策与推理，以数据高效的方式学习使用外部音频工具。其核心方法如下：

- **问题重构与系统设计**：
    - 将工具使用明确为一个离散决策问题，动作空间包括“直接推理”和调用K个特定工具之一。
    - 系统由三部分组成：一个轻量级、可训练的**路由策略模块（Router）**、一组外部音频工具、以及一个**冻结的音频推理模型（Reasoner）**。这种解耦设计是关键。

- **路由策略优化**：
    - **路由模块**：根据输入（音频、问题、选项）决定是直接推理还是调用某个工具。它仅学习调度策略，不学习音频感知或答案推理，极大降低了学习复杂度。
    - **相对结果奖励**：训练信号并非来自人工标注的工具使用标签，而是基于工具调用与直接推理的**相对效果比较**。具体而言：
        - 若调用工具后答案正确，而直接推理错误，则给予高额正奖励。
        - 若调用工具后答案错误，而直接推理正确，则给予高额负奖励。
        - 若两者结果相同（无论对错），则给予小额负奖励，以抑制冗余的工具调用。
    - 这种奖励机制使Router能自动学习在工具能带来**边际收益**时才调用，优先考虑功能有效性而非表面关键词，并将推理限制在工具明确的能力边界内。

- **优势**：通过仅优化轻量级路由策略并保持强大的推理主干冻结，该方法显著降低了样本复杂度，实现了极高的数据效率，同时避免了干扰模型已有的推理和感知表征。

3)  
- **评测任务**：在两个标准音频理解基准上进行了评估：**MMAU-mini**（涵盖声音、音乐、语音任务）和**MMAR**（涵盖单模态与多模态混合的音频推理任务）。
- **取得的效果**：
    - **性能提升**：在两个不同的骨干模型上，AudioRouter均一致提升了基线模型的平均准确率。例如，在Qwen2.5-Omni骨干上，MMAU-mini准确率从73.9%提升至77.8%，MMAR从63.0%提升至64.4%。
    - **数据高效性**：仅使用**1.5k**训练样本，即达到了优于或媲美需要使用**38k至971k**样本的端到端基线的性能，实现了**25倍至647倍**的数据节省。
    - **路由有效性**：与随机路由或基于其他大模型的路由策略相比，AudioRouter学习到的路由策略能做出更有效的决策，带来了最佳的整体推理性能。
</div>

</details>

---
