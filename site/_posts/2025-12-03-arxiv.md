---
layout: post
title: "arXiv Daily – 2025-12-03"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-03（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-02 08:50 — 2025-12-03 08:50
- 抓取总数：10 篇 | 本页显示：10 篇（去重/过滤后）

## Perceptual evaluation of Acoustic Level of Detail in Virtual Acoustic Environments
- **Authors**: Stefan Fichna, Steven van de Par, Bernhard U. Seeber, Stephan D. Ewert
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.02891v1](https://arxiv.org/abs/2512.02891v1)
- **PDF**: [https://arxiv.org/pdf/2512.02891v1](https://arxiv.org/pdf/2512.02891v1)

虚拟声学环境能够创建并模拟逼真且生态效度高的日常生活场景，这对于听力研究与听力学至关重要。其中，混响室内环境尤为重要。在实时应用中，房间声学模拟需要进行简化，但为捕捉所有感知相关效应所需的声学细节层次仍不明确。本研究探讨了在三个真实环境（带连通厨房的客厅、酒吧及地铁站）模拟中改变声学细节层次的影响。声学细节层次通过生成不同数量的早期反射镜像声源，或排除各环境特有的几何房间细节来进行调整。模拟的感知评估采用头戴式耳机，与对应真实环境中假人头录制的双耳房间脉冲响应进行比较，或使用扬声器进行。研究评估了脉冲信号、电贝斯演奏片段和语音片段在整体感知上的差异，同时评价了 plausibility（感知合理性）、言语可懂度及声像外化程度。结果表明，在保持与假人头录音相近的感知合理性、言语可懂度及声像外化效果的前提下，大幅降低声学细节层次是可行的。只要扩散性后期混响得到恰当呈现，早期反射的数量与精确度似乎影响较小。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：虚拟声学环境在听力研究、听力学及娱乐应用中至关重要。实时应用需对房间声学模拟进行简化，但**必要的声学细节水平（ALOD）尚不明确**，以确保感知相关的所有效果得以保留。
- **既有方法问题**：先前研究通常**孤立考察特定环境、信号或重放方法**，缺乏跨环境、跨任务的统一评估。此外，ALOD的简化多基于几何模型简化（如减少多边形数量），**未能系统控制声学模拟中特定感知相关特征**（如早期反射数量、散射、耦合空间效应）的独立影响，限制了结论的普适性。

2)  
- **核心方法**：本研究采用**感知驱动的ALOD定义**，基于已简化的“代理鞋盒”几何模型，通过**系统调制房间脉冲响应的特定特征**来改变ALOD，而非单纯减少几何细节。使用Room Acoustics SimulatoR (RAZR) 生成合成双耳房间脉冲响应（BRIR）和扬声器重放信号。
- **解决方式**：
  - **统一评估框架**：在**同一组参与者**中，对**三个真实环境**（客厅+厨房、酒吧、地铁站）进行测试，涵盖**耳机与扬声器两种重放方式**，并与真实环境录制的BRIR直接比较。
  - **多维度感知测量**：综合评估**整体差异、合理性、言语可懂度、空间音频质量（如距离、音色、混响）及外部化**，使用多种刺激（脉冲、语音、音乐片段），以揭示不同任务对ALOD的敏感度。
  - **可控的ALOD调制**：从最高ALOD（包含散射、耦合空间等所有RAZR特征）开始，**逐步省略特定组件**（如降低早期反射阶数、移除附近反射面、忽略双斜率衰减），从而**孤立考察各声学特征对感知的影响**。
  - **跨环境与重放方式的比较**：分析ALOD简化在不同声学环境（如耦合空间、大混响空间）及不同重放方式下的感知后果，明确**哪些简化可被容忍而不损害感知有效性**。

3)  
- **任务与效果**：
  - **言语可懂度**：在多数环境下保持稳定，即使使用高度简化的鞋盒镜像源模型（ISM），只要早期反射和扩散混响得到基本保留。仅在地铁站中，ISM因反射稀疏导致可懂度轻微下降。
  - **合理性与整体差异**：语音刺激对ALOD简化相对鲁棒；而**宽带瞬态脉冲**对ALOD降低**高度敏感**，简化模拟（如ISM）的合理性与整体差异评分显著较差。
  - **空间音频质量与外部化**：最高ALOD模拟（RAZR）在合理性、空间音频质量和外部化上接近真实录制效果。**扬声器重放显著提升外部化感知**，耳机重放中高ALOD模拟也能达到类似真实录音的外部化水平。
  - **环境依赖性**：在声学复杂的客厅（耦合空间）中，ALOD简化的影响更明显；在混响较强的酒吧和地铁站中，感知容忍度更高。
</div>

</details>

---

## Exploring Definitions of Quality and Diversity in Sonic Measurement Spaces
- **Authors**: Björn Þór Jónsson, Çağrı Erdem, Stefano Fasciani, Kyrre Glette
- **Categories**: cs.SD, cs.NE
- **arXiv**: [https://arxiv.org/abs/2512.02783v1](https://arxiv.org/abs/2512.02783v1)
- **PDF**: [https://arxiv.org/pdf/2512.02783v1](https://arxiv.org/pdf/2512.02783v1)

数字声音合成技术为探索包含数百万种配置的广阔参数空间提供了可能。质量多样性进化算法为利用这一潜力提供了有前景的途径，但其成功与否取决于恰当的声音特征表示方法。现有质量多样性方法主要采用手工设计的描述符或监督分类器，这可能引入非预期的探索偏差，并将发现范围限制在熟悉的声音区域内。本研究探讨了在质量多样性搜索过程中，利用无监督降维方法自动定义并动态重构声音行为空间的技术。我们应用主成分分析和自编码器将高维音频特征映射到结构化网格中供MAP-Elites算法使用，并通过定期重新训练模型实现动态重构。在两种实验场景下的对比表明，自动方法在避免专家引入偏差的同时，实现了比手工设计行为空间显著更高的多样性。动态行为空间重构能保持进化压力并防止停滞现象，其中主成分分析在降维技术中表现最为有效。这些研究成果推动了自动化声音发现系统的发展，使其能够在无需人工干预或监督训练限制的情况下探索广阔的参数空间。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：数字声音合成涉及探索包含数百万配置的庞大参数空间。质量多样性进化算法是有效方法，但其成功依赖于合适的声学特征表示。  
- **既有方法问题**：现有方法主要依赖手工设计的声学描述符或有监督分类器，可能引入非预期的探索偏差，并将发现限制在熟悉的声学区域内。

2)  
- **核心方法**：论文采用无监督降维技术自动定义和动态重构声学行为空间，以解决手工或有监督方法带来的偏差和限制问题。  
- **具体方案**：  
  - 使用主成分分析和自编码器将高维音频特征投影到结构化网格，供MAP-Elites算法使用。  
  - 通过定期重新训练降维模型，实现行为空间的动态重构，以适应进化过程中发现的声音分布变化。  
  - 将质量评估与行为空间定义解耦：质量评估通过参考声音相似性或无参考音频问题检测进行，而多样性则由降维模型定义的行为空间驱动。  
- **优势**：  
  - 避免了专家偏见，允许探索超出预定义分类的声学区域。  
  - 动态重构防止进化停滞，维持探索压力。  
  - PCA在多样性和计算效率间取得最佳平衡，而自编码器在感知质量上表现更好。

3)  
- **任务与效果**：在基于CPPN-DSP网络的声学发现任务中，方法取得了以下效果：  
  - **多样性提升**：无监督方法（特别是PCA）发现的声学多样性显著高于手工定义的行为空间。  
  - **质量相当**：在保持声音质量的同时，实现了更广泛的声学探索。  
  - **动态重构有效**：动态重构行为空间维持了进化压力，避免了早期收敛，促进了持续探索。
</div>

</details>

---

## Towards Language-Independent Face-Voice Association with Multimodal Foundation Models
- **Authors**: Aref Farhadipour, Teodora Vukovic, Volker Dellwo
- **Categories**: eess.AS, cs.SD, eess.IV
- **arXiv**: [https://arxiv.org/abs/2512.02759v1](https://arxiv.org/abs/2512.02759v1)
- **PDF**: [https://arxiv.org/pdf/2512.02759v1](https://arxiv.org/pdf/2512.02759v1)

本文介绍了苏黎世大学计算语言学团队提交至FAME2026挑战赛的系统。该赛事聚焦于独特多语言场景下的跨模态验证任务，特别针对训练阶段未见的语言类型。我们探索了两种架构：其一是基于对比学习与正交投影损失从头训练的双编码器基线系统；其二是采用LoRA技术微调ImageBind多模态基础模型的方案。为应对赛事数据稀缺及语言限制，我们从VoxBlink中构建了阿拉伯语外部数据集。性能最优的ImageBind-LoRA系统展现出卓越的跨语言泛化能力：尽管仅使用阿拉伯语语音进行微调，其在评估集（英语与德语）上仍实现了24.73%的等错误率，最终荣获赛事第二名。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：人机交互发展推动多模态生物识别系统需求，要求系统在单一模态缺失时仍能进行跨模态验证。  
- **既有问题**：  
  - 传统方法依赖单模态，对语言变化敏感，难以处理未见/未听过的语言。  
  - 数据稀缺与语言限制制约模型泛化能力，易导致过拟合。  

2)  
- **核心方法**：提出两种架构，重点是基于ImageBind与LoRA的预训练基础模型方法。  
  - **系统1（CLIP风格双编码器）**：使用ResNet34和IR101-ArcFace分别编码语音和面部特征，通过对比损失和正交投影损失优化，但易受数据量限制影响。  
  - **系统2（ImageBind-LoRA）**：  
    - 利用ImageBind预训练对齐多模态表示，冻结主干网络，仅通过LoRA微调注意力层（参数量降至510万），降低计算成本并避免灾难性遗忘。  
    - 关键创新：从VoxBlink构建外部阿拉伯语数据集进行微调，通过两阶段训练（分类头训练后LoRA微调）和对称对比损失优化。  
  - **策略融合**：在开发阶段使用Z-score归一化融合多个子系统得分，提升鲁棒性。  
- **解决思路**：  
  - 通过大规模预训练基础模型捕获跨模态的通用身份特征，减少对特定语言的依赖。  
  - 使用LoRA实现参数高效微调，结合外部多语言数据增强语言无关性。  

3)  
- **任务**：FAME 2026挑战赛中的跨模态验证任务，涉及英语、德语、乌尔都语等未见/未听过语言的语音-面部关联。  
- **效果**：  
  - ImageBind-LoRA系统在仅用阿拉伯语数据微调后，在评估集（英语和德语）上取得24.73%的等错误率，获得比赛第二名。  
  - 相比CLIP风格基线（EER约48%），性能显著提升，并展现出跨语言一致泛化能力。
</div>

</details>

---

## SAND Challenge: Four Approaches for Dysartria Severity Classification
- **Authors**: Gauri Deshpande, Harish Battula, Ashish Panda, Sunil Kumar Kopparapu
- **Categories**: cs.SD, cs.AI, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.02669v1](https://arxiv.org/abs/2512.02669v1)
- **PDF**: [https://arxiv.org/pdf/2512.02669v1](https://arxiv.org/pdf/2512.02669v1)

本文针对神经退行性疾病语音分析（SAND）挑战中的构音障碍严重程度分类问题，系统研究了四种不同的建模方法。所有模型均基于同一套语音录音数据集，执行相同的五分类任务。我们探索了以下方法：（1）ViT-OF方法，利用视觉变换器处理语谱图图像；（2）1D-CNN方法，采用八个一维卷积神经网络并结合多数投票融合；（3）BiLSTM-OF方法，使用九个双向长短期记忆网络并结合多数投票融合；（4）分层XGBoost集成方法，通过两阶段学习框架结合声门特征与共振峰特征。文中详细阐述了每种方法，并在包含53名说话者的验证集上比较了其性能。结果表明，虽然基于特征工程的XGBoost集成方法取得了最高的宏观F1分数（0.86），但深度学习模型（ViT、CNN、BiLSTM）也获得了具有竞争力的F1分数（0.70），并为该问题提供了互补的分析视角。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：研究聚焦于神经退行性疾病（如肌萎缩侧索硬化症）伴随的构音障碍严重程度自动分类。SAND挑战赛要求将患者语音分为五个严重等级（1-4级及健康对照5级）。
- **既有问题**：任务面临两大核心挑战：
  - **数据严重不平衡**：训练集中各类别样本数差异极大（如最严重的1级仅4例，健康5级有86例），易导致模型偏向多数类。
  - **数据量有限且存在性别不平衡**：总训练样本仅219例，且男女比例不均（约1.28:1），增加了建模难度。

2)  
论文提出了四种互补的建模方法，从不同角度应对上述挑战：

- **VIT-AVE方法**：
  - 利用在图像识别上预训练的Vision Transformer（ViT）分析语谱图图像，通过迁移学习缓解数据不足。
  - 针对每位说话人的8段语音，采用**平均损失训练策略**：计算所有语音的损失并平均后再反向传播，使模型学习整体严重程度而非依赖单段语音。
  - 预测时对8段语音的概率输出进行平均（软融合），提升决策鲁棒性。

- **1D-CNN方法**：
  - 为5个元音和3个音节分别训练8个独立的1D-CNN模型，输入为手工设计的**相位相关声学特征**（如相位倒谱系数），以捕捉构音障碍的细微频谱特征。
  - 采用**多数投票融合**：汇总8个模型的预测结果，以多数票决定最终类别，降低单段语音误判的影响。
  - 通过特征选择（保留最具判别力的特征和语音类型）进一步提升性能。

- **BILSTM-OF方法**：
  - 为每种语音类型训练9个BiLSTM模型（8段独立语音+1段拼接所有语音的复合输入），输入为STFT幅度谱，以建模语音的**时序动态特性**。
  - 同样使用**多数投票融合**，但通过筛选（仅保留表现最佳的节奏音节和复合语音模型）优化集成效果。
  - 该方法特别擅长捕捉节奏音节中可能加剧的言语异常（如含糊、节奏不规则）。

- **分层XGBoost集成方法**：
  - **核心创新**：结合领域知识，提取**声门脉冲特征**（通过SEDREAMS算法）和**共振峰频率**等12个声学特征，并加入年龄、性别等人口统计学信息。
  - **两阶段分层架构**：
    - 第一阶段：训练8个专门的XGBoost二元分类器，针对特定子问题（如按性别、年龄区分相邻严重等级）。
    - 第二阶段：将第一阶段的8个预测结果与编码后的人口统计特征一起输入决策树，进行最终五分类。
  - 该方法通过**任务分解**有效处理了数据异质性（性别、年龄影响）和类别不平衡问题。

3)  
所有方法在SAND验证集（53名说话人）上评估：
- **分层XGBoost**表现最佳，宏平均F1分数达**0.86**，准确率约0.89，仅在相邻类别间有轻微混淆。
- **深度学习方法**取得竞争性结果：BILSTM-OF的F1为**0.70**，VIT-AVE为**0.68**，1D-CNN为**0.64**。
- 结果表明，在数据有限场景下，**结合领域知识的特征工程方法**（XGBoost）更具优势；而**数据驱动的深度学习方法**通过预训练、数据增强和集成策略也能达到良好性能，并为问题提供了互补视角。
</div>

</details>

---

## Pianist Transformer: Towards Expressive Piano Performance Rendering via Scalable Self-Supervised Pre-Training
- **Authors**: Hong-Jie You, Jie-Jing Shao, Xiao-Wen Yang, Lin-Han Jia, Lan-Zhe Guo, Yu-Feng Li
- **Categories**: cs.SD, cs.AI, cs.MM
- **arXiv**: [https://arxiv.org/abs/2512.02652v1](https://arxiv.org/abs/2512.02652v1)
- **PDF**: [https://arxiv.org/pdf/2512.02652v1](https://arxiv.org/pdf/2512.02652v1)

现有方法依赖小规模标注数据集进行有监督学习来实现富有表现力的音乐演奏渲染，这限制了数据和模型规模的扩展，尽管存在大量未标注音乐数据，正如视觉和语言领域的情况。为弥补这一差距，我们提出 Pianist Transformer，其核心贡献包括：1）统一的 MIDI 数据表示方法，无需显式标注即可学习音乐结构与表达的共享原理；2）高效的非对称架构，在不牺牲渲染质量的前提下支持更长上下文与更快推理；3）基于 10B 标记和 1.35 亿参数的自监督预训练流程，释放了数据与模型规模对表现力演奏渲染的潜力；4）达到先进水平的演奏模型，在客观指标和主观听感评分上均接近人类水准。总体而言，Pianist Transformer 为音乐领域实现类人演奏合成开辟了可扩展的技术路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现有方法依赖监督学习，需要大量对齐的乐谱-演奏数据，这类数据稀缺且标注成本高，限制了模型与数据的规模扩展。
- **既有问题**：
  - **数据瓶颈**：监督范式无法利用海量未标注的演奏MIDI数据（>10万小时）。
  - **表示限制**：现有方法依赖乐谱的结构化特征（如小节、节拍），这些特征在未对齐的演奏MIDI中无法提取，阻碍了无监督数据的利用。
  - **扩展性差**：对抗训练等替代方案训练不稳定，生成质量有限，缺乏稳定、可扩展的范式。

2)  
论文通过**Pianist Transformer**框架，引入可扩展的自监督预训练范式来解决上述问题，其核心方法包括：

- **统一MIDI表示**：
  - 设计了一种细粒度的、事件驱动的统一MIDI标记化方案，将乐谱和演奏编码在相同的离散词汇表中。
  - 每个音符用8个标记表示音高、力度、时长和起奏间隔，并包含额外的踏板控制标记。
  - **关键作用**：该表示不依赖高级音乐概念（如小节），使得海量未对齐的演奏MIDI数据可以直接用于预训练，打破了数据利用的壁垒。

- **高效的非对称架构**：
  - 采用编码器-解码器Transformer，并引入**编码器序列压缩**：利用音符的固定8标记结构，将其聚合为一个向量，将编码器序列长度压缩8倍，自注意力计算成本降低64倍。
  - 采用**非对称层分配**：使用深编码器（10层）和浅解码器（2层）。这种设计将大部分计算集中在高度并行的编码过程，而浅层解码器在生成时延迟低、内存占用小。
  - **综合效果**：该架构实现了更长的上下文处理、更快的推理速度（比对称架构快2.1倍），且不牺牲渲染质量，满足了实际应用的低延迟需求。

- **可扩展的训练流程**：
  - **自监督预训练**：在包含100亿标记的大规模未标注MIDI语料库上，使用掩码去噪目标进行预训练。模型通过重建被破坏的音乐片段，内化音乐结构（如和声、旋律走向）等先验知识。
  - **监督微调**：在少量对齐的乐谱-演奏数据上，以序列到序列的方式微调模型，学习将乐谱结构映射为具体的演奏表达参数（如时序、力度）。
  - **后处理**：提出**表达性速度映射**算法，将模型输出的绝对时间转换为可编辑的速度曲线，使生成的MIDI文件能与数字音频工作站兼容，便于实际音乐制作。

3)  
- **任务**：在**表达性钢琴演奏渲染**任务上进行了评估，即从符号乐谱生成具有人类表现力的钢琴演奏。
- **效果**：
  - **客观指标领先**：在ASAP测试集上，在速度、时长、起奏间隔、踏板等关键表达维度的Jensen-Shannon散度和交集面积指标上，全面优于VirtuosoNet、ScorePerformer等基线模型，最接近人类演奏的分布。
  - **主观评价达人类水平**：
    - 在包含39名参与者的听感研究中，其生成演奏的“人类相似度”评分与人类钢琴家的演奏在统计上无显著差异。
    - 在整体偏好排名中，其“首选率”（32.7%）甚至略高于人类演奏（30.8%），显著优于所有基线模型。
  - **风格鲁棒性强**：在巴洛克、古典、浪漫等不同风格的音乐上均能保持高质量和稳定的表现力，而基线模型在古典和巴洛克音乐上性能显著下降。
</div>

</details>

---

## Hear What Matters! Text-conditioned Selective Video-to-Audio Generation
- **Authors**: Junwon Lee, Juhan Nam, Jiyoung Lee
- **Categories**: cs.CV, cs.LG, cs.MM, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.02650v1](https://arxiv.org/abs/2512.02650v1)
- **PDF**: [https://arxiv.org/pdf/2512.02650v1](https://arxiv.org/pdf/2512.02650v1)

本文提出了一项新任务：文本条件选择性视频到音频生成，其目标是从包含多个对象的视频中仅生成用户指定的声音。该能力在多媒体制作中尤为重要，因为通常需要对每个声源的音频轨道进行独立处理，以实现精确的编辑、混音和创作控制。然而，现有方法通常一次性生成混合了多个声源的声音，这主要是由于视觉特征相互纠缠，且区域提示或文本提示往往难以准确指定目标声源。为此，我们提出了SelVA模型，这是一种新颖的文本条件视频到音频生成模型，它将文本提示作为目标声源的显式选择器，并通过调制视频编码器来清晰提取与提示相关的视频特征。所提出的补充标记通过高效的参数调优抑制与文本无关的激活，从而增强跨注意力机制，实现鲁棒的语义与时序对齐。此外，SelVA采用自增强策略以克服单声道音频轨道监督数据的不足。我们在VGG-MONOAUDIO基准上对SelVA进行评估，该基准专门为此任务构建，包含干净的单一声源视频数据。大量实验与消融研究一致验证了SelVA在音频质量、语义对齐和时序同步方面的有效性。代码与演示可在 https://jnwnlee.github.io/selva-demo/ 获取。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在多媒体制作中，音频通常需要针对单个声源进行独立编辑和混音，以实现精确的创意控制。  
- **既有方法的问题**：现有的视频到音频（V2A）生成模型一次性生成混合所有声源的单一音轨，无法根据用户意图选择性生成特定声源的声音。这主要是因为：  
  - 视觉特征通常是纠缠的，包含不相关的信息。  
  - 现有方法依赖区域提示（如分割掩码）来指定声源，但这类方法计算开销大，且难以处理非物体声源（如雨声）或视觉遮挡的情况。

2)  
论文提出的核心方法 SELVA 通过以下机制解决上述问题：  
- **文本引导的视频编码器**：  
  - 将文本提示作为明确的声源选择器，通过一个轻量级的交叉注意力块来调制视频编码器，使其提取与文本相关的视频特征。  
  - 引入可学习的补充令牌（[SUP]），将其预置到文本嵌入中。这些令牌在交叉注意力中抑制与文本无关的视觉激活，从而增强对目标声源相关区域的关注，实现更鲁棒的语义和时间对齐。  
- **选择性声音生成器**：  
  - 采用基于多模态扩散变换器（MM-DiT）的生成器，以文本调制后的视频特征和文本嵌入为条件，生成目标声源的音频。  
  - 生成器仅对处理视频特征的特定模块（如初始投影层和自适应层归一化模块）进行微调，保持大部分参数冻结，实现参数高效的学习。  
- **自增强训练策略**：  
  - 提出一种两阶段训练方案，无需显式的单声道音频监督。  
  - **第一阶段**：通过教师-学生蒸馏，训练文本引导的视频编码器。学生模型从混合视频（目标视频与干扰视频拼接）中，根据目标文本提取特征，并回归教师模型从干净目标视频提取的特征。  
  - **第二阶段**：冻结训练好的视频编码器，仅微调生成器中与视频特征交互的模块，使用条件流匹配目标进行训练。  
- **整体优势**：该方法避免了依赖计算昂贵的分割模型，能处理非物体声源，并通过文本实现了灵活、直观的声源控制。

3)  
- **任务**：在文本条件下的选择性视频到音频生成任务上进行了评估。  
- **效果**：  
  - 在专门构建的基准测试集 VGG-MONOAUDIO 上，SELVA 在音频质量、语义对齐（与文本/视频的匹配度）和时间同步性方面均优于现有最先进方法。  
  - 定量指标（如FAD、CLAP、DeSync）和人类主观评估均显示其性能领先。  
  - 模型能够成功从包含多个同时或交织声源的复杂视频中，选择性地生成与指定文本相符且与视频时间同步的高质量音频。
</div>

</details>

---

## Spoken Conversational Agents with Large Language Models
- **Authors**: Chao-Han Huck Yang, Andreas Stolcke, Larry Heck
- **Categories**: cs.CL, cs.MA, cs.NE, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.02593v1](https://arxiv.org/abs/2512.02593v1)
- **PDF**: [https://arxiv.org/pdf/2512.02593v1](https://arxiv.org/pdf/2512.02593v1)

口语对话系统正朝着原生语音大语言模型的方向演进。本教程系统梳理了从级联式自动语音识别/自然语言理解架构到端到端、检索增强与视觉融合系统的发展路径。重点阐述文本大语言模型在语音领域的适配方法、跨模态对齐技术及语音-文本联合训练策略；综述相关数据集、评估指标，以及针对多口音场景的鲁棒性研究，并对级联与端到端架构、语音识别后修正、流式处理等关键技术方案进行比较分析。教程进一步将工业级语音助手与当前开放域及任务导向型系统相联系，突出可复现的基线模型，并围绕隐私保护、安全性与评估体系等开放性问题展开探讨。参会者将掌握实用技术方案，并获得清晰的系统级发展路线图。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于语音接口的大语言模型（如GPT-4o、Gemini）在语音识别、翻译、口语理解等经典任务上表现出色，超越了以往开源基准。  
- **既有问题**：  
  - 缺乏对语音模态如何融入LLM以实现真正多模态理解的系统设计与机制研究。  
  - 语音模型与LLM的交互，特别是在分层、自博弈认知智能体中的结合，尚处于初步探索阶段。  
  - 现有研究对多样性（如口音、方言、社会方言）和公平性问题的处理仍不充分。

2)  
- **核心方法概述**：本教程通过系统性综述与理论结合实践的方式，提出并探讨了构建语音对话智能体的多层次方法框架，旨在解决语音-语言多模态融合的挑战。  
- **具体解决路径**：  
  - **跨模态对齐理论**：引入基于人口风险度量与模型可迁移性估计的理论基础，指导非文本模态（语音）与文本表征的对齐，为语音增强的LLM设计提供理论支撑。  
  - **联合语音-文本预训练与后对齐**：探讨端到端的多模态语音-语言模型，采用生成式自回归方法与联合语音-文本分词技术，实现语音与文本目标的统一学习，提升多模态理解能力。  
  - **分层智能体架构**：探索语音模型与LLM在分层、自博弈智能体中的交互机制，通过对话自博弈、情境化对话系统等技术，增强对话系统的覆盖范围、鲁棒性与多轮交互能力。  
  - **多样性适应与公平性处理**：强调通过情境学习适应、检索增强聚类、风格迁移等技术，使模型能更好地处理口音、方言及社会方言变异，并推动建立面向多样性的评估指标，以降低模型偏见。

3)  
- **任务与效果**：  
  - 在**语音识别**、**机器翻译**、**口语理解**等任务上，基于语音接口的LLM显著超越了传统开源基准性能。  
  - 通过端到端多模态模型与联合预训练，在**语音生成翻译**、**多轮对话**（开放域与任务导向）中实现了更无缝、高质量的输出。  
  - 在**多样性适应**方面，针对口音、方言的模型鲁棒性得到提升，并通过公平性评估框架推动了对不同说话者群体的性能均衡。
</div>

</details>

---

## Generative Multi-modal Feedback for Singing Voice Synthesis Evaluation
- **Authors**: Xueyan Li, Yuxin Wang, Mengjie Jiang, Qingzi Zhu, Jiang Zhang, Zoey Kim, Yazhe Niu
- **Categories**: cs.SD, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.02523v1](https://arxiv.org/abs/2512.02523v1)
- **PDF**: [https://arxiv.org/pdf/2512.02523v1](https://arxiv.org/pdf/2512.02523v1)

歌唱语音合成技术已取得显著进展，能够生成音高准确、风格统一的歌声。随着模型能力的提升，对可靠评估与优化方法的需求日益迫切。然而，当前基于奖励机制的方法通常依赖单一数值评分，难以捕捉乐句处理、表现力等多维度特征，且依赖高成本标注，限制了方法的可解释性与泛化能力。为此，我们提出一种生成式反馈（即奖励模型）框架，为歌唱语音合成评估提供多维度的语言与音频反馈。该框架利用音频-语言模型生成涵盖旋律、内容、听觉质量等维度的文本与音频评述。模型通过在融合人类音乐反应数据与多模态大语言模型合成评述的混合数据集上进行微调，提升了数据多样性与语言丰富性。定量实验验证了所提数据集与训练策略的有效性，表明该框架能够生成音乐准确性高、可解释性强的评估结果，适用于指导生成模型的优化。代码已开源：[https://github.com/opendilab/VocalCritic](https://github.com/opendilab/VocalCritic)

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：歌唱语音合成（SVS）技术已能生成音高准确、风格一致的歌声，但对其输出进行可靠评估和优化仍具挑战。
- **既有方法问题**：
  - 现有奖励模型通常输出单一数值分数，无法分解评估维度，缺乏可解释性。
  - 难以量化歌唱中如乐句连贯性等主观、复杂的表现维度。
  - 依赖大规模、高质量的人工标注数据，获取成本高且易引入噪声。

2)  
- **核心方法**：提出一个生成式多模态反馈（奖励模型）框架，为SVS评估提供多维度的语言和音频评论。
- **解决方案**：
  - **多维度输出**：模型接收歌声音频片段和包含歌曲背景、评论者风格的文本提示，通过音频-语言模型生成覆盖旋律、内容、听觉质量等多方面的文本和音频评论，突破了单一数值评分的局限。
  - **混合数据训练**：构建了双源数据集进行监督微调（SFT）。
    - **人类反应数据**：从B站反应视频中提取，通过ASR和语音分离技术，获得“音乐-反应文本-反应语音”三元组，提供了真实、多样且富含领域知识的反馈。
    - **MLLM生成数据**：使用多模态大语言模型为不同流派歌曲生成系统化、标准化的评论，确保了评论风格的覆盖度和一致性。
  - **联合训练目标**：模型基于共享的LLM骨干网络，并行训练文本生成头和音频生成头。总损失函数是文本交叉熵损失和音频交叉熵损失的加权和，迫使共享编码器学习能同时支持生成正确语义内容和恰当韵律表达的表示，实现了模态间的对齐。
  - **提升可解释性与泛化性**：生成的自然语言和音频反馈本身具有高可解释性。混合数据策略结合了人类数据的知识深度与MLLM数据的系统性，共同提升了模型的鲁棒性和知识容量上限。

3)  
- **评估任务与效果**：论文在提出的基于LLM的评测基准上进行了实验，该基准包含单选题（SCQ）和开放式问答题（OEQ），并从音乐准确性、完整性、事实性、新颖性等维度衡量评论质量。
- **主要结果**：
  - **SCQ任务**：使用纯人类数据微调的模型取得了0.60的最高准确率，超过了GPT-4o-Audio等基线模型。使用混合数据微调的模型取得了0.65的最佳准确率。
  - **OEQ任务**：纯MLLM数据训练保持了均衡的OEQ性能。混合数据设置取得了最佳权衡，在获得最高SCQ准确率的同时，保持了相对稳定的OEQ性能。
  - **定性分析**：模型成功学会了根据音乐输入生成文本和音频反馈，最终模型表现出带有情感语调甚至哼唱等类人行为，表明联合监督有效引导模型实现了更具体化的音乐理解。
</div>

</details>

---

## VibOmni: Towards Scalable Bone-conduction Speech Enhancement on Earables
- **Authors**: Lixing He, Yunqi Guo, Haozheng Hou, Zhenyu Yan
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.02515v1](https://arxiv.org/abs/2512.02515v1)
- **PDF**: [https://arxiv.org/pdf/2512.02515v1](https://arxiv.org/pdf/2512.02515v1)

真无线立体声耳机及VR/AR头显等耳戴设备日益普及，但其紧凑设计为嘈杂环境下的通话、语音助手交互等语音相关应用带来了挑战。现有仅依赖全向麦克风的语音增强系统难以有效抑制环境噪声（如多人说话声）。为此，我们提出VibOmni——一种轻量级端到端多模态语音增强系统，利用耳戴设备中广泛搭载的惯性测量单元采集骨传导振动信号。该系统通过双分支编码器-解码器深度神经网络融合音频与振动特征。针对配对音频-振动数据稀缺的问题，我们提出一种新型数据增强技术，基于有限录音数据建模骨传导函数，仅需4.5%的频谱图相似度误差即可生成合成振动数据。此外，多模态信噪比估计器支持持续学习与自适应推理，无需设备端反向传播即可优化动态嘈杂环境下的性能。在32位志愿者使用不同设备的真实数据集测试中，VibOmni将语音质量感知评估指标提升达21%，信噪比提升26%，移动设备端词错误率降低约40%且延迟显著减少。35名参与者的用户研究表明，87%的用户更倾向于选择VibOmni而非基线系统，验证了其在多样化声学环境中的部署有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
研究背景与既有方法的问题：
- **背景**：耳戴式设备（如TWS耳机、VR/AR头显）在嘈杂环境中进行语音通信或语音助手交互时，语音质量常因环境噪声而下降。
- **既有方法问题**：
  - 仅依赖全向麦克风的语音增强系统难以处理竞争说话人等环境噪声。
  - 基于麦克风阵列的波束成形方法因麦克风间距过近而性能受限。
  - 纯音频的深度学习方法在领域变化时性能不稳定，而引入其他模态（如摄像头、雷达）则需额外硬件，不适用于现有耳戴设备。

2)  
论文核心方法如何解决上述问题：
VibOmni 提出一种轻量级、端到端的**多模态语音增强系统**，核心创新包括：

- **多模态数据融合网络**：
  - 设计双分支编码器-解码器深度神经网络，分别处理音频和骨传导振动信号。
  - 通过投影层对齐两种模态因采样率差异导致的频域维度，并采用DPRNN模块在特征层面分离语音。
  - 引入辅助解码器重建低频语音，避免模型因音频信息更丰富而忽略振动输入，通过双损失函数平衡多模态学习。

- **数据增强解决配对数据稀缺**：
  - 提出**骨传导函数（BCF）建模**，从有限录音中估计BCF（建模为高斯分布）。
  - 将BCF应用于大规模公共音频数据集（如LibriSpeech），生成合成振动数据，仅需4.5%的谱图相似度误差，大幅降低数据收集开销。

- **自适应训练与推理应对动态环境**：
  - 设计**多模态信噪比估计器**，结合音频和振动更准确估计SNR。
  - 基于SNR估计实现**持续自监督学习**，仅使用野外采集的带噪数据进行模型优化，无需干净语音标签。
  - 采用**自适应推理机制**，根据噪声水平动态调整模型深度（如分离器模块数），在低噪声时减少计算，平衡性能与效率。

3)  
在哪些任务上取得了怎样的效果：
- **任务**：在嘈杂环境下的语音增强任务，包括合成噪声和真实场景噪声（如竞争说话人、环境噪声、音乐）。
- **效果**：
  - 客观指标：在32名志愿者的真实数据集上，相比基线方法，**PESQ提升最高达21%，SNR提升26%，WER降低约40%**。
  - 计算效率：在移动设备上延迟降低**31倍**，支持实时处理。
  - 用户主观评价：35名参与者的用户研究中，**87%的用户偏好VibOmni**，认为其语音质量更优、可懂度更高。
</div>

</details>

---

## Continual Learning for Singing Voice Separation with Human in the Loop Adaptation
- **Authors**: Ankur Gupta, Anshul Rai, Archit Bansal, Vipul Arora
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.02432v1](https://arxiv.org/abs/2512.02432v1)
- **PDF**: [https://arxiv.org/pdf/2512.02432v1](https://arxiv.org/pdf/2512.02432v1)

近年来，基于深度学习的歌声分离研究取得了显著进展。然而，现有方法大多未考虑用户与模型的交互优化，这在实际应用中至关重要，因为真实场景中的音乐曲目在风格和乐器使用上往往与原始训练数据存在差异。本文提出一种基于深度学习的交互式持续学习框架，用于歌声分离，允许用户针对新目标歌曲对分离模型进行微调。该框架采用基于U-Net的基础模型架构，通过生成掩码从频谱图中分离人声，随后引入人机交互环节：用户通过标记若干误报区域（即提取人声中本应为静音的部分）提供反馈。我们提出了两种持续学习算法。实验证明，在数据集内和跨数据集场景下，所提算法相比基础模型能有效提升歌声分离性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于深度学习的歌声分离模型性能优异，但通常无法让用户交互式地调整模型以适应特定歌曲或新流派/乐器，限制了其在训练数据分布外的真实场景中的应用。
- **既有方法问题**：
  - 现有方法主要关注提升基准指标，缺乏让模型持续适应新数据（如新乐器或流派）的机制。
  - 少数允许用户交互的方法（如基于频谱图编辑或基频反馈）需要用户具备音乐专业知识且耗时，且容易导致模型对特定歌曲过拟合，难以持续学习。

2)  
论文提出了一种结合**人在回路**的**持续学习框架**，使用户能以简单反馈微调预训练的歌声分离模型，并避免过拟合与灾难性遗忘。核心方法如下：

- **基础模型与反馈机制**：
  - 采用U-Net架构作为基础分离模型，输入为混合音频的频谱图，输出用于分离人声的掩码。
  - 设计了一个简单的人机交互任务：用户只需在模型分离出的人声轨迹中，标记出本应为静音（即模型误判为人声）的片段（时间戳）。该任务无需用户具备专业音乐知识。

- **两种持续学习微调算法**：
  1. **基于零人声目标的适应**：
     - 将用户标记的片段（应静音区域）的目标掩码设为零，与原始训练数据（作为“回放”样本）混合进行微调。
     - 使用回放式持续学习策略，在微调时同时使用新标注数据和存储的部分原始训练数据样本，以防止模型遗忘旧知识并缓解过拟合。
  2. **基于合成轨道的适应**：
     - 为提供更丰富的学习信号，创建**合成音乐轨道**：将原始训练数据中歌曲的“真实人声”与用户标记片段（来自新歌曲）重复拼接形成的“人工伴奏”叠加。
     - 模型从这些合成轨道中提取数据点进行学习，同样与回放样本结合微调。此方法提供了关于人声和噪声的联合信息，学习更有效。

- **关键设计优势**：
  - **简易性**：用户反馈任务极其简单（标记静音区域），降低了交互门槛。
  - **持续学习能力**：通过回放样本和精心设计的微调目标，模型在适应新歌曲时能保持泛化性，避免对少数样本过拟合或遗忘原有分离能力。
  - **架构无关性**：所提框架不依赖于U-Net，可应用于其他深度学习分离模型。

3)  
- **任务与效果**：
  - 在**数据集内**（MUSDB-18）和**跨数据集**（从MUSDB-18适应到CC-Mixter）两种设置下评估。
  - 所提方法（尤其是合成轨道适应法）在**人声分离信号失真比**上取得显著提升：
    - 在MUSDB-18上，微调后模型在HITL集和测试集上的平均SDR均优于基础模型。
    - 在跨数据集适应中，模型在CC-Mixter的HITL集和测试集上的平均SDR也大幅提高，且中位数SDR保持稳定，表明模型有效适应新数据分布的同时未破坏原有性能。
  - 验证了方法的**持续学习特性**：在多轮迭代适应中，模型性能持续提升且未出现明显过拟合或灾难性遗忘。
</div>

</details>

---
