---
layout: post
title: "arXiv Daily – 2026-02-17"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-17（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-16 08:50 — 2026-02-17 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## SA-SSL-MOS: Self-supervised Learning MOS Prediction with Spectral Augmentation for Generalized Multi-Rate Speech Assessment
- **Authors**: Fengyuan Cao, Xinyu Liang, Fredrik Cumlin, Victor Ungureanu, Chandan K. A. Reddy, Christian Schuldt, Saikat Chatterjee
- **Categories**: eess.AS, cs.LG
- **arXiv**: [https://arxiv.org/abs/2602.14785v1](https://arxiv.org/abs/2602.14785v1)
- **PDF**: [https://arxiv.org/pdf/2602.14785v1](https://arxiv.org/pdf/2602.14785v1)

设计一个能够评估多采样率（16-48 kHz）语音平均意见得分（MOS）的语音质量评估系统是一项具有挑战性的任务。这一挑战主要源于缺乏包含多采样率语音样本且带有MOS标注的训练数据集。尽管自监督学习模型在语音质量评估中已被广泛采用以提升性能，但其关键局限在于这些模型通常在16 kHz语音上进行预训练，从而忽略了更高采样率（如48 kHz）所包含的高频信息。为解决这一问题，我们提出了一种频谱图增强的自监督学习方法，通过并行分支架构引入高频特征（最高支持48 kHz采样率）。此外，我们进一步设计了一种两阶段训练策略：模型首先在大型48 kHz数据集上进行预训练，随后在较小的多采样率数据集上进行微调。实验结果表明，利用自监督学习特征所忽略的高频信息对于准确的多采样率语音质量评估至关重要，且所提出的两阶段训练方法在有限的多采样率数据条件下显著提升了模型的泛化能力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：非侵入式语音质量评估（SQA）常依赖自监督学习（SSL）模型提取特征，但现有SSL模型（如Wav2Vec2）通常在16 kHz语音上预训练。  
- **既有问题**：  
  - 处理高采样率（如48 kHz）语音时，需下采样至16 kHz，导致高频信息丢失，影响感知质量评估。  
  - 多速率MOS标注数据集稀缺，且不同数据集的MOS评分分布因“范围均衡偏差”存在差异，难以直接合并训练。  
  - 仅用有限多速率数据（如AudioMOS）训练的模型泛化能力不足。

2)  
- **核心方法**：提出**SA-SSL-MOS**，通过**双分支架构**融合SSL特征与频谱图特征，以保留高频信息。  
  - **SSL分支**：输入下采样至16 kHz，通过预训练SSL模型（如Wav2Vec2）提取特征。  
  - **频谱图分支**：输入上采样至48 kHz，转换为频谱图后经卷积模块处理，捕获高频成分。  
  - 两分支特征拼接后，通过映射模块预测MOS均值与方差，使用高斯负对数似然损失优化。  

- **训练策略**：采用**两阶段训练**解决数据稀缺与泛化问题：  
  - **预训练阶段**：在大型单速率数据集（如48 kHz的NISQA）上训练，使模型学习丰富声学表示。  
  - **微调阶段**：在少量多速率数据（如AudioMOS）上轻量微调，避免过拟合并适应多速率分布。  
- **优势**：  
  - 频谱图分支弥补了SSL模型的高频信息缺失，提升对高保真语音的评估准确性。  
  - 两阶段训练利用大规模数据提升泛化能力，缓解多速率数据不足与分布偏差问题。

3)  
- **评估任务与效果**：  
  - **多速率SQA**：在AudioMOS测试集上，SA-SSL-MOS结合两阶段训练取得最佳性能（如UTT LCC达0.848）。  
  - **泛化能力**：在6个外部数据集（如NISQA子集、TCD-VoIP）上评估，两阶段训练的SA-SSL-MOS在多数任务中优于基线，显著提升跨语言、采样率的泛化性（如NISQA TEST P501的SRCC达0.926）。  
  - **局限性**：在中文数据集（Tencent）上因语言不匹配略逊于基线，但整体验证了高频信息与两阶段训练的有效性。
</div>

</details>

---

## Disentangling Pitch and Creak for Speaker Identity Preservation in Speech Synthesis
- **Authors**: Frederik Rautenberg, Jana Wiechmann, Petra Wagner, Reinhold Haeb-Umbach
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.14686v1](https://arxiv.org/abs/2602.14686v1)
- **PDF**: [https://arxiv.org/pdf/2602.14686v1](https://arxiv.org/pdf/2602.14686v1)

本文提出一种能够在保持说话人感知身份的同时，精准调整语音中吱嘎声感知质量的系统。尽管高吱嘎声概率通常与低基频相关，但需注意这一规律仅在说话人群体的统计层面成立，并不适用于所有具体场景。为实现基频与吱嘎声的解耦，我们在语音合成系统的训练数据集中增加了基于条件连续归一化流的说话人特征调控模块。实验表明，该系统在多种吱嘎声调控强度下均能显著提升说话人验证性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音合成中，常需修改感知语音质量（如嘎裂声），同时保持说话人身份。现有方法基于条件连续归一化流（CCNF）学习语音质量与说话人表征的联合分布。
- **既有问题**：训练数据（如LibriTTS-R）中，嘎裂声概率与基频（音高）存在显著的**说话人间**负相关（即高嘎裂声常伴随低音高）。这导致修改嘎裂声时，音高随之改变，从而损害说话人身份保持。

2)  
论文通过**数据集自适应**来解耦音高与嘎裂声，训练改进的CCNF模型以实现更纯净的“说话人内”嘎裂声操控。具体步骤包括：
- **数据增强**：对LibriTTS-R数据集进行音高移位，将每个话语的音高轮廓调整至其性别平均音高，并添加随机扰动。这大幅削弱了训练数据中音高与嘎裂声的语料库内相关性。
- **模型训练**：在增强后的数据集上训练CCNF操纵模块。该模块以说话人嵌入和包含嘎裂声概率等属性的条件向量为输入，通过归一化流学习条件分布，从而生成符合目标嘎裂声强度但音高保持中性的新说话人嵌入。
- **解耦效果**：增强数据使模型能够区分“说话人间”与“说话人内”的关联。在合成时，系统能独立控制嘎裂声强度，而避免不必要的音高偏移，从而更好地保持原始说话人身份。

3)  
- **任务**：在语音合成中进行嘎裂声强度操控，并评估说话人身份保持效果。
- **效果**：在说话人验证任务上，使用增强数据训练的模型（adapted-flow与combined-flow）在广泛的嘎裂声操控强度范围内，其等错误率（EER）显著低于基线模型（base-flow）。这表明身份保持能力大幅提升。同时，合成语音的声学特征（如H1-H2、谐噪比）仍与嘎裂声强度正常相关，证明操控有效性得以保留。
</div>

</details>

---

## Data Augmentation for Pathological Speech Enhancement
- **Authors**: Mingchi Hou, Enno Hermann, Ina Kodrasi
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.14671v1](https://arxiv.org/abs/2602.14671v1)
- **PDF**: [https://arxiv.org/pdf/2602.14671v1](https://arxiv.org/pdf/2602.14671v1)

当前先进的语音增强模型在面对病理语音时，由于声学特征异常及数据稀缺，性能显著下降。本文系统研究了数据增强策略在提升病理说话人语音增强性能方面的作用，并对预测式和生成式语音增强模型进行了评估。我们考察了三种数据增强方法——变换增强、生成增强和噪声增强，并通过客观语音增强指标评估其效果。实验结果表明，噪声增强能够持续带来最显著且最稳健的性能提升，变换增强可带来中等程度的改进，而生成增强效果有限，且随着合成数据量的增加可能损害模型性能。此外，我们发现数据增强的效果因语音增强模型而异，对预测式语音增强模型的增益更为明显。尽管实验证明数据增强能够改善病理语音的增强效果，但神经典型语音与病理语音之间的性能差距依然存在，这凸显了未来需要针对病理语音开发更具针对性的数据增强策略。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前先进的语音增强模型在处理病理性语音时性能显著下降，主要因为病理性语音具有非典型的声学特性，且公开可用的数据集规模小、质量参差不齐。
- **既有方法问题**：现有研究多集中于神经典型语音，对病理性语音关注不足；数据增强在语音增强中的应用探索有限，且已有研究仅针对单一预测模型和神经典型语音，缺乏系统性评估。

2)  
- **核心方法**：本文系统评估了三种数据增强策略对病理性语音增强的效果，使用预测性模型和生成性模型进行实验。
    - **变换增强**：包括音高偏移、时间拉伸和SpecMix，通过修改波形或时频表示增加数据多样性。
    - **生成增强**：利用零样本文本转语音模型合成新语音，以扩展数据集。
    - **噪声增强**：通过改变信噪比生成更多噪声-干净语音对，不改变原始干净语音。
- **解决思路**：
    - 针对数据稀缺问题，通过增强策略扩大训练数据规模，提升模型对病理性语音的适应能力。
    - 通过控制增强比例，分析不同策略对模型性能的影响，找到最优配置。
    - 实验表明，噪声增强能显著提升模型鲁棒性，变换增强提供中等改进，而生成增强效果有限甚至可能损害性能。

3)  
- **任务与效果**：在西班牙语PC-GITA数据集上，使用预测性和生成性语音增强模型进行实验。
    - **噪声增强**：在预测模型上，所有增强比例均带来显著提升；在生成模型上，适度比例（100%）效果最佳。
    - **变换增强**：SpecMix在预测模型中表现稳定，时间拉伸也有改善；但在生成模型中效果因策略而异。
    - **生成增强**：仅在小比例（25%）时略有改善，比例增加会导致性能下降。
- **总体**：数据增强缩小了病理性与神经典型语音的性能差距，但差距依然存在，表明需要针对病理性语音设计更有效的增强策略。
</div>

</details>

---

## Probing Human Articulatory Constraints in End-to-End TTS with Reverse and Mismatched Speech-Text Directions
- **Authors**: Parth Khadse, Sunil Kumar Kopparapu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.14664v1](https://arxiv.org/abs/2602.14664v1)
- **PDF**: [https://arxiv.org/pdf/2602.14664v1](https://arxiv.org/pdf/2602.14664v1)

端到端（e2e）文本转语音（TTS）系统是一种深度架构，通过学习从精心整理的数据集中建立文本字符串与声学语音模式之间的关联。训练完成的TTS模型应能捕捉语音产生的所有方面，如音素时长、说话人特征及语调等，以确保合成语音的自然度和可懂度。人类语音具有复杂性，涉及发音构型（ACs）之间的平滑过渡。由于解剖结构限制，某些发音构型难以模仿或在彼此间转换。本文通过实验探究人类解剖结构所施加的约束是否对端到端TTS系统的训练产生影响。我们采用两种端到端TTS架构进行实验：自回归模型Tacotron-2与非自回归模型VITS-TTS。研究中构建了三种TTS系统：（a）正向文本、正向语音（传统端到端TTS）；（b）反向文本、反向语音（反向端到端TTS）；（c）反向文本、正向语音（反向文本正向语音端到端TTS）。实验表明，端到端TTS系统完全由数据驱动。值得注意的是，反向端到端TTS系统生成的语音在保真度、感知可懂度及自然度方面均表现更优。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：端到端文本转语音系统旨在学习文本与语音声学模式之间的映射。人类语音的产生受到发音器官解剖结构的约束，导致某些发音配置之间的转换存在困难。  
- **既有方法的问题**：传统端到端TTS系统（如Tacotron-2、VITS）通常使用正向文本和正向语音进行训练，但并未明确探究这些系统是否受益于人类发音的自然约束，或仅仅是数据驱动的映射。

2)  
- **核心方法**：论文通过设计三种不同的训练数据配对方式，实验性地探究了人类发音约束对端到端TTS训练的影响：  
  - **常规配对**：正向文本与正向语音（e2e-TTS）。  
  - **反向配对**：反向文本与反向语音（r-e2e-TTS）。  
  - **错配配对**：反向文本与正向语音（rtfs-e2e-TTS）。  
- **解决思路**：  
  - 通过比较不同配对下的模型性能，检验TTS系统是否依赖于人类发音的自然模式。  
  - 实验使用了两种主流架构：自回归的Tacotron-2和非自回归的VITS-TTS，以确保结论的普适性。  
  - 引入字节对编码进行文本标记化，以探究更高效的文本表示是否影响语音生成质量。  
- **关键发现**：  
  - r-e2e-TTS在客观指标（如词错误率）和主观听感（平均意见得分）上均优于常规e2e-TTS，表明模型并非必须依赖人类发音的自然约束。  
  - rtfs-e2e-TTS在Tacotron-2架构下也能有效学习，进一步证明端到端TTS系统本质上是数据驱动的映射过程。  
  - 使用BPE编码能进一步提升语音识别准确率，说明更丰富的文本表示有助于捕捉语音上下文。

3)  
- **任务与效果**：  
  - **语音合成任务**：在Tacotron-2和VITS-TTS架构上，使用LJSpeech数据集进行实验。  
  - **客观效果**：r-e2e-TTS生成的语音在词错误率和字符错误率上显著低于常规e2e-TTS（例如，WER相对降低约34-38%）。  
  - **主观效果**：通过平均意见得分评估，r-e2e-TTS在自然度和可懂度上均获得更高评分。  
  - **最佳性能**：rtfs-e2e-TTS（Tacotron-2架构）取得了最低的WER（8.31%），表明非常规数据配对也能产生高质量的语音。
</div>

</details>

---

## LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio
- **Authors**: Naveen Vakada, Kartik Hegde, Arvind Krishna Sridhar, Yinyi Guo, Erik Visser
- **Categories**: eess.AS, cs.AI, cs.LG
- **arXiv**: [https://arxiv.org/abs/2602.14612v1](https://arxiv.org/abs/2602.14612v1)
- **PDF**: [https://arxiv.org/pdf/2602.14612v1](https://arxiv.org/pdf/2602.14612v1)

长时音频在工业与消费场景中日益普遍，但人工审阅多小时的录音并不现实，这促使我们需要能够通过自然语言查询、以精确时间定位且幻觉最小化的系统来应对。现有的音频-语言模型虽展现出潜力，但由于上下文长度限制，长音频问答仍具挑战。本文提出LongAudio-RAG（LA-RAG），一种混合框架，其将大语言模型（LLM）的输出基于检索到的带时间戳的声学事件检测结果，而非原始音频。该系统将多小时的音频流转化为结构化事件记录并存储于SQL数据库中，在推理时解析自然语言中的时间指代、识别查询意图、仅检索相关事件，并基于这些受限证据生成答案。为评估性能，我们通过拼接保留时间戳的录音并生成基于模板的检测、计数与摘要类问答对，构建了一个合成的长音频评测基准。最后，我们在混合边缘-云环境中部署了该框架，以验证其实用性：音频定位模型在物联网级硬件上本地运行，而LLM则部署于GPU服务器。该架构实现了边缘侧的低延迟事件提取与云端的高质量语言推理。实验表明，相比原始检索增强生成（RAG）或文本转SQL方法，结构化的事件级检索显著提升了准确性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：工业与消费场景中多小时长音频日益普遍，但人工审听不现实，需要能回答自然语言查询、提供精确时间定位且幻觉最小的系统。
- **既有方法问题**：
  - **上下文长度限制**：现有音频-语言模型因上下文长度和计算限制，无法直接处理多小时原始音频。
  - **时间表达解析困难**：自然语言时间表达（如12/24小时制、相对短语）高度多变，解析错误会损害下游推理。
  - **幻觉问题**：在长日志上进行开放式生成容易产生幻觉，除非响应被明确约束在可验证证据上。
  - **检索质量依赖**：传统检索增强生成（RAG）的性能严重依赖检索质量，对模糊或未充分指定的查询效果不佳。

2) **论文核心方法如何解决上述问题**
论文提出了 **LongAudio-RAG (LA-RAG)**，一个混合框架，通过将大语言模型（LLM）的输出**锚定在检索到的、带时间戳的声学事件检测结果**上，而非原始音频，来解决上述问题。其核心流程如下：

- **结构化事件提取与存储**：
  - 使用一个开集词汇的**音频定位模型（AGM）**，将多小时的音频流转换为结构化的**事件记录**（包含事件名称、起止时间戳、置信度等属性）。
  - 这些记录被存储在**SQL数据库**中，便于高效、精确的查询。

- **查询时处理流程**：
  1.  **查询重述**：利用聊天历史重写用户查询，使其更清晰明确，减少幻觉。
  2.  **时间解析**：通过**规则提取器与LLM回退相结合**的方法，将自然语言时间表达（如“早班后两小时”）解析为具体的起止时间区间。
  3.  **意图分类**：采用**基于关键词和嵌入相似度的混合分类器**，识别查询属于检测、计数、摘要还是异常分析。
  4.  **检索与生成**：
      - 根据解析出的时间区间和意图，从SQL数据库中**检索相关事件**。
      - 对于特定查询，使用**嵌入相似度进行Top-K过滤**，仅保留最相关事件。
      - 最后，将过滤后的、带时间戳的事件证据输入LLM，通过**特定意图的提示模板**生成最终答案，确保回答基于证据、事实准确。

- **系统部署架构**：
  - 采用**边-云混合部署**：轻量级AGM在**边缘设备（如IoT硬件）** 上运行，实现低延迟、保护隐私的音频处理；LLM和嵌入服务在**云端GPU服务器**上运行，提供强大的语言推理能力。这种设计减少了带宽使用，并实现了可扩展性。

3) **在哪些任务上取得了怎样的效果**
论文在构建的合成长音频基准（涵盖家庭IoT和工业IoT场景）上进行了评估，主要任务和效果如下：

- **评估任务**：事件检测（是/否查询）、事件计数、自然语言摘要。
- **主要效果**：
  - **显著优于基线**：LA-RAG在检测、计数和摘要任务上的**整体准确率（76.88% / 68.92%）** 显著优于传统的RAG方法（~49%）和Text-to-SQL方法（~41%）。
  - **高效低延迟**：LA-RAG的**端到端延迟最低**（约0.5秒），远快于基于Audio Flamingo 3的RAG方法（~5.5秒）。
  - **时间解析有效**：结合规则与LLM的时间解析模块，在测试集上达到**77.78%的总体准确率**，对简单案例实现100%准确率。
  - **模型规模分析**：实验表明，中等规模的指令调优或MoE模型（如Phi-3.5-MoE）与高质量事件表示结合，即可取得强劲性能，无需依赖超大规模LLM。
</div>

</details>

---

## CLAP-Based Automatic Word Naming Recognition in Post-Stroke Aphasia
- **Authors**: Yacouba Kaloga, Marina Laganaro, Ina Kodrasi
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.14584v1](https://arxiv.org/abs/2602.14584v1)
- **PDF**: [https://arxiv.org/pdf/2602.14584v1](https://arxiv.org/pdf/2602.14584v1)

传统自动单词命名识别系统因失语症患者言语不流畅及发音错误而难以准确识别，限制了该群体自动化评估的可靠性。本文提出一种基于对比语言-音频预训练（CLAP）的自动单词命名识别方法，通过利用文本-音频对齐技术应对这一挑战。该方法将单词命名识别视为音频-文本匹配问题，将语音信号与文本提示映射至共享嵌入空间，从而即使在困难录音中也能识别目标词汇。在两组法语脑卒中后失语症患者语音数据集上的评估表明，该方法准确率最高可达90%，优于现有的基于分类和自动语音识别的基线方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：脑卒中后失语症患者存在言语不流畅和发音错误，导致传统自动命名识别系统难以准确评估其命名能力。
- **既有方法问题**：
  - **基于ASR的方法**：依赖健康人语音训练的模型，对病理语音识别错误率高；孤立词缺乏上下文，进一步增加识别难度。
  - **基于验证的方法**：需针对每个新词重新训练，扩展性差；对临床录音中的多轮尝试、重叠语音等场景适应性不足。

2)  
- **核心方法**：将单词命名识别重构为**音频-文本对比匹配问题**，基于对比语言-音频预训练（CLAP）框架实现。
- **具体解决路径**：
  - **共享嵌入空间**：使用预训练的wav2vec2作为音频编码器，DistilRoBERTa-base作为文本编码器，将语音信号和文本提示映射到同一向量空间。
  - **对比学习优化**：通过对称对比损失函数，拉近正确发音与其对应文本描述的距离，推远错误发音与无关文本的距离。
  - **文本提示设计**：正确发音对应具体目标词描述（如“单词pomme的正确发音”），错误发音使用通用负标签（如“错误发音”），引导模型关注语义意图而非声学细节。
- **优势体现**：
  - **强鲁棒性**：通过文本锚点聚焦于目标词意图，对发音变异、不完整发音、重叠语音等不敏感。
  - **零样本泛化**：无需针对新词重新训练，可直接处理未见过词汇，适合临床灵活应用。
  - **端到端适配**：在失语症语音数据上微调编码器，提升对病理语音的特征提取能力。

3)  
- **任务**：法语脑卒中后失语症患者的单词命名识别（判断发音正确与否）。
- **效果**：
  - 在第一个数据集（轻度障碍为主）上准确率达**90%**，显著超过分类基线（85%）和ASR基线（86%）。
  - 在第二个数据集（中度障碍为主）上准确率达**85%**，优于基线（分类79%，ASR 80%），且在所有评估指标上均取得最佳性能。
  - 模型对发音错误、不流畅及重叠语音具有较强鲁棒性，展现出临床应用的潜力。
</div>

</details>

---

## Preliminary sonification of ENSO using traditional Javanese gamelan scales
- **Authors**: Sandy H. S. Herho, Rusmawan Suwarman, Nurjanna J. Trilaksono, Iwan P. Anwar, Faiz R. Fajary
- **Categories**: physics.soc-ph, cs.SD, physics.ao-ph
- **arXiv**: [https://arxiv.org/abs/2602.14560v1](https://arxiv.org/abs/2602.14560v1)
- **PDF**: [https://arxiv.org/pdf/2602.14560v1](https://arxiv.org/pdf/2602.14560v1)

声化——将数据映射为非语音音频——为表征复杂动力系统提供了一个尚未充分探索的通道。本研究以厄尔尼诺-南方涛动（ENSO）这一低维气候混沌的典型范例为测试案例，通过复杂系统诊断方法评估文化情境化的声化设计。基于 Niño 3.4 海表温度异常指数（1870–2024）的参数映射声化技术，我们将 ENSO 的变异性编码至两种传统爪哇甘美兰五声音阶体系（佩洛格与斯连德罗），并采用四种作曲策略生成音频，随后在二维声学相空间中将其作为轨迹进行分析。基于递归的诊断、凸包几何分析及耦合分析表明，该声化流程保留了关键动力学特征：交替模式产生最高的轨迹递归率，呼应了 ENSO 的准周期性；分层复调模式探索了最广的相空间区域；两种音阶体系在频谱亮度与能量之间引发了性质不同的耦合机制——佩洛格以反相位为主，而斯连德罗则接近相互独立。相空间轨迹分析为在复杂系统背景下比较不同声化设计提供了严谨的几何框架。感知验证仍有待进行；本研究贡献了用于评估此类映射的动力学系统方法论。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：ENSO（厄尔尼诺-南方涛动）是一个典型的低维气候混沌系统，其复杂动态难以直观表达。传统可视化方法依赖图形素养且仅使用单一感官通道。  
- **既有方法问题**：现有气候数据可听化研究几乎完全基于西方音乐体系（如十二平均律），未探索不同音阶系统对动态结构编码的差异，忽略了音乐传统本身作为复杂系统的潜力，错失了利用不同文化音乐结构映射气候动态的机会。

2)  
- **核心方法**：本研究采用参数映射可听化，将1870–2024年的Niño 3.4海表温度异常指数映射到两种爪哇甘美兰五声音阶（pelog和slendro），并设计了四种作曲策略（分层、交替、旋律、频谱）。  
- **解决方式**：  
  - **动态保留**：通过将温度异常映射为音高（暖异常对应高音），变化率映射为MIDI力度，编码了ENSO的状态与演化速率。  
  - **多尺度分解**：使用3、12、24、36个月的移动平均，捕捉ENSO从季节到年代际的多尺度变率，模拟甘美兰的分层复调结构。  
  - **相空间分析**：从音频中提取感知相关特征（如频谱质心、RMS能量），构建二维声学相空间轨迹，并应用非线性动力学诊断（如凸包面积、路径长度、重现率、亮度-能量耦合相关性）来量化可听化输出的动态特征。  
- **方法论贡献**：提供了一套基于复杂系统科学的几何框架，用于客观比较不同可听化设计在保留源系统动态特性方面的差异，而非依赖主观感知评估。

3)  
- **任务与效果**：在ENSO数据可听化任务上，该方法成功生成了八个声学变体，并通过相空间分析量化了其动态特性：  
  - **音阶差异**：Slendro音阶产生 consistently 更高的频谱质心（亮度）。  
  - **作曲模式差异**：分层模式探索了更广的相空间区域但重现率低；交替模式因轮询策略产生了最高的轨迹重现率，放大了ENSO的准周期性。  
  - **耦合机制**：Pelog音阶导致亮度与能量呈强反相关，而Slendro音阶则呈现弱相关或正相关，体现了音阶结构与映射算法相互作用产生的涌现特性。  
- **总体效果**：证明了基于文化的音阶系统可以作为结构化的动态映射框架，且相空间轨迹分析能系统区分不同可听化设计的动态特征。
</div>

</details>

---
