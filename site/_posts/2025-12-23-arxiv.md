---
layout: post
title: "arXiv Daily – 2025-12-23"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-23（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-22 08:50 — 2025-12-23 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning
- **Authors**: Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu
- **Categories**: cs.SD, cs.CV, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.19687v1](https://arxiv.org/abs/2512.19687v1)
- **PDF**: [https://arxiv.org/pdf/2512.19687v1](https://arxiv.org/pdf/2512.19687v1)

我们提出感知编码器视听模型（PE-AV），这是一个通过规模化对比学习训练而成、用于音频与视频理解的新编码器系列。基于PE架构，PE-AV在多个关键方面做出贡献：将表征能力扩展至音频领域，并原生支持跨音频-视频、音频-文本及视频-文本模态的联合嵌入。PE-AV统一的跨模态嵌入实现了语音检索等新型任务，并在标准音频与视频基准测试中取得了最先进的性能。为实现这一突破，我们构建了强大的视听数据引擎，为约亿量级的音视频对生成高质量文本描述，从而实现了跨模态的大规模一致性监督。我们的音频数据涵盖语音、音乐和通用音效，避免了以往工作中常见的单一领域局限性。通过设计十组对比学习目标，我们证明扩展跨模态与描述类型的配对能增强模态对齐并提升零样本性能。此外，我们通过帧级对比目标对PE-AV进行微调，开发出PE-A-Frame模型，实现了音频帧与文本的细粒度对齐，可支持声音事件检测等任务。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：人类感知依赖视觉、听觉和语言的互补信息。现有研究通过对比学习对齐单模态（如图像-文本或音频-文本），但多模态对齐仍面临挑战。
- **问题**：
  - **数据不平衡**：现有方法（如ImageBind、LanguageBind）依赖单一“锚点”模态（如图像或文本）连接其他模态，导致跨模态数据规模与多样性不匹配，性能受限。
  - **音频-视频领域滞后**：相比视觉-语言学习，音频-视频领域数据代表性不足，性能落后。
  - **领域局限**：先前音频模型多专注于单一领域（如语音或音乐），缺乏通用性。

2) **论文核心方法如何解决上述问题**
论文提出**PEAV**（Perception Encoder Audiovisual），一个通过大规模对比学习训练的音频-视频-文本编码器家族，其核心方法包括：

- **构建强大的多模态数据引擎**：
  - 开发两阶段数据引擎，为约1亿个无标注音频-视频对生成高质量合成字幕。
  - **第一阶段**：利用LLM（Llama 3.1 8B）结合多个弱音频字幕模型（EnCLAP、CoNeTTE）的输出及其置信度分数，以及视频字幕，生成音频、视觉和视听字幕。
  - **第二阶段**：使用基于PLM的模型生成细粒度视频字幕，并训练PLM-AV多模态LLM来改进音频字幕，融入语音相关属性（转录、语言ID、口音）。
  - 合成字幕在多样性和质量上优于弱字幕器，并与真实字幕互补，混合使用效果更佳。

- **扩展对比学习目标**：
  - 将对比目标扩展到多达**十个跨模态对**，涵盖视频、音频及多种文本字幕（音频字幕、视频字幕、视听字幕）之间的对齐。
  - 包括单模态对齐损失（如音频-音频字幕、视频-视频字幕）和融合模态对齐损失（如音频-视频到音频字幕）。
  - 在微调阶段额外加入两个联合嵌入对（如音频+视频字幕到视频），实现更精细的跨模态检索控制。
  - 实验表明，增加对比对数量能持续加强跨模态对齐，提升零样本性能。

- **模型架构与训练**：
  - 采用分离的音频和视觉编码塔，后接联合音频-视觉编码器。
  - 使用PE作为视频帧编码器，DAC-VAE编码原始音频波形，ModernBERT作为文本编码器。
  - 通过轻量级时序Transformer捕获视频动态，并使用时序对齐（最近邻插值）融合音频和视频特征。
  - 在三个规模（0.1-1.1B参数）上训练音频编码器，均显示零样本性能的持续提升。

- **领域覆盖与统一嵌入**：
  - 音频编码器支持**语音、音乐和通用音效**，突破先前模型的单领域限制。
  - 学习统一的跨模态嵌入，原生支持音频-视频、音频-文本和视频-文本的联合编码，实现新颖任务（如语音检索）。

3) **在哪些任务上取得了怎样的效果**
PEAV在广泛的零样本音频、视频、语音和音乐任务上取得了**最先进的性能**：
- **音频任务**：
  - **检索**：在AudioCaps上，文本到音频检索R@1从35.4提升至45.8；在VALOR上达到35.1 R@1。
  - **分类**：在VGGSound上，分类准确率从36.0提升至47.1；在NSynth、GTzan和ESC50上也创下新SOTA。
- **视频任务**：
  - **检索**：在ActivityNet上，文本到视频检索R@1从60.4提升至66.5。
  - **分类**：在Kinetics-400上，分类准确率从76.9提升至78.9，超越参数量大2-4倍的模型。
- **语音任务**：
  - **转录检索**：在VCTK上实现85.6 R@1（基线接近0），首次实现有效的语音到转录检索。
  - **分类**：在Dynamic-SUPERB的语言识别、口音分类等任务上表现优异。
- **联合模态任务**：在音频+文本到视频等联合检索任务上显著优于基线，展示出互补信息整合的优势。
- **下游应用**：通过微调得到的PEA-Frame在开放和封闭词汇的声音事件检测任务上取得顶级结果，尤其在DESED等真实数据集上表现强劲。
</div>

</details>

---

## MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery
- **Authors**: Angelo Ortiz Tandazo, Manel Khentout, Youssef Benchekroun, Thomas Hueber, Emmanuel Dupoux
- **Categories**: cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.19612v1](https://arxiv.org/abs/2512.19612v1)
- **PDF**: [https://arxiv.org/pdf/2512.19612v1](https://arxiv.org/pdf/2512.19612v1)

本文提出MauBERT，作为HuBERT的多语言扩展，通过利用发音特征实现鲁棒的跨语言音素表征学习。我们在55种语言中基于音素-发音特征映射进行监督式预训练，延续了HuBERT的训练框架。所提出的模型通过多语言数据学习预测发音特征或音素，从而获得能够捕捉多语言音学特性的语言无关表征。通过全面的ABX可区分性测试，我们证明MauBERT模型相比当前最先进的多语言自监督学习模型，能产生更具上下文不变性的表征。此外，该模型仅需少量自监督微调（10小时语音数据）即可有效适应未见语言及非正式语音场景。这为在自监督语音模型中注入语言归纳偏置提供了一种有效途径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：从原始音频自动发现未知语言的语音单元，对低资源语言研究和儿童语言习得建模具有重要意义。现有方法主要基于自监督学习（SSL），但存在两个关键问题。
- **既有方法的问题**：
  - **单元对应性差**：SSL发现的声学单元与音素等语言学单元不一一对应，单元更短、数量更多，且缺乏对说话人身份和语音上下文的完全不变性，导致编码比特率较高。
  - **数据需求大**：当前SSL算法需要大量干净语音数据（如960小时至100万小时），这在低资源语言场景中难以获得，也与儿童语言习得的实际数据量不符。

2)  
论文提出**MAUBERT**，一种基于HuBERT的多语言扩展模型，通过引入**发音特征**作为归纳偏置，以解决上述问题。核心方法包括两个阶段：

- **多语言预训练**：
  - **基础架构**：以在英语上预训练的HuBERT-base为编码器，保持卷积特征提取器冻结，微调Transformer编码器。
  - **监督目标**：使用来自55种语言的VoxCommunis语料库及其通过PanPhon工具提取的发音特征（AF）和音素标注。模型被训练来预测这些**三元值发音特征**或**音素**。
  - **模型变体**：提出MAUBERT-FEAT（通过AF瓶颈预测音素）和MAUBERT-PHONE（直接预测音素）两个版本，均采用加权求和、双向LSTM等下游模块进行任务特定预测。
  - **目的**：通过多语言监督微调，使模型学习到**语言无关的语音表示**，捕获跨语言的语音特性，增强上下文不变性。

- **少样本语言适应**：
  - **自监督微调**：对于未见语言，使用少量（如10小时）无标注语音进行自适应。首先对MAUBERT学习到的表示进行聚类（如K-means、基于频率的特征/音素选择）生成伪标签，然后采用掩码语言建模方式，训练模型预测这些聚类标签。
  - **监督微调**：在有标注情况下，直接使用真实音素进行掩码预测训练。
  - **解决思路**：
    - **引入通用语音归纳偏置**：通过多语言发音特征/音素的监督学习，为模型注入强语言学偏置，使其能从有限数据中学习更抽象、不变的语音表示。
    - **实现少样本适应**：结合预训练的通用表示和高效的微调策略，使模型能快速适应新语言或语域（如随意语音），降低对海量数据的依赖。

3)  
- **评估任务**：在**零资源语音挑战2017（ZRC2017）** 的基准上进行评估，包括5种开发语言（斯瓦希里语、泰米尔语、泰语、土耳其语、乌克兰语）和5种测试语言（英语、法语、德语、普通话、沃洛夫语）。主要使用**ABX可区分性测试**来衡量语音表示的语音不变性（包括音素和三音素层面、说话人内/跨说话人、上下文内/任意上下文条件）。
- **取得的效果**：
  - **零样本性能**：MAUBERT模型（尤其是PHONE变体）在ABX任务上优于或媲美大规模多语言SSL基线模型（如XEUS），在任意上下文音素ABX上表现最佳，显示出更强的上下文不变性。
  - **少样本适应**：仅用10小时语音进行**自监督微调**后，ABX错误率显著降低；进行**监督微调**后，性能提升更明显，在测试语言上达到最佳水平（平均ABX错误率约3.39%）。
  - **跨语域鲁棒性**：在**随意语音**（英语和法语）上，经过自监督微调的MAUBERT模型也表现出良好的适应能力。
  - **语音清单发现**：基于MAUBERT-FEAT的特征向量频率分布，能够为未见语言自动发现候选音素清单，在精确率和召回率之间取得平衡，为低资源语言的语音学分析提供了潜在工具。
</div>

</details>

---

## Real-Time Streamable Generative Speech Restoration with Flow Matching
- **Authors**: Simon Welker, Bunlong Lay, Maris Hillemann, Tal Peer, Timo Gerkmann
- **Categories**: eess.SP, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.19442v1](https://arxiv.org/abs/2512.19442v1)
- **PDF**: [https://arxiv.org/pdf/2512.19442v1](https://arxiv.org/pdf/2512.19442v1)

基于扩散的生成模型近年来对语音处理领域产生了重大影响，展现出极高的语音自然度并催生了新的研究方向。然而，由于其计算密集的特性——涉及多次调用大型深度神经网络，这些模型在实时通信中的应用仍显滞后。

本文提出Stream.FM，一种基于流的帧因果生成模型，其算法延迟为32毫秒，总延迟为48毫秒，为实时通信中的生成式语音处理开辟了道路。我们提出了一种缓冲流式推理方案和优化的深度神经网络架构，展示了在固定计算预算下，通过学习型少步数值求解器如何提升输出质量，探索了模型权重压缩以在计算与质量权衡中找到最优平衡点，并贡献了一个总延迟为24毫秒的语音增强任务变体。

我们的研究超越了理论延迟的探讨，证明了高质量的流式生成语音处理可在当前消费级GPU上实现。Stream.FM能够以流式方式处理多种语音任务：语音增强、去混响、编解码器后滤波、带宽扩展、短时傅里叶变换相位恢复以及梅尔声码器合成。通过全面评估和MUSHRA听力测试验证，Stream.FM在生成式流式语音修复任务中达到了先进水平，与非流式变体相比仅出现合理程度的质量下降，同时在更低延迟下超越了我们先前的生成式流式语音增强工作（Diffusion Buffer）。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：生成式模型（如扩散模型）在语音处理中展现出高自然度，但其计算密集性（需多次调用大型DNN）阻碍了在实时通信中的应用。  
- **既有问题**：  
  - 现有方法常缺乏真实硬件上的流式实现验证，或依赖离线处理的实时因子（RTF）评估，低估了流式场景的实际计算负担。  
  - 如Diffusion Buffer等方法在低延迟设置下质量下降显著，且算法延迟较高（≥180ms），难以满足实时需求（如VoIP的数十毫秒延迟）。

2)  
- **核心方法Stream.FM**：基于流匹配（Flow Matching）的帧因果生成模型，通过以下设计实现低延迟流式推理：  
  - **缓冲流式推理方案**：采用多层因果卷积网络，每层维护滚动缓存，仅在新帧到达时计算单帧输出，避免冗余计算，实现算法延迟32ms（总延迟48ms）。  
  - **优化DNN架构**：改造U-Net（NCSN++），移除时间维度的步长卷积，仅沿频率下采样，使用时间扩张因果卷积增加上下文，并替换归一化层为子带分组批归一化（SGBatchNorm）。  
  - **学习型ODE求解器**：针对固定计算预算（如NFE=4或5），训练自定义Runge-Kutta求解器系数，提升输出质量（如减少语音幻觉、改善高频细节）。  
  - **模型权重压缩**：对卷积层进行SVD分解与截断，平衡计算量与质量，允许在压缩后增加NFE以进一步提升性能。  
- **解决思路**：将生成过程与物理时间解耦，通过缓存机制实现帧级并行计算，确保流式推理结果与离线批量处理一致，同时保持高质量。

3)  
- **任务与效果**：Stream.FM在六项语音修复任务上实现流式处理，均达到或超越现有流式方法的性能：  
  - **语音增强（SE）**：在EARS-WHAM和VoiceBank-DEMAND数据集上，PESQ、DiMOS等指标优于DEMUCS、Diffusion Buffer等基线，MUSHRA听力测试评分接近非流式模型。  
  - **去混响、带宽扩展、编解码器后滤波、STFT相位恢复、Mel声码化**：在各自任务中显著提升输出质量（如带宽扩展的WER降低至10.5%），且学习型求解器在模糊任务（如带宽扩展）中效果尤佳。  
- **整体**：在消费级GPU上实现总延迟≤48ms的实时处理，建立了生成式流式语音修复的新标杆。
</div>

</details>

---

## DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners
- **Authors**: Wenyu Luo, Jinhui Chen
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.19374v1](https://arxiv.org/abs/2512.19374v1)
- **PDF**: [https://arxiv.org/pdf/2512.19374v1](https://arxiv.org/pdf/2512.19374v1)

语音清晰度评估对众多语音相关应用至关重要。然而，大多数客观清晰度指标具有侵入性，因为它们除了待评估的失真或处理信号外，还需要纯净的参考语音。此外，现有指标（如STOI）主要针对正常听力听众设计，其对听力受损者语音清晰度的预测准确性仍有限。另一方面，GESI（Gammachirp包络相似度指数）虽可用于评估听力受损者的清晰度，但同样具有侵入性，因其依赖参考信号。这一要求限制了其在实际场景中的应用。

为突破此限制，本研究提出DeepGESI——一种基于深度学习的非侵入式模型，能够在不依赖任何纯净参考语音的情况下，准确高效地预测听力受损者的语音清晰度。实验结果表明，在第二届Clarity预测挑战赛（CPC2）数据集的测试条件下，DeepGESI预测的GESI分数与实际GESI分数呈现强相关性。此外，相较于传统方法，所提模型的预测速度显著提升。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音清晰度评估对助听器等应用至关重要。传统主观测试成本高，而客观评估方法（如STOI、GESI）多为侵入式，需纯净参考语音，限制了实际应用。
- **既有方法问题**：
  - 侵入式依赖：主流客观指标（如STOI、GESI）需纯净参考语音，现实中常不可得。
  - 适用性局限：多数方法针对正常听力者设计，对听损者的预测精度不足。
  - 计算效率低：如GESI涉及复杂听觉滤波与调制分析，计算开销大，难以实时部署。

2)  
**DeepGESI通过非侵入式深度学习架构解决上述问题，核心方法如下：**
- **非侵入式设计**：模型仅需处理后的语音信号，无需纯净参考语音。通过端到端训练，直接预测GESI分数，摆脱了对参考信号的依赖。
- **针对听损者的优化**：以GESI（专为听损者设计）为预测目标，继承了其考虑听力损失（如听阈图依赖处理、调制域线索）的能力，从而更准确评估听损者的语音清晰度。
- **高效轻量架构**：
  - **特征提取**：结合短时傅里叶变换（STFT）与可学习滤波器组（LFB），后者基于SincNet实现，可优化窄带成分（如基频、共振峰）的捕捉。
  - **注意力机制与位置编码**：采用多头注意力模块捕获长程依赖，并结合RoPE位置编码，能处理任意长度序列并增强泛化能力。
  - **激活函数**：使用Maxout替代ReLU，更好地保留负值范围内的语音线索，提升轻量模型的特征表示能力。
  - **损失函数**：结合句子级与帧级预测误差（加权均方误差），同时建模全局与局部声学特征。
- **计算效率提升**：轻量设计大幅降低计算成本，实现实时预测。

3)  
- **任务**：在CPC2数据集上评估听损者的语音清晰度，预测GESI分数。
- **效果**：
  - **高精度**：在可见数据上，MSE=0.0011，LCC=0.9613，SRCC=0.9561；在未见数据上仍保持稳定（MSE=0.0034，LCC=0.9289），显示强泛化能力。
  - **高效率**：平均每语句预测仅需0.005秒，较GESI（9.27秒）和HASPI（1.26秒）提速显著，适合实时应用。
</div>

</details>

---

## Sonified Quantum Seizures. Sonification of time series in epileptic seizures and simulation of seizures via quantum modelling
- **Authors**: Maria Mannone, Paulo Vitor Itaborai, Omar Costa Hamido, Miriam Goldack, Norbert Marwan, Peppino Fazio, Patrizia Ribino
- **Categories**: quant-ph, cs.ET, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.19272v1](https://arxiv.org/abs/2512.19272v1)
- **PDF**: [https://arxiv.org/pdf/2512.19272v1](https://arxiv.org/pdf/2512.19272v1)

本研究将声化策略与量子计算应用于癫痫发作事件的分析。首先，我们对选定通道（源自真实皮层脑电图数据）的信号进行声化处理，生成多音轨序列。随后，我们提出两种量子模拟方法以模拟类似的癫痫发作事件，并对模拟结果进行声化处理。通过对比真实数据与模拟结果的声化呈现，可以揭示二者之间的相似性与差异性，从而有助于优化计算机模型。这一开创性方法展示了量子计算与声化技术相结合如何拓展真实数据研究的视角，并为癫痫发作的分析与预测构建了新的测试平台。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：癫痫预测至关重要，通常依赖于脑电图（EEG）或侵入性皮层电图（ECoG）数据。现有分析方法包括时域、频域和时频分析，旨在识别癫痫发作的源头和传播模式。  
- **既有问题**：传统方法主要依赖视觉化数据（如图谱），可能难以直观捕捉多通道信号中复杂的时空动态和同步变化。此外，癫痫发作具有概率性特征，而经典建模在模拟这种随机性和多通道交互方面存在局限。

2)  
论文提出了一种结合**声化**与**量子计算**的创新方法，以增强癫痫数据的分析与模拟：  
- **声化真实数据**：首先，将患者ECoG时间序列（多通道信号）映射为音高-起始序列，生成多声部声音。通过听觉呈现，研究者能直观感知癫痫发作期间信号的异常模式（如发作起始与结束的听觉变化）。  
- **量子方法模拟癫痫事件**：  
  - **技术一：量子信息检索**：采用量子概率幅度调制（QPAM）对单通道（如MST4）信号进行编码，通过量子电路计算滑动窗口的统计矩（如峰度）。将结果声化后，可捕捉癫痫发作的关键特征（如大振幅振荡），作为原始声化的补充。  
  - **技术二：量子建模与仿真**：基于横向场伊辛模型构建量子系统，将16个ECoG通道映射为耦合系数，并利用技术一的输出控制横向场。通过量子电路模拟时间演化，计算每个量子比特的期望值（即处于|1⟩态的概率），从而生成反映癫痫动态的量子数据。  
- **整合与比较**：将量子模拟结果（如伊辛模型的相变信号）通过频率调制融入原始声化，增强对癫痫发作起始、传播及结束后活动的标识。通过比较真实数据与量子模拟的声化结果，可评估模型相似性，进而优化仿真。

3)  
- **任务**：癫痫发作数据的分析与仿真。  
- **效果**：  
  - 声化成功将多通道ECoG信号转换为可听序列，听觉与频谱图均清晰显示癫痫发作期（约120-200秒）的模式改变。  
  - 量子模拟（特别是伊辛模型）能产生与真实数据相似的动态，如清晰识别发作起始的相变，并指示发作后的持续高活动状态。  
  - 该方法为癫痫研究提供了新框架，通过声化与量子计算的结合，增强了数据探索能力，并为未来预测系统的开发奠定基础。
</div>

</details>

---

## JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis
- **Authors**: Fan Yu, Tao Wang, You Wu, Lin Zhu, Wei Deng, Weisheng Han, Wenchao Wang, Lin Hu, Xiangyu Liang, Xiaodong He, Yankun Huang, Yu Gu, Yuan Liu, Yuxuan Wang, Zhangyu Xiao, Ziteng Wang, Boya Dong, Feng Dang, Jinming Chen, Jingdong Li, Jun Wang, Yechen Jin, Yuan Zhang, Zhengyan Sheng, Xin Wang
- **Categories**: cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.19090v1](https://arxiv.org/abs/2512.19090v1)
- **PDF**: [https://arxiv.org/pdf/2512.19090v1](https://arxiv.org/pdf/2512.19090v1)

大型语音生成模型正从单说话人、短句合成向多说话人、长对话生成演进。当前的长时语音生成模型主要局限于二元轮转式交互。为此，我们提出JoyVoice——一种新颖的拟人化基础模型，专为灵活、无边界地合成最多八位说话人而设计。与传统的级联系统不同，JoyVoice采用统一的端到端Transformer-DiT架构，直接利用自回归隐层表示作为扩散模型输入，实现了整体端到端优化。我们进一步提出了一种工作于12.5 Hz低比特率的MM-Tokenizer，它融合了多任务语义损失与MMSE损失，能有效建模语义与声学信息。此外，模型通过大规模数据扰动实现了鲁棒的文本前端处理。实验表明，JoyVoice在多语言生成（中文、英文、日文、韩文）和零样本语音克隆任务中均达到领先水平。该模型在Seed-TTS-Eval基准测试和多说话人长时对话语音克隆任务中均取得顶尖结果，展现出卓越的音频质量与泛化能力。其在长时语音的韵律连贯性、多说话人对话的节奏丰富性、副语言自然度方面均有显著提升，同时具备优异的可懂度。我们建议读者访问演示页面https://jea-speech.github.io/JoyVoice试听效果。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：大语音生成模型正从单说话人、短句合成向多说话人、长对话生成演进。当前主流长语音生成模型主要局限于两人轮转式交互，难以支持更灵活、无边界（如多达八人）的对话合成。  
- **既有方法问题**：  
  - **两阶段模型**（如先自回归生成离散语义令牌，再用扩散模型合成声学特征）：存在信息瓶颈（离散令牌丢失细粒度信息）、优化不一致（两阶段独立训练）、错误传播以及因令牌序列过长导致的推理效率低下和幻觉风险。  
  - **一阶段连续特征模型**（如基于VAE表示）：局部DiT的感受野有限可能损害音色相似性，且若提示语音含噪声，会通过连续表示传播导致合成不稳定。此外，现有模型对多说话人、长上下文、多轮对话的零样本合成在稳定性和自然度方面仍面临挑战。

2)  
论文提出**JoyVoice**，一个高效、可扩展的零样本TTS合成器，通过以下核心方法解决上述问题：  
- **统一的端到端Transformer-DiT架构**：  
  - 采用自回归Transformer（基于Qwen）预测离散令牌，其**隐藏状态直接作为全局因果DiT（扩散Transformer）的条件输入**，实现从文本到声学的端到端联合优化。  
  - 通过双向梯度传播（公式4），使自回归模型学习到的表示既优化令牌预测，又有利于连续声学细节重建，从而缓解信息瓶颈，提升内容清晰度和系统稳定性。  
- **高效MM-Tokenizer**：  
  - 基于Whisper-large-v3，通过多任务学习（结合ASR、SER、AED等语义任务损失与声学重建损失）训练，在**12.5 Hz的低帧率下**实现语义与声学信息的高效建模。  
  - 高压缩率利于长上下文建模，且端到端框架对令牌压缩更具鲁棒性（12.5 Hz性能接近25 Hz）。  
- **多说话人多轮对话建模**：  
  - 采用**统一序列化输入**（公式5-6），将整个多轮对话的文本、说话人标签和语音令牌连续拼接，无需显式话语分割，通过注意力机制隐式学习说话人与内容的对应关系，保持对话连贯性并捕捉跨轮依赖。  
- **动态分块流匹配**：  
  - 在DiT中采用动态分块因果注意力，训练时随机分块大小，推理时可灵活调整以支持流式与非流式合成，适应不同延迟需求。  
- **课程学习与数据增强**：  
  - 两阶段课程学习：先在大规模单说话人短音频上预训练，再混合长音频和多说话人数据微调，提升长音频多说话人生成的稳定性。  
  - 大规模数据扰动（如文本归一化/反归一化、多音字、罕见字）减少对前端文本预处理模块的依赖，增强模型鲁棒性。  
- **强化学习优化**：  
  - 在后训练阶段引入声学偏好优化（APO），基于字符错误率构建偏好对，在声学令牌级别进行优化，减少发音错误和不自然输出。

3)  
JoyVoice在以下任务中取得**最先进（SOTA）效果**：  
- **多语言零样本语音克隆**：在SEED-TTS-Eval基准测试中，中/英文测试集上取得最低字符/词错误率（如中文CER 0.97%，英文WER 1.69%）和最高说话人相似度（如0.836），超越CosyVoice等基线。  
- **多说话人长对话语音克隆**：在自建的JoyVoice-MSMT-eval基准（2-4人对话）上，中英文的cpWER显著低于VibeVoice等模型，展现出优异的内容一致性、音色稳定性和长音频韵律连续性。  
- **流式合成**：JoyVoice-Streaming在动态分块下保持与非流式版本相近的性能，支持灵活延迟控制，并在相似度上优于CosyVoice 3-0.5B。  
- **单说话人合成**：即使作为多功能模型，在单说话人任务上也达到SOTA，CER/WER相对改进显著（如test-hard上相对提升8.6%）。
</div>

</details>

---

## Enhancing Fully Formatted End-to-End Speech Recognition with Knowledge Distillation via Multi-Codebook Vector Quantization
- **Authors**: Jian You, Xiangfeng Li, Erwan Zerhouni
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.18967v1](https://arxiv.org/abs/2512.18967v1)
- **PDF**: [https://arxiv.org/pdf/2512.18967v1](https://arxiv.org/pdf/2512.18967v1)

传统自动语音识别模型通常输出缺乏标点和大写格式的规范化文本，需依赖后处理模型提升可读性，但这种级联系统设计会引入额外复杂性与延迟。为应对这一挑战，学界正致力于开发能直接预测标点与大写的端到端语音识别模型，然而该领域仍待深入探索。本文提出一种增强型全格式化端到端语音识别模型，通过多码本向量量化技术实现知识蒸馏。实验结果表明，无论是否包含标点与大写格式，该模型在词错误率及标点错误率上均显著优于先前研究。在LibriSpeech-PC测试集clean与other子集上的评估显示，本模型取得了当前最优性能。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：传统自动语音识别模型通常输出无标点、全小写的归一化文本，可读性差，且不利于下游任务。为恢复格式，需额外后处理模型，形成级联系统。
- **既有问题**：级联系统增加了复杂性和延迟；后处理模型常忽略声学特征（如韵律），仅依赖文本特征易出错；现有端到端格式化ASR研究尚不充分，部分模型（如Whisper）无法流式处理。

2)  
- **核心框架**：提出基于RNN-T的端到端ASR模型，直接输出带标点和大写的格式化文本。采用高性能Zipformer作为声学编码器，以RNN-T损失为主损失进行训练。
- **知识蒸馏设计**：
  - **教师模型**：使用在LibriSpeech上微调的大规模自监督模型HuBERT。
  - **蒸馏方式**：从教师模型中间层提取嵌入，通过多码本向量量化压缩为码本索引；学生模型（Zipformer中间层）学习预测这些索引，通过交叉熵损失进行知识迁移。
  - **损失融合**：总损失为RNN-T格式化损失与知识蒸馏损失的加权和，通过超参数α平衡。
- **解决思路**：
  - **提升表征质量**：蒸馏使学生模型中间层学习到教师模型丰富的声学-语言表征，有助于更准确地预测文本（包括标点和大写）。
  - **利用声学信息**：教师模型的中间嵌入包含韵律等声学线索，可弥补纯文本后处理的不足。
  - **促进任务协同**：更准确的文本预测改善标点预测，而标点与大写存在关联（如句号后首字母大写），从而相互促进。
  - **高效实现**：码本索引可预先计算并高压缩存储，训练时几乎无额外计算开销，推理时无需增加模块。

3)  
- **任务与数据集**：在LibriSpeech-PC（test-clean和test-other子集）上进行全格式化ASR评估，指标包括词错误率（含不同格式变体）和标点错误率。
- **效果**：
  - 在无外部语言模型时，模型（6500万参数）在WER PC和PER上均优于或媲美更大参数模型（如E2E Conformer）。
  - 结合外部语言模型后，取得最优性能：在test-clean和test-other上，WER PC分别降至6.65%和9.05%，相对此前最佳提升约13%和8%；PER也显著降低。
  - 消融实验证实知识蒸馏对提升格式化性能（尤其在流式模式下）的有效性，且训练开销增加很小。
</div>

</details>

---
