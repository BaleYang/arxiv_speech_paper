---
layout: post
title: "arXiv Daily – 2026-02-05"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-05（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-04 08:50 — 2026-02-05 08:50
- 抓取总数：8 篇 | 本页显示：8 篇（去重/过滤后）

## Fine-Grained Frame Modeling in Multi-head Self-Attention for Speech Deepfake Detection
- **Authors**: Tuan Dat Phuong, Duc-Tuan Truong, Long-Vu Hoang, Trang Nguyen Thi Thu
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.04702v1](https://arxiv.org/abs/2602.04702v1)
- **PDF**: [https://arxiv.org/pdf/2602.04702v1](https://arxiv.org/pdf/2602.04702v1)

基于Transformer的模型在语音深度伪造检测中表现出色，这主要得益于多头自注意力机制的有效性。该机制能够提供帧级注意力分数，这对于检测语音时序维度上局部小范围出现的深度伪造痕迹尤为重要，因此细粒度帧建模对于准确捕捉细微伪造线索至关重要。本研究提出一种用于基于多头自注意力的语音深度伪造检测的细粒度帧建模方法：首先通过多头投票模块筛选出信息量最大的帧，随后借助跨层优化模块对这些选定帧进行精细化处理，以增强模型学习细微伪造线索的能力。实验结果表明，该方法在LA21、DF21和ITW数据集上分别实现了0.90%、1.88%和6.64%的等错误率，性能优于基线模型。多个基准测试中一致的性能提升，验证了所提细粒度建模方法在鲁棒性语音深度伪造检测中的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：深度伪造语音技术（如TTS和VC）的进步对生物识别安全构成严重威胁。基于Transformer的模型因其多头自注意力（MHSA）机制在检测中表现出色，该机制能捕获帧级依赖关系。  
- **既有方法问题**：传统MHSA将注意力输出视为全局聚合特征，忽略了细粒度的时间动态信息。合成语音的伪造痕迹（如不自然的过渡或特定音素）常出现在局部、稀疏的时间区域，而全局注意力可能稀释或忽略这些细微线索，导致检测能力受限。  

2)  
论文提出**细粒度帧建模（FGFM）**方法，通过两个核心模块增强MHSA对局部伪造痕迹的捕捉能力：  
- **多头投票（MHV）模块**：  
  - 将每个MHSA注意力头视为弱学习器，基于注意力图对每头投票选出最具信息量的帧（如每头选24帧）。  
  - 通过高斯核卷积对投票结果进行平滑增强，减少噪声，聚焦于语音丰富的关键区域。  
- **跨层精炼（CLR）模块**：  
  - 聚合不同层中MHV选出的帧，与分类令牌拼接后输入额外卷积块，提取跨层全局判别特征。  
  - 使用交叉注意力机制交换跨层特征与精炼特征的信息，并通过动态聚合前馈（DAFF）模块强化特征关联，最终丰富分类令牌的上下文信息。  
- **整体设计优势**：FGFM显式建模局部细粒度帧，避免全局平均化导致的线索丢失，同时利用多头多样性和层次信息提升对细微伪造痕迹的敏感性。  

3)  
在多个语音深度伪造检测任务上取得显著效果：  
- **ASVspoof 2021 LA数据集**：EER为0.90%，相对基线降低7.2%。  
- **ASVspoof 2021 DF数据集**：EER为1.88%，相对基线降低27.1%。  
- **跨领域In-the-Wild（ITW）数据集**：EER为6.64%，相对基线降低21.1%，展现了强泛化能力。  
- 该方法在Conformer和Transformer架构上均稳定提升性能，在多项基准中达到或超越当前最优水平。
</div>

</details>

---

## UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization
- **Authors**: Dongchao Yang, Yuanyuan Wang, Dading Chong, Songxiang Liu, Xixin Wu, Helen Meng
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.04683v1](https://arxiv.org/abs/2602.04683v1)
- **PDF**: [https://arxiv.org/pdf/2602.04683v1](https://arxiv.org/pdf/2602.04683v1)

本研究针对音频语言模型中的两个基础性问题展开：(1) 如何设计一种既能服务于理解任务又能支持生成任务的音频分词器作为中间表示；(2) 如何构建一个能够像大语言模型那样在少样本与零样本场景下泛化的音频基础模型。为此，我们提出以下两项贡献：首先，我们设计了ReasoningCodec——一种将音频分解为两类离散编码的分词器：(i) 推理令牌，用于编码与文本对齐的高层分析与规划表示，以支持音频理解与分层生成；(ii) 重建令牌，用于编码富含语义的声学线索以实现高保真波形重建。该设计在理解性能上媲美强连续表示，同时在生成质量与重建保真度上超越了现有离散分词器。其次，我们提出了一种面向文本与音频的统一自回归架构，配合多阶段训练与多任务数据构建方案。基于此框架，我们在1000亿文本令牌与600亿音频令牌上训练了UniAudio 2.0模型。在涵盖语音、环境声与音乐的广泛任务评测中，UniAudio 2.0在领域内任务上表现优异，并在未见任务中展现出强大的少样本与零样本泛化能力。演示页面、代码与模型权重将通过\href{https://dongchaoyang.top/UniAudio2Demo/}{https://dongchaoyang.top/UniAudio2Demo/}发布。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频语言模型旨在统一音频理解与生成任务，但现有方法在泛化、扩展和任务多样性上不及文本大语言模型。  
- **既有问题**：  
  - **音频表示设计**：连续表示（如SSL特征）利于理解但难以用于自回归生成；离散音频编解码器利于生成但缺乏文本对齐的高层抽象，限制理解能力。  
  - **模型架构**：现有统一自回归Transformer平等处理文本和音频token，导致音频感知抽象不足，且易遗忘预训练文本知识。  
  - **训练数据**：缺乏大规模、多样化的多任务音频-文本数据，制约模型泛化。

2)  
论文通过**ReasoningCodec**和**功能分层专业化架构**解决上述问题：  
- **ReasoningCodec音频分词器**：  
  - 将音频分解为**推理token**（编码文本对齐的高层分析与规划表示，支持理解和分层生成）和**重建token**（编码语义丰富的声学细节，实现高保真波形重建）。  
  - 通过多专家特征提取、查询量化与GRPO强化学习优化，平衡理解与生成需求。  
- **统一自回归架构**：  
  - **功能分层专业化**：下层为音频理解专家（专注音频感知抽象），中层为跨模态专家（从预训练LLM初始化，对齐文本与音频），上层为音频生成专家（建模细粒度声学）。  
  - 音频专家层仅处理音频token，保留文本处理路径，避免知识遗忘。  
- **数据与训练策略**：  
  - 构建涵盖语音、声音、音乐的大规模多任务数据，并引入**听觉句子**（将多段相关音频/文本组合为长序列），统一任务构造。  
  - 采用四阶段训练：音频理解预热→音频生成预热→音频-文本预训练→音频-文本中期训练，逐步整合能力并提升泛化。

3)  
UniAudio 2.0在以下任务中取得显著效果：  
- **已见任务**：在语音（TTS、ASR）、声音（音频描述与生成）、音乐（音乐生成、歌曲生成）等任务上，性能与当前SOTA模型（如MiMo-Audio、Qwen2.5-Omni）相当或更优。  
- **少样本任务**：在语音去噪、语音转换、情感分类等任务上，1-shot和2-shot设置下均优于基线模型，展现强大泛化能力。  
- **零样本任务**：在文本问答、语音对话、构音障碍语音识别等未见任务中表现良好，验证其作为音频基础模型的潜力。
</div>

</details>

---

## Audio ControlNet for Fine-Grained Audio Generation and Editing
- **Authors**: Haina Zhu, Yao Xiao, Xiquan Li, Ziyang Ma, Jianwei Yu, Bowen Zhang, Mingqi Yang, Xie Chen
- **Categories**: cs.SD, cs.AI, cs.CL, cs.MM
- **arXiv**: [https://arxiv.org/abs/2602.04680v1](https://arxiv.org/abs/2602.04680v1)
- **PDF**: [https://arxiv.org/pdf/2602.04680v1](https://arxiv.org/pdf/2602.04680v1)

本研究聚焦于细粒度文本到音频（T2A）生成任务。尽管现有模型能够根据文本描述合成高质量音频，但在响度、音高和声音事件等属性上往往缺乏精确控制。不同于以往针对特定控制类型重新训练模型的方法，我们提出在预训练的T2A主干模型基础上训练ControlNet模型，以实现对响度、音高和事件序列的可控生成。我们引入了T2A-ControlNet和T2A-Adapter两种架构，并证明T2A-Adapter能以更高效的结构实现强大的控制能力。该模型仅增加3800万参数，即在AudioSet-Strong数据集上同时取得事件级别和片段级别F1分数的领先性能。我们进一步将该框架扩展至音频编辑领域，提出T2A-Editor模型，支持通过指令指定的时间位置进行音频事件的删除与插入。我们将公开模型、代码、数据集流程及评测基准，以支持未来可控音频生成与编辑的研究。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：文本到音频（T2A）生成模型在音频质量和文本对齐方面取得了显著进展，但普遍缺乏对音频属性的精细控制。
- **既有方法的问题**：
  - 现有模型主要通过文本描述进行粗粒度控制，难以精确指定声音事件的时间位置（如起止时间）或同时控制多个事件。
  - 在信号层面（如响度动态、音高轮廓）的控制能力有限。
  - 现有方法通常需要为特定控制类型重新训练整个模型，训练复杂且灵活性不足。

2)  
论文提出了 **Audio ControlNet** 框架，通过在预训练的T2A骨干模型（FluxAudio）上附加轻量级辅助网络来实现精细控制，无需重新训练骨干模型。其核心方法包括：

- **统一的结构化控制信号表示**：将所有控制输入（响度、音高、声音事件）统一表示为与音频时间线对齐的时序序列，并设计了相应的特征提取器。
  - **响度**：提取音频信号的RMS能量并平滑处理。
  - **音高**：估计基频并应用连续小波变换，再通过可学习的码本进行量化嵌入。
  - **声音事件**：使用事件卷（标注了事件类别及其起止时间），通过CLAP文本编码器获取语义嵌入，并聚合为时序表示。

- **两种辅助网络架构**：
  - **T2A-ControlNet**：复制骨干模型的部分层，以控制条件为输入，通过零卷积将残差分量注入到骨干模型的每一层。
  - **T2A-Adapter**：采用更轻量的编码器（由1D卷积和SiLU激活组成）从控制条件中提取特征，然后通过交叉注意力机制注入到骨干模型的前几层。该方法参数量更少，效率更高。

- **扩展到音频编辑任务**：提出了 **T2A-Editor**。它基于T2A-Adapter的范式，通过引入参考音频和基于事件卷的编辑指令（如“在X秒到Y秒插入/移除某声音”），将T2A模型转换为支持时间定位插入和移除的音频编辑模型。

3)  
论文在以下任务上取得了显著效果：
- **精细控制生成**：在AudioSet-Strong数据集上评估。
  - **声音事件控制**：T2A-Adapter在事件级F1分数（54.36）和片段级F1分数（68.26）上达到了最先进水平，仅引入3800万额外参数。
  - **响度与音高控制**：T2A-Adapter在响度控制上平均绝对误差（MAE）为1.40，优于基线模型；T2A-ControlNet在音高控制上MAE为119.28。
- **音频编辑**：在插入和移除任务上，使用FlexSED指标评估，T2A-Editor（尤其是结合LoRA后）的结果显著接近真实目标，有效实现了基于指令的时间定位编辑。
</div>

</details>

---

## HoliAntiSpoof: Audio LLM for Holistic Speech Anti-Spoofing
- **Authors**: Xuenan Xu, Yiming Ren, Liwei Liu, Wen Wu, Baoxiang Li, Chaochao Lu, Shuai Wang, Chao Zhang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.04535v1](https://arxiv.org/abs/2602.04535v1)
- **PDF**: [https://arxiv.org/pdf/2602.04535v1](https://arxiv.org/pdf/2602.04535v1)

随着语音合成与编辑技术的快速发展，语音伪造问题日益严峻。然而，现有方法大多将伪造检测视为二分类任务，忽略了不同伪造技术会同时对多个相互关联的语音属性及其语义效应进行篡改。本文提出首个用于整体语音反伪造分析的音频大语言模型框架 HoliAntiSpoof，将伪造分析重新定义为统一的文本生成任务，实现对伪造方法、受影响的语音属性及其语义影响的联合推理。为支持语义层面的分析，我们构建了 DailyTalkEdit 这一新型反伪造基准数据集，模拟真实对话场景中的篡改操作并提供语义影响标注。大量实验表明，HoliAntiSpoof 在多种设定下均优于传统基线方法，初步研究还显示上下文学习能进一步提升其跨领域泛化能力。这些发现表明，音频大语言模型不仅能提升语音伪造检测性能，还可实现对伪造行为及其语义影响的可解释分析，为构建更可信、可解释的语音安全体系提供了新方向。相关数据与代码均已公开。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：语音合成与编辑技术的进步使语音伪造（深度伪造）威胁加剧。传统反欺骗方法主要将问题视为二分类（真/假），忽略了不同伪造技术会同时影响语音的多个耦合属性（如信号特征、副语言特征、语义内容）及其语义影响。  
- **既有方法问题**：二分类框架无法全面分析伪造行为，且现有方法多关注信号层面的真实性，缺乏对语义影响的评估。这导致模型可能过拟合特定模式，泛化能力不足，难以应对不断演化的新型伪造技术。  

2)  
- **统一任务框架**：HoliAntiSpoof首次将语音反欺骗重新定义为统一的文本生成任务，基于音频大语言模型（ALLM）进行整体分析。它将多个子任务（真伪分类、伪造方法识别、伪造区域定位、语义影响分析）整合为结构化文本（JSON格式）的生成，使模型能联合推理多层面信息。  
- **模型架构与训练**：  
  - 以Qwen2.5-Omni为骨干，结合音频编码器提取声学特征。  
  - 通过监督微调（SFT）训练模型生成结构化分析结果；进一步采用上下文学习微调（ICLFT），使模型能通过少量参考示例适应新领域，提升零样本泛化能力。  
  - 引入DoRA高效微调技术，在保持预训练模型能力的同时适配反欺骗任务。  
- **语义数据构建**：为支持语义分析，论文构建了DailyTalkEdit数据集（基于对话的伪造场景）并为PartialEdit添加语义标注，模拟真实对话操纵并标注语义影响，填补了现有数据集的空白。  
- **特征增强**：探索结合专用反欺骗编码器提取的特征，补充原始音频特征，以更好地捕捉低层级伪造线索。  

3)  
- **任务与效果**：HoliAntiSpoof在多项任务上超越传统基线模型：  
  - **真伪分类**：在领域内（如ASVSpoof2019）和跨领域测试集上均达到SOTA准确率（如混合测试集96.16%）。  
  - **伪造方法识别**：宏平均F1分数达95.12%。  
  - **伪造区域定位**：在部分伪造语音中，段级F1分数达91.33%，显著优于传统方法（均低于60%）。  
  - **语义影响分析**：通过LLM评估生成分析的合理性（平均评分3.5/5）。  
- **泛化能力**：通过ICLFT，在跨语言和未知伪造方法的场景中（如HAD数据集）表现出更强的适应性和鲁棒性。
</div>

</details>

---

## Universal Robust Speech Adaptation for Cross-Domain Speech Recognition and Enhancement
- **Authors**: Chien-Chun Wang, Hung-Shin Lee, Hsin-Min Wang, Berlin Chen
- **Categories**: eess.AS, cs.CL, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.04307v1](https://arxiv.org/abs/2602.04307v1)
- **PDF**: [https://arxiv.org/pdf/2602.04307v1](https://arxiv.org/pdf/2602.04307v1)

针对自动语音识别与语音增强任务的预训练模型，在匹配的噪声与信道条件下已展现出卓越性能。然而，当面临领域偏移时，尤其是在未见的噪声与信道失真条件下，这些模型常出现显著的性能下降。为此，本文提出URSA-GAN——一个统一且具备领域感知能力的生成式框架，专门用于缓解噪声与信道条件不匹配问题。该框架采用双嵌入架构，包含分别通过有限领域内数据预训练的噪声编码器与信道编码器，以捕获领域相关表征。这些嵌入向量作为条件输入至基于生成对抗网络的语音生成器，使其合成在声学特征上与目标领域对齐、同时保持语音内容一致性的语音信号。为进一步提升泛化能力，我们提出动态随机扰动技术，通过在生成过程中向嵌入向量引入受控的随机变化，增强模型对未见领域的鲁棒性。实验结果表明，URSA-GAN能有效降低多种噪声与不匹配信道场景下的语音识别字错误率，并提升语音增强的感知评价指标。值得注意的是，在同时包含信道与噪声退化的复合测试条件下，URSA-GAN展现出优异的泛化能力，使语音识别性能相对提升16.16%，语音增强指标相对提升15.58%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：预训练的自动语音识别和语音增强模型在匹配的噪声和信道条件下表现优异，但在实际应用中常面临域偏移问题，如未见过的噪声类型和信道失真，导致性能严重下降。  
- **既有方法问题**：传统域适应方法（如对抗训练、数据增强）通常需要大量目标域标注数据或复杂训练流程，难以扩展；现有数据模拟方法多关注整体域特性，忽略细粒度、语句级变化，且往往单独处理噪声或信道失真，缺乏统一框架。

2)  
- **核心框架**：提出URSA-GAN，一个统一的生成对抗网络框架，通过双嵌入架构（噪声编码器和信道编码器）分别捕获目标域的噪声和信道特征。这些嵌入用于条件化GAN生成器，合成在声学上与目标域对齐且保留语音内容的语音。  
- **关键创新**：  
  - **双编码器设计**：噪声编码器基于BEATs预训练模型，信道编码器基于MFA-Conformer，均用少量目标域数据微调，以提取实例级、解耦的声学表示。  
  - **动态随机扰动**：在生成过程中向嵌入注入受控噪声，增强模型对未见域的鲁棒性。  
  - **特征融合与损失函数**：采用FiLM机制在多层级融合噪声和信道嵌入；结合对抗损失、块级对比学习损失、噪声重建损失和信道一致性损失，确保生成语音的逼真度和内容保留。  
- **解决思路**：仅需少量无标注目标域数据，通过生成模拟数据来适配下游ASR和SE模型，从而同时缓解噪声和信道失配问题。

3)  
- **任务与效果**：在多个数据集上评估URSA-GAN，涵盖信道失配、噪声失配及两者混合的复杂条件。  
  - **自动语音识别**：在HAT和TAT数据集上，相对基线降低CER达20.51%和9.87%；在混合噪声/信道的HAT-ESC数据集上，相对改进16.16%。  
  - **语音增强**：在VBD数据集上，PESQ和STOI指标显著提升，相对改进15.58%；在HAT-ESC上PESQ从1.99提升至2.30。  
- **结论**：URSA-GAN在跨域ASR和SE任务中均取得稳定提升，尤其在混合失真条件下展现强泛化能力。
</div>

</details>

---

## DementiaBank-Emotion: A Multi-Rater Emotion Annotation Corpus for Alzheimer's Disease Speech (Version 1.0)
- **Authors**: Cheonkam Jeong, Jessica Liao, Audrey Lu, Yutong Song, Christopher Rashidian, Donna Krogh, Erik Krogh, Mahkameh Rasouli, Jung-Ah Lee, Nikil Dutt, Lisa M Gibbs, David Sultzer, Julie Rousseau, Jocelyn Ludlow, Margaret Galvez, Alexander Nuth, Chet Khay, Sabine Brunswicker, Adeline Nyamathi
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.04247v1](https://arxiv.org/abs/2602.04247v1)
- **PDF**: [https://arxiv.org/pdf/2602.04247v1](https://arxiv.org/pdf/2602.04247v1)

本研究推出DementiaBank-Emotion——首个针对阿尔茨海默病（AD）语音的多标注者情感标注语料库。通过对108名说话者的1,492条话语进行埃克曼六种基本情感及中性情感的标注，我们发现AD患者表达非中性情感的比例（16.9%）显著高于健康对照组（5.7%；p < .001）。探索性声学分析揭示一种可能的分离现象：对照组说话者在表达悲伤时表现出显著的基频调制（较基线降低3.45个半音），而AD说话者变化极小（较基线增加0.11个半音；交互作用p = .023），但该发现基于有限样本（悲伤情感：对照组n=5，AD组n=15）仍需进一步验证。在AD语音内部，响度能有效区分情感类别，表明情感-韵律映射机制得到部分保留。我们公开了该语料库、标注规范及校准研讨材料，以支持临床人群情感识别研究。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：阿尔茨海默病（AD）患者的言语分析在计算语言学领域已取得进展，但情感维度研究不足。现有情感识别数据集（如IEMOCAP）主要基于健康人群的表演性或脚本化言语，缺乏针对临床人群（尤其是认知障碍患者）的资源。
- **既有方法的问题**：
  - 临床言语（如AD患者）的情感表达常因病理性的声音质量变化（如情感平淡、基频变化减少）而变得复杂。
  - 标准情感分析和自动语音情感识别系统难以处理临床细微差别，例如将用于应对词汇查找困难的“笑声”误判为“喜悦”。
  - 缺乏结合语言学焦点、语用意图和临床专业知识的细粒度标注框架。

2)  
- **构建首个AD言语情感标注语料库**：论文发布了DementiaBank-Emotion（v1.0），这是首个针对AD言语的多标注者情感标注语料库。它包含108名说话者（54名AD患者，54名健康对照）的1,492个话语，标注了Ekman的六种基本情感和中性类别。
- **采用多学科标注与校准流程**：
  - 标注团队由临床专家（护理研究人员）和技术人员（计算机科学等）组成，确保标签既反映临床现实，又适合计算分析。
  - 通过多次校准研讨会，解决了AD言语中特有的标注挑战，例如区分“快乐的笑声”与用于“保全面子”或应对困难的笑声。
  - 制定了详细的标注指南（v2.0），强调优先考虑韵律线索（如音高、响度）而非词汇内容，并确立了“默认中性”原则。
- **实施层次化裁决算法确定最终标签**：采用基于多数共识和置信度加权的算法来确定每个话语的“黄金标准”情感标签，并对无法达成共识的话语标记为“模糊”。
- **进行深入的语料库分析**：
  - 发现AD患者表达非中性情感的比例显著高于健康对照（16.9% vs. 5.7%）。
  - 探索性声学分析表明，AD患者在表达情感时可能存在“声学扁平化”现象，例如在表达悲伤时，其基频调制远小于健康说话者。
  - 在AD言语内部，响度能有效区分不同情感类别（如喜悦和惊讶的响度更高），表明情感-韵律映射得到部分保留。

3)  
- **任务与效果**：
  - **语料库构建任务**：成功创建并发布了首个针对AD言语的多标注者情感标注语料库（DementiaBank-Emotion v1.0），包括标注数据、指南和校准材料。
  - **情感分布分析任务**：量化了AD患者与健康对照在情感表达上的显著差异，为理解AD的情感维度提供了实证基础。
  - **声学关联分析任务**：初步揭示了AD言语中情感表达的声学特征（如响度的区分作用，以及可能的基频调制减弱），为后续开发更适应临床言语的情感识别模型提供了关键见解和数据支持。
</div>

</details>

---

## Frontend Token Enhancement for Token-Based Speech Recognition
- **Authors**: Takanori Ashihara, Shota Horiguchi, Kohei Matsuura, Tsubasa Ochiai, Marc Delcroix
- **Categories**: cs.SD, cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.04217v1](https://arxiv.org/abs/2602.04217v1)
- **PDF**: [https://arxiv.org/pdf/2602.04217v1](https://arxiv.org/pdf/2602.04217v1)

语音信号的离散化表征作为连续特征的替代方案，在自动语音识别（ASR）和语音语言模型等应用中具有高效性。然而，这类表征（例如从自监督学习语音模型的聚类输出中获得的语义或音素标记）易受环境噪声干扰，可能导致后端任务性能下降。本研究提出一种前端系统，用于从含噪语音中估计纯净语音标记，并在基于语义标记的ASR后端上进行评估。我们依据输入/输出域设计了四类增强模型：波形到波形、标记到标记、连续SSL特征到标记以及波形到标记。这些模型的训练独立于ASR后端。在CHiME-4数据集上的实验表明，波形到标记增强在前端模型中取得最佳性能，且多数情况下优于基于连续SSL特征的ASR系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于自监督学习（SSL）模型生成的离散语义或语音学标记（token）在语音识别（ASR）等任务中具有高效性，但其对**环境噪声敏感**，导致后端任务性能下降。  
- **既有方法问题**：现有研究主要关注基于连续特征（如FBANK或SSL特征）的ASR系统的噪声鲁棒性，并常结合语音增强前端。然而，对于**基于离散标记的ASR系统**，如何设计有效的前端增强方法（例如应在波形、连续特征还是标记层面进行增强）尚未得到充分探索。

2)  
论文提出四种前端增强模型，根据输入/输出表示类型进行系统分类和比较：  
- **波形到波形增强（W2W-E）**：传统语音增强方法，将带噪波形转换为增强波形，再通过SSL模型提取标记。  
- **标记到标记增强（T2T-E）**：直接学习从带噪标记序列到干净标记序列的映射，使用嵌入层和轻量网络（如E-Branchformer）。  
- **连续特征到标记增强（V2T-E）**：以SSL模型的加权和特征为输入，通过分类器（如MLP、TCN或E-Branchformer）预测干净标记，旨在构建对噪声不变的标记器。  
- **波形到标记增强（W2T-E）**：**核心创新方法**，直接以带噪波形为输入，**微调整个SSL模型**（仅添加一个线性分类层），通过CTC损失预测干净标记序列。  

**解决思路与优势**：  
- **模块化设计**：所有前端与ASR后端独立训练，便于替换和更新。  
- **信息利用**：W2T-E和V2T-E利用SSL模型的丰富层次特征，比仅基于标记的T2T-E包含更多语音信息。  
- **效率与性能平衡**：W2T-E虽训练成本较高，但推理时无需中间表示，系统更简洁，且通过端到端优化充分利用SSL模型能力，显著提升噪声鲁棒性。

3)  
- **任务**：在CHiME-4数据集（单通道）的噪声环境下进行自动语音识别（ASR）评估。  
- **效果**：  
  - **W2T-E取得最佳性能**：在多数噪声条件下，其词错误率（WER）优于所有其他前端，甚至超过基于连续SSL特征的ASR系统。  
  - **标记压缩高效**：通过子词建模（BPE）将标记序列长度平均缩短约68%，提升了处理效率。  
  - **前端模块化有效**：W2T-E增强的标记可适用于不同ASR后端（如CTC或注意力编码器-解码器模型），均带来显著性能提升。
</div>

</details>

---

## PFluxTTS: Hybrid Flow-Matching TTS with Robust Cross-Lingual Voice Cloning and Inference-Time Model Fusion
- **Authors**: Vikentii Pankov, Artem Gribul, Oktai Tatanov, Vladislav Proskurov, Yuliya Korotkova, Darima Mylzenova, Dmitrii Vypirailenko
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.04160v1](https://arxiv.org/abs/2602.04160v1)
- **PDF**: [https://arxiv.org/pdf/2602.04160v1](https://arxiv.org/pdf/2602.04160v1)

本文提出PFluxTTS，一种混合式文本转语音系统，旨在解决流匹配TTS中的三个关键问题：稳定性与自然度的权衡、跨语言音色克隆能力不足，以及低速率梅尔特征导致的音频质量受限。我们的核心贡献包括：（1）通过推理时向量场融合，结合时长引导模型与无对齐模型的双解码器架构；（2）在基于FLUX的解码器中采用连续语音提示嵌入序列，实现无需提示文本转录的跨语言音色特征鲁棒克隆；（3）改进型PeriodWave声码器配合超分辨率技术，将输出提升至48 kHz。在跨语言真实场景数据测试中，PFluxTTS显著优于F5-TTS、FishSpeech和SparkTTS，其自然度（MOS 4.11）与ChatterBox相当，同时词错误率降低23%（6.9% vs. 9.0%），并在说话人相似度上超越ElevenLabs（SMOS提升0.32）。该系统在多数开源模型失效的复杂场景中仍保持鲁棒性，且仅需短参考音频而无需额外训练。音频示例详见：https://braskai.github.io/pfluxtts/

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：基于流匹配（FM）的TTS系统在快速、高保真合成方面取得进展，但仍存在三个核心问题。  
- **既有方法的问题**：  
  - **对齐与自然度的权衡**：显式时长预测模型（如Matcha-TTS）稳定性好但自然度受限；无对齐模型（如F5-TTS）自然度高但易出现词跳过等不稳定问题。  
  - **跨语言音色克隆薄弱**：多数系统依赖固定维度的说话人嵌入，无法捕捉时变音色细节，尤其在长提示或多语言场景下效果差。  
  - **声码器质量受限**：从低帧率梅尔特征重建全频带48 kHz音频的研究不足，影响音质。

2)  
- **双解码器混合架构**：  
  - 独立训练**时长引导（DG）模型**（基于FLUX架构，含显式时长预测）和**无对齐（AF）模型**（基于DiT风格，隐式对齐）。  
  - 在推理时通过**向量场融合**将两者结合：使用分段常数混合系数α(t)，在前段以DG为主稳定对齐，后段切换至AF以提升自然度。  
  - 两者共享DG预测的总时长T，确保特征图对齐。  
- **鲁棒的跨语言音色克隆**：  
  - 在DG路径中，**语音提示编码器**将可变长提示转换为K=16的嵌入序列，通过FLUX解码器中的注意力与文本交互，保留细粒度时变音色。  
  - AF路径为稳定性采用固定维提示嵌入。  
  - 训练时使用随机裁剪的提示段，并屏蔽目标对应部分以避免内容泄漏。  
- **超分辨率声码器**：  
  - 改进**PeriodWave声码器**，增加上/下采样模块，支持从低帧率梅尔特征（24 kHz，跳幅512）直接合成48 kHz全频带音频。  
  - 引入**提示感知条件**，通过ConvNeXt V2提取的全局提示嵌入补偿高频细节损失。

3)  
- **任务与效果**：  
  - **跨语言音色克隆与合成**：在真实多语言数据集（VoxLingua-dev、mTEDx）上评估，以英语为目标语言，非英语为提示。  
  - **主观指标**：自然度MOS达4.11，与ChatterBox相当；说话人相似度SMOS为3.51，超过ElevenLabs（+0.32）。  
  - **客观指标**：词错误率（WER）6.9%，低于ChatterBox（9.0%）；说话人相似度（SPK-SIM）0.68，优于基线。  
  - **声码器性能**：在VCTK和mTEDx上取得最佳对数谱距离（LSD），显示超分辨率有效提升音质。
</div>

</details>

---
