---
layout: post
title: "arXiv Daily – 2026-02-11"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-02-11（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-02-10 08:50 — 2026-02-11 08:50
- 抓取总数：4 篇 | 本页显示：4 篇（去重/过滤后）

## Covo-Audio Technical Report
- **Authors**: Wenfu Wang, Chenxing Li, Liqiang Zhang, Yiyang Zhao, Yuxiang Zou, Hanzhao Li, Mingyu Cui, Hao Zhang, Kun Wei, Le Xu, Zikang Huang, Jiajun Xu, Jiliang Hu, Xiang He, Zeyu Xie, Jiawen Kang, Youjun Chen, Meng Yu, Dong Yu, Rilin Chen, Linlin Di, Shulin Feng, Na Hu, Yang Liu, Bang Wang, Shan Yang
- **Categories**: cs.SD, cs.CL, eess.AS
- **arXiv**: [https://arxiv.org/abs/2602.09823v1](https://arxiv.org/abs/2602.09823v1)
- **PDF**: [https://arxiv.org/pdf/2602.09823v1](https://arxiv.org/pdf/2602.09823v1)

本研究提出Covo-Audio，一个拥有70亿参数、可直接处理连续音频输入并生成音频输出的端到端语言音频大模型（LALM）。通过大规模精选预训练与针对性后训练，该模型在语音-文本建模、口语对话、语音理解、音频理解及全双工语音交互等广泛任务中，取得了同规模模型中最先进或具有竞争力的性能。大量评估表明，预训练基础模型在多项基准测试中展现出强大的语音-文本理解与语义推理能力，优于同规模的代表性开源模型。此外，面向对话优化的变体Covo-Audio-Chat表现出卓越的口语对话能力，包括理解、上下文推理、指令跟随以及生成情境适宜且具共情力的回应，验证了其在现实对话助手场景中的适用性。进一步演进的全双工模型Covo-Audio-Chat-FD在口语对话能力与全双工交互行为上均实现显著提升，展现了其实用场景下的鲁棒性。为降低端到端LALM在自然对话系统中的部署成本，我们提出一种智能-语音解耦策略，将对话智能与语音渲染分离，从而在保持对话性能的同时，以极少量文本转语音（TTS）数据实现灵活的语音定制。总体而言，我们的成果凸显了70亿参数模型在融合复杂音频智能与高层语义推理方面的巨大潜力，并为构建更强大、更通用的LALM提供了一条可扩展的技术路径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **传统级联架构**：依赖独立的ASR、LLM、TTS模块，存在信息丢失和错误传播问题，阻碍真实对话体验。
- **近期大音频语言模型**：如采用Thinker-Talker架构的模型，虽通过中间文本推理步骤提升了文本智能，但牺牲了端到端的语音指令跟随能力和直接对话可控性，且难以处理全双工动态交互。
- **端到端统一模型的痛点**：在开发生产级应用时，普遍存在**智能与音色深度耦合**的问题，导致数据准备困难，并阻碍了灵活的语音定制。

2) **论文核心方法如何解决上述问题**
Covo-Audio 提出了一套端到端的统一架构与训练策略，核心方法包括：

- **层次化三模态语音-文本交错框架**：
    - **三模态融合**：在统一序列中整合连续声学特征、离散语音token和自然语言文本，弥合高保真韵律细节与鲁棒语义结构之间的鸿沟。
    - **层次化策略**：短语级交错实现声学片段与词汇单元的细粒度对齐；句子级交错保持长话语的全局语义完整性和韵律流。这确保了模型在遵循LLM生成模式的同时，能捕捉精确的声学细节。

- **缓解智能-音色耦合的技术**：
    - **智能-音色解耦**：通过多说话人训练，将说话人特征与对话智能解耦。
    - **上下文适应方法**：将高质量的TTS数据重新格式化为带有掩码文本损失的伪对话数据，并纳入训练。这能在仅需少量TTS数据的情况下，保持推理能力的同时实现高保真音色自然度，从而实现经济灵活的语音定制。

- **原生全双工语音交互**：
    - **架构与训练修改**：将音频编码器改为分块流式处理，并以1:4的比例交错用户流和模型流，实现同时听说的低延迟全双工。
    - **训练策略**：将全双工交互直接纳入大规模预训练阶段，随后使用半双工和全双工数据集进行轻量级监督微调。这使得模型能掌握流畅的轮流发言、暂停处理、用户打断和反馈信号等动态。

- **系统化的预训练与后训练**：
    - **两阶段预训练**：第一阶段通过大规模ASR数据对齐音频编码器与LLM；第二阶段通过多任务目标（ASR、TTS、音频建模、语音-文本交错等）深度融合语音与文本模态。
    - **针对性后训练**：设计涵盖通用智能、口语对话、语音理解、语音生成、音频理解的多任务配方，并融入基于KL蒸馏的策略以防止智能退化，同时构建情感感知数据以培养共情能力。

3) **在哪些任务上取得了怎样的效果**
Covo-Audio 及其变体在广泛的音频-语言任务上取得了先进或具有竞争力的效果：
- **语音-文本建模**：在预训练评估中，于A2A故事续写、A2T/T2T任务以及ASR/TTS基准上，性能媲美或超越同规模开源模型（如GLM-4-Voice-Base）。
- **口语对话**：在URO-Bench和VCB Bench上，Covo-Audio-Chat在语音理解、推理、口语对话、指令跟随及鲁棒性方面表现全面优异，尤其在中文推理和对话任务中领先。在VStyle共情基准上，也取得了领先的客观评分。
- **全双工交互**：Covo-Audio-Chat-FD 在URO-Bench上的对话能力大幅超越Moshi和Freeze-Omni等开源基线，同时在轮流发言、暂停处理、打断和反馈等全双工行为指标上达到高成功率（如暂停处理97.6%）。
- **语音与音频理解**：在CoVoST2语音翻译、AIR-Bench副语言理解（如情感识别）以及MMAU/MMSU音频问答基准上，均取得了同规模模型中的最佳或极具竞争力的性能。
</div>

</details>

---

## Evaluation of acoustic Green's function in rectangular rooms with general surface impedance walls
- **Authors**: Matteo Calafà, Yuanxin Xia, Jonas Brunskog, Cheol-Ho Jeong
- **Categories**: eess.AS, cs.CE, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.09594v1](https://arxiv.org/abs/2602.09594v1)
- **PDF**: [https://arxiv.org/pdf/2602.09594v1](https://arxiv.org/pdf/2602.09594v1)

针对壁面完全反射的矩形房间，声学房间模态及格林函数模态展开已广为人知。对于近似刚性边界，也存在一阶近似方法；然而，当前的分析方法无法适用于更一般的边界条件，例如当壁面吸声显著时。本研究通过纳入考虑软壁边界的附加一阶渐近项，对前人研究进行了全面拓展。此外，我们提出了一种半解析、高效且可靠的计算矩形房间内格林函数的方法，并通过数值测试进行了描述与验证。在截断阶数足够大的情况下，所得误差可忽略不计，使得该方法适合作为数值模拟的基准。研究还探讨了谱基正交性与完备性等相关问题，为所提方法的有效性提供了通用框架。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：矩形房间声学格林函数的计算在房间声学中至关重要，但现有方法存在局限。  
- **既有问题**：  
  - 传统方法（如有限元法）因脉冲源奇异性导致收敛慢。  
  - 谱表示法（本征函数展开）需截断无限级数，计算成本高，且仅适用于完美反射或近似刚性边界。  
  - 先前研究仅考虑“硬墙”近似，无法处理高吸声或一般阻抗边界条件。

2)  
- **核心方法**：提出一种半解析、高效可靠的本征函数展开法，计算一般表面阻抗矩形房间的格林函数。  
- **关键创新**：  
  - **扩展一阶渐近分析**：引入四组互补的渐近近似（硬墙、软墙、负电抗、高非对称墙），覆盖任意阻抗值，突破了以往仅限硬墙的限制。  
  - **算法设计**：  
    - 利用渐近解作为牛顿-拉弗森迭代的初值，精确求解本征值方程。  
    - 通过分离变量，将高维问题分解为一维子问题组合。  
    - 基于本征函数正交性和完备性理论，构建格林函数的级数表达式。  
  - **理论验证**：证明了在矩形房间中，本征函数在几乎任意阻抗下均构成完备正交基，确保了展开式的有效性。

3)  
- **验证任务与效果**：  
  - **本征值近似**：数值实验验证了四组渐近解在各自阻抗区间的准确性。  
  - **格林函数计算**：在2D矩形房间中，与高分辨率有限元法结果高度吻合，误差可忽略；计算效率显著优于传统数值方法。  
  - **实验对比**：在真实3D矩形房间的测量中，计算得到的格林函数与实测脉冲响应在模态峰值位置和阻尼程度上均表现出良好一致性。
</div>

</details>

---

## TVTSyn: Content-Synchronous Time-Varying Timbre for Streaming Voice Conversion and Anonymization
- **Authors**: Waris Quamer, Mu-Ruei Tseng, Ghady Nasrallah, Ricardo Gutierrez-Osuna
- **Categories**: eess.AS, cs.CL, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.09389v1](https://arxiv.org/abs/2602.09389v1)
- **PDF**: [https://arxiv.org/pdf/2602.09389v1](https://arxiv.org/pdf/2602.09389v1)

实时语音转换与说话人匿名化需要在保证清晰度与自然度的前提下，实现低延迟的因果合成。现有系统存在核心表征不匹配问题：语音内容随时间变化，而说话人身份却以静态全局嵌入形式注入。本文提出一种可流式合成的语音合成器，通过内容同步的时变音色表征，实现身份与内容在时间粒度上的对齐。全局音色记忆模块将单一音色实例扩展为多个紧凑的音色维度；帧级内容通过注意力机制与该记忆交互，门控机制调节音色变化，球面插值则在保持身份几何结构的同时实现平滑的局部调整。此外，采用分解式向量量化瓶颈对内容进行正则化，以减少残留的说话人信息泄漏。最终系统可实现端到端的流式合成，GPU延迟低于80毫秒。实验表明，相较于当前最优的流式基线模型，本系统在自然度、说话人转换效果和匿名化性能上均有提升，证明了时变音色表征是一种在严格延迟限制下可扩展的隐私保护与高表现力语音合成方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：实时语音转换与说话人匿名化需在低延迟下保持语音清晰自然。现有流式系统存在核心表征不匹配问题：内容表征是时变的，而说话人身份通常被注入为一个静态的全局嵌入向量。
- **既有方法的问题**：这种动态内容与静态身份表征的不匹配限制了语音的表现力，常导致音色过度平滑，尤其在发音、情感或重音变化时。为增强匿名化而采用的强内容瓶颈（如离散化）又会抑制口音、情感等有意义的语音变化，或引入伪影。

2)  
论文提出 **TVTSyn**，一个端到端可流式合成的系统，通过引入**内容同步的时变音色**表征来解决上述问题。其核心方法包含以下几个关键设计：

- **全局音色记忆**：将一个全局说话人嵌入向量扩展为一组紧凑的“音色面”。这些面由可学习的通用音色原型和说话人特定的调制共同构成，从而分解并丰富了音色表征。
- **内容驱动的注意力机制**：在每一帧，内容嵌入通过注意力机制从GTM中检索最相关的音色面，实现了音色与语音内容的帧级同步。
- **门控与球面线性插值**：一个学习的门控网络控制音色变化的程度。通过球面线性插值将检索到的时变音色与全局音色基进行融合，在保持身份几何结构（即说话人整体特征）的同时，允许平滑的局部变化。
- **因式分解的向量量化瓶颈**：在内容编码器中引入VQ瓶颈，将连续特征压缩并离散化，有效减少内容中残留的说话人线索，同时保留语言信息。
- **完全因果的流式架构**：编码器仅使用基于掩码的有限未来信息访问（~80毫秒），解码器完全因果。整个系统在GPU上实现<80毫秒的端到端延迟。

这些设计共同作用，**解决了静态身份与动态内容之间的表征不匹配**，使得音色能够随内容动态、自然地变化，从而在严格延迟约束下，更好地平衡了自然度、说话人相似性与隐私保护。

3)  
论文在**语音转换**和**说话人匿名化**两个任务上评估了TVTSyn的效果：
- **语音转换**：在多个数据集上，TVTSyn在目标说话人相似性上达到最佳（与真实语音的说话人内相似度相当），同时保持了接近最优的语音自然度评分，显著优于其他流式基线模型。
- **说话人匿名化**：遵循VoicePrivacy Challenge 2024协议，TVTSyn在保护隐私（更高的等错误率EER）和保持可用性（更低的词错误率WER）之间取得了优越的平衡。其WER（5.35%）优于其他流式基线，同时实现了强匿名化（EER达47.55%），并有意抑制了情感等副语言信息以增强隐私。
- **实时性能**：在GPU上实现<80毫秒的延迟，在CPU上约132毫秒，实时因子远小于1，满足严格的实时流式处理要求。
</div>

</details>

---

## Performance Comparison of CNN and AST Models with Stacked Features for Environmental Sound Classification
- **Authors**: Parinaz Binandeh Dehaghania, Danilo Penab, A. Pedro Aguiar
- **Categories**: eess.AS, cs.SD
- **arXiv**: [https://arxiv.org/abs/2602.09321v1](https://arxiv.org/abs/2602.09321v1)
- **PDF**: [https://arxiv.org/pdf/2602.09321v1](https://arxiv.org/pdf/2602.09321v1)

环境声音分类（ESC）因其在智慧城市监测、故障检测、声学监控和制造质量控制等领域的广泛应用而受到广泛关注。为提升卷积神经网络（CNN）的性能，特征堆叠技术被用于将互补的声学描述子聚合为更丰富的输入表示。本文研究了基于CNN的模型采用多种堆叠特征组合的效果，包括对数梅尔频谱（LM）、频谱对比度（SPC）、色度特征（CH）、音调网络特征（TZ）、梅尔频率倒谱系数（MFCC）和伽马通倒谱系数（GTCC）。实验在广泛使用的ESC-50和UrbanSound8K数据集上进行，涵盖多种训练策略：在ESC-50上预训练、在UrbanSound8K上微调，并与基于大规模数据集（如AudioSet）预训练的音频频谱变换器（AST）模型进行对比。该实验设计能够分析特征堆叠CNN与基于变换器的模型在不同训练数据规模和预训练多样性下的性能差异。结果表明，当缺乏大规模预训练或充足训练数据时，特征堆叠CNN在计算效率和数据利用效率上更具优势，尤其适用于资源受限和边缘端声音分类场景。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：环境声音分类（ESC）在智能城市监控、故障检测等领域应用广泛。传统方法依赖手工特征（如MFCC）和浅层分类器，但泛化能力有限。CNN能直接从频谱图中学习特征，但难以捕捉音频序列中的长程依赖。
- **既有方法问题**：基于Transformer的模型（如AST）通过自注意力机制建模全局依赖，在大型数据集上表现优异，但其性能严重依赖大规模预训练和大量计算资源，在数据有限或资源受限的实际应用中面临挑战。

2)  
论文核心方法是**采用特征堆叠的轻量级CNN模型**，并与AST进行系统对比，以解决数据效率与计算资源问题。具体方案如下：
- **特征堆叠策略**：
    - 提取多种互补的声学特征，包括Log-Mel频谱图、MFCC、频谱对比度、Chroma、Tonnetz和GTCC。
    - 将所有特征图统一调整为128×128尺寸，并沿通道维度堆叠，形成类似多通道图像的结构，作为CNN的输入。
- **模型设计**：
    - 设计了两种CNN架构：**CNN-1（基线）** 为轻量级四层卷积网络；**CNN-2** 在CNN-1基础上增加了批归一化和Dropout以提升训练稳定性。
    - 模型在ESC-50数据集上预训练，然后在UrbanSound8K数据集上进行微调（仅微调分类层），利用迁移学习适应新数据。
- **与AST的对比**：
    - 在相同数据集（ESC-50、UrbanSound8K）上，系统比较了特征堆叠CNN与AST的性能。
    - 实验设置了多种训练模式，包括从头训练、迁移学习，以及AST在大型数据集AudioSet上的预训练，以分析在不同数据规模和预训练条件下的表现差异。
- **解决的核心问题**：
    - **数据效率**：通过堆叠多种手工特征，为CNN提供了更丰富、互补的声学表示，使其在中等规模数据上就能获得良好性能，降低了对大规模预训练数据的依赖。
    - **计算效率**：轻量级CNN的参数量（约15万）远少于AST（8600万），推理速度快（CNN-1约18毫秒/样本，AST约1.1秒/样本），更适合资源受限的边缘部署场景。

3)  
- **任务与数据集**：在**环境声音分类（ESC）** 任务上，使用**ESC-50**和**UrbanSound8K**两个基准数据集进行评估。
- **取得的效果**：
    - **特征堆叠CNN表现优异**：最佳配置（CNN-1使用MFCC+GTCC+CH+LM特征）在UrbanSound8K上取得了**92.46%** 的验证准确率，显著优于使用单一特征的模型。
    - **对比AST的优势**：在未使用大规模预训练的情况下，特征堆叠CNN的性能（最高92.46%）远超在相同条件下训练的AST（43%）。即使AST经过AudioSet预训练后准确率达99%，但其计算成本极高。
    - **效率突出**：CNN-1在保持高精度的同时，具有最低的推理延迟和参数量，证明了其在计算和资源受限场景下的实用价值。
</div>

</details>

---
