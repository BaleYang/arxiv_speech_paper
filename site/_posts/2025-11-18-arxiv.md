---
layout: post
title: "arXiv Daily – 2025-11-18"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-11-18（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-11-17 08:50 — 2025-11-18 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets
- **Authors**: Máté Gedeon, Piroska Zsófia Barta, Péter Mihajlik, Tekla Etelka Gráczi, Anna Kohári, Katalin Mády
- **Categories**: cs.CL, cs.AI, cs.SD, eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.13529v1](https://arxiv.org/abs/2511.13529v1)
- **PDF**: [https://arxiv.org/pdf/2511.13529v1](https://arxiv.org/pdf/2511.13529v1)

自动语音识别（ASR）的发展主要受益于高资源语言的大规模数据集，而匈牙利语等语言因缺乏自然对话语料库而处于弱势。为弥补这一空白，我们基于匈牙利语语音库BEA中未处理的部分，构建了两个新数据集——BEA-Large与BEA-Dialogue。BEA-Large在BEA-Base基础上扩展了433名说话人的255小时自发语音，并补充了细粒度的分段元数据；BEA-Dialogue则包含85小时自然对话，其按说话人无关方式划分的对话结构可支持会话ASR与说话人日志研究。通过公开ASR模型构建可复现基线，微调后的Fast Conformer模型在自发语音和重复语音上的词错误率分别降至14.18%与4.8%。说话人日志实验的错误率介于13.05%-18.26%，为后续研究提供参照。结果表明会话ASR仍面临严峻挑战，尤其受不流利表达、语音重叠及非正式说话模式的影响。通过发布数据集与基线系统，我们旨在推动匈牙利语语音技术发展，并为其他语言构建自然对话基准提供方法论框架。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2511.13529v1）
</div>

</details>

---

## Systematic evaluation of time-frequency features for binaural sound source localization
- **Authors**: Davoud Shariat Panah, Alessandro Ragano, Dan Barry, Jan Skoglund, Andrew Hines
- **Categories**: eess.AS, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2511.13487v1](https://arxiv.org/abs/2511.13487v1)
- **PDF**: [https://arxiv.org/pdf/2511.13487v1](https://arxiv.org/pdf/2511.13487v1)

本研究系统评估了双耳声源定位中的时频特征设计，重点探究不同条件下特征选择对模型性能的影响。我们采用卷积神经网络模型，分析基于幅度的特征（幅度谱图、双耳电平差-ILD）与基于相位的特征（相位谱图、双耳相位差-IPD）的多种组合效果。针对头相关传递函数失配的域内与域外数据测试表明：精心设计的特征组合往往优于模型复杂度的提升。虽然ILD+IPD的双特征组合已能满足域内定位需求，但要实现多样化内容的泛化定位，则需要结合通道谱图与ILD、IPD的复合特征输入。采用最优特征组合后，我们的轻量级CNN模型取得了具有竞争力的性能。这些发现揭示了特征设计在双耳声源定位中的关键作用，并为特定领域与通用场景的定位任务提供了实践指导。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2511.13487v1）
</div>

</details>

---

## PASE: Leveraging the Phonological Prior of WavLM for Low-Hallucination Generative Speech Enhancement
- **Authors**: Xiaobin Rong, Qinwen Hu, Mansur Yesilbursa, Kamil Wojcicki, Jing Lu
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.13300v1](https://arxiv.org/abs/2511.13300v1)
- **PDF**: [https://arxiv.org/pdf/2511.13300v1](https://arxiv.org/pdf/2511.13300v1)

生成式模型在语音增强任务中展现出卓越性能，其感知质量显著优于传统判别式方法。然而，现有生成式增强方法常忽视强噪声下的幻觉风险，导致语义内容错误或说话人特征失真的语言幻觉与声学幻觉问题。我们认为语言幻觉源于模型对有效音系结构约束的缺失，是更具本质性的挑战。虽然语言模型通过建模离散令牌分布能有效捕捉语音底层结构，但现有方法难以从噪声污染表征中学习，导致先验知识污染并引发幻觉。为突破这些局限，我们提出音系锚定语音增强器（PASE），该生成式增强框架利用预训练WavLM模型中嵌入的鲁棒音系先验来抑制幻觉。首先，通过表征蒸馏将WavLM适配为去噪专家，净化其最终层特征。在模型内在音系先验引导下，该过程在实现鲁棒去噪的同时最大限度降低语言幻觉。为进一步减少声学幻觉，我们采用双流表征训练声码器：高层音素表征提供纯净语义内容，底层声学表征保留说话人身份与韵律特征。实验结果表明，PASE不仅在感知质量上超越主流判别式模型，更以显著降低的语言与声学幻觉率大幅优于现有生成式方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2511.13300v1）
</div>

</details>

---

## Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs
- **Authors**: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang
- **Categories**: cs.SD, cs.AI
- **arXiv**: [https://arxiv.org/abs/2511.13273v1](https://arxiv.org/abs/2511.13273v1)
- **PDF**: [https://arxiv.org/pdf/2511.13273v1](https://arxiv.org/pdf/2511.13273v1)

大型音频语言模型（LALMs）近期在语音识别、音频描述和听觉问答任务中展现出显著进展。然而，这些模型是否能感知空间动态特性（尤其是声源运动）仍不明确。本研究揭示了当前音频语言模型中存在的系统性运动感知缺陷。为探究该问题，我们提出了首个专门评估听觉运动理解能力的基准AMPBench——该受控问答基准旨在检验音频语言模型能否通过双耳音频推断移动声源的方向与轨迹。综合定量与定性分析表明，现有模型难以可靠识别运动线索或区分方向模式，平均准确率低于50%，暴露出听觉空间推理的根本性缺陷。本研究揭示了人类与模型在听觉空间推理能力上的本质差距，不仅为未来音频语言模型的空间认知增强提供了诊断工具，更指明了新的研究方向。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：大型音频语言模型在语音识别、音频描述等语义任务上取得进展，但缺乏对声音空间动态（如声源运动）的感知能力。  
- **既有问题**：  
  - 模型训练数据多为单通道或弱空间化音频，缺乏双耳时间差、强度差等关键空间线索。  
  - 现有模型仅关注语义理解，无法推断声源运动方向或轨迹，导致空间推理能力严重不足。  

2)  
- **提出AMPBench基准**：通过物理模拟生成双耳音频，构建可控的问答任务，专门评估模型对声源运动方向与轨迹的感知能力。  
- **核心方法设计**：  
  - **任务形式**：包括多项选择题（从四个方向选项中识别正确运动轨迹）和判断题（验证文本描述是否与音频运动匹配）。  
  - **音频合成**：在6m×6m空间内模拟声源直线运动，基于双耳时间差、强度差、多普勒效应等物理参数生成音频，确保空间线索主导且非空间干扰最小化。  
  - **任务变体**：  
    - 固定音高运动：测试基础空间感知能力。  
    - 可变音高运动：检验模型分离空间线索与频谱干扰的能力。  
    - 可变速度运动：评估对运动连续性的时间适应性。  
- **解决思路**：通过隔离语义与空间推理，强制模型依赖双耳线索而非语言先验，从而暴露并诊断其运动感知缺陷。  

3)  
- **评估任务**：在AMPBench上对Qwen2-Audio-7B等主流模型进行零样本测试，涵盖方向识别与轨迹验证任务。  
- **效果**：  
  - 模型平均准确率低于50%，多项选择题准确率接近随机猜测（25%），判断题准确率约50%。  
  - 所有模型均无法可靠区分运动方向，且性能对音频信噪比变化不敏感，表明其未有效利用空间线索。  
  - 结果揭示了当前模型在听觉运动感知上的系统性缺陷，为开发空间感知能力更强的音频语言模型提供了诊断基础。
</div>

</details>

---

## FoleyBench: A Benchmark For Video-to-Audio Models
- **Authors**: Satvik Dixit, Koichi Saito, Zhi Zhong, Yuki Mitsufuji, Chris Donahue
- **Categories**: cs.SD, cs.AI, eess.AS
- **arXiv**: [https://arxiv.org/abs/2511.13219v1](https://arxiv.org/abs/2511.13219v1)
- **PDF**: [https://arxiv.org/pdf/2511.13219v1](https://arxiv.org/pdf/2511.13219v1)

视频到音频生成（V2A）技术在影视后期制作、增强现实/虚拟现实及声音设计领域日益重要，尤其适用于生成与屏幕动作同步的拟音效果。拟音任务需同时满足语义层面（与可见事件匹配）与时间层面（与动作时序同步）的音频生成要求。然而，由于缺乏专门针对拟音场景的评估基准，现有评估方法与实际应用需求存在脱节。研究发现，过往评估数据集中74%的视频存在音画关联性弱的问题，且内容过度集中于语音和音乐这类非拟音应用场景。为填补这一空白，我们推出FoleyBench——首个专为拟音式V2A评估设计的大规模基准测试集。该数据集包含5,000组（视频、真实音频、文本描述）三元样本，每个样本均呈现可见声源，且音频与屏幕事件具备因果关联。我们通过自动化可扩展流程，从YouTube和Vimeo的互联网视频中构建该数据集。与既有数据集相比，FoleyBench的视频在专为拟音设计的声学分类体系中覆盖更全面的声音类别。每个片段还标注了声源复杂度、UCS/AudioSet分类及视频时长等元数据，支持对模型性能与失效模式的细粒度分析。通过对多个前沿V2A模型进行基准测试，我们从音频质量、音画对齐、时序同步及音文一致性四个维度开展评估。测试样本详见：https://gclef-cmu.org/foleybench

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：视频到音频生成在影视后期等领域至关重要，但现有评估数据集存在显著问题。  
- **既有方法问题**：  
  - 过去数据集（如VGGSound）中74%的视频音频与视觉内容对齐性差，且以语音和音乐为主，不适用于Foley场景。  
  - 缺乏针对Foley声音的细粒度分类和元数据，无法系统评估模型在不同声音类型和复杂度上的表现。  

2)  
- **核心方法**：提出FoleyBench，首个专为Foley风格视频到音频评估设计的大规模基准。  
- **解决方案**：  
  - **数据集构建**：通过自动化多阶段流程从互联网视频中筛选5,000个高质量视频-音频-文本三元组，确保声音源可见且与视觉事件因果关联。  
  - **内容过滤**：  
    - 使用YAMNet去除含语音或音乐的片段。  
    - 利用Gemini模型进一步剔除音频与视觉不匹配的片段，提升数据质量。  
  - **元数据标注**：为每个片段添加AudioSet/UCS标签、声音类型（离散事件vs连续环境音）和源复杂度（单源vs多源），支持细粒度分析。  
  - **评估维度**：涵盖音频质量、音视频对齐、时间同步和音频-文本一致性，全面衡量模型性能。  

3)  
- **评估任务与效果**：  
  - 在FoleyBench上测试多种先进模型，发现MMAudio在多数指标（如音频质量和同步性）上表现最佳。  
  - 不同模型各有优势：Seeing & Hearing在语义对齐上领先，V-AURA在时间同步上优异，LOVA在长视频音频质量上更优。  
  - 模型在离散事件和背景声音生成上存在挑战，而文本条件能显著提升性能。
</div>

</details>

---

## Towards Practical Real-Time Low-Latency Music Source Separation
- **Authors**: Junyu Wu, Jie Liu, Tianrui Pan, Jie Tang, Gangshan Wu
- **Categories**: cs.SD, cs.MM
- **arXiv**: [https://arxiv.org/abs/2511.13146v1](https://arxiv.org/abs/2511.13146v1)
- **PDF**: [https://arxiv.org/pdf/2511.13146v1](https://arxiv.org/pdf/2511.13146v1)

近年来，音乐源分离的深度学习研究取得显著进展，但实时低延迟方向尚未获得足够关注，而该技术对助听器、音频流实时混音及现场演出等应用具有重要价值。当前研究存在模型规模不断增大的趋势，这限制了其在特定场景的适用性。本文基于双路径TFC-TDF UNET（DTTNet）提出轻量级实时低延迟模型RT-STT，通过通道扩展实现特征融合技术，并论证了单路径建模在实时任务中优于双路径架构。此外，我们探索了量化方法以进一步缩短推理时间。实验表明，RT-STT在参数量显著减少、推理速度大幅提升的同时，性能优于当前最先进模型。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音乐源分离技术广泛应用于音乐混音、信息检索等领域，但现有深度学习方法主要关注离线处理，模型规模大、计算成本高，无法满足实时低延迟场景的需求。  
- **既有问题**：  
  - 现有模型依赖深层网络和非因果结构，导致高延迟和低计算效率。  
  - 实时模型如HS-TasNet参数冗余（如42M参数），限制了实际应用场景。  
  - 性能与离线模型存在显著差距，且对高采样率和未来上下文依赖性强。  

2)  
- **核心方法**：提出轻量级实时低延迟模型RT-STT，基于DTTNet改进，通过以下方法解决上述问题：  
  - **特征融合与通道扩展**：  
    - 在解码器中通过通道扩展卷积合并多源通道，替代独立卷积，增强源间交互建模。  
    - 在减少推理时间的同时提升性能（如cSDR从4.92 dB提升至5.17 dB）。  
  - **单路径建模**：  
    - 用单路径模块（专注时间维度）替代双路径模块（交替建模时间和频率），适应实时场景中较小的窗口尺寸和较浅的网络深度。  
    - 实验表明单路径在性能略优（cSDR从5.16 dB提升至5.17 dB）的同时，推理时间减少0.9 ms。  
  - **量化技术**：  
    - 应用训练后量化（PTQ）将精度降至16位浮点，推理时间从5.8 ms降至1.01 ms，性能无显著下降（cSDR保持5.17 dB）。  
    - 验证量化在音频分离中的有效性，为低资源部署提供支持。  
  - **整体架构优化**：采用浅层网络和因果实现，显著减少参数量（仅383K）和延迟（窗口尺寸1024样本，延迟23 ms）。  

3)  
- **任务与效果**：在MUSDB18-HQ数据集上评估音乐源分离任务，分离目标包括人声、鼓、贝斯和其他乐器：  
  - **性能**：RT-STT的cSDR达5.17 dB，优于当前最佳实时模型HS-TasNet（4.65 dB）。  
  - **效率**：量化后推理速度提升至HS-TasNet的3倍（1.01 ms vs. 3.90 ms），参数量减少至不足1M（HS-TasNet为42M）。  
  - **应用潜力**：适用于助听器、实时混音等低延迟场景，为资源受限平台提供可行方案。
</div>

</details>

---

## A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning
- **Authors**: Liuyi Jin, Pasan Gunawardena, Amran Haroon, Runzhi Wang, Sangwoo Lee, Radu Stoleru, Michael Middleton, Zepeng Huo, Jeeeun Kim, Jason Moats
- **Categories**: cs.LG, eess.AS, eess.IV
- **arXiv**: [https://arxiv.org/abs/2511.13078v1](https://arxiv.org/abs/2511.13078v1)
- **PDF**: [https://arxiv.org/pdf/2511.13078v1](https://arxiv.org/pdf/2511.13078v1)

针对急救医护人员在高压环境下需快速做出关键决策的挑战，本文提出EMSGlass智能眼镜系统。该系统由两大核心技术支撑：EMSNet——首个面向急救服务的多模态多任务模型，以及EMSServe——专为急救场景设计的低延迟多模态服务框架。EMSNet通过融合文本、生命体征与场景图像数据，构建对急救事件的统一实时感知。基于真实多模态急救数据集训练的EMSNet可同步支持五项关键任务，其准确率显著优于当前最优单模态基线。基于PyTorch构建的EMSServe创新性地引入模态感知模型分割器与特征缓存机制，在异构硬件上实现自适应高效推理，同时解决了现场多模态数据异步到达的难题。经优化后，EMSServe在急救场景中的多模态推理速度较直接PyTorch实现提升1.9至11.7倍。六位专业急救人员的用户研究证实，EMSGlass通过直观的镜片交互有效提升实时态势感知能力、决策速度与操作效率。研究中的定性分析进一步为扩展系统功能指明方向，推动多模态智能技术与现实急救工作流的深度融合，助力构建新一代人工智能急救系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

（全文解读失败：404 Client Error: Not Found for url: https://arxiv.org/pdf/2511.13078v1）
</div>

</details>

---
