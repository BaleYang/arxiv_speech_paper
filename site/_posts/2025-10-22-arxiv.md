---
layout: post
title: "arXiv Daily – 2025-10-22"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-10-22（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-10-21 08:50 — 2025-10-22 08:50
- 抓取总数：11 篇 | 本页显示：11 篇（去重/过滤后）

## Diffusion Buffer for Online Generative Speech Enhancement
- **Authors**: Bunlong Lay, Rostislav Makarov, Simon Welker, Maris Hillemann, Timo Gerkmann
- **Categories**: eess.AS, cs.LG, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.18744v1](http://arxiv.org/abs/2510.18744v1)
- **PDF**: [http://arxiv.org/pdf/2510.18744v1](http://arxiv.org/pdf/2510.18744v1)

在线语音增强任务长期由预测模型主导，这类模型的核心优势在于对数据流中的每帧输入信号仅需单次神经网络调用即可完成增强。相比之下，基于生成式方法的语音增强模型通常需多次调用网络，导致计算复杂度难以满足多数在线增强场景的需求。本研究提出扩散缓冲区——一种基于扩散模型的生成式语音增强系统，其在消费级GPU上实现每帧单次网络调用的在线处理能力。该方法的核心理念在于将物理时间与扩散模型去噪步序对齐：通过随时间推进的渐进式去噪机制，使历史帧获得更充分的降噪处理。系统以预设延迟输出增强语音帧，并对应包含前瞻帧信息。本文在前期工作基础上，专门设计了与扩散缓冲区前瞻机制相匹配的二维卷积UNet架构。实验表明该网络能显著提升性能，尤其在低算法延迟条件下表现突出。此外，采用数据预测损失函数替代去噪分数匹配损失，可在推理阶段灵活权衡算法延迟与音质。搭载新型神经网络与损失函数的增强版扩散缓冲区，将算法延迟从320-960毫秒大幅降至32-176毫秒，且性能进一步提升。已有研究表明离线扩散模型在未知噪声场景下优于预测模型，本工作验证了在线运行的扩散缓冲区在未知噪声语音数据上同样超越预测类方法。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：在线语音增强传统上依赖预测模型，因其每帧仅需一次网络调用，计算效率高。生成式模型（如扩散模型）虽在未见数据上表现更优，但需多次网络调用，计算复杂度过高，难以满足在线应用的实时性要求。  
- **既有问题**：扩散模型通过反向过程去噪，需大量神经网络评估，导致计算延迟高（如超过30次调用），无法在消费级GPU上实现在线处理。  

2)  
- **核心方法**：提出**扩散缓冲区**，将扩散时间步与物理时间对齐。缓冲区存储最近帧，新帧置于高噪声步，旧帧逐步去噪，每帧仅需一次网络调用。  
- **关键改进**：  
  - **块因果架构**：设计2D卷积UNet（BC-NCSN++），使其感受野与缓冲区前瞻约束对齐，避免处理零填充数据，提升低延迟下的性能。  
  - **损失函数优化**：用数据预测损失替代去噪分数匹配损失，支持在推理时灵活权衡延迟与质量，并允许动态调整输出帧。  
- **效果提升**：算法延迟从320–960 ms降至32–176 ms，且在消费级GPU（如RTX 2080Ti）上实现在线处理。  

3)  
- **任务与效果**：在语音增强任务中，扩散缓冲区在匹配数据集（EARS-WHAM-v2）上性能接近预测模型，但在未见噪声数据（如脉冲噪声）上显著优于预测模型，具体提升包括：  
  - 更高的语音质量（PESQ）和可懂度（ESTOI）。  
  - 更强的噪声抑制能力（SI-SIR提升约7 dB）。
</div>

</details>

---

## Adapting Language Balance in Code-Switching Speech
- **Authors**: Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel
- **Categories**: cs.CL, cs.LG, cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.18724v1](http://arxiv.org/abs/2510.18724v1)
- **PDF**: [http://arxiv.org/pdf/2510.18724v1](http://arxiv.org/pdf/2510.18724v1)

尽管大型基础模型在标准测试集上表现优异，但在语码转换场景中仍存在明显不足。当数据稀缺无法作为性能不佳的惯常解释时，问题可能源于语码转换片段在训练数据中出现频率较低，且第二语言的嵌入特征往往隐而不显。与其期待模型自行习得这种低频规律，为训练过程引入标注信息或许更为有效。评估模型在语码转换数据上的性能时，需精确定位识别错误影响最大的转换边界点，从而使分析聚焦于这些关键位置的错误。

基于此洞见，我们通过主语言与嵌入语言之间的差异特征来凸显语码转换边界，强化模型在这些关键位置的学习能力。这种简洁而有效的可微分替代机制缓解了生成过程中的语境偏差——即语码转换的核心挑战——从而增强模型鲁棒性。在阿拉伯语与中英双语场景的实验表明，该方法能更准确地预测语码转换位置，其有效性通过替换错误率的显著降低得到验证。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：代码切换（多语言混合）在自动语音识别中具有重要社会语言学意义，但现有大型基础模型在代码切换测试中表现不佳。  
- **既有问题**：训练数据中嵌入语言（如英语）的标记数量远少于矩阵语言，导致模型难以学习切换点；标准词错误率评估无法凸显切换点的错误，掩盖了关键问题。  

2)  
- **核心方法**：提出基于标记权重的交叉熵损失函数，通过为嵌入语言标记分配更高权重（如α=1.5），使模型在训练时重点关注代码切换位置。  
- **实现机制**：  
  - 利用脚本差异（如拉丁字母与阿拉伯文字）自动识别嵌入语言标记。  
  - 在损失计算中加权梯度更新，直接优化切换点的预测能力。  
- **优势**：  
  - 无需额外模块或语言标注，轻量级适配现有模型；  
  - 通过可微损失函数对齐评估指标PIER，显著降低切换点的替换与删除错误。  

3)  
- **任务与效果**：  
  - **阿拉伯语-英语（ArzEn）**：最佳权重（α=1.5）下，PIER相对降低6.45%，嵌入语言替换错误显著减少；  
  - **汉语-英语（ASCEND/SEAME）**：在跨语料测试中，混合错误率降低9.33-26.33%，同时保持矩阵语言识别稳定性。  
- **关键成果**：在代码切换点错误减少与整体识别精度间取得平衡，显著提升模型鲁棒性。
</div>

</details>

---

## Bayesian Low-Rank Factorization for Robust Model Adaptation
- **Authors**: Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel
- **Categories**: cs.CL, cs.LG, cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.18723v1](http://arxiv.org/abs/2510.18723v1)
- **PDF**: [http://arxiv.org/pdf/2510.18723v1](http://arxiv.org/pdf/2510.18723v1)

大规模语音基础模型在众多领域展现出卓越性能，但常需适配本地化需求（如语码转换场景下说话人在同一语句中混合使用多语言）。直接对这些模型进行微调易导致目标域过拟合，并削弱基础模型的广泛能力。为解决该问题，我们探索面向语音基础模型的贝叶斯因子化适配器，通过引入接近零的先验分布获得稀疏化适配矩阵，从而在适应特定领域时保持通用性能。我们将该方法应用于Whisper模型，并在多语种语码转换场景中进行评估。结果表明，在显著降低基础模型灾难性遗忘的同时，仅产生微小的适配损失。相较于LoRA方法，本方案在新领域性能仅下降4%的情况下实现了54%的逆向性能增益。这些发现凸显了贝叶斯适配方法在微调语音基础模型时保持泛化能力的有效性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：大规模语音基础模型在多领域表现优异，但面对特定场景（如语码转换）时需适应本地需求。直接微调易导致目标域过拟合，并削弱基础模型的通用能力。  
- **既有方法问题**：传统低秩适应（LoRA）在部署时需合并适配器权重至基础模型，易引发灾难性遗忘，尤其在分布差异大的数据上微调后，原始任务性能显著下降。  

2)  
- **核心方法**：提出贝叶斯低秩适应（BLoRA），通过以下机制解决过拟合与遗忘问题：  
  - **先验约束**：对LoRA参数施加零均值高斯先验，约束权重更新幅度，鼓励稀疏化适配矩阵。  
  - **变分优化**：引入证据下界（ELBO）目标，结合交叉熵损失与KL散度惩罚，平衡新域适应与基础模型保留。  
  - **稀疏性增强**：通过先验迫使多数权重接近零，仅关键参数显著更新，减少对基础模型潜在空间的破坏性修改。  
- **优势**：  
  - 单步微调中显著降低基础模型性能退化；  
  - 适配器权重分布更稀疏，提升泛化鲁棒性；  
  - 无需多任务数据或复杂路由机制，适用于资源受限场景。  

3)  
- **任务与效果**：在多种语码转换语音识别任务（如西班牙语-英语、阿拉伯语-英语、汉语-英语）上评估：  
  - **新域适应**：BLoRA仅使目标域错误率轻微上升（约4%），接近标准LoRA性能；  
  - **基础模型保留**：相比LoRA，BLoRA将反向评估错误率降低54%，在部分任务中几乎消除遗忘（如SEAME数据集仅退化0.13%）。
</div>

</details>

---

## MLMA: Towards Multilingual with Mamba Based Architectures
- **Authors**: Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti
- **Categories**: cs.CL, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.18684v1](http://arxiv.org/abs/2510.18684v1)
- **PDF**: [http://arxiv.org/pdf/2510.18684v1](http://arxiv.org/pdf/2510.18684v1)

多语言自动语音识别（ASR）始终面临严峻挑战，尤其在平衡高资源与低资源语言性能方面更为突出。序列建模最新进展表明，超越Transformer的架构可能提供更优的可扩展性与效率。本文提出MLMA——基于Mamba架构的多语言ASR建模方法，该架构是专为长上下文序列处理优化的高效状态空间模型。通过Mamba机制，MLMA隐式融合语言感知条件与共享表征，实现对多样化语言的鲁棒识别。在标准多语言基准测试中，MLMA相较基于Transformer的架构展现出可比性能。这些成果印证了Mamba作为可扩展、高效且精准的多语言语音识别骨干网络的巨大潜力。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：多语言自动语音识别（ASR）面临高低资源语言性能不平衡的挑战。现有系统多为语言特定模型，限制了可扩展性，且低资源语言因标注数据稀缺而性能较差。  
- **既有方法问题**：Transformer架构虽在多语言ASR中占主导地位，但存在计算和内存成本高的问题，尤其对长序列和变长语音（如语码转换）处理效率低，难以适应多语言场景的语音速率和韵律多样性。  

2)  
- **核心方法**：MLMA采用Mamba架构（一种高效状态空间模型），结合ConMamba编码器，通过以下机制解决问题：  
  - **高效长序列处理**：Mamba具有线性时间复杂性，能有效建模长距离依赖，降低计算开销，支持多语言语音中的变长输入和时序不规则性。  
  - **局部与全局特征整合**：ConMamba编码器融合卷积层（提取局部特征）和Mamba层（捕获全局依赖），提升对多语言声学和语音结构的建模能力。  
  - **语言感知与共享表示**：模型隐式集成语言感知条件机制，利用跨语言共享表示，增强对低资源语言和语码转换的鲁棒性。  
  - **可扩展性**：Mamba的流式处理机制（如lookahead和UMA）支持实时ASR，并允许添加语言而无显著计算负担。  

3)  
- **任务与效果**：在涵盖6种欧洲语言的多语言基准测试中，MLMA取得以下成果：  
  - **多语言ASR**：在CommonVoice、VoxPopuli等数据集上，词错误率（WER）表现稳健（如英语WER 23.2%，意大利语13.0%），优于Conformer基线。  
  - **跨语言泛化**：在未训练数据（如FLEURS）上保持竞争力，显示强泛化能力。  
  - **效率提升**：在保持准确性的同时，推理速度显著快于Transformer模型，验证了其作为高效多语言ASR骨干的潜力。
</div>

</details>

---

## Noise-Conditioned Mixture-of-Experts Framework for Robust Speaker Verification
- **Authors**: Bin Gu, Lipeng Dai, Huipeng Du, Haitao Zhao, Jibo Wei
- **Categories**: cs.SD, cs.MM, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.18533v1](http://arxiv.org/abs/2510.18533v1)
- **PDF**: [http://arxiv.org/pdf/2510.18533v1](http://arxiv.org/pdf/2510.18533v1)

在噪声干扰下实现稳健的说话人验证仍面临严峻挑战。传统深度学习方法通过构建抗多种背景噪声的统一说话人表征空间取得显著进展，而本文提出一种噪声条件混合专家框架，将特征空间解耦为专用于说话人验证的噪声感知子空间。具体而言，我们设计了噪声条件专家路由机制、基于通用模型的专家特化策略以及信噪比衰减课程学习方案，共同提升模型在多元噪声环境下的鲁棒性与泛化能力。该方法能依据输入信号中的噪声信息自动路由至专家网络，各专家在保持说话人身份特征的同时分别聚焦不同噪声特性。综合实验表明，本方法在各项基准测试中均保持稳定优势，证实显式构建噪声相关特征模型可在不牺牲验证精度前提下显著增强系统鲁棒性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1.  
- **研究背景**：说话人验证在真实噪声环境中性能显著下降，现有方法主要分为两类：  
  - 语音增强前端：通过降噪提升输入质量，但架构复杂且存在误差传播问题。  
  - 噪声不变表征学习：依赖统一特征空间对齐干净与噪声样本，可能削弱说话人判别性，且多损失函数调优复杂。  
- **核心问题**：统一特征空间难以兼顾噪声鲁棒性与说话人特征保真度，极端噪声下性能受限。

2.  
- **框架设计**：提出噪声条件混合专家框架，将特征空间分解为噪声感知子空间，通过轻量噪声分类器动态路由输入至专用专家网络。  
- **核心方法**：  
  - **噪声条件专家路由**：基于输入频谱估计噪声类型，通过温度缩放Softmax分配专家权重，训练时聚合所有专家梯度，推理时仅激活最优专家，平衡性能与效率。  
- **通用模型专家特化**：  
  - 阶段一：所有专家共享参数学习通用表征，构建基础模型。  
  - 阶段二：引入噪声条件加权梯度，使专家逐步特化至不同噪声场景，保留通用损失以维持跨条件判别力。  
- **信噪比衰减课程学习**：  
  - 训练时按指数衰减规律逐步降低增强数据的信噪比，从易到难提升模型适应性。  
  - 采用截断高斯分布采样，平衡课程进度与数据多样性，保障训练稳定性。  
- **优势**：通过显式噪声建模与条件子空间分解，避免统一空间中的特征折衷，显著提升噪声鲁棒性。

3.  
- **任务与效果**：在VoxCeleb1数据集上添加MUSAN与Nonspeech100噪声进行评测：  
  - 在0–20dB信噪比范围内，相对基线平均等错误率降低12.6%（3.73%→3.26%），在音乐、多人声等噪声下提升显著。  
  - 跨域测试中，对未见过噪声（Nonspeech100）平均错误率降低27%（4.92%→3.59%），验证泛化能力。  
- **对比优势**：超越语音增强与表征学习的主流方法（如Diff-SV、NISRL），在极端噪声（0dB）下错误率降低超20%。
</div>

</details>

---

## A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker Verification
- **Authors**: Bin Gu, Lipeng Dai, Huipeng Du, Haitao Zhao, Jibo Wei
- **Categories**: cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.18530v1](http://arxiv.org/abs/2510.18530v1)
- **PDF**: [http://arxiv.org/pdf/2510.18530v1](http://arxiv.org/pdf/2510.18530v1)

在噪声环境下学习鲁棒的说话人表征面临显著挑战，需同时兼顾判别性与噪声不变性。本文提出一种基于锚点的分阶段学习策略，用于鲁棒说话人表征学习。具体而言，该方法首先训练基础模型以建立判别性说话人边界，随后从中提取锚点嵌入作为稳定参照。最后复制基础模型并在噪声输入上进行微调，通过强制其逼近对应的固定锚点嵌入实现正则化，从而在失真条件下保持说话人身份。实验结果表明，该策略相较于传统联合优化方法更具优势，尤其在提升噪声鲁棒性的同时保持判别能力。由于能够分别处理边界稳定与变异抑制，所提方法在多种噪声条件下均展现出稳定改进。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：说话人验证在真实噪声环境中性能显著下降，需同时学习区分性特征和噪声不变特征。  
- **既有方法问题**：  
  - 现有方法依赖联合优化多损失函数，需精细平衡类内紧凑性和类间分离性。  
  - 过度压缩类内距离可能导致说话人间决策边界模糊，而过度强调类间分离会阻碍同一说话人的干净与噪声样本对齐。  
  - 训练过程复杂，超参数调整困难，易导致次优性能。  

2)  
- **核心方法**：提出分阶段学习策略，通过固定锚点引导框架解决噪声鲁棒性与区分性间的平衡问题。  
- **具体步骤**：  
  - **阶段一**：训练基础模型，专注于从干净语音中学习区分性说话人边界，生成锚点嵌入作为稳定参考。  
  - **阶段二**：复制基础模型，处理噪声输入，通过最小化噪声样本嵌入与对应固定锚点嵌入的距离（使用指数余弦距离），增强噪声鲁棒性。  
  - 同时加入分类损失作为正则化，防止类间可分性崩溃。  
- **创新点**：  
  - 固定锚点分支提供稳定学习信号，避免动态更新导致的振荡和模型坍塌。  
  - 分阶段优化将边界稳定性和变异抑制分离，简化训练过程。  

3)  
- **任务与效果**：在VoxCeleb1数据集上评估，覆盖多种噪声类型（如Babble、Music）和信噪比条件。  
- **结果**：  
  - 在干净和噪声条件下均优于基线及现有方法（如NDAL、NISRL），平均EER降低至3.06%。  
  - 在未知噪声（Nonspeech100）上表现优异，验证了强泛化能力。  
  - 可视化显示类内方差显著抑制，类间分离保持清晰，提升了噪声环境下的鲁棒性。
</div>

</details>

---

## ProLAP: Probabilistic Language-Audio Pre-Training
- **Authors**: Toranosuke Manabe, Yuchi Ishikawa, Hokuto Munakata, Tatsuya Komatsu
- **Categories**: eess.AS, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.18423v1](http://arxiv.org/abs/2510.18423v1)
- **PDF**: [http://arxiv.org/pdf/2510.18423v1](http://arxiv.org/pdf/2510.18423v1)

现有语言-音频联合表征学习框架通常依赖确定性嵌入，假设音频与文本存在一一对应关系。然而现实场景中，语言-音频本质上是多对多关系：同一音频片段可由多种文本描述，反之亦然。为此，我们提出概率化语言-音频预训练模型ProLAP，通过联合嵌入空间中的概率分布扩散来建模这种多元特性。为有效训练模态内层次关系，我们引入两个目标函数：（一）层次包含损失，增强对输入语义层次结构的理解；（二）掩码排斥损失，在优化层次包含损失时提升学习效率。该训练策略使模型即使从小规模数据集中也能学习数据内在的层次结构，这与先前依赖大规模数据集的概率方法形成鲜明对比。实验表明，ProLAP在音频-文本检索任务上优于现有确定性方法。此外，通过本文提出的音频遍历任务实验，我们验证了ProLAP能够捕捉合理的语义层次结构。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：现有语言-音频联合表示学习框架（如CLAP）通常依赖确定性嵌入，假设音频与文本间存在一对一对应关系。  
- **问题**：现实场景中，音频与文本的关系本质上是多对多的，例如同一音频片段可由多个不同描述（如“吉他声”或“弦乐器声”）表示，而确定性方法无法捕捉这种不确定性和语义层次结构。  

2)  
- **核心方法**：ProLAP将输入（音频或文本）建模为联合嵌入空间中的概率分布（高斯分布），通过以下损失函数解决多对多关系和语义层次学习问题：  
  - **概率对比损失（PPCL）**：基于修正相似度度量，替代传统余弦相似度，以对齐跨模态分布。  
  - **包含损失**：建模跨模态（音频⊂文本）和模态内（掩码输入⊂原始输入）的不确定性包含关系，例如文本描述比对应音频更不确定。  
  - **分层包含损失**：通过递归掩码层级（如M₀→Mₗ）强制模型学习细粒度语义层次（如“乐器→吉他→电吉他”）。  
  - **掩码排斥损失**：防止掩码输入的表征退化，通过负样本对比学习提升掩码内容的区分性。  
- **优势**：结合上述损失，ProLAP仅需小规模数据即可学习语义层次，而无需依赖大规模数据集。  

3)  
- **任务与效果**：  
  - **音频-文本检索**：在AudioCaps和ClothoV2数据集上，ProLAP在R@1、R@5等指标上均优于CLAP及其变体，尤其在跨领域数据中表现更鲁棒。  
  - **音频遍历任务**：新任务中，ProLAP通过嵌入空间路径检索层次化文本，在HierarAudioCaps上R@1达15.16%，显著优于基线，证明其能有效捕获语义层次关系。
</div>

</details>

---

## SegTune: Structured and Fine-Grained Control for Song Generation
- **Authors**: Pengfei Cai, Joanna Wang, Haorui Zheng, Xu Li, Zihao Ji, Teng Ma, Zhongliang Liu, Chen Zhang, Pengfei Wan
- **Categories**: cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.18416v1](http://arxiv.org/abs/2510.18416v1)
- **PDF**: [http://arxiv.org/pdf/2510.18416v1](http://arxiv.org/pdf/2510.18416v1)

当前歌曲生成技术虽能基于歌词或全局文本提示生成音乐，但普遍难以建模歌曲随时间变化的属性，导致对音乐结构与动态特性的细粒度控制不足。本文提出SegTune——一种支持结构化精细控制的非自回归歌曲生成框架。该框架通过用户或大语言模型为歌曲段落指定局部音乐描述，实现分段层级的控制：分段提示经时间维度广播至对应时间窗口，而全局提示则作用于整首歌曲以保持风格一致性。为精准划分段落时长并实现歌词与音乐的精确对齐，我们设计了基于大语言模型的时长预测器，以自回归方式生成LRC格式的带时间戳歌词。此外，我们构建了大规模数据流水线用于采集含对齐歌词与提示的高质量歌曲，并提出新评估指标以衡量段落对齐精度与人声属性一致性。实验表明，SegTune在可控性与音乐连贯性方面均优于现有基线模型。演示内容详见：https://cai525.github.io/SegTune_demo

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：歌曲生成技术已能基于歌词或全局文本提示生成歌曲，但现有系统大多无法建模歌曲随时间变化的属性。  
- **既有问题**：  
  - 依赖全局文本描述，难以捕捉音乐动态变化（如乐器、情感在段落间的差异）。  
  - 非自回归架构需同时处理作曲与音频渲染，模型容量压力大。  
  - 缺乏对特定段落（如副歌）的精细化控制，无法满足专业创作需求。  

2)  
- **核心方法**：SegTune提出非自回归框架，支持全局与段落级双粒度控制。  
  - **段落级提示注入**：通过提示编码器将用户或LLM生成的段落描述广播至对应时间窗口，全局提示覆盖全曲以保持风格一致性。  
  - **LLM时长预测**：微调Qwen3-4B模型，自回归生成带时间戳的LRC格式歌词，解决以往依赖人工指定时长的问题，提升歌词与音乐对齐精度。  
  - **数据与评估优化**：构建大规模高质量歌曲流水线，包含歌词对齐与多级文本描述；新增段落对齐度与歌手属性一致性指标，完善评估体系。  
- **技术优势**：  
  - 实现动态属性（如情绪强度、乐器编排）的局部调控。  
  - 通过条件流匹配训练与分类器无关引导，平衡生成质量与控制灵活性。  

3)  
- **任务与效果**：  
  - **歌曲生成**：在中文流行歌曲数据集上，SegTune在AudioBox-aesthetic与SongEval指标中均优于基线（如YuE、DiffRhythm+），尤其在连贯性、结构清晰度方面提升显著。  
  - **指令跟随**：全局Mulan分数达0.47，段落级对齐分数领先；歌手性别控制准确率最高达96.7%，年龄控制精度超基线模型。  
  - **消融验证**：段落控制策略与LLM时长预测模块均被证明对提升音乐质量与控制精度关键。
</div>

</details>

---

## MVDR Beamforming for Cyclostationary Processes
- **Authors**: Giovanni Bologni, Martin Bo Møller, Richard Heusdens, Richard C. Hendriks
- **Categories**: eess.AS, cs.SD
- **arXiv**: [http://arxiv.org/abs/2510.18391v1](http://arxiv.org/abs/2510.18391v1)
- **PDF**: [http://arxiv.org/pdf/2510.18391v1](http://arxiv.org/pdf/2510.18391v1)

传统声学波束成形器假设噪声在短时帧内平稳，这一前提限制了其对准周期性噪声源（如乐器、风扇、发动机）中频间相关性的利用。此类信号具有周期性时变统计特性，更适合用循环平稳过程建模。本文提出循环最小方差无失真响应（cMVDR）波束成形器，通过扩展传统MVDR框架，联合利用空间与频谱相关性以提升降噪性能，尤其在低信噪比场景中。该方法基于频移刷新（FRESH）滤波技术，通过组合输入信号的频移副本实现对跨频率相干分量的抑制或增强。针对谐波分量偏离基频整数倍的非谐波问题，我们提出数据驱动策略：通过周期图分析估计共振频率，并基于频率间隔计算频移量。理论与实验结果表明，性能提升与频谱相关性强度正相关。在真实录音数据中，cMVDR相较于MVDR在尺度不变信噪比（SI-SDR）上最高可获得5dB增益，且仅需单麦克风即可保持有效性。代码已开源：https://github.com/Screeen/cMVDR。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：传统声学波束成形器（如MVDR）假设噪声在短时帧内平稳，忽略了周期性噪声源（如乐器、风扇）的频率间相关性。  
- **既有问题**：  
  - 平稳性假设限制了利用周期噪声的频谱冗余性。  
  - 谐波方法假设噪声频率为基频整数倍，但实际信号存在非谐波性偏差，导致性能下降。  
  - 现有方法依赖先验循环频率知识，在声学场景中不切实际。  

2)  
- **核心方法**：提出循环MVDR（cMVDR），基于FRESH滤波理论，通过频移信号构建多频带模型，联合利用空间和频谱相关性。  
- **解决思路**：  
  - **多频带建模**：将接收信号与其频移版本拼接，形成扩展的频谱-空间协方差矩阵。  
  - **优化目标**：在保持目标信号无失真约束下，最小化包含频移分量的接收功率。  
  - **数据驱动频移选择**：  
    - 通过周期图估计噪声谐振频率。  
    - 使用差值策略（Δ策略）计算频移，适应非谐波特性。  
    - 基于频谱相干性阈值筛选有效频移，提升计算效率。  
  - **理论优势**：噪声频谱相关性越强，cMVDR的降噪效果越显著，尤其在低信噪比下优于传统MVDR。  

3)  
- **任务与效果**：  
  - **合成噪声实验**：在谐波噪声中，cMVDR比MVDR提升最高10 dB SI-SDR；非谐波噪声下Δ策略仍保持优势。  
  - **真实噪声实验**：针对乐器、引擎等周期性噪声，cMVDR在低SNR时SI-SDR提升达5 dB，STOI改善0.1，单麦克风场景也有效。  
  - **鲁棒性**：在混响和低SNR环境中，cMVDR频谱线抑制能力更强，语音清晰度显著提升。
</div>

</details>

---

## ParaStyleTTS: Toward Efficient and Robust Paralinguistic Style Control for Expressive Text-to-Speech Generation
- **Authors**: Haowei Lou, Hye-Young Paik, Wen Hu, Lina Yao
- **Categories**: cs.SD, eess.AS
- **arXiv**: [http://arxiv.org/abs/2510.18308v1](http://arxiv.org/abs/2510.18308v1)
- **PDF**: [http://arxiv.org/pdf/2510.18308v1](http://arxiv.org/pdf/2510.18308v1)

在语音合成系统中控制说话风格已成为学界与工业界日益关注的焦点。现有方法多依赖参考音频引导风格生成，但此类方案常因隐私顾虑与可获取性限制难以实际应用。近期虽有研究利用大语言模型通过自然语言提示实现风格控制，但其存在计算成本高、可解释性弱及对提示词表述敏感等缺陷，制约了在实时与资源受限环境中的适用性。本文提出ParaStyleTTS——一种轻量化且可解释的语音合成框架，仅通过文本提示即可实现富有表现力的风格控制。该框架采用创新的双层风格适配架构，将韵律特征与副语言语音风格建模分离，从而实现对情感、性别、年龄等要素的细粒度鲁棒控制。相较于基于大语言模型的方法，ParaStyleTTS在不同提示表述下均能保持稳定的风格呈现，尤其适合设备端与低资源场景的实际部署。实验表明，本方法在生成高质量语音时性能媲美最先进的大语言模型系统，同时推理速度提升30倍，参数总量减少8倍，CUDA内存占用降低2.5倍。此外，ParaStyleTTS在副语言说话风格控制方面展现出更优的鲁棒性与可控性，为风格可控的语音合成提供了实用高效的解决方案。演示页面见https://parastyletts.github.io/ParaStyleTTS_Demo/，代码仓库见https://github.com/haoweilou/ParaStyleTTS。

<details>
<summary>详细解读</summary>

<div markdown="1">

1. **研究背景与既有方法的问题**
- **参考音频方法**：依赖参考音频引导风格生成，但存在隐私问题和可访问性限制，难以广泛应用。
- **大语言模型方法**：通过自然语言提示控制风格，但存在以下问题：
  - 计算成本高，不适合实时或资源受限环境。
  - 缺乏可解释性，风格控制不透明。
  - 对提示措辞敏感，风格实现不稳定。

2. **论文核心方法如何解决上述问题**  
ParaStyleTTS提出一种轻量级、可解释的TTS框架，通过以下创新解决既有问题：
- **两级风格适配架构**：
  - **韵律风格适配器**：在音素级别建模韵律特征（如音调、重音），使用门控Tanh单元融合音素与韵律嵌入，实现细粒度控制。
  - **副语言风格适配器**：在句子级别建模全局特征（如情感、年龄、性别），通过FiLM机制将风格嵌入调制到音素表示中，确保风格一致性。
- **端到端设计**：
  - 直接生成波形，无需外部声码器或复杂流水线，提升效率。
  - 结合变分自编码器和标准化流，学习富有表现力的潜在表示。
- **独立处理风格与内容**：
  - 音素序列与风格提示分别编码，避免大语言模型中风格与内容的隐式纠缠。
  - 时间复杂度为O(N² + M²)，优于大语言模型的O((N+M)²)，减少计算开销。
- **轻量化优势**：
  - 模型参数减少8倍，推理速度提升30倍，CUDA内存使用降低2.5倍。
  - 支持实时部署和边缘设备应用。

3. **在哪些任务上取得了怎样的效果**  
- **多语言语音生成**：在英语和中文任务中，生成语音的智能性MOS达4.65，自然性MOS达4.36，接近最优的大语言模型基线。
- **副语言风格控制**：
  - 情感分类准确率达54.00%，年龄分类准确率达57.50%，性别分类准确率达100%，均优于CosyVoice。
  - t-SNE可视化显示风格嵌入形成清晰聚类，证明模型能区分不同年龄、性别和情感。
- **资源效率**：在NVIDIA 3060 Ti GPU上，推理时间仅121毫秒，模型参数量为52M，适用于低资源环境。
</div>

</details>

---

## Adaptive Per-Channel Energy Normalization Front-end for Robust Audio Signal Processing
- **Authors**: Hanyu Meng, Vidhyasaharan Sethu, Eliathamby Ambikairajah, Qiquan Zhang, Haizhou Li
- **Categories**: eess.AS, cs.SD, eess.SP
- **arXiv**: [http://arxiv.org/abs/2510.18206v1](http://arxiv.org/abs/2510.18206v1)
- **PDF**: [http://arxiv.org/pdf/2510.18206v1](http://arxiv.org/pdf/2510.18206v1)

在音频信号处理中，可学习前端通过优化任务特定表征，已在多种任务中展现出优异性能。然而这类前端一旦训练完成，其参数即固定不变，导致推理过程缺乏灵活性，在动态复杂声学环境下的鲁棒性受限。本文提出一种新型自适应音频前端范式，通过闭环神经控制器替代静态参数化方法。具体而言，我们简化了可学习前端LEAF架构，集成神经控制器实现动态调节的逐通道能量归一化，从而获得自适应表征能力。该神经控制器同时利用当前与缓存的过往子带能量，在推理过程中实现输入依赖的自适应调节。在多类音频分类任务上的实验结果表明，无论在纯净还是复杂声学条件下，所提出的自适应前端均持续优于现有固定式与可学习前端。这些发现揭示了神经自适应机制将成为新一代音频前端的重要发展方向。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频信号处理中，可学习前端通过优化任务特定表征在多任务中表现优异，但训练后参数固定，缺乏推理时的灵活性，限制了在动态复杂声学环境下的鲁棒性。  
- **既有方法问题**：传统可学习前端无法适应输入条件变化（如信号内容或环境噪声），导致在复杂场景下性能下降；现有自适应方法多聚焦于频谱分解滤波器调整，尚未充分利用子带能量归一化的动态适应潜力。  

2)  
- **简化PCEN**：将原始四参数PCEN简化为仅含两个关键参数（SimpPCEN），去除影响较小的偏移和平滑因子，保留核心增益控制参数，在保持性能的同时提升效率。  
- **自适应PCEN模块**：基于LEAF框架引入APCEN，通过神经控制器动态调整SimpPCEN参数，实现时频依赖的动态范围压缩，适应输入信号的能量变化。  
- **神经控制器设计**：  
  - 输入为当前帧能量与历史帧输出，通过双向GRU捕捉时序动态；  
  - 使用两层MLP预测参数，Sigmoid约束范围，确保稳定性；  
  - 控制器仅4.32K参数，轻量高效，支持实时推理。  
- **解决核心问题**：通过动态参数调整，前端能根据声学上下文自适应增强关键特征并抑制干扰，提升在噪声、背景音乐等复杂环境下的鲁棒性。  

3)  
- **任务与效果**：在音频分类任务（声学场景、音乐流派、情感识别、说话人识别）上评估：  
  - 干净条件下，LEAF-APCEN在多数任务中准确率最高，情感识别提升8.40%，说话人识别提升14.47%；  
  - 复杂声学条件下（含噪声、响度变化），其性能显著优于固定与可学习前端，尤其在音乐分类中防止后端分类器崩溃，保持稳定性。  
- **优势**：自适应机制通过时变增益平衡噪声抑制与语音保留，增强特征判别性，验证了神经适应性在提升泛化与鲁棒性方面的有效性。
</div>

</details>

---
