---
layout: post
title: "arXiv Daily – 2026-01-16"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2026-01-16（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2026-01-15 08:50 — 2026-01-16 08:50
- 抓取总数：7 篇 | 本页显示：7 篇（去重/过滤后）

## VoiceSculptor: Your Voice, Designed By You
- **Authors**: Jingbin Hu, Huakang Chen, Linhan Ma, Dake Guo, Qirui Zhan, Wenhao Li, Haoyu Zhang, Kangxiang Xia, Ziyu Zhang, Wenjie Tian, Chengyou Wang, Jinrui Liang, Shuhan Guo, Zihang Yang, Bengu Wu, Binbin Zhang, Pengcheng Zhu, Pengyuan Xie, Chuan Xie, Qiang Zhang, Jie Liu, Lei Xie
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2601.10629v1](https://arxiv.org/abs/2601.10629v1)
- **PDF**: [https://arxiv.org/pdf/2601.10629v1](https://arxiv.org/pdf/2601.10629v1)

尽管文本转语音（TTS）技术发展迅速，但开源系统仍缺乏真正遵循指令、对核心语音属性（如音高、语速、年龄、情感和风格）进行细粒度控制的能力。本文提出VoiceSculptor，一个开源统一系统，通过将基于指令的语音设计与高保真语音克隆集成于单一框架，填补了这一空白。该系统能够直接从自然语言描述生成可控的音色，支持通过检索增强生成（RAG）进行迭代优化，并提供跨多维度属性级编辑。设计完成的语音将被渲染为提示波形，并输入克隆模型，从而为下游语音合成实现高保真音色迁移。VoiceSculptor在InstructTTSEval-Zh评测中达到开源领域最先进水平，并已全面开源代码与预训练模型，以推动可复现的指令控制TTS研究发展。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：当前开源文本转语音系统在遵循自然语言指令、对音高、语速、年龄、情感等核心语音属性进行细粒度控制方面存在不足。  
- **既有方法问题**：  
  - 传统非大语言模型方法依赖预定义提示或学习到的嵌入，可扩展性和表达能力有限。  
  - 近期基于大语言模型的方法通过嵌入对齐实现控制，但将多维语音属性压缩为低带宽表示，导致控制粗糙、隐式，难以精确操纵单个属性或忠实解释复杂组合指令。

2)  
论文提出 **VoiceSculptor**，一个统一的开源框架，通过以下核心方法解决上述问题：  
- **基于思维链的细粒度属性建模**：  
  - 将高层自然语言指令显式分解为跨多个声学与风格属性的结构化推理步骤。  
  - 将这些步骤建模为辅助属性标记，引导模型逐步解释抽象描述并映射到具体声学实现，实现对韵律、风格和说话人特征的精确解耦控制。  
- **检索增强生成**：  
  - 在推理时检索语义相关的指令示例和属性知识，支持迭代指令优化并提升对域外描述的泛化能力。  
- **统一框架集成**：  
  - 将语音设计模块与语音克隆模块结合，设计出的语音可作为提示波形输入下游合成模型（如CosyVoice2），实现高保真音色转换。  
- **训练策略增强**：  
  - 引入随机属性标记丢弃策略，防止模型过度依赖显式属性标记，鼓励其从指令和上下文推断属性，提升鲁棒性。  
  - 使用文本侧交叉熵损失联合建模指令文本与音频标记，加强文本理解与指令跟随能力。

3)  
- **主要任务**：在中文指令跟随TTS基准 **InstructTTSEval-Zh** 上进行评估。  
- **取得效果**：  
  - 在属性感知与合成准确率、描述-语音一致性、响应精确度等多项指标上，超越了所有开源及部分商业基线模型。  
  - 其语音设计模块在开源模型中达到了最先进的性能，实现了对自然语言指令的准确解释和可控语音属性的可靠生成。  
  - 验证了设计出的语音表示能有效作为条件信号，用于下游语音合成任务，支持语音设计与合成的解耦部署。
</div>

</details>

---

## HeartMuLa: A Family of Open Sourced Music Foundation Models
- **Authors**: Dongchao Yang, Yuxin Xie, Yuguo Yin, Zheyu Wang, Xiaoyu Yi, Gongxi Zhu, Xiaolong Weng, Zihan Xiong, Yingzhe Ma, Dading Cong, Jingliang Liu, Zihang Huang, Jinghan Ru, Rongjie Huang, Haoran Wan, Peixu Wang, Kuoxi Yu, Helin Wang, Liming Liang, Xianwei Zhuang, Yuanyuan Wang, Haohan Guo, Junjie Cao, Zeqian Ju, Songxiang Liu, Yuewen Cao, Heming Weng, Yuexian Zou
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.10547v1](https://arxiv.org/abs/2601.10547v1)
- **PDF**: [https://arxiv.org/pdf/2601.10547v1](https://arxiv.org/pdf/2601.10547v1)

本文介绍了一系列开源音乐基础模型，旨在推动跨任务与跨模态的大规模音乐理解与生成。该框架包含四个核心组件：(1) HeartCLAP，一种音频-文本对齐模型；(2) HeartTranscriptor，专为真实音乐场景优化的鲁棒歌词识别模型；(3) HeartCodec，一种低帧率（12.5 Hz）高保真音乐编解码器，能够在捕捉长程音乐结构的同时保留细粒度声学细节，并支持高效的自回归建模；(4) HeartMuLa，一种基于大语言模型的歌曲生成模型，能够在丰富的用户可控条件下（如文本风格描述、歌词及参考音频）合成高保真音乐。此外，该模型提供两种专用模式：(i) 细粒度音乐属性控制，允许用户通过自然语言提示指定歌曲不同段落（如前奏、主歌、副歌）的风格；(ii) 简短动感的音乐生成，适用于短视频背景音乐。最后，当模型参数扩展至70亿时，HeartMuLa性能显著提升。我们首次证明，利用学术规模的数据与GPU资源即可复现达到Suno级别的商业级系统。我们期望这些基础模型能为未来研究提供有力基准，并推动多模态内容生产的实际应用。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：音乐生成与理解领域快速发展，但现有系统存在显著局限。
- **问题**：
  - **封闭性**：许多模型依赖专有数据集或闭源流程，限制了可复现性与下游研究。
  - **控制粗糙**：对音乐属性的控制通常较粗粒度，缺乏文本描述与音频实现间的鲁棒对齐。
  - **长程连贯性不足**：难以在长片段（如完整歌曲）中保持音乐结构的长期连贯性。
  - **多条件生成挑战**：联合基于风格描述、歌词和参考音频进行端到端可控歌曲生成仍具挑战。

2) **论文核心方法如何解决上述问题**
论文提出了一个开源音乐基础模型家族 **HeartMuLa**，通过四个核心组件构建统一框架，以解决上述问题：

- **HeartCLAP（音频-文本对齐模型）**：
  - 学习音乐语义的共享嵌入空间，实现精确的音乐标注和跨模态检索。
  - 为下游生成任务提供对齐基础，改善了文本描述与生成音频之间的语义一致性。

- **HeartTranscriptor（歌词识别模型）**：
  - 针对真实音乐场景（复杂伴奏、歌唱语音）优化，提供准确的歌词转录。
  - 通过高质量数据集微调Whisper模型，显著提升了在音乐音频上的识别鲁棒性，为生成提供准确的歌词条件。

- **HeartCodec（音乐编解码器）**：
  - **低帧率（12.5 Hz）与高保真**：在极低帧率下捕获长程音乐结构，同时保留细粒度声学细节。
  - **高效建模**：其紧凑的离散表示支持高质量重建，并实现了高效的自回归建模，解决了长序列建模的扩展性问题。
  - **分层重建**：采用基于流匹配的解码器，结合Reflow蒸馏和SQ-Codec微调，在保证质量的同时提升了推理效率。

- **HeartMuLa（基于LLM的歌曲生成模型）**：
  - **分层架构**：采用全局-局部Transformer。全局模型预测粗粒度语义（RVQ第0层），局部模型预测细粒度声学细节（剩余层），兼顾效率与保真度。
  - **多条件控制**：接受丰富的用户可控条件，包括文本风格描述、带结构标记的歌词以及参考音频嵌入。
  - **渐进式训练范式**：包含热身、预训练、监督微调和直接偏好优化（DPO）四个阶段，逐步提升模型的长程依赖建模、结构控制和感知质量。
  - **推理加速**：通过KV-Cache对齐、FlashAttention和CUDA Graph等系统级优化，显著降低了长序列生成时的延迟。

**整体贡献**：该框架首次证明，利用学术规模的数据和GPU资源，可以复现出Suno级别的商业级系统。其开源特性促进了可复现性、可扩展性和社区广泛采用。

3) **在哪些任务上取得了怎样的效果**
- **音乐编解码（HeartCodec）**：在VISQOL、FAD、FD等客观重建指标上达到SOTA，同时保持了高语音保真度（低WER）和美学质量。
- **音乐生成（HeartMuLa）**：
  - **多语言歌曲生成**：在涵盖英、中、日、韩、西五语的HeartBeats-Benchmark上评估，在**歌词清晰度（PER）** 上显著领先，达到最低错误率（如英文0.09）。
  - **综合质量**：在音乐性、结构连贯性、风格一致性等客观指标（SongEval, AudioBox）上表现稳定且具竞争力，与顶级闭源模型（如Suno-v5）相当。
  - **主观评价**：在多项主观听测维度（如音乐性、和谐度、结构）上，MOS分数显著优于其他开源基线，接近顶级闭源模型。
- **辅助任务**：
  - **音频-文本检索（HeartCLAP）**：在WikiMT-X基准上，文本到音乐和音乐到文本的检索Recall@1等指标显著优于Laion-CLAP和MuQ-MuLan。
  - **歌词识别（HeartTranscriptor）**：在多个多语言数据集上，词错误率（WER）/字错误率（CER）达到最优，显著优于原始Whisper及其他专用模型。
</div>

</details>

---

## Stable Differentiable Modal Synthesis for Learning Nonlinear Dynamics
- **Authors**: Victor Zheleznov, Stefan Bilbao, Alec Wright, Simon King
- **Categories**: cs.SD, cs.LG, eess.AS, physics.comp-ph
- **arXiv**: [https://arxiv.org/abs/2601.10453v1](https://arxiv.org/abs/2601.10453v1)
- **PDF**: [https://arxiv.org/pdf/2601.10453v1](https://arxiv.org/pdf/2601.10453v1)

模态方法是物理建模合成领域长期采用的一种方法。该方法可扩展至非线性问题，包括弦的高振幅振动情形。通过模态分解，可得到一组密集耦合的非线性常微分方程系统。近期标量辅助变量技术的研究成果，为此类非线性系统构建了显式且稳定的数值求解器。另一方面，机器学习方法（特别是神经常微分方程）已成功实现从数据中自动建模非线性系统。本研究探讨了如何将标量辅助变量技术与神经常微分方程相结合，构建出能够学习非线性动力学的稳定可微分模型。所提出的方法利用系统模态线性振动的解析解，使得训练后系统的物理参数仍易于获取，无需在模型架构中引入参数编码器。作为概念验证，我们生成了弦非线性横向振动的合成数据，并证明该模型可通过训练复现系统的非线性动力学特性。文中同时提供了声音示例。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：物理建模合成旨在通过求解描述声学系统动力学的微分方程来生成声音。传统方法（如模态合成）虽能处理非线性问题，但机器学习方法（如神经ODE）在从数据中自动建模非线性系统方面显示出潜力。  
- **既有方法的问题**：  
  - 机器学习方法通常缺乏数值稳定性保证，在训练时间区间外进行时间外推时，解精度会迅速下降。  
  - 现有模型在训练后难以灵活调整采样率或物理参数（如影响音高、音色的参数），或需依赖参数编码器，导致参数量增加且需大量配置数据。

2)  
论文提出一种结合物理先验与机器学习的稳定可微分模态合成模型，核心方法如下：  
- **模态分解与结构分离**：  
  - 对分布式系统（如弦振动）进行模态分解，得到有限维ODE系统。  
  - 将问题分解为线性振动部分（已知解析解）与非线性耦合部分，后者仅描述模态间无量纲、无记忆的非线性相互作用。  
- **神经ODE与物理约束**：  
  - 用神经网络参数化非线性函数 \( f_\theta(q) \)，并将其嵌入物理约束的ODE框架（式14），其中线性部分由物理参数（刚度、阻尼等）显式定义。  
  - 采用梯度网络（GradNet）作为 \( f_\theta(q) \) 的架构，因其可解释为闭式非负势函数的梯度，满足稳定性要求。  
- **标量辅助变量（SAV）技术**：  
  - 引入SAV方法对势函数进行二次化，并添加控制项以减少数值漂移。  
  - 结合显式时间离散化方案（式11），保证数值能量守恒与非负性，从而确保模拟的长期稳定性。  
- **训练策略**：  
  - 使用“离散化再优化”方法，通过数值求解器的内部操作进行反向传播。  
  - 采用教师强制技术，将目标轨迹分段并提供真实初始条件，以加速训练并缓解梯度消失/爆炸问题。  
- **优势**：模型在训练后可直接调整物理参数（如张力、长度）和采样率，无需重新训练或参数编码器，实现了灵活性与可控性。

3)  
- **任务**：非线性横向弦振动的建模与合成。  
- **效果**：  
  - 在训练集、验证集和测试集上，模型能准确预测位移轨迹与音频输出（初始100ms的相对MSE约 \(10^{-4}\) 量级）。  
  - 模型可泛化至未见过的物理参数（如基频、刚度）、采样率及时间尺度，且保持性能稳定。  
  - 相比纯线性解，模型显著提升了高阶模态的预测精度（误差降低达四个数量级），并成功复现了非线性效应（如音高滑移、幻象分音）。
</div>

</details>

---

## RSA-Bench: Benchmarking Audio Large Models in Real-World Acoustic Scenarios
- **Authors**: Yibo Zhang, Liang Lin, Kaiwen Luo, Shilinlu Yan, Jin Wang, Yaoqi Guo, Yitian Chen, Yalan Qin, Zhenhong Zhou, Kun Wang, Li Sun
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.10384v1](https://arxiv.org/abs/2601.10384v1)
- **PDF**: [https://arxiv.org/pdf/2601.10384v1](https://arxiv.org/pdf/2601.10384v1)

尽管音频大模型（ALMs）已展现出卓越的性能，但在实际部署中其鲁棒性仍显脆弱。现有评估方法主要依赖合成的高斯噪声或简单的单源干扰，未能捕捉真实物理环境中复杂、多层次的声音动态——即“声学生态”。为弥补这一生态差距，我们提出了 **RSA-Bench**，一个通过高保真听觉场景模拟对ALMs进行压力测试的综合性鲁棒性基准。与传统方法不同，我们通过将多样化的环境声景（涵盖**牧场**、**极端天气**、**教室**和**户外**等场景）在多种干扰强度下自然叠加到纯净语音信号上，构建评估样本。通过在从基础感知到复杂推理的六项核心任务上评估模型，本研究揭示了三个宏观层面的发现：**（I）感知与认知鸿沟**：模型在低层次识别任务中保持相对稳健，但在压力下的高阶推理任务中出现**功能性崩溃**；**（II）场景敏感性**：“类人声”干扰（如背景笑声）比机械噪声更具破坏性，这对模型的听觉注意力机制提出了挑战；**（III）去噪悖论**：标准语音增强方法往往会加剧性能下降，因为ALMs对去噪伪影引入的语义失真高度敏感。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：音频大模型在受控环境下表现出色，但在真实复杂声学场景中部署时，其鲁棒性存疑。  
- **既有方法的问题**：现有评估主要依赖合成的高斯噪声或简单的单源干扰，无法捕捉真实物理环境中错综复杂、多层次的“声学生态”动态，导致评估与现实脱节。

2)  
论文提出了名为 **RSA-Bench** 的鲁棒性基准测试，通过高保真听觉场景模拟来系统性地评估音频大模型在真实复杂噪声下的表现。其核心方法旨在解决传统评估的“生态鸿沟”问题，具体体现在：  

- **构建高保真“声学生态”场景**：  
  - 设计了四个具有代表性的真实声学场景：牧场、极端天气、教室和户外。  
  - 每个场景的噪声均由真实环境录音构成，模拟了目标语音与多样化背景声音交织的复杂性。  

- **采用多源叠加策略以模拟真实干扰**：  
  - 通过将1到4个不同的真实噪声源与干净语音信号自然叠加，生成评估样本。  
  - 通过调整叠加噪声源的数量（K=1至4），系统地控制环境复杂度，形成从简单到极端的压力梯度。  

- **覆盖全面的任务以揭示能力断层**：  
  - 基准测试包含六个核心任务，涵盖从基础感知到复杂推理的完整能力谱系。  
  - **感知与副语言学任务**：包括自动语音识别、性别识别和情绪识别，评估模型在干扰下保持信号保真度的能力。  
  - **认知推理任务**：包括数学推理、语音问答和语音指令跟随，评估模型在噪声中进行逻辑处理和语义理解的高阶能力。  

- **实现精细化的性能分析**：  
  - 通过对每个原始样本生成17种测试条件（1个干净版本 + 4个场景 × 4种复杂度），能够量化模型性能随环境复杂性增加而下降的轨迹。  
  - 这种方法超越了单一信噪比或合成噪声的测试，首次系统性地揭示了音频大模型在复杂声学压力下的行为模式与脆弱性。

3)  
在RSA-Bench的六个核心任务上对11个主流音频大模型进行了评估，主要效果与发现如下：  
- **任务表现差异显著**：模型在低阶感知任务（如性别识别）上相对稳健，但在高阶认知任务（如数学推理、指令跟随）上性能急剧下降，出现“功能崩溃”。  
- **场景敏感性突出**：户外场景（含类人声干扰，如笑声）对模型破坏性最强；教室场景（噪声有节奏间隙）中模型表现相对最好。  
- **去噪策略普遍失效**：应用标准语音增强方法进行预处理，不仅未能恢复性能，反而常因引入失真伪影而导致模型表现进一步恶化。
</div>

</details>

---

## Self-supervised restoration of singing voice degraded by pitch shifting using shallow diffusion
- **Authors**: Yunyi Liu, Taketo Akama
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.10345v1](https://arxiv.org/abs/2601.10345v1)
- **PDF**: [https://arxiv.org/pdf/2601.10345v1](https://arxiv.org/pdf/2601.10345v1)

在歌唱音频制作中，音高变换是一项核心功能。然而，传统信号处理方法存在众所周知的权衡问题，例如共振峰偏移和机械感音色失真，且音高偏移幅度越大，这些问题越显著。本文通过将音高变换重构为修复问题，旨在实现高质量的歌唱音频音高变换：给定一段经过音高变换（因而包含伪影）的音频，我们在保持其旋律与时序的同时，恢复出自然听感的演唱效果。具体而言，我们采用一个轻量级的梅尔谱扩散模型，该模型由帧级声学特征（如基频、音量及内容特征）驱动。我们通过自监督方式构建训练数据对：施加音高变换并逆向还原，以模拟真实伪影，同时保留原始真实数据。在精选的歌唱数据集上，与代表性经典基线方法相比，所提方法显著降低了音高变换伪影，这一结论通过统计指标和成对声学测量均得到验证。结果表明，基于修复的音高变换方法有望成为歌唱制作流程中抗伪影音高变换的有效途径。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：传统信号处理方法（如重采样、相位声码器、PSOLA、分析-合成声码器）在实现人声变调时，存在固有的音质与保真度权衡问题。  
- **既有方法的问题**：  
  - 大范围变调时，易产生共振峰偏移、机器人音色、相位失真、瞬态模糊等可听伪影。  
  - 数据驱动方法（如基于扩散的歌声合成/转换系统）通常依赖说话人嵌入，难以直接应用于未见歌手的、与说话人无关的通用变调任务。

2)  
论文提出一种**基于浅层扩散的自监督修复方法**，将变调重构为一个修复问题：对因变调而产生伪影的音频，恢复其自然音质，同时保持旋律和节奏。核心方法如下：  

- **自监督训练数据构建**：  
  - 使用WORLD声码器对原始音频进行正向变调（±12个半音内），再反向变调回原调，生成带有伪影的失真音频。  
  - 以失真音频为输入，原始音频为重建目标，构成训练对，无需人工标注的完美变调样本。  

- **浅层扩散修复模型**：  
  - 在梅尔谱域采用轻量级DDPM模型（100步噪声过程，推理时用DPM-Solver加速）。  
  - 模型以帧级声学特征为条件：包括F0（基频）、音量包络以及ContentVec提取的内容特征。  
  - 通过自适应归一化层将条件特征注入U-Net去噪网络。  
  - 训练目标结合标准扩散L2损失、梅尔谱L1损失和F0 L1损失，以稳定音高和能量重建。  

- **推理流程**：  
  - 对输入音频用WORLD进行目标变调，得到带伪影的梅尔谱。  
  - 以该梅尔谱为初始状态，注入少量噪声后，通过少量去噪步骤（浅层扩散）逐步修复伪影。  
  - 最终将修复后的梅尔谱用NSF-HiFiGAN声码器合成为波形。  

- **方法优势**：  
  - 自监督训练避免了配对数据需求。  
  - 利用WORLD提供准确、稳健的F0轨迹，确保音高保真度。  
  - 浅层扩散作为轻量级修正器，专注于去除伪影，而非重新估计音高或内容，提高了对未见歌手的泛化能力。

3)  
- **任务**：在**歌声变调修复**任务上进行了评估，测试集为未见过的歌唱音频数据集（包含多种唱法和歌手）。  
- **效果**：  
  - 在统计分布指标（FAD、KID、MMD）上大幅优于传统方法（PSOLA、WORLD）及数据驱动基线（SiFiGAN、Diff-Pitcher等），表明重建音频的全局分布最接近真实歌声。  
  - 在成对信号指标上，取得了最佳的频谱收敛度、MFCC距离、F0误差和清浊音错误率，显著降低了伪影。  
  - 结合WORLD后，在较大范围变调（±12半音）时仍能保持优异的音高准确性，同时显著改善音质。
</div>

</details>

---

## MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts
- **Authors**: Yuxuan Lou, Kai Yang, Yang You
- **Categories**: cs.CL, cs.AI, cs.LG, cs.SD
- **arXiv**: [https://arxiv.org/abs/2601.10272v1](https://arxiv.org/abs/2601.10272v1)
- **PDF**: [https://arxiv.org/pdf/2601.10272v1](https://arxiv.org/pdf/2601.10272v1)

本文提出MoST（语音与文本混合模型），这是一种新型多模态大语言模型，通过我们提出的模态感知专家混合架构（MAMoE）实现了语音与文本处理的无缝集成。当前多模态模型通常使用相同参数处理不同模态表征，忽视了其内在的表征差异，为此我们引入了专用路由路径，能够根据输入类型将词元导向适配对应模态的专家。MAMoE通过两个互补组件同时增强模态特异性学习与跨模态理解：捕获领域特定模式的模态专用专家组，以及促进模态间信息传递的共享专家。基于此架构，我们开发了高效的转换流程：首先在ASR和TTS数据集上进行策略性后训练，随后使用精心构建的语音-文本指令数据集进行微调，从而适配预训练的MoE语言模型。该流程的关键特性在于完全依赖可公开获取的开源数据集即可实现优异性能与数据效率。在ASR、TTS、音频语言建模及口语问答基准上的综合评估表明，MoST在参数量相当的情况下持续超越现有模型。消融研究证实，模态专用路由机制与共享专家设计对所有测试领域的性能提升均有显著贡献。据我们所知，MoST是基于专家混合架构构建的首个完全开源的语音-文本大语言模型。\footnote{模型、训练代码、推理代码及训练数据发布于 https://github.com/NUS-HPC-AI-Lab/MoST}

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：追求自然的人机交互需要无缝整合语音和文本模态。现有大型语言模型在文本处理上表现出色，但扩展到语音面临挑战。  
- **既有方法的问题**：  
  - 主流多模态模型通常使用相同的参数处理不同模态，忽视了语音（连续、高维）和文本（离散、符号）在表示上的固有差异。  
  - 这种统一处理可能导致表示干扰，稀释各模态独特的统计特性和结构细节，阻碍捕获语音中语音变化或文本中复杂语义关系等细粒度模式的能力。  

2)  
论文提出 **MoST** 模型，其核心是 **模态感知专家混合** 架构，通过以下机制解决上述问题：  
- **模态感知路由机制**：  
  - 根据输入标记的模态指示符（如文本为0，音频为1），将标记定向到相应的模态专用专家组。  
  - 文本标记主要路由至文本专家和共享专家，音频标记则路由至音频专家和共享专家。  
  - 这种设计实现了**专用学习**：模态专用专家可以无干扰地捕获语音或文本特有的复杂模式。  
- **共享专家设计**：  
  - 引入并行共享专家块，处理所有通过MAMoE层的标记。  
  - 共享专家作为桥梁，促进跨模态的知识传递和对齐，这对于自动语音识别和文本到语音合成等任务至关重要。  
- **高效训练流程**：  
  - 从预训练的MoE LLM出发，首先在大型ASR和TTS数据集上进行针对性后训练，以初始化模态专用专家并使共享专家适应跨模态任务。  
  - 随后，使用精心构建的语音-文本指令数据集进行指令微调，增强模型的通用性和可控性。  
  - 整个流程完全依赖可公开访问的开源数据，确保了数据效率和强性能。  

3)  
MoST在以下任务上取得了显著效果：  
- **自动语音识别**：在LibriSpeech-clean上词错误率低至2.0%，在跨数据集场景（如VoxPopuli）上也表现出色。  
- **文本到语音合成**：在多个数据集上达到最先进性能，例如在LibriSpeech-clean上词错误率为6.0%。  
- **音频语言建模**：在sTopic-StoryCloze等基准测试中取得最佳性能，平均表现优于或媲美Phi-4 Multimodal等强基线。  
- **口语问答**：在Llama Q、Trivial QA和WebQ等基准上，其语音到文本和语音到语音任务均取得竞争性或最优结果，展现了强大的端到端语音理解与生成能力。
</div>

</details>

---

## Nearest Kronecker Product Decomposition Based Subband Adaptive Filter: Algorithms and Applications
- **Authors**: Jianhong Ye, Haiquan Zhao
- **Categories**: eess.AS, cs.IT
- **arXiv**: [https://arxiv.org/abs/2601.10078v1](https://arxiv.org/abs/2601.10078v1)
- **PDF**: [https://arxiv.org/pdf/2601.10078v1](https://arxiv.org/pdf/2601.10078v1)

近年来，基于最近克罗内克积分解的归一化最小均方算法在收敛性能上已展现出优于传统NLMS算法的优势。然而，该算法在处理高度相关输入信号时收敛速度显著下降。为解决此问题，本文提出一种基于I型NKP的归一化子带自适应滤波算法，即NSAF-NKP-I。但该算法的计算开销远高于NLMS-NKP算法。值得注意的是，我们提出的增强型II型NKP-NSAF算法在保持同等收敛性能的同时，显著降低了计算复杂度。为进一步提升算法在脉冲噪声干扰下的鲁棒性，我们开发了两种鲁棒性变体：基于最大相关熵准则的鲁棒NSAF-NKP算法和基于对数准则的鲁棒NSAF-NKP算法。此外，本文对所提算法的计算复杂度、步长范围及理论稳态性能进行了详细分析。为增强NSAF-NKP-II算法在复杂非线性环境中的实用性，我们进一步设计了两种非线性实现方案：基于三角函数链接网络的NKP-NSAF算法和基于Volterra级数展开的NKP-NSAF算法。在主动噪声控制系统中，我们进一步提出了滤波-x型NSAF-NKP-II算法。通过在回声消除、稀疏系统辨识、非线性处理及主动噪声控制场景中的仿真实验，验证了所提算法相较于现有先进方法的优越性。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：自适应滤波算法（如NLMS）在处理高度相关输入信号时收敛性能下降。现有方法（如AP、RLS算法）虽能提升收敛速度，但计算复杂度高。  
- **既有问题**：NKP分解技术（如NLMS-NKP）虽能提升稀疏系统收敛速度，但在处理强相关信号时收敛率仍不足，且现有NKP方法（如RLS-NKP、APA-NKP）涉及矩阵运算，计算开销大。此外，传统算法在脉冲噪声和非线性环境中鲁棒性差。

2)  
- **核心方法**：提出基于NKP分解的子带自适应滤波框架，通过子带处理提升对相关信号的适应性，并设计两种算法变体以平衡性能与复杂度。  
  - **NSAF-NKP-I算法**：将NKP分解与子带结构结合，通过分析滤波器组将输入信号分解为近似白化的子带信号，加速收敛。但计算复杂度较高。  
  - **NSAF-NKP-II算法**：改进子带NKP模型结构，在保持与NSAF-NKP-I相当收敛性能的同时，显著降低计算复杂度。  
  - **鲁棒扩展**：为增强抗脉冲噪声能力，基于最大相关熵准则（MCC）和对数准则（LC）分别提出RNSAF-NKP-MCC和RNSAF-NKP-LC算法，通过缩放函数在脉冲噪声出现时减小步长。  
  - **非线性扩展**：针对非线性环境，提出基于三角函数链路网络（TFLN）和Volterra级数展开的TFLN-NKP-NSAF和Volterra-NKP-NSAF算法。  
  - **ANC应用**：在主动噪声控制中，进一步提出滤波-x结构的NKP-FxNSAF算法。  
- **理论分析**：给出了NSAF-NKP-II算法的稳态性能模型、步长范围分析及计算复杂度对比。

3)  
- **任务与效果**：  
  - **系统辨识与回声消除**：在稀疏系统辨识和高相关输入下，NSAF-NKP-II较NLMS-NKP、NSAF等算法收敛更快，稳态误差更低；在回声消除中，ERLE性能显著提升。  
  - **脉冲噪声环境**：RNSAF-NKP-MCC和RNSAF-NKP-LC在α稳定噪声中保持稳定收敛，优于NKP-GMCC等现有鲁棒算法。  
  - **非线性处理**：TFLN-NKP-NSAF和Volterra-NKP-NSAF在不对称扬声器失真和对称软限幅非线性场景中，较TFLN-NKP-NLMS等算法收敛更快、稳态精度更高。  
  - **主动噪声控制**：NKP-FxNSAF在ANC系统中较FxLMS、FxNSAF等算法收敛更快，噪声抑制效果更优。
</div>

</details>

---
