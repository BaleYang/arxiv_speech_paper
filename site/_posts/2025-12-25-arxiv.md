---
layout: post
title: "arXiv Daily – 2025-12-25"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-25（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-24 08:50 — 2025-12-25 08:50
- 抓取总数：5 篇 | 本页显示：5 篇（去重/过滤后）

## Towards Practical Automatic Piano Reduction using BERT with Semi-supervised Learning
- **Authors**: Wan Ki Wong, Ka Ho To, Chuck-jee Chau, Lucas Wong, Kevin Y. Yip, Irwin King
- **Categories**: cs.SD, cs.SC
- **arXiv**: [https://arxiv.org/abs/2512.21324v1](https://arxiv.org/abs/2512.21324v1)
- **PDF**: [https://arxiv.org/pdf/2512.21324v1](https://arxiv.org/pdf/2512.21324v1)

本研究提出了一种基于半监督机器学习的新型自动钢琴缩编方法。钢琴缩编是重要的音乐转换过程，可作为演奏与分析的乐谱草图辅助音乐家与作曲家。该过程的自动化是极具挑战性的研究课题，但能带来巨大便利，因为人工进行钢琴缩编需耗费大量时间与精力。虽然监督式机器学习常被用于学习输入-输出映射，但难以获取大量标注数据。我们旨在通过半监督学习解决此问题，从而利用古典音乐中丰富的可用数据，以极少甚至无需标注的方式完成该任务。为此，我们构建了包含音乐简化与和声重构的两步法框架，并基于现有机器学习框架MidiBERT提出并实现了两种可行方案。实验表明，我们的方案能够生成实用且真实的缩编样本，其输出结果准确性高，仅需少量后处理调整。本研究为半监督学习在自动钢琴缩编中的应用奠定了基础，为后续研究者提供了可借鉴的框架以产出更先进的成果。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：钢琴缩编是将多乐器管弦乐谱转换为双手钢琴谱的重要音乐处理任务，能极大帮助音乐家分析、演奏与创作。传统手动缩编耗时耗力，亟需自动化方法。
- **既有方法问题**：
  - 监督学习方法受限于**标注数据稀缺**，缺乏大规模高质量的管弦乐谱与对应钢琴缩编谱配对数据集。
  - 现有方法多基于**规则或统计模型**（如隐马尔可夫模型），泛化能力有限，且难以灵活生成符合音乐性与可演奏性的结果。

2)  
论文提出一种基于**半监督学习**的两阶段框架，结合预训练BERT模型解决数据稀缺与生成质量难题：
- **整体策略**：采用“**音乐简化后和声化**”流程。先简化原谱（选取关键音符），再和声化（调整并补充音符），最终通过后处理确保可演奏性。
- **数据利用**：
  - 使用大量**无标注古典音乐MIDI数据**（如Symbolic Orchestral Database与GiantMIDI-Piano）进行预训练，提升模型对古典音乐的理解。
  - 仅依赖少量**对齐标注数据**（Live Orchestral Piano数据库）进行下游任务微调。
- **核心方法**：
  - **预训练**：基于MidiBERT架构，使用掩码语言建模任务在古典音乐数据上预训练，学习音乐表示。
  - **方案一（MB-NR）**：将缩编视为**音符二分类任务**（保留/丢弃）。输入管弦乐谱序列，输出每个音符的保留概率，通过二值交叉熵损失训练。
  - **方案二（MB-R2F）**：构建**序列到序列生成模型**。输入为经天际线算法提取的旋律与低音线简化谱，输出完整钢琴谱。采用编码器-解码器结构（两个MidiBERT），通过交叉注意力机制生成和谐且对齐的音符。
- **技术亮点**：
  - 引入**特殊空小节标记**（⟨ABS⟩）解决Compound Word表示中休止小节缺失问题。
  - 通过**半监督范式**，充分利用丰富无标注数据，缓解标注数据不足的瓶颈。
  - 后处理模块（聚类、简化、移调）进一步优化可读性与可演奏性。

3)  
- **任务**：自动钢琴缩编，输入多轨管弦乐MIDI，输出双轨钢琴MIDI。
- **效果**：
  - **客观指标**：MB-R2F在**音调相似性**上表现最佳（均值0.58），优于MB-NR（0.39）与规则基线DBM（0.74），表明其能更好保持原曲和声特征。
  - **主观评估**：
    - **辨别测试**：MB-NR生成结果最接近人工缩编，未能被显著区分（假设未被拒绝），**真实性最高**。
    - **专业评价**：MB-NR在**音乐保真度、自然度与缩编质量**上均获最高分（约3.6/5），且难度适中，被专业钢琴师评为最佳。
  - **总体**：两种方案均优于规则基线，其中MB-NR在生成**实用、真实且需后处理调整最少**的缩编谱方面表现突出。
</div>

</details>

---

## USE: A Unified Model for Universal Sound Separation and Extraction
- **Authors**: Hongyu Wang, Chenda Li, Xin Zhou, Shuai Wang, Yanmin Qian
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.21215v1](https://arxiv.org/abs/2512.21215v1)
- **PDF**: [https://arxiv.org/pdf/2512.21215v1](https://arxiv.org/pdf/2512.21215v1)

声音分离（SS）与目标声音提取（TSE）是处理复杂声学场景的基础技术。现有声音分离方法难以确定未知数量的声源，而目标声音提取方法则需要精确指定的线索才能达到最佳性能。本文提出一个统一框架，将声音分离与目标声音提取协同结合，以克服各自的局限。该架构包含两个互补组件：1）编码器-解码器吸引子（EDA）网络，可自动推断声源数量及对应的声学线索以进行声音分离；2）多模态融合网络，能精准解析用户提供的多样化线索（声学、语义或视觉）以执行目标声音提取。通过跨任务一致性约束的联合训练，我们构建了一个连接两种范式的统一潜在空间。在推理阶段，系统可自适应地运行于全自动声音分离模式或线索驱动的目标声音提取模式。实验表明，本方法在两项任务中均表现优异，其中声音分离相比基线提升1.4 dB SDR，目标声音提取准确率达到86%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
   - **通用声音分离 (USS)**：旨在从混合音频中分离任意类型和数量的声源。但现有方法通常需要预先知道声源数量，这在真实场景中往往未知，限制了其应用。
   - **目标声音提取 (TSE)**：利用关于目标声音的先验线索（如音频、文本或视觉）从混合音中提取特定声源。但它存在两个主要问题：
     - 依赖高质量、可获取的线索，若线索缺失或质量低，性能会显著下降。
     - 当声源数量已知时，其性能通常低于专门的分离方法。

2) **论文核心方法如何解决上述问题**
   论文提出了 **USE**，一个统一声音分离与目标提取的框架。其核心是通过**联合训练**和**语义空间对齐**，使模型能自适应地处理两种任务。

   - **统一架构与两个核心模块**：
     - **编码器-解码器吸引子网络**：自动从混合音频中推断声源数量，并为每个声源生成“吸引子”嵌入表示。这解决了分离任务中声源数量未知的问题。
     - **多模态线索网络**：能够接受并融合文本、视频、声音标签等任意模态（或组合）的用户线索，生成统一的“线索”嵌入。这增强了对低质量或缺失线索的鲁棒性。

   - **关键创新：语义空间对齐与联合训练**：
     - 通过设计**对齐损失**，在训练阶段强制将“吸引子”嵌入和“线索”嵌入映射到同一个语义空间。这使得两者可以相互替代。
     - 采用**两阶段训练策略**：第一阶段训练分离和EDA网络；第二阶段随机使用吸引子或线索嵌入进行训练，并施加对齐约束，使模型同时精通两种任务。

   - **自适应推理**：
     - **无线索时**：启用EDA模块，进行全自动的通用声音分离。
     - **有线索时**：启用线索网络，用线索嵌入替代吸引子，进行目标声音提取。
     - 这种设计弥合了SS和TSE之间的鸿沟，使模型既能利用线索提升提取精度，又能在无线索时自主分离。

3) **在哪些任务上取得了怎样的效果**
   - **通用声音分离**：在LibriMix和AudioSet等数据集上，USE在2-6个声源的混合场景中均取得优异性能。例如，在未知说话人数量的语音分离任务上，性能优于基线；在FUSS数据集上，4混音分离的SI-SNRi达到11.9 dB，显著领先于TDCN++（7.4 dB）。
   - **目标声音提取**：在AudioSet数据集上，USE在使用多模态线索时，提取性能（SNRi）相比强基线DCCRN提升最高达35.4%。即使在单模态或低质量线索下，也表现出强大的鲁棒性。
   - **附加能力**：EDA模块的声源计数准确率在2混音场景下超过80%；吸引子与线索的匹配准确率在2混音下达到86%，验证了语义空间对齐的有效性。
</div>

</details>

---

## GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model
- **Authors**: Haoyang Li, Xuyi Zhuang, Azmat Adnan, Ye Ni, Wei Rao, Shreyas Gopal, Eng Siong Chng
- **Categories**: eess.AS, cs.AI, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.20978v1](https://arxiv.org/abs/2512.20978v1)
- **PDF**: [https://arxiv.org/pdf/2512.20978v1](https://arxiv.org/pdf/2512.20978v1)

基于语言模型的生成式建模为目标说话人提取提供了新的研究方向，在提升泛化能力与生成高保真语音方面展现出潜力。本文提出GenTSE，一种仅含解码器的两阶段生成式语言模型方法：第一阶段预测粗粒度语义标记，第二阶段生成细粒度声学标记。通过分离语义与声学表示，该方法提升了解码稳定性，并生成更忠实、内容对齐的目标语音。两阶段均采用连续的自监督学习或编解码器嵌入，相比离散提示方法能提供更丰富的上下文信息。为减少曝光偏差，我们提出冻结语言模型条件训练策略，使语言模型基于早期检查点预测的标记进行条件生成，从而缩小教师强制训练与自回归推理之间的差距。进一步采用直接偏好优化方法，使输出更符合人类感知偏好。在Libri2Mix数据集上的实验表明，GenTSE在语音质量、可懂度与说话人一致性方面均优于此前基于语言模型的系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：目标说话人提取旨在从混合语音中分离出指定说话人的声音。传统判别式方法泛化能力差，且可能损害语音保真度。
- **既有方法问题**：
  - 现有基于语言模型的生成方法多采用单阶段模型直接预测细粒度声学码本，建模难度高。
  - 部分方法引入非生成式组件或使用离散化参考提示，破坏了端到端生成性或丢失了细粒度说话人信息。
  - 自回归训练中的“曝光偏差”问题被忽视，导致训练与推理不匹配，影响语音质量和说话人一致性。

2)  
论文提出 **GenTSE**，一个完全生成式的两阶段解码器语言模型框架，通过以下方式解决问题：

- **核心架构：粗到细的层次化生成**
  - **第一阶段（语义提取）**：使用仅解码器的语言模型，以参考语音和混合语音的连续WavLM嵌入为条件，自回归地预测目标语音的**粗粒度语义令牌**（通过k-means离散化WavLM特征得到）。
  - **第二阶段（声学生成）**：另一个仅解码器语言模型以第一阶段预测的语义令牌、以及参考和混合语音的连续DAC声学嵌入为条件，自回归地生成**细粒度声学令牌**（使用单码本的神经编解码器SimCodec）。
  - **优势**：将语义和声学建模分离，降低了直接建模高熵声学细节的复杂度，使解码更稳定，生成的目标语音内容更准确、保真度更高。使用连续嵌入作为条件，比离散提示提供了更丰富的上下文信息。

- **优化策略一：冻结语言模型条件训练**
  - **问题**：曝光偏差——训练时使用真实历史令牌（教师强制），而推理时依赖模型自身的预测历史，导致不匹配。
  - **解决方案**：首先训练基础模型（θ, φ）。然后复制参数创建可训练模型（θ‘, φ’），并冻结基础模型。在训练可训练模型时，使用冻结基础模型生成的预测令牌作为条件输入，替代真实历史令牌。
  - **效果**：让模型在训练阶段就适应自身预测的上下文，有效缩小了训练与推理的差距，提升了自回归推理的稳定性和输出质量。

- **优化策略二：直接偏好优化**
  - **问题**：基于令牌似然的训练目标无法可靠地捕捉人类感知的语音自然度、清晰度等偏好。
  - **解决方案**：在声学阶段引入DPO。使用一个冻结的参考模型采样多个候选声学令牌序列，用UTMOS（一种与人类评分高度相关的MOS预测器）评分，构建偏好对（高分 vs 低分）。DPO目标函数直接优化模型，使其输出分布更倾向于高分（人类偏好）的序列。
  - **效果**：以简单稳定的方式将模型输出与人类感知偏好对齐，无需修改推理架构，有效提升了感知语音质量。

3)  
- **任务**：在标准目标说话人提取基准数据集 **Libri2Mix** 上进行评估。
- **效果**：
  - 在**语音质量**（DNSMOS、UTMOS、NISQA）、**说话人一致性**（SECS）和**可懂度**（SpeechBERT）多项指标上，超越了所有对比的基于语言模型的生成方法（如TSELM-L, LLaSE-G1, Metis）。
  - 与先进的判别式方法（如USEF-SepFormer）相比，取得了更高的感知语音质量和说话人相似度，但在词错误率降低（dWER）指标上略逊。
  - 消融实验验证了所提两阶段架构、连续嵌入条件、FLC和DPO策略的有效性，均带来了显著的性能提升。
</div>

</details>

---

## Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study
- **Authors**: Zhongren Dong, Haotian Guo, Weixiang Xu, Huan Zhao, Zixing Zhang
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.20948v1](https://arxiv.org/abs/2512.20948v1)
- **PDF**: [https://arxiv.org/pdf/2512.20948v1](https://arxiv.org/pdf/2512.20948v1)

神经精神疾病（如阿尔茨海默病、抑郁症和自闭症谱系障碍）常伴随语言与声学特征的异常，这些异常为早期检测提供了潜在的生物标志物。尽管多模态方法前景广阔，但多语言泛化能力不足及缺乏统一评估框架等问题依然存在。为应对这些挑战，本研究提出FEND（基于基础模型的神经精神疾病评估框架），这是一个融合语音与文本模态的综合性多模态框架，旨在实现覆盖全生命周期的阿尔茨海默病、抑郁症和自闭症谱系障碍检测。通过整合涵盖英语、汉语、希腊语、法语和荷兰语的13个多语言数据集，我们系统评估了多模态融合的性能。实验结果表明：多模态融合在阿尔茨海默病和抑郁症检测中表现优异，但因数据集异质性在自闭症谱系障碍检测中效果欠佳。研究同时发现模态不平衡是普遍问题，多模态融合未能超越最佳单模态模型。跨语料库实验显示，在任务与语言一致的情境下模型表现稳健，但在多语言及任务异质场景中性能显著下降。通过提供全面的基准测试及对性能影响因素的深入分析，FEND推动了自动化、全生命周期覆盖及多语言的神经精神疾病评估领域发展。我们鼓励研究者采用FEND框架进行公平比较与可重复研究。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经精神障碍（如阿尔茨海默病、抑郁症、自闭症谱系障碍）常伴随语言和声学异常，为早期检测提供了潜在生物标志物。传统诊断依赖专家评估，耗时、主观且难以扩展。
- **既有方法问题**：现有研究多局限于单一数据集、特定语言（主要为英语）或私有评估指标，缺乏统一的多维基准。这导致方法间难以公平比较，阻碍了对模型跨疾病、跨语言和跨年龄组性能的系统理解，限制了实际应用和泛化能力。

2)  
- **提出FEND框架**：为解决上述问题，论文提出了FEND（基于基础模型的神经精神障碍评估）框架。它是一个统一、标准化的多模态评估基准，旨在系统评估基础模型在神经精神障碍检测中的性能。
- **核心方法设计**：
    - **多维度覆盖**：框架涵盖三种代表性障碍（AD、抑郁症、ASD），整合语音和文本模态，并跨越多种语言（英、中、希腊、法、荷）及全年龄段。
    - **系统化评估流程**：
        - **单模态分析**：使用预训练的基础模型（如WavLM用于语音，E5用于文本）提取特征，通过简单的MLP分类器评估各模态单独的性能。
        - **多模态融合**：采用多种经典融合算法（如序列级的MulT、图级的GraphMFN，以及话语级的TFN、LMF、Attention等）整合语音和文本特征，评估融合效果。
    - **标准化与可复现性**：提供统一的数据预处理、评估指标（加权准确率WA、未加权准确率UA、加权F1值WF1）和训练协议（固定超参数、冻结基础模型参数），确保公平比较和结果可复现。
- **如何解决问题**：
    - **解决碎片化与不可比性**：通过统一的框架和基准，使不同模型、方法和数据集能在相同条件下进行公平比较。
    - **评估多语言泛化**：利用13个多语言数据集，系统测试模型在跨语言场景下的性能，揭示了英语中心模型在非英语数据上的局限。
    - **深入分析性能影响因素**：不仅报告结果，还深入分析了影响性能的关键因素，如模态不平衡、跨语料库泛化挑战等，为未来研究提供具体指导。

3)  
- **评估任务与效果**：FEND在阿尔茨海默病（AD）、抑郁症和自闭症谱系障碍（ASD）的检测任务上进行了全面评估。
    - **AD与抑郁症检测**：多模态融合（尤其是基于注意力或张量融合的方法）在多数单语数据集上表现优异，能超越最佳单模态模型。例如，在ADReSS和D-VLOG数据集上，多模态融合取得了更高的WF1分数。
    - **ASD检测**：多模态融合未能显著超越最佳单模态（语音）基线，主要受数据集异质性和模态不平衡影响。
    - **跨语言与跨任务泛化**：在任务和语言一致的场景下模型表现稳健，但在多语言和任务异构的设置中性能明显下降，凸显了泛化挑战。
- **总体贡献**：FEND通过提供广泛的基准和深入分析，推动了自动化、全年龄段包容和多语言神经精神障碍评估领域的发展。
</div>

</details>

---

## SACodec: Asymmetric Quantization with Semantic Anchoring for Low-Bitrate High-Fidelity Neural Speech Codecs
- **Authors**: Zhongren Dong, Bin Wang, Jing Han, Haotian Guo, Xiaojun Mo, Yimin Cao, Zixing Zhang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.20944v1](https://arxiv.org/abs/2512.20944v1)
- **PDF**: [https://arxiv.org/pdf/2512.20944v1](https://arxiv.org/pdf/2512.20944v1)

针对低比特率下神经语音编解码器在声学保真度与语义丰富性之间难以兼顾的根本性权衡问题，本文提出SACodec。该新型编解码器基于非对称双量化器构建，并引入我们提出的语义锚定机制，从而在结构上实现了语义信息与声学细节的量化解耦。语义锚定通过一个轻量级投影器完成，将声学特征与冻结的大规模mHuBERT码本对齐，在注入语言学先验的同时确保码本得到充分利用。随后，针对声学细节，采用带有SimVQ的残差激活模块，使单层量化器（声学路径）能够精准恢复细粒度信息。在仅1.5 kbps的比特率下，SACodec在保真度与语义表现上均达到新的最优水平：主观听音测试证实其重建质量在感知上高度接近原始音频，同时其编码表征在下游任务中展现出显著提升的语义丰富度。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
- **背景**：大语言模型与语音领域深度融合，低码率神经语音编解码器成为关键，旨在生成更短的离散标记序列以提升计算效率。
- **既有方法问题**：
  - **多层残差向量量化方法**：在极低码率下性能急剧下降，量化误差累积导致可听伪影，且产生的多流标记增加了下游建模复杂性。
  - **单码本范式方法**：仅以信号失真为目标进行声学优化，生成的表示缺乏显式语义结构，不利于需要深度内容理解的任务。
  - **语义增强策略**：依赖外部知识蒸馏或内生自监督学习，通常需要保留复杂的多层量化后端或引入显著的训练开销，难以在低码率下兼顾重建质量与语义丰富性。

2) **论文核心方法如何解决上述问题**
论文提出 **SACodec**，其核心是**非对称双量化器**架构，通过解耦语义与声学细节的量化，协同解决上述问题。

- **语义锚定模块**：
  - **机制**：使用一个轻量级可学习的投影器，将编码器的声学特征与一个**冻结的、大规模预训练的 mHuBERT 语义码本**对齐。
  - **作用**：直接注入强大的语言先验知识，作为“语义锚”。通过码本空间投影策略，实现了接近100%的码本利用率，有效防止了传统VQ中的码本崩溃问题，确保了语义表示的丰富性和稳定性。

- **残差激活模块**：
  - **机制**：计算声学潜在表示与语义嵌入之间的残差，该残差包含说话人音色、韵律等副语言信息。使用**单层VQ并结合SimVQ技术**对该残差进行量化。
  - **作用**：SimVQ通过一个可学习的基对冻结的随机初始化系数矩阵进行重参数化，实现了码本的全局更新和完全激活。这使得单个量化层能够以最小的架构复杂性和码率开销，高保真地恢复细粒度的声学细节。

- **协同工作与训练**：
  - 两个模块**顺序工作**，其量化输出通过逐元素相加融合，共同输入解码器以重建波形。
  - 整个模型在GAN框架下进行端到端训练。损失函数不对称地加权，对语义分支施加更强的约束，以确保与外部语义空间的对齐；对动态学习的残差分支则使用较弱的正则化。
  - 这种设计**同时实现了**：1) 通过外部冻结码本直接、高效地注入语义；2) 通过高效的SimVQ单层量化器捕捉声学细节；3) 输出单一的标记序列，简化了下游集成。

3) **在哪些任务上取得了怎样的效果**
- **声学重建质量**：在1.5 kbps极低码率下，在LibriTTS（干净/嘈杂）和LJSpeech数据集上，**客观指标**均超越所有基线模型。**主观MUSHRA测试**中，其感知质量中位数达96.8，与真实音频的97.5极为接近，证明了近乎无损的重建保真度。
- **语义表示丰富性**：在**ARCH基准测试**中评估下游分类任务。
  - **压缩域**：仅使用语音数据训练的SACodec，其标记的语义准确性显著优于同类型的单码本编解码器，甚至与使用多领域数据训练的高码率基线模型表现相当。
  - **重建域**：重建后的波形在语义任务上仍保持高准确性，证明了其语义信息能完整通过生成管道，实现了端到端的语义保真度。
</div>

</details>

---
