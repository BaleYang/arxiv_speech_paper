---
layout: post
title: "arXiv Daily – 2025-12-25"
tags: [arxiv, cs.SD, eess.AS]
---

以下为 2025-12-25（CST，北京时间）窗口内更新的论文中文译文：
- 时间窗口（锚点 08:50 CST）：2025-12-24 08:50 — 2025-12-25 08:50
- 抓取总数：5 篇 | 本页显示：5 篇（去重/过滤后）

## Towards Practical Automatic Piano Reduction using BERT with Semi-supervised Learning
- **Authors**: Wan Ki Wong, Ka Ho To, Chuck-jee Chau, Lucas Wong, Kevin Y. Yip, Irwin King
- **Categories**: cs.SD, cs.SC
- **arXiv**: [https://arxiv.org/abs/2512.21324v1](https://arxiv.org/abs/2512.21324v1)
- **PDF**: [https://arxiv.org/pdf/2512.21324v1](https://arxiv.org/pdf/2512.21324v1)

本研究提出了一种基于半监督机器学习的新型自动钢琴缩编方法。钢琴缩编是重要的音乐转换过程，可为演奏者与作曲家提供用于表演和分析的音乐草稿。该过程的自动化是极具挑战性的研究课题，但能带来巨大便利——人工进行钢琴缩编需要耗费大量时间与精力。虽然监督式机器学习常被用于学习输入-输出映射关系，但获取大量标注数据十分困难。我们通过半监督学习解决此问题，从而能够利用古典音乐中丰富的可用数据，以极少甚至无需标注的方式完成该任务。为此，我们构建了包含音乐简化与和声重构的两步式框架，并基于现有机器学习框架MidiBERT提出并实现了两种可行方案。实验表明，我们的方案能够生成实用且真实的缩编样本，其输出结果准确性高，仅需少量后处理调整。本研究为半监督学习在自动钢琴缩编中的应用奠定了基础，为后续研究者实现更先进成果提供了参考。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：钢琴缩编是将管弦乐总谱转换为双手可弹奏的钢琴谱的重要音乐转换任务，能极大帮助音乐家、作曲家和演奏者。传统手动缩编耗时耗力，因此自动化需求迫切。
- **既有方法的问题**：
  - 监督学习方法面临**标注数据稀缺**的挑战，缺乏大规模高质量的管弦乐谱与对应钢琴缩编谱配对数据集。
  - 现有方法多基于**规则或统计模型**（如HMM），其灵活性和音乐表现力有限，难以生成自然且符合音乐性的缩编谱。

2)  
论文提出一种基于BERT与半监督学习的自动钢琴缩编方法，核心是“先简化后和声化”的两步策略，并利用MidiBERT框架构建了两种解决方案：
- **整体策略**：利用大量未标注的古典音乐MIDI数据（如GiantMIDI-Piano、Symbolic Orchestral Database）进行预训练，使模型学习音乐表示；随后在少量标注数据（Live Orchestral Piano数据库）上进行微调，以解决标注数据不足的问题。
- **方法一：音符缩减（MB-NR）**
  - 将缩编视为**二分类任务**，输入为管弦乐谱的音符序列，输出为每个音符的保留（1）或丢弃（0）标签。
  - 使用预训练的MidiBERT作为编码器，通过二元交叉熵损失进行微调，学习从对齐的管弦乐-钢琴谱对中提取关键音符（如旋律和低音）。
- **方法二：序列到序列生成（MB-R2F）**
  - 将缩编视为**生成任务**，输入为从管弦乐谱中提取的旋律和低音线（通过skyline算法），输出为完整的钢琴谱。
  - 构建**Seq2Seq模型**：使用两个MidiBERT分别作为编码器和解码器（添加交叉注意力），通过自回归方式生成音符序列，以交叉熵损失进行训练。
- **技术细节**：
  - 采用Compound Word（CP）音符表示法进行符号化，并引入⟨ABS⟩等特殊令牌处理空小节问题。
  - 通过**后处理模块**（如聚类分声部、简化音符、移调）进一步提升输出谱的可演奏性和可读性。

3)  
- **任务**：自动钢琴缩编，输入为多轨管弦乐MIDI，输出为双轨钢琴MIDI。
- **效果**：
  - **客观指标**：在音调相似性上，MB-R2F（均值0.58）优于MB-NR（0.39），但均低于规则基线DBM（0.74），因后者直接保留更多原始音符。
  - **主观评估**：
    - **辨别测试**：MB-NR生成的结果最接近人工缩编，未能被显著识别为计算机生成（假设未被拒绝）。
    - **专业评估**：由专业钢琴家评分，MB-NR在音乐保真度（3.8）、自然度（3.6）和缩编质量（3.6）上均最高，且难度适中（2.9），综合表现优于MB-R2F和规则基线。
  - **总体**：两种基于BERT的方法均能生成实用且真实的缩编谱，仅需少量后处理调整，其中MB-NR在感知质量上尤为突出。
</div>

</details>

---

## USE: A Unified Model for Universal Sound Separation and Extraction
- **Authors**: Hongyu Wang, Chenda Li, Xin Zhou, Shuai Wang, Yanmin Qian
- **Categories**: eess.AS
- **arXiv**: [https://arxiv.org/abs/2512.21215v1](https://arxiv.org/abs/2512.21215v1)
- **PDF**: [https://arxiv.org/pdf/2512.21215v1](https://arxiv.org/pdf/2512.21215v1)

声音分离（SS）与目标声音提取（TSE）是处理复杂声学场景的基础技术。现有SS方法难以确定未知数量的声源，而TSE方法则需要精确指定的线索才能达到最佳性能。本文提出一个统一框架，将SS与TSE协同结合以克服各自的局限。该架构包含两个互补组件：1）编码器-解码器吸引子（EDA）网络，可自动推断SS所需的声源数量及对应声学线索；2）多模态融合网络，能精准解析用户提供的多样化线索（声学、语义或视觉）以执行TSE。通过跨任务一致性约束的联合训练，我们构建了一个连接两种范式的统一潜在空间。在推理阶段，系统可自适应地运行于全自动SS模式或线索驱动的TSE模式。实验表明，本方法在两项任务中均表现优异，其中SS相比基线提升1.4 dB SDR，TSE准确率达到86%。

<details>
<summary>详细解读</summary>

<div markdown="1">

1) **研究背景与既有方法的问题**
   - **通用声音分离 (USS)**：旨在从混合音频中分离任意类型和数量的声源，但现有方法通常需要预先知道声源数量，这在真实场景中往往未知，限制了其应用。
   - **目标声音提取 (TSE)**：利用关于目标声音的先验线索（如音频、文本或视觉）从混合中提取特定声源，但存在两个主要问题：
     - 线索可能质量低下或在实际中无法获取，导致性能下降或提取错误。
     - 当声源数量已知时，TSE方法的性能通常低于专门的分离方法。

2) **论文核心方法如何解决上述问题**
   论文提出了一个统一的框架 **USE**，通过协同结合声音分离 (SS) 和目标声音提取 (TSE) 来克服各自的局限性。其核心架构与训练策略如下：

   - **统一架构设计**：
     - **编码器-解码器吸引子网络**：自动推断混合音频中的声源数量，并为每个声源生成对应的“吸引子”嵌入表示，从而在无需先验知识的情况下完成声音分离。
     - **多模态线索网络**：能够接收并融合用户提供的任意模态（文本、视频、音频标签）的线索，生成统一的“线索”嵌入表示，用于驱动目标声音提取。
     - **共享的分离主干网络**：接收上述吸引子或线索嵌入，生成分离或提取后的音频信号。

   - **关键创新：语义空间对齐**：
     - 在训练阶段，通过引入**对齐损失**，强制将EDA网络生成的吸引子嵌入与多模态线索网络生成的线索嵌入映射到同一个统一的语义潜在空间中。
     - 对齐损失结合了均方误差损失和InfoNCE对比损失，确保相同声源的吸引子和线索在嵌入空间中彼此接近，从而实现语义层面的统一。

   - **灵活的自适应推理**：
     - **无线索模式**：系统使用EDA网络自动估计声源数量和吸引子，执行完全自主的通用声音分离。
     - **有线索模式**：当有任何模态的线索可用时，系统使用线索网络生成嵌入，并以此替换吸引子嵌入，执行精准的目标声音提取。
     - 两种模式共享同一个分离主干，得益于统一的嵌入空间，可以无缝切换。

   - **两阶段联合训练策略**：
     - **第一阶段**：专注于训练分离主干和EDA网络，优化声源计数和分离质量。
     - **第二阶段**：引入多模态线索，以随机比例（如30% EDA嵌入，70% 线索嵌入）作为分离网络的输入，并施加对齐损失，联合优化分离、提取和嵌入对齐任务。

   通过上述设计，USE模型不仅解决了SS中声源数量未知的问题，也增强了TSE对低质量或缺失线索的鲁棒性，并在一个框架内实现了两种任务性能的共同提升。

3) **在哪些任务上取得了怎样的效果**
   - **通用声音分离**：在LibriMix和AudioSet等数据集上，USE模型在2-mix和3-mix场景下均取得了优异的分离性能（SNRi提升显著），并且能够有效处理声源数量未知的情况，声源数量估计准确率在2-mix场景下超过80%。
   - **目标声音提取**：在包含多模态（文本、视频、标签）线索的AudioSet数据集上，USE模型在所有线索配置下均显著优于基线模型（如DCCRN）。例如，在使用全部三种线索时，SNRi相比基线提升了约29%-35%，并且在单一线索可用时也表现出强大的鲁棒性。
   - **统一性能**：经过联合训练后，模型在保持甚至略微提升TSE性能的同时，并未牺牲SS能力，验证了统一框架的有效性。可视化分析也证实了吸引子与线索在语义空间中的成功对齐。
</div>

</details>

---

## GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model
- **Authors**: Haoyang Li, Xuyi Zhuang, Azmat Adnan, Ye Ni, Wei Rao, Shreyas Gopal, Eng Siong Chng
- **Categories**: eess.AS, cs.AI, cs.LG
- **arXiv**: [https://arxiv.org/abs/2512.20978v1](https://arxiv.org/abs/2512.20978v1)
- **PDF**: [https://arxiv.org/pdf/2512.20978v1](https://arxiv.org/pdf/2512.20978v1)

基于语言模型的生成式建模已成为目标说话人提取领域的一个有前景的方向，其具备提升泛化能力与生成高保真语音的潜力。本文提出GenTSE，一种仅含解码器的两阶段生成式语言模型方法：第一阶段预测粗粒度语义标记，第二阶段生成细粒度声学标记。通过分离语义与声学信息，该方法稳定了解码过程，并生成更忠实、内容对齐的目标语音。两个阶段均使用连续的自监督学习或编解码器嵌入，相比离散提示方法提供了更丰富的上下文信息。为减少曝光偏差，我们采用一种冻结语言模型条件训练策略，使语言模型基于早期检查点预测的标记进行条件生成，从而缩小教师强制训练与自回归推理之间的差距。此外，我们应用直接偏好优化方法，以更好地使输出符合人类感知偏好。在Libri2Mix数据集上的实验表明，GenTSE在语音质量、可懂度及说话人一致性方面均优于此前基于语言模型的系统。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：目标说话人提取旨在从混合语音中分离出指定说话人的语音。传统判别式方法泛化能力差，且会损害语音保真度。
- **既有方法问题**：
  - 现有基于语言模型的生成方法存在挑战：单阶段模型需在高熵空间中联合建模内容、音色和细节声学结构，难度大。
  - 一些两阶段方法引入了非生成组件，破坏了端到端的生成性。
  - 部分方法依赖离散化的参考提示，会丢失细粒度的说话人信息。
  - 自回归训练中的“曝光偏差”问题会降低语音保真度与说话人一致性。

2)  
论文提出 **GenTSE**，一个完全生成式的两阶段解码器语言模型架构，通过以下核心方法解决上述问题：

- **粗到细的层次化建模**：
  - **第一阶段（语义提取）**：使用仅解码器的语言模型，以参考语音和混合语音的连续WavLM嵌入为条件，自回归地预测目标语音的**粗粒度语义令牌**（通过k-means离散化WavLM特征获得）。
  - **第二阶段（声学生成）**：以第一阶段预测的语义令牌、以及参考和混合语音的连续DAC声学嵌入为条件，另一个仅解码器语言模型自回归地生成**细粒度声学令牌**（使用单码本的SimCodec编码器）。
  - **优势**：将语义与声学解耦，降低了直接建模底层声学细节的复杂度，生成了更忠实、内容对齐的目标语音。使用连续嵌入作为条件，比离散提示提供了更丰富的上下文信息。

- **缓解曝光偏差：冻结语言模型条件化**：
  - **问题**：训练时的教师强制与推理时的自回归存在不匹配，导致曝光偏差。
  - **方法**：首先训练基础模型（θ, φ）。然后复制参数创建可训练模型（θ‘, φ’），同时冻结基础模型。在训练可训练模型时，使用冻结基础模型生成的预测令牌作为条件输入，替代真实令牌。
  - **效果**：让模型在训练时即暴露于自身预测的历史，从而缩小训练与推理的差距，提升解码稳定性和输出质量。

- **对齐人类感知偏好：直接偏好优化**：
  - **问题**：基于令牌似然的优化目标不能可靠地反映人类感知的语音自然度、清晰度等。
  - **方法**：在声学阶段应用DPO。使用一个冻结的参考模型采样多个候选声学令牌序列，用UTMOS（一种MOS预测器）评分，构建偏好对（优 vs. 劣）。DPO目标函数直接优化模型，使其输出分布偏向于更受偏好的语音。
  - **效果**：提供了一种稳定、简洁的方法来将模型输出与人类听觉偏好对齐，无需修改推理架构或增加额外参数。

3)  
在 **Libri2Mix** 数据集上的实验表明，GenTSE在以下任务和指标上取得了显著效果：
- **语音质量**：在DNSMOS、UTMOS、NISQA等多个基于DNN的语音质量预测指标上，超越了所有对比的基于语言模型的基线方法，也优于领先的判别式方法。
- **说话人一致性**：在说话人嵌入余弦相似度指标上取得最高分，表明能更好地保留目标说话人音色。
- **可懂度**：在差分词错误率上优于其他生成式方法，但在该指标上略低于某些判别式方法。通过SpeechBERT衡量的语义相似度也表现优异。
- **消融实验**验证了其两阶段设计、连续条件输入、FLC和DPO策略的有效性。
</div>

</details>

---

## Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study
- **Authors**: Zhongren Dong, Haotian Guo, Weixiang Xu, Huan Zhao, Zixing Zhang
- **Categories**: cs.CL, cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.20948v1](https://arxiv.org/abs/2512.20948v1)
- **PDF**: [https://arxiv.org/pdf/2512.20948v1](https://arxiv.org/pdf/2512.20948v1)

神经精神疾病（如阿尔茨海默病、AD）、抑郁症和自闭症谱系障碍（ASD）常伴随语言与声学特征异常，这些异常可作为早期检测的潜在生物标志物。尽管多模态方法前景广阔，但多语言泛化能力不足及缺乏统一评估框架等问题依然存在。为应对这些挑战，本研究提出FEND（基于基础模型的神经精神疾病评估框架），这是一个融合语音与文本模态的综合性多模态框架，旨在实现覆盖全生命周期的AD、抑郁症及ASD检测。通过整合涵盖英语、汉语、希腊语、法语和荷兰语的13个多语言数据集，我们系统评估了多模态融合性能。实验结果表明：多模态融合在AD与抑郁症检测中表现优异，但因数据集异质性在ASD检测中效果欠佳。同时，我们发现模态不平衡是普遍问题，多模态融合未能超越最佳单模态模型。跨语料库实验显示，在任务与语言一致场景下性能稳健，但在多语言及任务异构场景中性能显著下降。通过提供广泛的基准测试及对性能影响因素的深入分析，FEND推动了自动化、全生命周期覆盖及多语言的神经精神疾病评估领域发展。我们鼓励研究者采用FEND框架进行公平比较与可重复研究。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经精神障碍（如阿尔茨海默病、抑郁症、自闭症谱系障碍）常伴随语言和声学异常，为早期检测提供了潜在生物标志物。传统诊断依赖专家评估，耗时、主观且难以扩展。
- **既有方法问题**：现有研究多局限于单一数据集、特定语言（主要是英语）或私有评估指标，缺乏统一的多维基准。这导致方法间难以公平比较，阻碍了对模型跨疾病、跨语言和跨年龄组性能的系统理解，限制了实际应用和泛化能力。

2)  
- **提出FEND框架**：为解决上述问题，论文提出了FEND（基于基础模型的神经精神障碍评估）框架。这是一个全面的多模态评估框架，旨在系统评估声学和语言基础模型在神经精神障碍检测中的性能。
- **核心设计**：
    - **统一基准**：FEND为阿尔茨海默病、抑郁症和自闭症谱系障碍的检测建立了统一的基准，覆盖多种语言（英语、中文、希腊语、法语、荷兰语）和全年龄段，共使用13个数据集。
    - **标准化流程**：框架采用标准化的预处理、评估和分析方法，确保了实验的可比性和可复现性。评估指标统一为加权准确率、未加权准确率和加权F1分数。
    - **双模块评估**：
        - **单模态分析模块**：分别评估语音和文本基础模型提取疾病特征的能力。使用预训练基础模型（如WavLM、E5）提取特征，后接一个简单的多层感知机进行分类。
        - **多模态融合模块**：系统评估多种经典融合算法（如序列级的MFN、MulT和话语级的TFN、Attention等），以整合语音和文本信息，提升检测性能。
- **解决的关键问题**：
    - **碎片化研究**：通过提供统一的跨数据集、跨语言、跨模态评估平台，解决了研究碎片化、无法公平比较的问题。
    - **泛化能力评估**：通过跨语料库实验，系统评估了模型在任务一致、语言一致场景下的鲁棒性，以及在多语言、任务异构场景下的性能下降，揭示了泛化挑战。
    - **深入分析**：框架不仅报告性能指标，还深入分析了影响结果的关键因素，如模态不平衡（多模态融合未能超越最佳单模态模型）和数据集异质性（尤其在自闭症检测中），为未来模型选择和方法设计提供了具体指导。

3)  
- **评估任务与效果**：FEND在阿尔茨海默病、抑郁症和自闭症谱系障碍的检测任务上进行了全面评估。
    - **单模态性能**：在阿尔茨海默病和抑郁症检测中，文本模态略优于语音模态；在自闭症检测中，语音模态显著优于文本模态。WavLM-Large在语音任务中表现突出，E5-Large在文本任务中表现良好。
    - **多模态融合效果**：多模态融合在阿尔茨海默病和抑郁症检测中普遍提升了性能（尤其在单语言数据集上），但在自闭症检测中因数据集异质性未能超越最佳单模态模型。
    - **跨语料库泛化**：在任务和语言一致的场景下模型表现稳健，但在多语言和任务异构的设置中性能出现明显下降。
</div>

</details>

---

## SACodec: Asymmetric Quantization with Semantic Anchoring for Low-Bitrate High-Fidelity Neural Speech Codecs
- **Authors**: Zhongren Dong, Bin Wang, Jing Han, Haotian Guo, Xiaojun Mo, Yimin Cao, Zixing Zhang
- **Categories**: cs.SD
- **arXiv**: [https://arxiv.org/abs/2512.20944v1](https://arxiv.org/abs/2512.20944v1)
- **PDF**: [https://arxiv.org/pdf/2512.20944v1](https://arxiv.org/pdf/2512.20944v1)

针对低比特率下神经语音编解码器面临声学保真度与语义丰富度难以兼顾的根本性挑战，本文提出SACodec——一种基于非对称双量化器的新型编解码器，其核心在于我们提出的语义锚定机制。该设计策略性地将语义信息与声学细节的量化过程解耦。语义锚定通过一个轻量级投影器实现，将声学特征与一个冻结的大规模mHuBERT码本对齐，从而注入语言学先验知识并确保码本得到充分利用。随后，针对声学细节，采用带有SimVQ的残差激活模块，使单层量化器（声学路径）能够精准恢复细粒度信息。在仅1.5 kbps的比特率下，SACodec在保真度与语义表现上均达到新的最优水平：主观听音测试证实其重建质量在感知上高度接近原始音频，同时其编码表征在下游任务中展现出显著提升的语义丰富度。

<details>
<summary>详细解读</summary>

<div markdown="1">

1)  
- **研究背景**：神经语音编解码器在低码率下需平衡声学保真度与语义丰富性。现有方法面临瓶颈：
  - **多层残差向量量化**：在极低码率（如1.5 kbps）下，量化误差累积导致可听伪影，且多流令牌增加下游建模复杂性。
  - **单码本范式**：虽简化下游集成，但仅优化声学失真，缺乏显式语义结构，限制内容理解任务。
  - **语义增强策略**：依赖外部知识蒸馏或内生学习，常需复杂多层后端或引入高训练开销，难以兼顾低码率目标。

2)  
论文提出 **SACodec**，通过**非对称双量化器**架构解决上述问题，核心方法如下：

- **语义锚定模块**：
  - 使用**冻结的大规模 mHuBERT 语义码本**作为先验知识源。
  - 通过轻量级投影器将编码器声学特征对齐到该码本，实现语义注入，同时避免码本崩溃。
  - 此设计确保语义层码本完全利用，为令牌提供丰富语言学结构。

- **残差激活模块**：
  - 量化声学残差（原始特征减去语义嵌入），捕获音色、韵律等细节。
  - 采用 **SimVQ 技术**：将残差码本重构为冻结随机矩阵与可学习基的乘积，实现全局更新，保证码本完全激活。
  - 仅使用单层 VQ，以最小架构复杂度和码率开销恢复细粒度声学信息。

- **整体架构与训练**：
  - 编码器-双量化器-解码器以端到端方式训练，结合对抗损失、多尺度谱重建损失等。
  - 双量化器输出嵌入相加后送入解码器重建波形，在低码率下协同优化声学与语义目标。

3)  
- **任务与效果**：
  - **声学重建质量**：在 1.5 kbps 码率下，于 LibriTTS 和 LJSpeech 数据集上，UTMOS、PESQ 等客观指标均超越所有基线，接近无损音频；主观 MUSHRA 测试中得分接近真实音频（中位数 96.8 vs. 97.5）。
  - **语义表示能力**：在 ARCH 基准测试中，压缩域平均准确率达 0.4809，优于同类语音编解码器；重建域准确率 0.6311，显示端到端语义保真度高，且在跨领域任务中表现稳健。
  - **整体优势**：以 1.5 kbps 实现声学保真度与语义丰富性的最佳平衡，为下游语音语言模型提供高效、高质量的离散令牌。
</div>

</details>

---
